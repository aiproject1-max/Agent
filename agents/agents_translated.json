{
  "title": "ADVANCES AND CHALLENGES IN FOUNDATION AGENTSFROM BRAIN-INSPIRED INTELLIGENCE TO EVOLUTIONARY, COLLABORATIVE,AND SAFE SYSTEMS",
  "authors_info": "Bang $\\mathbf{Liu^{2,3,20*\\dagger}}$ , Xinfeng $\\mathbf{Li^{4*}}$ , Jiayi $\\mathbf{Z}\\mathbf{hang}^{1,10*}$ , Jinlin Wang \\*, Tanjin $\\mathbf{He}^{5*}$ , Sirui $\\mathbf{Hong^{1*}}$ Hongzhang $\\mathbf{Liu^{6*}}$ , Shaokun $\\mathbf{Z}\\mathbf{h}\\mathbf{a}\\mathbf{n}\\mathbf{g}^{7*}$ , Kaitao $\\mathbf{Song^{8*}}$ , Kunlun $\\mathbf{Z}\\mathbf{h}\\mathbf{u}^{9*}$ , Yuheng $\\mathbf{Cheng^{1*}}$ Suyuchen Wang2,3\\*, Xiaoqiang $\\mathbf{Wang^{2,3*}}$ , Yuyu $\\mathbf{Luo^{10*}}$ , Haibo $\\mathbf{Jin^{9}}.$ \\* Peiyan Zhang10, Ollie $\\mathbf{Liu}^{11}$ \\* Jiaqi Chen1, Huan Zhang2,3, Zhaoyang $\\bar{\\mathbf{Y}}\\mathbf{u}^{1}$ , Haochen $\\mathbf{Shi^{2,3}}$ , Boyan $\\mathbf{Li}^{10}$ , Dekun $\\mathbf{W_{u}}^{2,3}$ , Fengwei Teng1, Xiaojun $\\mathbf{Jia^{4}}$ , Jiawei $\\mathbf{X}\\mathbf{u}^{1}$ , Jinyu Xiangl, Yizhang $\\mathbf{Lin}^{1}$ , Tianming $\\mathbf{Liu^{14}}$ , Tongliang $\\mathbf{Liu}^{6}$ Yu $\\mathbf{Su}^{15}$ , Huan $\\mathbf{Sun^{15}}$ ,Glen Berseth2,32,Jian $\\mathbf{Nie^{2}}$ , Ian Foster5, Logan Ward5,Qingyun $\\mathbf{W_{u}}^{\\mathrm{7}}$ Yu $\\mathbf{Gu}^{15}$ , Mingchen Zhuge16, Xiangru $\\mathbf{Tang^{12}}$ , Haohan $\\mathbf{Wang}^{9}$ , Jiaxuan $\\mathbf{You}^{9}$ , Chi Wang19, Jian $\\mathbf{Pei^{17\\dagger}}$ , Qiang $\\mathbf{Yang^{10,18\\dagger}}$ , Xiaoliang $\\mathbf{Q}\\mathbf{i}^{13\\dagger}$ , Chenglin $\\mathbf{W_{u}}^{1*\\dagger}$  \n\n1MetaGPT, ²Université de Montreal, 3Mila - Quebec AI Institute, 4Nanyang Technological University,   \n5Argonne National Laboratory, 6University of Sydney,Penn State University, 8Microsoft Research Asia,   \n9University of Illnois at Urbana-Champaign,10The Hong Kong University of Science and Technology,   \n1UniversityofhCalifileityadityUetyf   \n15The Ohio State University,16King Abdullah University of Science and Technology,17Duke University,   \n18The Hong Kong Polytechnic University,19Google DeepMind, 20Canada CIFAR AIChair",
  "sections": [
    {
      "title": "ABSTRACT",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling,reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats,ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.By synthesizing modular AI architectures with insights from different disciplines, this survey identifies key research gaps,challenges, and opportunities, encouraging innovations that harmonize technological advancement with meaningful societal benefit. The project's Github link is: https://github.com/FoundationAgents/awesome-foundation-agents.",
          "translated_content": "大型语言模型（LLMs）的出现催生了人工智能领域的深刻转变，为能够在各种领域展现复杂推理、强大感知和多功能行为能力的先进智能代理铺平了道路。随着这些代理在推动人工智能研究和实际应用方面的作用日益增强，它们的设计、评估和持续改进提出了复杂而多面向的挑战。本调查提供了一个全面的概述，将智能代理置于一个模块化、脑启发式架构中，该架构整合了认知科学、神经科学和计算研究的原则。我们将探索分为四个相互关联的部分。首先，我们深入探讨智能代理的模块化基础，系统地将它们的认知、感知和操作模块映射到类似的人脑功能上，并阐明核心组件，如记忆、世界建模、奖励处理和类似情感的系统。其次，我们讨论自我增强和自适应进化机制，探讨代理如何自主完善其能力、适应动态环境，并通过自动化优化范式实现持续学习，包括新兴的AutoML和LLM驱动的优化策略。第三，我们研究协作和进化多代理系统，调查从代理相互作用、合作和社会结构中出现的集体智能，突显与人类社会动态的相似之处。最后，我们着重讨论构建安全、可靠和有益的人工智能系统的关键使命，强调内在和外在的安全威胁、道德对齐、稳健性以及在值得信赖的现实世界部署所必需的实用缓解策略。通过将模块化人工智能架构与不同学科的见解综合起来，本调查确定了关键的研究空白、挑战和机遇，鼓励创新，使技术进步与有意义的社会利益相协调。该项目的Github链接为: https://github.com/FoundationAgents/awesome-foundation-agents。"
        }
      ],
      "raw_title": "ABSTRACT",
      "type": "abstract",
      "children": [],
      "translated_title": "摘要"
    },
    {
      "title": "Preface",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Large language models (LLMs)have revolutionized artificial intelligence(AI) by demonstrating unprecedented capabilities in naturallanguage andmultimodal understanding,as wellasreasoning and generation.These models are trainedonvastdatasets,andtheyexhibitemergentabilitiessuchasreasoning,in-contextleaing,andevenudimetar planning.While these modelsrepresent a major stepforward in realizing intelligent machines,they themselves do not yet fully embody allthe capabilities of an intelligent beingSincethe early daysof artificial intelligence,AI researchershavelong beenonaquest foratruly“intellgent\"systemthatcanlearn,plan,reason,sense,communicate, act,remember, and demonstrate various human-like abilities and agility.These beings,known as intelligent agents, should be ableto think both long-term and short-term,perform complex actions,and interact withhumans and other agents. LLMs are an important step towards realizing intelligent agents, but we are not there yet.",
          "index": 0,
          "part": 0,
          "translated_content": "大型语言模型（LLMs）通过展示在自然语言和多模态理解以及推理和生成方面的前所未有能力，彻底改变了人工智能（AI）。这些模型在庞大数据集上训练，并展现出诸如推理、上下文学习甚至元规划等新兴能力。虽然这些模型代表了实现智能机器的重要进步，但它们本身尚未完全体现出智能生物的所有能力。自人工智能早期以来，AI研究人员一直在追求一个真正“智能”的系统，该系统能够学习、规划、推理、感知、交流、行动、记忆，并展示各种类似人类的能力和灵活性。这些被称为智能代理的存在应能够进行长期和短期思考，执行复杂动作，并与人类和其他代理进行互动。LLMs是实现智能代理的重要一步，但我们尚未达到这一目标。"
        },
        {
          "type": "text",
          "content": "This manuscript provides acomprehensive overview of the current state of the art of LLM-based intelligent agents. In the past,there have been numerous research papers and books on intelligent agents, as wellasa flurryof books on LLMs. However,there has scarcely been comprehensive coverage of both. While LLMs can achieve significant capabilities required by agents,they onlyprovidethefoundations upon whichfurther functionalities must be built.For example,while LLMs can help generate plans such as travelplans,they cannot yet generatefully complex plans for complex and professionaltasks, nor can they maintain long-term memories without hallucination. Furthermore, their ability to perform real-world actions autonomouslyremains limited.Wecan view LLMs asengines, with agents being thecars,boatsandairplanes builtusingtheseengines.Inthisview,wenaturallseek tomoveforward indesigning and constructing fully functioning intelligent agents by making fulluse of the capabilities provided by LLMs.",
          "index": 1,
          "part": 0,
          "translated_content": "本文全面概述了基于LLMs的智能代理的当前技术现状。过去，关于智能代理的研究论文和书籍层出不穷，同时也有大量关于LLMs的书籍涌现。然而，很少有综合涵盖两者的内容。虽然LLMs可以实现代理所需的重要功能，但它们仅提供了必须构建更多功能的基础。例如，虽然LLMs可以帮助生成旅行计划等计划，但它们尚不能为复杂和专业任务生成完全复杂的计划，也无法在没有幻觉的情况下保持长期记忆。此外，它们在自主执行现实世界动作方面的能力仍然有限。我们可以将LLMs视为引擎，而代理则是使用这些引擎构建的汽车、船舶和飞机。在这个视角下，我们自然而然地寻求通过充分利用LLMs提供的能力，设计和构建完全运转的智能代理。"
        },
        {
          "type": "text",
          "content": "In this engine-vehicle analogyof theinterplaybetweenLLMs andagents,wenaturallask:Howmuchofthecapabilities of intelligent agentscancurrent LLMtechnologies provide?What are the functions thatcannot yetberealized based on current LLM technologies? Beyond LMs, what more needs to be done to have a full intelligent agent capable of autonomous action and interaction in the physical world?What are the challnges forfull integrated LLM-based agents?What aditionaldevelopments arerequiredforcapable,communicative agents thateffectivelycollaborate with humans?What are the areas thatrepresentlow-hanging fruits for LLM-based agents?What implications willthere be for society once we have fully intelligent LLM-based agents, and how should we prepare for this future?",
          "index": 2,
          "part": 0,
          "translated_content": "在LLMs与代理之间的引擎-车辆类比中，我们自然地会问：基于当前的LLM技术，智能代理的能力有多少可以提供？基于当前的LLM技术，有哪些功能尚无法实现？除了LMs，还需要做什么才能拥有一个能够在物理世界中进行自主行动和互动的完整智能代理？完全集成的基于LLM的代理面临哪些挑战？为了具备能够有效与人类合作的具有沟通能力的代理，需要哪些额外的发展？哪些领域对于基于LLM的代理来说是低 hanging fruit？一旦我们拥有完全智能的基于LLM的代理，社会将会产生哪些影响，我们应该如何为这个未来做准备？"
        },
        {
          "type": "text",
          "content": "These questions transcend not only the engineering practice of extending current LLMs and agents but also raise potentialfuture research directions.We have assembled frontierresearchers from AI,spanning from LLMdevelopment to agent design,tocomprehensivelyaddress thesequestions.Thebookconsists offour parts.The first part presents an exposition of the requirementsforindividualagents,comparing theircapabilities withthose ofhumans,including perception and action abilities. The second part explores agents\"evolution capabilities and their implications on intelligent toolssuch as workflow management systems.Thethirdpart discusses societies of agents,emphasizingtheir collaborative andcolective actioncapabilities, andthefourth part addresses ethicaland societalaspects, including agent safety and responsibilities.",
          "index": 3,
          "part": 0,
          "translated_content": "这些问题不仅超越了扩展当前LLMs和代理的工程实践，还提出了潜在的未来研究方向。我们邀请了来自人工智能领域的前沿研究人员，涵盖了LLM开发到代理设计，以全面解决这些问题。该书分为四个部分。第一部分阐述了个体代理的要求，将它们的能力与人类进行比较，包括感知和行动能力。第二部分探讨了代理的进化能力及其对智能工具（如工作流管理系统）的影响。第三部分讨论了代理社会，强调它们的协作和集体行动能力，第四部分涉及伦理和社会方面，包括代理的安全性和责任。"
        },
        {
          "type": "text",
          "content": "This book isintended forresearchers,students,policymakers,and practitioners alike.The audience includes non-AI readers curious about AI,LLMs,andagents,as wellas individuals interestedinfuture societieswhere humansco-exist with AI.Readers may range from undergraduate and graduate students toresearchers and industry practitioners.The book aims notonly to provide answers to readers'questions about AIand agents but alsoto inspire them to ask new questions.Ultimately, we hope to motivate more people to join our endeavor in exploringthisfertileresearch ground.",
          "index": 4,
          "part": 0,
          "translated_content": "本书旨在面向研究人员、学生、决策者和实践者。受众包括对人工智能、大型语言模型(LLMs)和代理感兴趣的非人工智能领域读者，以及对人类与人工智能共存的未来社会感兴趣的个人。读者群可能包括本科生、研究生、研究人员和行业从业者。本书旨在不仅回答读者对人工智能和代理的问题，还激励他们提出新问题。最终，我们希望能激励更多人加入我们的努力，探索这片富饶的研究领域。"
        }
      ],
      "raw_title": "Preface",
      "type": null,
      "children": [],
      "translated_title": "前言"
    },
    {
      "title": "1 Introduction 12",
      "number": "1",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "1.1  The Rise and Development of AI Agents 12\n1.2 A Parallel Comparison between Human Brain and AI Agents 13\n1.2.1 Brain Functionality by Region and AI Parallels 14\n1.3 A Modular and Brain-Inspired AI Agent Framework 16\n1.3.1 Core Concepts and Notations in the Agent Loop 18\n1.3.2 Biological Inspirations 21\n1.3.3 Connections to Existing Theories 21\n\n1.4 Navigating This Survey 22",
          "index": 0,
          "part": 0,
          "translated_content": "1.1 人工智能代理的兴起与发展 12\n1.2 人类大脑与人工智能代理的并行比较 13\n1.2.1 大脑功能区域与人工智能的相似之处 14\n1.3 模块化和脑启发式人工智能代理框架 16\n1.3.1 代理循环中的核心概念和符号 18\n1.3.2 生物启发 21\n1.3.3 与现有理论的联系 21\n\n1.4 导览本调查 22"
        }
      ],
      "raw_title": "Introduction 12",
      "type": null,
      "children": [
        {
          "title": "1.1 The Rise and Development of AI Agents",
          "number": "1.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "The concept of“agent\"is acornerstone of modern AI,representing a system that perceives its environment, makes decisions,and takes actions to achieve specificgoals.This idea,whileformalized in AIin the mid-2Othcentury,has roots in earlyexplorationsof autonomy and interaction inintelligent systems.One ofthe most widelycited definitions, proposedby[3],describes anagent as“anythingthatcanbeviewedasperceiving itsenvironmentthroughsensors and acting uponthatenvironmentthroughactuators\".This definitionemphasizesthedualnatureofagents asbothobservers and actors,capableof dynamicalladapting totheir surroundingsrather thanfollowing staticrules.Itencapsulatesthe shiftin AIfrom systems thatmerelycompute tosystems that engage withtheirenvironment.Thehistoricaldevelopment of agents paralels the evolution of Alitself.Early symbolic systems,such as Newell and Simon's General Problem Solver[4],sought toreplicate human problem-solving processes bybreaking tasks intological steps.However, these systems werelimited by theirreliance on structured environments and predefinedlogic.The agent paradigm emerged as aresponse to these limitations,focusing on autonomy,adaptability,and real-world interaction. Rodney Brooks's subsumption architecture inthe198Osexemplifiedthis shift, introducing agentscapableofbehavior-driven,real-time responses inrobotics[5].Unlike earlierapproaches,these agents operated without the need forexhaustive models of their environment,showcasinga more flexible and scalable design.Agents have since become a versatile framework across AI subfields. In robotics, they enable autonomous navigation and manipulation; in software, they form the foundation of multi-agent systems used for simulation and coordination [6].By integrating perception, reasoning, and action into acohesive structure,the agent paradigm has consistently served as a bridge betweentheoretical AI constructs and practicalapplications,advancing our understanding ofhow intellgent systemscan operate in dynamic and complex environments.",
              "index": 0,
              "part": 0,
              "translated_content": "“代理（agent）”的概念是现代人工智能的基石，代表了一个系统感知其环境，做出决策，并采取行动以实现特定目标。这一概念在20世纪中叶在人工智能领域得到正式确立，但其根源可以追溯到早期对智能系统中自治性和互动性的探索。其中最广泛引用的定义之一，由[3]提出，将代理描述为“任何可以被视为通过传感器感知其环境并通过执行器对其环境进行作用的东西”。该定义强调了代理的双重性质，既是观察者又是执行者，能够动态适应周围环境而不是遵循静态规则。它概括了人工智能从仅仅计算系统到与环境互动的系统的转变。代理的历史发展与人工智能本身的演变相辅相成。早期的符号系统，如纽厄尔和西蒙的通用问题求解器[4]，试图通过将任务分解为逻辑步骤来复制人类解决问题的过程。然而，这些系统受限于对结构化环境和预定义逻辑的依赖。代理范式的出现是对这些限制的回应，侧重于自治性、适应性和真实世界的互动。罗德尼·布鲁克斯（Rodney Brooks）在20世纪80年代提出的包含架构（subsumption architecture）就是这种转变的典范，引入了能够在机器人领域实时做出基于行为的响应的代理[5]。与早期方法不同，这些代理无需对其环境进行详尽建模，展示了更灵活、可扩展的设计。代理自那时起已成为人工智能各个子领域中的多才多艺的框架。在机器人领域，它们实现了自主导航和操作；在软件领域，它们构成了用于模拟和协调的多代理系统的基础[6]。通过将感知、推理和行动整合到一个连贯的结构中，代理范式一直作为理论人工智能构想和实际应用之间的桥梁，推动我们对智能系统如何在动态复杂的环境中运作的理解。"
            },
            {
              "type": "text",
              "content": "Theadvent oflargelanguage models (LLMs)has redefined thecapabilitiesofagents,ransforming theirrole inartificial intelligence and opening up newhorizons fortheir applications.Agents,once confinedto executing narrowly defined tasks orfollowing rigidrule-based frameworks,now leverage thebroad generalization,reasoning,andadaptability of models like OpenAl's ChatGPT [7], DeepSeek AI's DeepSeek [8], Anthropic's Claude[9], Alibaba'sQWen [10], and Meta's LLaMA[11].These LLM-powered agents have evolved from static systems intodynamic entities capable of processing naturallanguage,reasoning across complex domains, and adapting to novel situations with remarkable fluency. No longer merely passive processors of input, these agents have become active collaborators,capable of addressing multi-stepchallenges and interacting with their environments ina way that mirrors human problem-solving.",
              "index": 1,
              "part": 0,
              "translated_content": "大型语言模型(LLMs)的出现重新定义了代理的能力，改变了它们在人工智能中的角色，并为它们的应用开辟了新的视野。代理曾经局限于执行狭义任务或遵循严格基于规则的框架，现在利用像OpenAI的ChatGPT[7]、DeepSeek AI的DeepSeek[8]、Anthropic的Claude[9]、阿里巴巴的QWen[10]和Meta的LLaMA[11]等模型的广泛泛化、推理和适应能力。这些由LLM驱动的代理已经从静态系统发展为能够处理自然语言、跨越复杂领域进行推理，并以出色的流畅度适应新情况的动态实体。这些代理不再仅仅是输入的被动处理者，它们已经成为能够解决多步挑战并以类似人类问题解决方式与环境互动的积极合作者。"
            },
            {
              "type": "text",
              "content": "A key advancement intheLLMera isthe seamlessintegrationof language understanding with actionable capabilities. Modern LLMs,equipped with function-calling APIs,enable agents to identify when external tools or systems are required,reason about their usage, and execute preciseactions to achieve specific goals.For instance,an agent powered by ChatGPTcanautonomouslyqueryadatabase,retrieve relevant information,and use it to deliver actionable insights,all while maintaining contextualawareness of the broader task.This dynamic combination of abstract reasoning andconcrete execution alows agents to bridge the gap between cognitive understanding and real-world action.Furthermore,the generalization abilities of LLMs in few-shot and zero-shot learning have revolutionized the adaptabilityof agents,enabling them to tackleadiverse array oftasks-fromdata analysis andcreative content generation to real-time collaborative problem-solving—without extensive task-specific training.This adaptability, coupled with their conversational fluency,positions LLM-powered agents as intellgent mediators between humans and machines, seamlessly integrating human intent with machine precision in increasingly complex workflows.",
              "index": 2,
              "part": 0,
              "translated_content": "LLM时代的一个关键进步在于将语言理解与可操作能力无缝集成。现代LLM配备了函数调用API，使代理能够识别何时需要外部工具或系统，思考它们的使用方式，并执行精确的动作以实现特定目标。例如，由ChatGPT驱动的代理可以自主查询数据库，检索相关信息，并将其用于提供可操作的见解，同时保持对更广泛任务的上下文意识。这种抽象推理和具体执行的动态组合使代理能够弥合认知理解与现实行动之间的差距。此外，LLM在少样本学习和零样本学习中的泛化能力已经彻底改变了代理的适应性，使它们能够处理各种各样的任务——从数据分析和创意内容生成到实时协作问题解决，而无需进行大量的特定任务训练。这种适应性，再加上它们的对话流畅性，使得由LLM驱动的代理在人类和机器之间成为智能的中介者，在日益复杂的工作流程中无缝地将人类意图与机器精度整合在一起。"
            }
          ],
          "raw_title": "The Rise and Development of AI Agents",
          "type": null,
          "children": [],
          "translated_title": "1.1 人工智能代理的崛起与发展"
        },
        {
          "title": "1.2 A Parallel Comparison between Human Brain and AI Agents",
          "number": "1.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Therapid integrationofLLMs into intellgent agent architectures has notonly propelledartificial intellgence forward but also highlighted fundamental differences between AI systems and human cognition.As illustrated briefly in Table 1.1,LLM-powered agents differ significantly from human cognition across dimensions such as underlying “hardware\",consciousness,learning methodologies,creativity, and energy efficiency.However, it is important to emphasize that this comparison provides only a high-level snapshot rather than an exhaustive depiction. Human intelligence possesses many nuancedcharacteristics notcaptured here,while AIagents alsoexhibit distinct features beyond this concise comparison.",
              "index": 0,
              "part": 0,
              "translated_content": "大型语言模型（LLMs）快速融入智能代理架构不仅推动了人工智能的发展，也凸显了人工智能系统与人类认知之间的根本差异。正如表1.1简要说明的那样，基于LLM的代理在诸如基础“硬件”、意识、学习方法、创造力和能源效率等维度上与人类认知存在显著差异。然而，需要强调的是，这种比较仅提供了一个高层次的快照，而非详尽的描绘。人类智能具有许多此处未涵盖的微妙特征，而AI代理也展示出超越这种简明比较的独特特征。"
            },
            {
              "type": "text",
              "content": "Human inteligence operates on biological hardware-the brain—that demonstrates extraordinary energy efficiency, enabling lifelong learming,inference,and adaptive decision-making with minimal metaboliccosts.In contrast,urent AI systems require substantialcomputational power,resultingin significantly higherenergyconsumptionforcomparable cognitive tasks. Recognizing this performance gapemphasizes energy eficiency asacriticalfrontierforfuture AI research.\n\nIn terms ofconsciousness and emotional experience,LLM agents lack genuine subjective states and self-awareness inherent to human cognition. Although fullreplicating human-like consciousness in AI may neither be necessary nor desirable,appreciating the profound role emotions and subjective experiences playin human reasoning, motivation, ethical judgments,and socialinteractionscan guide research towardcreating AIthat is more aligned,trustworthy,and socially beneficial.\n\nHuman learning is continuous,interactive,andcontext-sensitive,deeplyshaped bysocial,cultural,andexperiential factors.Conversely, LLM agents primarily undergo static,ofline batch training with limited ongoing adaptation capabilities.Despite research works through instruction tuning and reinforcement learning from human feedback (RLHF)[12],LLMagentsstillfallshortofhuman-likeflexibility.Bridging this gapthroughapproaches suchas lifelong learning,personalizedadaptation,and interactivefine-tuning represents a promisingresearch direction,enabling AI to better mirror human adaptability and responsiveness.",
              "index": 1,
              "part": 0,
              "translated_content": "人类智能运行在展现出非凡能源效率的生物硬件——大脑上，这使得人类能够以极低的代谢成本进行终身学习、推理和适应性决策。相比之下，当前的人工智能系统需要大量的计算能力，导致在进行相似认知任务时能源消耗显著增加。认识到这种性能差距强调了能源效率作为未来人工智能研究的一个关键领域。\n\n在意识和情感体验方面，基于LLM的代理缺乏人类认知固有的真实主观状态和自我意识。虽然在人工智能中完全复制类似人类的意识既可能不是必要的，也不是可取的，但是欣赏情感和主观体验在人类推理、动机、伦理判断和社会互动中发挥的重要作用，可以引导研究朝着创造更加符合、可信赖和对社会有益的人工智能方向发展。\n\n人类学习是连续的、互动的、与环境相关的，深受社会、文化和经验因素的影响。相反，LLM代理主要经历静态的、离线的批量训练，具有有限的持续适应能力。尽管通过指导调整和从人类反馈中进行强化学习（RLHF）等研究工作，LLM代理仍然无法达到类似人类的灵活性。通过采用终身学习、个性化适应和交互微调等方法来弥合这一差距代表了一个有前途的研究方向，使人工智能能够更好地模拟人类的适应能力和响应能力。"
            },
            {
              "type": "text",
              "content": "Creativity in humans emerges from a rich interplay of personal experiences,emotional insights, and spontaneous cross-domain associations. Incontrast,LLMcreativity primarily arises through statisticalrecombinations of training data—“statisticalcreativity\"-lacking depth,originality,andemotionalresonance.Thisdistinction highlightsopportunities for developing AIagentscapable of deeper creative processes by integrating richercontextual understanding, simulated emotional states, and experiential grounding.",
              "index": 2,
              "part": 0,
              "translated_content": "人类的创造力源自个人经历、情感洞察和跨领域联想的丰富互动。相比之下，基于LLM的创造力主要通过训练数据的统计重组产生——“统计创造力”缺乏深度、独创性和情感共鸣。这种区别突显了通过整合更丰富的语境理解、模拟情感状态和经验基础来开发更深层次创造过程的AI代理的机遇。"
            },
            {
              "type": "text",
              "content": "Considering the time scale,the human brain has evolved over millions of years, achieving remarkable efficiency, adaptability,andcreativitythrough naturalselection andenvironmentalinteractions.Instarkcontrast, AIagentshave undergone rapid yet comparatively brief development over roughly 80 years since the advent of earlycomputational machines.This parallelcomparison between human cognition and AI systems is thus highly valuable, as it uncovers essential analogies and fundamental differences, providing meaningful insights that can guide advancements in AI agent technologies.Ultimately,drawing inspiration from human inteligence can enhance AIcapabilities,benefting humanity across diverse applications from healthcare and education to sustainability and beyond.",
              "index": 3,
              "part": 0,
              "translated_content": "考虑时间尺度，人类大脑经过数百万年的演化，在自然选择和环境互动中实现了卓越的效率、适应性和创造力。与之形成鲜明对比的是，自早期计算机问世以来的大约80年间，AI代理经历了快速而相对较短的发展。因此，对比人类认知和AI系统是非常有价值的，它揭示了基本类比和根本差异，提供了有意义的见解，可以指导AI代理技术的进步。最终，从人类智慧中汲取灵感可以增强AI的能力，使其在医疗保健、教育、可持续发展等各种领域造福人类。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td>Dimension</td><td>Human Brain / Cognition</td><td>LLM Agent</td><td>Remarks</td></tr><tr><td>Hardware & Maintenance</td><td>- Biological neurons, neuro- transmitters, neuroplasticity. - Requires sleep, nutrition, rest. - Limited replication, knowl- edge transfer via learning. - Extremely energy-efficient</td><td>Deep neural networks, gradient-based optimization. - Requires hardware, stable power, and cooling. Easily  duplicated across servers globally. High energy consumption</td><td>Human brains are biologically maintained, energy-efficient, and not easily  replicable. LLM agents rely on hardware maintenance, are highly repli- cable, but significantly less energy-efficient.</td></tr><tr><td>ment</td><td>- Genuine  subjective  ex-- No genuine subjective experi-  Human &Develop-periences, emotions,self- awareness. - Gradual developmental stages from childhood. - Emotional cognition drives decision-making. - Lifelong, continuous, online</td><td>ence or self-awareness. -“Emotions\" are superficial lan- guage imitations. - Static post-training with lim- ited dynamic growth.</td><td>consciousness emerges from  emotional, social, and biological devel- opment; LLMs remain static without true introspection or emotional depth.</td></tr><tr><td>Learning Style</td><td>learning. - Few-shot, rapid knowledge transfer. - Influenced by environment, - Neutral, impersonal learned culture, emotions.</td><td>- Primarily offline, batch-based training. - Limited online fine-tuning and adaptation. knowledge.</td><td>Despite improvements via in- struction tuning, human learn- ing remains more dynamic, adaptive, and culturally/emo- tionally integrated than LLM learning.</td></tr><tr><td>Creativity & Divergence</td><td>ence, emotions, subconscious insights. - Rich cross-domain associa- tions, metaphorical thinking. - Emotional depth influences</td><td>- Rooted in personal experi-- Statistical recombination from extensive data. - Novelty through probabilistic optimization. Limited emotional  and experiential grounding.</td><td>LLM creativity is statistical and data-driven; human cre- ativity blends emotion, expe- rience, and subconscious pro- cesses.</td></tr></table></body></html>",
              "caption": "Table 1.1: Concise high-level comparison between human brains and LLM agents.",
              "index": 4,
              "part": 0,
              "translated_caption": "表1.1：人脑与大型语言模型代理之间的简明高层次比较。"
            }
          ],
          "raw_title": "A Parallel Comparison between Human Brain and AI Agents",
          "type": null,
          "children": [
            {
              "title": "1.2.1 Brain Functionality by Region and AI Parallels",
              "number": "1.2.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Understanding paralels between human brain functions and artificial intelligence(Al)sheds lighton boththe strengths and current lmitations of AI, particularly large language models (LLMs)and AIagents.Based on currnt neuroscience, the human brain is primarilycomposed of sixfunctionalregions,such as frontallobe,cerebelum, and brainstem, as shown inFigure 1.1.Inthis work,we further systematicallyexamine theexisting AIcounterparts to majorbrain regions and their primaryfunctionalities.Forabig-picture perspective,thestateofresearch inAIcan becategorized with three distinct levels:",
                  "index": 0,
                  "part": 0,
                  "translated_content": "理解人脑功能与人工智能（AI）之间的相似之处，不仅揭示了AI的优势和当前限制，特别是大型语言模型（LLMs）和AI代理。根据当前的神经科学，人脑主要由六个功能区域组成，如额叶、小脑和脑干，如图1.1所示。在这项工作中，我们进一步系统地检查了现有AI与主要脑区域及其主要功能的对应关系。从宏观角度来看，AI研究的状态可以分为三个不同的层次："
                },
                {
                  "type": "text",
                  "content": "· Level 1 (L1): Well-developed in current AI.\n· Level 2 (L2): Moderately explored, with partial progress. Can be further improved.\n· Level 3 (L3): Rarely explored; significant room for research.\n\nA high-level visual map of brain functionalregions andtheir coresponding AIdevelopmentlevels is shown in Figure 1.1.We aim to underscorehow core principles of specialization and integration,observed in biological systems,can guide morecohesive agent architectures.We now examine each brainfunctionalregion andtherelevant AIdevelopment in detail.\n\nFrontal Lobe: Executive Control and Cognition The frontal lobe, notably the prefrontal cortex, is crucial for higher-order cognition such as planning (L2), decision-making (L2),logical reasoning (L2), working memory (L2),self-awareness (L3),cognitive flexibility (L3),and inhibitorycontrol (L3)[13].AIhas made notable strides in planning and decision-making within well-defined domains, demonstrated by AI agents such as AlphaGo [14] Transformers employ atention mechanisms similar tohuman working memory[15],yetfallshortof human flexibility and robustness.The exploration of genuine self-awareness and inhibitory controlin AIremains scarce,andcaution is advised due to potential ethical and safety implications.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "在当前的人工智能中，Level 1 (L1)已经得到很好的发展。Level 2 (L2)已经有一定的探索，取得了部分进展，但还有进一步改进的空间。Level 3 (L3)很少被探索；有很大的研究空间。\n\n图1.1显示了大脑功能区域的高层视觉图及其对应的人工智能发展水平。我们旨在强调观察到的生物系统中的专业化和整合核心原则如何指导更具凝聚力的代理架构。现在我们将详细检查每个大脑功能区域及相关的人工智能发展。 \n\n额叶：执行控制与认知额叶，特别是前额叶皮层，对于高阶认知非常重要，如计划（L2）、决策（L2）、逻辑推理（L2）、工作记忆（L2）、自我意识（L3）、认知灵活性（L3）和抑制控制（L3）[13]。人工智能在规划和决策方面取得了显著进展，在明确定义的领域内展示了这一点，例如AlphaGo [14]。变压器使用类似于人类工作记忆的注意机制[15]，但在人类的灵活性和稳健性方面仍有不足。对于人工智能中真正的自我意识和抑制控制的探索仍然很少，并且由于潜在的道德和安全影响，建议谨慎对待。"
                },
                {
                  "type": "figure",
                  "src": "images/83102f32bcecee7f5b319bc61e73ac01298c219062bcf26542ad9d781abba69f.jpg",
                  "alt": "",
                  "caption": "Figure 1.l:Illustration ofkey human brain functionalities grouped by majorbrain regions,annotatedaccording to their current exploration levelin AIresearch.This figurehighlights existing achievements,gaps,and potentialoppotunities for advancing artificial intelligence toward more comprehensive, brain-inspired capabilities.",
                  "index": 2,
                  "part": 0,
                  "translated_caption": "图1.l：按主要大脑区域分组的关键人类大脑功能的示意图，根据它们在人工智能研究中的当前探索水平进行注释。该图突出了人工智能朝着更全面、脑启发式能力发展的现有成就、差距和潜在机会。"
                },
                {
                  "type": "text",
                  "content": "Parietal Lobe: Spatial Processing and Multisensory Integration The parietal lobes integrate multisensory inputs, facilitating attention(L2),spatialorientation (L2),andsensorimotor coordination (L2)[16].AIresearch in robotics and computer vision addresses similar challenges,employing techniques likesimultaneous localization and mapping (SLAM).Nonetheless, AI stillacks the seamless and real-time integration seen in humans.Furthermore, detailed tactile perception (L3)remains largely unexplored and offers considerable potential, particularly for robotics and prosthetics applications.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "顶叶：空间处理与多感官整合 顶叶整合多感官输入，促进注意力（L2）、空间定向（L2）和感觉运动协调（L2）[16]。在机器人学和计算机视觉中，人工智能研究解决类似的挑战，采用同时定位与地图构建（SLAM）等技术。然而，人工智能仍然缺乏人类所见到的无缝和实时整合。此外，详细的触觉知觉（L3）仍然鲜为人知，并且在机器人技术和假肢应用方面具有相当大的潜力。"
                },
                {
                  "type": "text",
                  "content": "Occipital Lobe: Visual Processing Specialized in visual perception (Ll),the occipitallobe efficiently processes visual stimulithrough hierarchical structures[13].AIexcels inbasicvisualrecognitiontasks,achieving human-levelor suprior performance using deep neural networks and vision transformers [15]. However, advanced capabilities such as contextual scene understanding (L2)and abstract visual reasoning remain challenging and are only moderately developed.\n\nTemporal Lobe: Language,Memory,and Auditory Processing The temporal lobes facilitate auditory processing (L1), language comprehension (L1), memory formation (L2), and semantic understanding (L2)[16].AI has notably advanced inlanguage and auditory processing,demonstrated by large language models (LLMs)capable of near-human speech recognition and language generation. However, robust episodic memory and lifelong learning capabilitiesremain limited, with AI systems frequently encountering issues like catastrophic forgeting.Grounding semantic understanding in multimodal experiences continues to be an active area of research.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "枕叶：视觉处理\n枕叶专门负责视知觉（L1），通过分层结构高效地处理视觉刺激[13]。人工智能在基本视觉识别任务上表现出色，利用深度神经网络和视觉转换器实现了与人类水平或更高水平的性能[15]。然而，高级功能，如语境场景理解（L2）和抽象视觉推理，仍然具有挑战性，发展程度仅属中等。\n\n颞叶：语言、记忆和听觉处理\n颞叶促进听觉处理（L1）、语言理解（L1）、记忆形成（L2）和语义理解（L2）[16]。人工智能在语言和听觉处理方面取得了显著进展，体现在能够实现接近人类语音识别和语言生成的大型语言模型（LLMs）上。然而，强大的情节记忆和终身学习能力仍然有限，人工智能系统经常遇到像灾难性遗忘这样的问题。将语义理解扎根于多模态体验仍然是一个活跃的研究领域。"
                },
                {
                  "type": "text",
                  "content": "Cerebellum: Coordination and Motor Learning The cerebellum primarily supports motor coordination (L2), precise skillearning (L2),and adaptive error correction (L2),with emergingroles incognitive timing and predictive modeling (cognitive timing, L3)[13]. Al-based robotics has achieved limited successes in emulating human-like dexterity. Real-time adaptive control remains challenging,though current research in reinforcement learning and meta-leaning shows promising initialresults.Cognitive functions of the cerebellum represent an underexplored yet promising frontier.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "小脑：协调和运动学习\n小脑主要支持运动协调（L2）、精细技能学习（L2）和适应性错误修正（L2），并在认知时间和预测建模（认知时间，L3）中扮演新兴角色[13]。基于人工智能的机器人技术在模拟类人灵巧性方面取得了有限的成功。实时自适应控制仍然具有挑战性，尽管当前在强化学习和元学习方面的研究显示出有希望的初步结果。小脑的认知功能代表了一个尚未被充分探索但有前景的前沿领域。"
                },
                {
                  "type": "text",
                  "content": "Brainstem: Autonomic Regulation and Reflexive Control The brainstem manages essential life-sustaining autonomic functions (L3)andrapid reflexive responses (L1),suchas basic motor reflexes[13].AI includes engineered reflexiveresponses,likeautomaticbraking inautonomous vehicles,ypically predefinedratherthanlearned.In contrast, the complexityof autonomic regulation and dynamic arousal states remains largely unexplored in AI,andtheir relevance may be limited due to fundamental differences between biological and artificial systems.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "脑干：自主调节和反射控制\n脑干管理基本的维持生命的自主功能（L3）和快速的反射性反应（L1），如基本的运动反射[13]。人工智能包括设计好的反射性反应，比如自动驾驶汽车中的自动制动，通常是预先定义的而非学习得来。相比之下，在人工智能中自主调节和动态唤醒状态的复杂性仍然大部分未被探索，由于生物和人工系统之间的基本差异，它们的相关性可能受到限制。"
                },
                {
                  "type": "text",
                  "content": "Limbic System: Emotion, Empathy, and Motivation The limbic system,comprising the amygdala and hippocampus, governs emotional processing (L3),reward mechanisms (L2),empathy (L3),stress regulation (L3),and motivationaldrives (L3)[13].AI'sreinforcementlearning algorithmsemulatereward-basedlearning superficialy,but nuanced emotional comprehension, genuine empathy, and internal motivational states remain significantly underdeveloped. Ethical concerns regarding emotional manipulation highlight the need for careful and responsible exploration.",
                  "index": 7,
                  "part": 0,
                  "translated_content": "边缘系统：情绪、共情和动机\n边缘系统由杏仁核和海马组成，主管情绪处理（L3）、奖励机制（L2）、共情（L3）、应激调节（L3）和动机驱动（L3）[13]。人工智能的强化学习算法模拟基于奖励的学习，但情绪理解的微妙差异、真正的共情和内在动机状态仍然显著不足。对于情绪操纵的伦理关切突显了对于谨慎和负责的探索的必要性。"
                },
                {
                  "type": "text",
                  "content": "Bridging Brain-Like Functions and Building Beneficial AI Until now, wehave witnessed the gap between human brain and machine intellgence.Neverthelesstheobjectiveisnot necessarilytoreplicateeveryfacetof humancognition within artificialintelligence systems.Rather,ouroverarching aimshould betodevelopintelligent agents thatare useful, ethical,safe,andbeneficialtosciety.Bycticallymparinghumanandartifcalintellgence,wehighlighttheexiting gapsand illuminate promising directions forinnovation.Thiscomparative perspectiveallows us toselectively integrate beneficial aspects of human cognition,such as energy-efficient processing, lifelong adaptive learning, emotional grounding,andrichcreativity,while simultaneously innovating beyond human limitations.Ultimately, this aproach aims to foster the creation of more capable, resilient, and responsible AI systems.",
                  "index": 8,
                  "part": 0,
                  "translated_content": "架桥脑样功能与构建有益人工智能至今，我们见证了人类大脑与机器智能之间的差距。然而，目标并不一定是在人工智能系统内复制人类认知的每一个方面。相反，我们的总体目标应该是开发对社会有用、符合伦理、安全和有益的智能体。通过比较人类和人工智能，我们突出了现有的差距，并阐明了创新的有希望方向。这种比较视角使我们能够有选择地整合人类认知的有益方面，比如高效处理、终身适应性学习、情感基础和丰富的创造力，同时超越人类的局限进行创新。最终，这种方法旨在促进更有能力、更具弹性和更负责任的人工智能系统的创建。"
                },
                {
                  "type": "text",
                  "content": "Furthermore,it is vital toconsider the evolving role ofhumans within ahybridHuman-AI society.The goal of AI should notbeto replace human roles entirely,butrather to augment and empowerhuman abilities,complementing human skills and judgment inareas where AIexcels,such as handling vast datasets,performing rapidcalculations,and automatingrepetitivetasks.Human oversight andinterpretabilityare essntialtoensure that powerfulAIsystemsremain controllable and aligned withhumanvalues andethical standards.Thus,thecoreobjective must bethe development of AI technologies that are transparent, interpretable, and responsive to human guidance.",
                  "index": 9,
                  "part": 0,
                  "translated_content": "此外，考虑人类在混合人工智能社会中的不断变化角色至关重要。人工智能的目标不应是完全取代人类角色，而应是增强和赋能人类能力，在AI擅长的领域，如处理海量数据、进行快速计算和自动化重复任务方面，补充人类技能和判断。人类监督和可解释性对于确保强大的AI系统保持可控性并与人类价值观和伦理标准保持一致至关重要。因此，核心目标必须是开发透明、可解释且对人类指导响应灵活的人工智能技术。"
                },
                {
                  "type": "text",
                  "content": "Human-centered AI design emphasizes collaboration, safety, and social responsibility, ensuring technological advancement proceeds in acontrolled,reliable manner. By placing humans at the centerof the AI ecosystem, wecan harness Al'spotentialtoenhancehuman productivitycreativity,anddecisionmakingfacilitating technicaland societal progress without compromising human autonomy or dignity. Ultimately,athoughtfulintegration of human intelligence and AI capabilities can pave the way for a sustainable, equitable, and prosperous future.",
                  "index": 10,
                  "part": 0,
                  "translated_content": "以人为中心的人工智能设计强调协作、安全和社会责任，确保技术进步以受控、可靠的方式进行。通过将人类置于人工智能生态系统的中心，我们可以利用人工智能的潜力，增强人类的生产力、创造力和决策能力，促进技术和社会进步，而不损害人类的自主权或尊严。最终，对人类智能和人工智能能力进行深思熟虑的整合，可以为一个可持续、公平和繁荣的未来铺平道路。"
                }
              ],
              "raw_title": "Brain Functionality by Region and AI Parallels",
              "type": null,
              "children": [],
              "translated_title": "1.2.1 大脑功能与区域及人工智能的相似性"
            }
          ],
          "translated_title": "1.2 人脑与人工智能代理之间的并行比较"
        },
        {
          "title": "1.3 A Modular and Brain-Inspired AI Agent Framework",
          "number": "1.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "One core issue in theLLMera isthelackofaunifiedframeworkthat integrates therichcognitive and functional components required byadvanced agents.While LLMsoffer exceptionallanguage reasoning capabilities, many current agent designs remain ad hoc-they incorporate modules like perception, memory,or planning in a piecemeal fashion, failing to approximate the well-coordinatedspecialization seen in biological systems such as the human brain. Unlike current LM agents,the human brain seamlessy balances perception, memory,reasoning,and actionthrough distinct yet interconnectedregions,facilitating adaptive responses tocomplex stimuli.LLM-driven agents,bycontrastoften stumblewhen tasks require cross-domain or multimodal integration,highlighting the needfora more holistic approach akin to thebrain'sfunctional diversityMotivated bythese paralls,our survey advocates drawing inspiration fromthe human brain to systematically analyze and design agent frameworks.This perspective shows that biological systems achieve general inteligence byblending specializedcomponents(for perception,reasoning,action,etc.)inatightly integrated fashion-an approach that could serve as a blueprint for strengthening current LLM-based agents.",
              "index": 0,
              "part": 0,
              "translated_content": "LLM时代的一个核心问题是缺乏一个统一的框架，该框架整合了先进代理所需的丰富认知和功能组件。虽然LLMs提供了出色的语言推理能力，但许多当前的代理设计仍然是临时的 - 它们以一种零碎的方式整合了感知、记忆或规划等模块，未能逼近生物系统（如人脑）中所见的良好协调的专业化。与当前的LM代理不同，人脑通过不同但相互连接的区域无缝平衡感知、记忆、推理和行动，促进对复杂刺激的适应性反应。相比之下，由LLM驱动的代理在需要跨领域或多模态集成的任务时往往遇到困难，突出了需要一种更全面的方法，类似于大脑的功能多样性。受到这些相似之处的启发，我们的调查主张从人脑中汲取灵感，系统地分析和设计代理框架。这一观点表明，生物系统通过将专门组件（用于感知、推理、行动等）紧密集成的方式实现了通用智能 - 这种方法可以作为加强当前基于LLM的代理的蓝图。"
            },
            {
              "type": "text",
              "content": "Neuroscientificresearchreveals thatthebrain leverages bothrational circuits(e.g.,the neocortex,enabling deliberation and planning)and emotional circuits (e.g.,the limbic system)to guide decision-making.Memory formation involves the hippocampus andcortical mechanisms,while reward signals,mediated by dopaminergic andother neuromodulatory pathways,reinforce behavior and learning.These biologicalinsights inspire several design principles for AI agents, including but not limited to:",
              "index": 1,
              "part": 0,
              "translated_content": "神经科学研究揭示，大脑利用理性电路（例如，新皮层，促使深思熟虑和规划）和情绪电路（例如，边缘系统）来引导决策。记忆形成涉及海马体和皮质机制，而奖励信号，通过多巴胺和其他神经调节途径介导，强化行为和学习。这些生物学见解启发了AI代理的几项设计原则，包括但不限于："
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td>Symbol</td><td>Meaning</td></tr><tr><td>W</td><td>The world with society systems that encapsulate both environment and intelligent beings (AI or human).</td></tr><tr><td>S</td><td> State space of the environment.</td></tr><tr><td> St ∈S</td><td>Environment's state at time t.</td></tr><tr><td></td><td>Observation space.</td></tr><tr><td>Ot∈O</td><td>Observation at time t (potentially shaped by attention or other perception filters).</td></tr><tr><td>A</td><td>Agent's action space.</td></tr><tr><td>at∈A</td><td>Action output by the agent at time t. This can be an external (physical) action or an internal (mental) action such as planning or decision-making.</td></tr><tr><td>M</td><td>Space of all mental states.</td></tr><tr><td>Mt∈M</td><td>Agent's mental state at time t, encompassing sub-components (memory, emotion, etc.).</td></tr><tr><td>Mmem</td><td>Memory component in Mt (e.g., short-term or long-term knowledge).</td></tr><tr><td>Mwm</td><td>World model component in Mt (internal representation of how the environment evolves).</td></tr><tr><td>Memo</td><td>Emotion component in Mt (internal valence, arousal, or affective states).</td></tr><tr><td>Mgoal</td><td>Goal component in Mt (objectives, desired outcomes, intentions).</td></tr><tr><td>Mrew</td><td>Reward/Learning signals in Mt (drives updates to preferences, values, or policy).</td></tr><tr><td>L</td><td>Learning function: L : M × A× O -→ M. Responsible for updating or learning the next mental state (e.g., memory, world model, emotion), based on the previous mental state Mt-1,the previous action at-1, and the new observation ot. Reflects how the agent acquires or revises knowledge, skills, or preferences.</td></tr><tr><td>R</td><td>Reasoning function: R : M -→ A. Responsible for deriving the next action at given the updated mental state Mt. Can involve planning, decision-making, or other internal logic.</td></tr><tr><td>C</td><td>Cognition function: C : M × A × O → M × A. Encapsulates both learning (L) and reasoning (R). Concretely, (Mt,at) = C(Mt-1,at-1,Ot) means the agent first learns the new mental state Mt = L(Mt-1,at-1,Ot),then reasons about the next action at = R(Mt).</td></tr><tr><td>E</td><td>Action execution (effectors): E : A -→ A.(Optional) transforms or finalizes at before applying it to the environment (e.g., converting a high-level command into low-level motor signals).</td></tr><tr><td>T</td><td>Environment transition: T : S × A -→ S. Defines how the environment state evolves from (st, at) to St+1.</td></tr></table></body></html>",
              "caption": "Table 1.2:Notation summary forthe revised agent framework,highlighting separate learning and reasoning function: within the overall cognition process.",
              "index": 2,
              "part": 0,
              "translated_caption": "表1.2: 修订后的智能代理框架符号总结，突出整体认知过程中的学习和推理功能的分离。"
            },
            {
              "type": "text",
              "content": "·Parallel, Multi-Modal Processing: The brain processes visual, auditory, and other sensory inputs in parallel through specialized cortical areas,integrating them in associative regions.Similarly, AI agents benefit from parallel processing of diverse sensor streams,fusing them in later stages for coherent understanding. · Hierarchical and Distributed Cognition: Reasoning, planning, emotional regulation, and motor control involve interactions between cortical and subcortical regions.Analogously, AI agents can employ modular architectures with subsystems dedicated to rational inference, emotional appraisal, and memory. ·Attention Mechanisms: Human attention prioritizes sensory data based on context, goals, and emotions.AI agents can replicate this by modulating perception through learned atention policies, dynamically adjusting focus based on internal states.",
              "index": 3,
              "part": 0,
              "translated_content": "·并行、多模式处理：大脑通过专门的皮质区域并行处理视觉、听觉和其他感官输入，并在联想区域中将它们整合。同样，AI代理受益于对不同传感器流的并行处理，在后续阶段将它们融合以实现连贯理解。·分层和分布式认知：推理、规划、情绪调节和运动控制涉及皮质和皮质下区域之间的相互作用。类似地，AI代理可以采用模块化架构，其中子系统专门用于理性推断、情绪评估和记忆。·注意机制：人类的注意力基于上下文、目标和情绪优先处理感官数据。AI代理可以通过调节感知的学习注意力策略来复制这一过程，根据内部状态动态调整焦点。"
            },
            {
              "type": "text",
              "content": "· Reward and Emotional Integration: Emotions are not merely noise but integral to decision-making, modulating priorities,enhancing vigilance, and guiding learing. Reward-driven plasticity facilitates habit formation and skill acquisition, a concept critical to reinforcement learning in AI agents. ·Goal Setting and Tool Usage: The human prefrontal cortex excels at setting abstract goals and planning action sequences, including tool uses. Similarly, AI agents require robust goal-management systems and adaptive action repertoires, driven by external rewards and intrinsic motivations.",
              "index": 4,
              "part": 0,
              "translated_content": "·奖励和情感整合：情绪不仅仅是噪音，而是决策中不可或缺的一部分，调节优先级，增强警惕，并引导学习。以奖励为驱动的可塑性促进习惯形成和技能习得，这一概念对于AI代理中的强化学习至关重要。·目标设定和工具使用：人类的前额叶皮质擅长设定抽象目标和规划行动序列，包括工具使用。同样，AI代理需要强大的目标管理系统和适应性行动方案，由外部奖励和内在动机驱动。"
            },
            {
              "type": "text",
              "content": "nese principles form the foundation of our proposed brain-inspired agent framework, where biological mechanisms rve as inspiration rather than direct replication.\n\nIn the following sections,weoutline ourframework'skeyconcepts,introducing aunifiedagent architecture basedon the perception-cognition-action loop enriched byreward signals andlearning processes.Each subsystem is carefully definedandinterconnectedtoensure transparencyinhow memory,world modelsemotions,goals,rewards, andleaing interact. We formalize cognition as a generalreasoning mechanism, with planning and decision-making framed as specific“mentalactions\"shaping behavior.Connections toestablished theories,such as Minsky's Societyof Mind[17], Buzsaki's inside-out perspective[18],and Bayesianactive inference[19],areexplored tohighlight the framework's generality and biological plausibility.",
              "index": 5,
              "part": 0,
              "translated_content": "这些原则构成了我们提出的基于大脑启发的代理框架的基础，其中生物机制作为灵感的来源而不是直接复制。\n\n在接下来的章节中，我们概述了我们框架的关键概念，介绍了一个基于感知-认知-行动循环的统一代理架构，通过奖励信号和学习过程进行丰富。每个子系统都被仔细定义和相互连接，以确保记忆、世界模型、情绪、目标、奖励和学习之间的相互作用的透明性。我们将认知形式化为一种通用的推理机制，规划和决策被构建为塑造行为的特定“心理行为”的具体行动。与已建立的理论，如明斯基的“心智社会”[17]、布扎基的内外透视[18]和贝叶斯主动推断[19]的联系被探讨，以突显该框架的普适性和生物可信度。"
            },
            {
              "type": "figure",
              "src": "images/e45d4a32b5d8e29284930de7efec6b3dee6de4a47936adc7cb0b15ecaefb5f72.jpg",
              "alt": "",
              "caption": "Figure 1.2: An overview of our general framework for describing an intelligent agent loop and agent society.",
              "index": 6,
              "part": 0,
              "translated_caption": "图1.2：我们描述智能代理循环和代理社会的一般框架概述。"
            }
          ],
          "raw_title": "A Modular and Brain-Inspired AI Agent Framework",
          "type": null,
          "children": [
            {
              "title": "1.3.1 Core Concepts and Notations in the Agent Loop",
              "number": "1.3.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Our architecture operates atthreeconceptuallevels:Society,Environment,and Agent.The Agent is then decomposed into three main subsystems: Perception, Cognition,and Action. Within Cognition, we identify key submodules: memory,world model,emotional state,goals,reward,learning,and reasoning processes (including“planning\"and “decision-making\"as specialactions produced with reasoning).Attention is primarilyhandled within perception and cognition. Before presenting the formal loop, we summarize our symbols in Table 1.2.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "我们的架构在三个概念层面操作：社会、环境和代理。然后，代理被分解为三个主要子系统：感知、认知和行动。在认知中，我们确定了关键的子模块：记忆、世界模型、情感状态、目标、奖励、学习和推理过程（包括“规划”和“决策”作为推理产生的特殊行为）。注意力主要在感知和认知中处理。在介绍正式循环之前，我们在表1.2中总结了我们的符号。"
                },
                {
                  "type": "text",
                  "content": "In the following, based on the notations in Table 1.2, we present our proposed agent loop.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "接下来，基于表1.2中的符号，我们提出了我们的代理循环。"
                }
              ],
              "raw_title": "Core Concepts and Notations in the Agent Loop",
              "type": null,
              "children": [],
              "translated_title": "1.3.1 代理循环中的核心概念和符号"
            },
            {
              "title": "1.3.2 Biological Inspirations",
              "number": "1.3.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Although our agent model is fundamentally computational, each submodule draws inspiration from well-studied biologicalcounterparts in the human brain.Below, we discussthese analogies in amanner that highlights boththe neuroscientific basis and the flexibility afforded by AI implementations.\n\nMemory (Hippocampus and Neocortex).Decades of neuroscience researchhave linked the hippocampus to episodic memory formation, while corticalregions are known tohouse semantic and proceduralknowledge[21,22].In humans, these memory subsystems cooperate to manage both short-term encoding and long-term consolidation. Our memory component, $M_{t}^{\\mathrm{mem}}$ , similarly aims to capture multi-scale learning by storing recent experiences and knowledge. This can be realizedthrougheither neuralnetwork weights(long-term)orexplicit bufers (short-term),thereby mirroring the hippocampal-cortical interplay.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "尽管我们的智能代理模型在根本上是计算化的，但每个子模块都从人类大脑中经过深入研究的生物学对应物中汲取灵感。下面，我们讨论这些类比，强调神经科学基础以及AI实现所提供的灵活性。\n\n记忆（海马体和新皮层）。几十年的神经科学研究将海马体与情节记忆形成联系起来，而皮层区域则被认为存储语义和程序知识。在人类中，这些记忆子系统合作管理短期编码和长期巩固。我们的记忆组件$M_{t}^{\\mathrm{mem}}$ 同样旨在通过存储最近的经验和知识来捕捉多尺度学习。这可以通过神经网络权重（长期）或显式缓冲区（短期）来实现，从而反映海马皮质相互作用。"
                },
                {
                  "type": "text",
                  "content": "World Model(Predictive Processing).A prominent theory incognitive neuroscience holds thatthe cortexoperates as a predictive machine,continually comparing incoming sensory data with generated expectations [23,19].The world model $M_{t}^{\\mathrm{wm}}$ reflects this idea by maintaining an internal representation of how the environment evolves over time. Just as cortical circuits integrate multisensory data to update these internal models,our framework allows $M_{t}^{\\mathrm{wm}}$ to be refined upon each new observation andrelevant reward oremotionalcues,ofering aBayesian or free-energy perspective on environmental dynamics.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "世界模型（预测处理）。认知神经科学中一个重要理论认为，皮质作为一个预测机器运作，不断将传入的感官数据与生成的期望进行比较。世界模型$M_{t}^{\\mathrm{wm}}$通过维护一个内部表示来反映环境随时间演变的方式。正如皮质回路整合多感官数据以更新这些内部模型一样，我们的框架允许$M_{t}^{\\mathrm{wm}}$在每次新观察和相关奖励或情绪提示时得以完善，提供了对环境动态的贝叶斯或自由能视角。"
                },
                {
                  "type": "text",
                  "content": "Emotion (Limbic System).Emotions, mediated by structures like the amygdala,hypothalamus, and limbic system, significantlymodulate attention,learningrates,and decision-making thresholds[24,25].By introducing anemotion component $M_{t}^{\\mathrm{{emo}}}$ ,our model captures how internal valence or arousal states can shift an agent's focus and behavior. Althoughcomputational“emotions\"are neither ful analogous to biological affct nor conscious feelings,they can guide adaptive heuristics-such as prioritizing urgent goals or responding quickly to perceived threats.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "情绪（边缘系统）。情绪，由杏仁核、下丘脑和边缘系统等结构介导，显著调节注意力、学习速率和决策门槛[24,25]。通过引入一个情绪组件$M_{t}^{\\mathrm{{emo}}}$，我们的模型捕捉了内部价值或唤醒状态如何转移代理的关注和行为。虽然计算“情绪”既不完全类似于生物影响，也不是意识感受，但它们可以引导适应性启发式，比如优先考虑紧急目标或快速回应感知到的威胁。"
                },
                {
                  "type": "text",
                  "content": "Goals and Reward (Prefrontal & Subcortical Circuits).Humans excel atformingabstract, long-term goals, an ability often associated withprefrontalcortexfunction[26,27].In paralel, subcorticalcircuits—particularlydopaminergic pathways-drive reinforcement signals that shape motivation and habit learning [28]. Our agent includes $M_{t}^{\\mathrm{goal}}$ for storing objectives and $M_{t}^{\\mathrm{rew}}$ for encoding reward signals, thus enabling a continuous feedback loop where goal formation andreward-based adaptation reinforce eachother.This mechanism allows for planned action sequences,tool usage, and more nuanced social interactions.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "目标与奖励（前额叶和皮层下环路）。人类擅长制定抽象的、长期的目标，这种能力通常与前额叶皮质功能相关联。与此同时，皮质下环路——特别是多巴胺通路——推动着塑造动机和习得习惯的强化信号。我们的智能体包括$M_{t}^{\\mathrm{goal}}$用于存储目标和$M_{t}^{\\mathrm{rew}}$用于编码奖励信号，从而实现目标形成和基于奖励的适应之间的连续反馈循环。这种机制允许计划的行动序列、工具使用以及更加微妙的社交互动。"
                },
                {
                  "type": "text",
                  "content": "Reasoning, Planning, and Decision-Making (Prefrontal Cortex).Finaly,the human prefrontal cortex integrates information from memory,sensory inputs,emotions, and reward pathways to carry out higher-order cognitive processes—suchaslogicalreasoning,planning,andexecutivecontrol[29,30]. Inour agentframework,thesecapabilities are subsumed by the reasoning sub-function, which-through modules like PlanFn and Decide—selects and executes actions (whether physical or purely mental).Bydistinguishing planning from on-the-fly decision-making,we capture how the agentcansimulatefuture scenarios,weighoutcomes,andthencommit toacourseofaction,akintothe flexible orchestration observed in prefrontal circuits.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "推理、规划和决策（前额叶皮质）。最后，人类前额叶皮质整合来自记忆、感觉输入、情绪和奖励途径的信息，执行高阶认知过程，如逻辑推理、规划和执行控制。在我们的智能体框架中，这些能力被推理子功能所包含，通过PlanFn和Decide等模块选择和执行行动（无论是物理行动还是纯粹的思维）。通过区分规划和即时决策，我们捕捉了智能体如何模拟未来场景、权衡结果，然后承诺采取行动方案，类似于前额叶回路中观察到的灵活编排。"
                }
              ],
              "raw_title": "Biological Inspirations",
              "type": null,
              "children": [],
              "translated_title": "1.3.2 生物启发"
            },
            {
              "title": "1.3.3 Connections to Existing Theories",
              "number": "1.3.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "3eyond these explicit neurobiological parallels,our architecture resonates with several important theories in AI, :ognitive science, and neuroscience.\n\nClassic Perception-Cognition-Action Cycle.We extend the traditional sense-think-act cycle outlined by [20] incorporating explicit mechanisms forattention(inP),leaming andemotion (in C),andreward signals that persist over time.This explicitness makes it easier to analyze how an agent's internal states and prior actions shape subsequent perception and cognition.\n\nMinsky's“Society of Mind\".[17] argued that intellgence arises from an ensembleof specialized“agents\"within a mind. Our submodules- $\\cdot\\mathrm{C}_{\\mathrm{mem}}$ $\\mathrm{C}_{\\mathrm{wm}}$ $\\mathrm{C}_{\\mathrm{emo}}$ $\\mathrm{C_{goal}}$ goal，C $\\mathrm{C}_{\\mathrm{rew}}$ -echo this decomposition, distributing key functions (memory,prediction,emotionalevaluation, goal-seting,etc.)acrossseparate yetinteractingcomponents.Inabroader “society\"context,each agent (or sub-agent)could coordinate cooperativelyor competitively, much like Minsky's internal agencies.Recent work on naturallanguage-based societies of mind[31] supports that agentic systemscan be represented using the original society-of-mindtheory,andcould incorporate socialstructures and economic models among agents.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "除了这些明确的神经生物学相似之外，我们的架构与人工智能、认知科学和神经科学中的几个重要理论相一致。\n\n经典的感知-认知-行动循环。我们扩展了传统的感知-思考-行动循环，其中包括明确的注意力机制（在P中）、学习和情感（在C中），以及持续一段时间的奖励信号。这种明确性使得分析代理的内部状态和先前行动如何塑造后续感知和认知变得更容易。\n\n明斯基的“心智社会”。明斯基认为，智能来自于心智中的一组专门化的“代理”。我们的子模块 - $\\cdot\\mathrm{C}_{\\mathrm{mem}}$ $\\mathrm{C}_{\\mathrm{wm}}$ $\\mathrm{C}_{\\mathrm{emo}}$ $\\mathrm{C_{goal}}$ goal，C $\\mathrm{C}_{\\mathrm{rew}}$ - 回应了这种分解，将关键功能（如记忆、预测、情感评估、目标设定等）分布在相互作用的独立组件之间。在更广泛的“社会”背景下，每个代理（或子代理）可以像明斯基的内部机构一样协调合作或竞争。最近关于基于自然语言的心智社会的研究支持了使用原始心智社会理论来表示代理系统，并且可以在代理之间整合社会结构和经济模型。"
                },
                {
                  "type": "text",
                  "content": "Buzsaki's Inside-Out Perspective.Neuroscientists [18]contend that the brain actively constructs and updates its perception instead of merely receiving inputs. In our model, $M_{t-1}$ —including emotional states, reward signals, and goals—directly influences the perception map P.This supports the inside-out stance that an agent's internal context drives the way it samples and interprets the environment, rather than passively reacting to it.\n\nPartially observable Markov decision process (POMDP).Our framework can be viewed as a generalization of the classical Partially Observable Markov Decision Process (POMDP) in several ways.First, whereas a POMDP specifies a probabilistic transition function $\\textstyle P(s_{t+1}\\mid s_{t},a_{t})$ over a(possibly finite) state space, we retain an environment transition T withoutrestricting it toa purely probabilisticor finite form,allowing forarbitraryoreven deterministic mappings. Second, in the standard POMDP setting, reward is typically defined as a scalar function of $(s_{t},a_{t})$ (possibly discounted over time). By contrast, we place reward signals inside the agent's mental state $(M_{t}^{\\mathrm{rew}})$ , letting them depend on—and co-evolve with-goals,emotion, and the world model rather than enforcing a single externally defined objective. Third,whilePOMDPagents generallyselect actions by maximizing anexpectedreturn (value function)ourreasoning sub-process is broader. It accounts for memory,emotion, and other mental-state factors,accommodating heuristic or socially driven decisions rather than strictly value-based choices.Finally,a POMDP does not explicitly define cognitive submodules such as memory oremotion-these must be collapsed into a monolithic“belief state\". In our framework,eachsub-component (memory, world model,emotion, goals,reward) is explicitly modeled and updated, mirroring biologicall inspired views of cognition. Hence,although our approach recovers the POMDP formulation as a specialcase (byenforcing aprobabilistic T,ascalar reward,andaminimal mental state),itadmits aricher varietyof environment transitions, internal states, and decision mechanisms.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "Buzsaki的内外视角。神经科学家们认为，大脑积极构建和更新其感知，而不仅仅是接收输入。在我们的模型中，$M_{t-1}$，包括情绪状态、奖励信号和目标，直接影响感知地图P。这支持了内外立场，即一个代理的内部背景驱动其对环境进行采样和解释的方式，而不是被动地对其做出反应。\n\n部分可观察马尔可夫决策过程（POMDP）。我们的框架可以被看作是经典部分可观察马尔可夫决策过程（POMDP）的泛化。首先，虽然POMDP规定了一个概率转移函数 $\\textstyle P(s_{t+1}\\mid s_{t},a_{t})$ 在（可能是有限的）状态空间上，我们保留了一个环境转移T，而不将其限制为纯粹的概率性或有限形式，允许任意甚至确定性的映射。其次，在标准POMDP设置中，奖励通常被定义为$(s_{t},a_{t})$的标量函数（可能随时间打折）。相比之下，我们将奖励信号放置在代理的心智状态$(M_{t}^{\\mathrm{rew}})$中，让它们依赖于目标、情绪和世界模型，并与之共同演化，而不是强制执行单一外部定义的目标。第三，虽然POMDP代理通常通过最大化预期回报（值函数）来选择行动，我们的推理子过程更广泛。它考虑记忆、情绪和其他心智状态因素，适应启发式或社会驱动的决策，而不仅仅是基于价值的选择。最后，POMDP并没有明确定义认知子模块，如记忆或情感，这些必须合并为一个单一的“信念状态”。在我们的框架中，每个子组件（记忆、世界模型、情感、目标、奖励）都被明确建模和更新，反映了对认知的生物启发观点。因此，尽管我们的方法作为一种特例恢复了POMDP的表述（通过强制执行概率性T、标量奖励和最小心智状态），但它允许更丰富的环境转移、内部状态和决策机制。"
                },
                {
                  "type": "text",
                  "content": "Active Inference and the Bayesian Brain. Active inference, aunifying framework advanced by[19],suggests that agents continually update internal generative models to minimize prediction error (or“free energy\"). Our use of $M^{\\mathrm{wm}}$ and $M^{\\mathrm{rew}}$ , together with planning and decision-making modules, can be interpreted in Bayesian terms. The agent atempts toreduce surprise byaligning its world modelwithnew dataand bychoosing actions that conform topredicted (or desired) outcomes.\n\nBiological Plausibility & Generality.While the mapping between brain circuits and agent submodules is made at a high level,it ofers an approach that isat once biologically inspired and modularly agnostic.Memory,emotion, goals,andrewardcan each be implemented by various AI paradigms—symbolic methods, neural networks,or hybrid approaches—thus preserving flexibility.By integrating these key ideas from neuroscience,cognitive science,and AI, we arrive atageneralframework thatcaptures theessentialpropertiesofintelligent behaviorwithout overconstraining implementation details.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "主动推理与贝叶斯大脑。主动推理是由Buzsaki提出的统一框架，表明代理不断更新内部生成模型，以最小化预测误差（或“自由能量”）。我们使用的$M^{\\mathrm{wm}}$和$M^{\\mathrm{rew}}$，连同规划和决策模块，可以用贝叶斯术语来解释。代理试图通过将其世界模型与新数据对齐，并选择符合预测（或期望）结果的行动来减少惊奇。\n\n生物可信性与普适性。虽然大脑回路与代理子模块之间的映射是在高层次上进行的，但它提供了一种既具有生物启发又模块化不可知的方法。记忆、情感、目标和奖励可以通过各种人工智能范式来实现，包括符号方法、神经网络或混合方法，从而保持了灵活性。通过将神经科学、认知科学和人工智能的关键思想整合在一起，我们得到了一个通用框架，捕捉了智能行为的基本特性，而不过度约束实施细节。"
                }
              ],
              "raw_title": "Connections to Existing Theories",
              "type": null,
              "children": [],
              "translated_title": "1.3.3 现有理论的关联"
            }
          ],
          "translated_title": "1.3 一个模块化且灵感源自大脑的人工智能代理框架"
        },
        {
          "title": "1.4 Navigating This Survey",
          "number": "1.4",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "This survey isstructured to provide acomprehensive, modular, and interdisciplinary examination of intellgent agents, drawing inspiration fromcognitive science,neuroscience,andother disciplines to guide the next waveof advancements in AI.Whilemanyexisting surveys[32,33,34,35,36,37,38,39,40]offr valuableinsights intovarious aspects of agent research,we provide adetailedcomparison oftheir focalpoints in Table1.3.Our work distinguishes itself by systematically comparing biological cognition with computational frameworks to identify synergies,gaps,and opportunities for innovation.By bridging thesedomains,we aim to providea unique perspective that highlights not only where agents excel but also where significant advancements are needed to unlock their fullpotential.",
              "index": 0,
              "part": 0,
              "translated_content": "本调查旨在提供对智能代理进行全面、模块化和跨学科审查，借鉴认知科学、神经科学和其他学科的启发，引导人工智能领域下一波进展。尽管许多现有调查为智能代理研究的各个方面提供了有价值的见解，但我们在表1.3中详细比较了它们的焦点。我们的工作通过系统比较生物认知与计算框架，以识别协同作用、差距和创新机会，从而展现出独特的视角。通过搭建这些领域之间的桥梁，我们旨在不仅突出智能代理的优势所在，还要指出需要取得重大进展以释放其全部潜力的地方。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td> Survey</td><td>Cognition</td><td>Memory</td><td>World Model</td><td>Reward</td><td>Action</td><td>Self Evolve</td><td>MultiAgent</td><td>Safety</td></tr><tr><td>Zhang et al. [39]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td><td>0</td></tr><tr><td>Guo et al. [38]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>0</td><td>·</td><td>·</td><td>0</td></tr><tr><td>Yu et al. [40]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td><td>·</td></tr><tr><td>Wang et al. [35]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td> Masterman et al. [37]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td>Xi et al. [34]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>·</td><td>·</td><td>·</td></tr><tr><td>Huang et al. [33]</td><td>·</td><td>·</td><td>0</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td></tr><tr><td>Durante et al. [32]</td><td>·</td><td>·</td><td>0</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td></tr><tr><td>This Manuscript</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td></tr></table></body></html>",
              "caption": "Table 1.3: Summary of existing reviews with diferent focal points. ·indicates primary focus while $\\scriptscriptstyle\\mathrm{~o~}$ indicates secondary or minor focus.",
              "index": 1,
              "part": 0,
              "translated_caption": "表1.3：具有不同焦点的现有评论摘要。·表示主要焦点，而$\\scriptscriptstyle\\mathrm{~o~}$表示次要或较小焦点。"
            },
            {
              "type": "text",
              "content": "The survey is divided into four key parts:\n\n·In Part I: Modular Design of Inteligent Agents, we introduce the core modules of agents, including the cognition module,which serves as the“brain\"of the agent; the perception systems for interpreting sensory input; as well as the action systems for interacting with the external world. Within the cognition system, we further discuss the memory, world modeling,emotion, goal, and reward systems, analyzing their current progress, limitations, and research challenges.\n\n·In Part II:Self-Enhancement in Intelligent Agents, we shift focus to the capability of agents to evolve and optimize themselves. We explore mechanisms like adaptive learning,self-reflection, and feedback-driven improvement,inspired bythe human ability to grow and refine skills over time.This part also addresses the importance of dynamic memory systems and continuous knowledge integration for agents to remain relevant and effective in changing environments.\n·In Part II: Collaborative and Evolutionary Intelligent Systems, we examine how agents interact with each other and their environments to solve complex, large-scale problems. We discuss multi-agent systems, highlighting their applications in fields such as robotics, medical systems and scientific discovery. This part explores multi-agent system topologies and agent protocol, tracing the evolution of communication and collaboration from static to dynamic frameworks. We align agents with human collaboration paradigms, examining how interaction patterns shape the co-evolution of inteligence and how multi-agent systems adapt their decision-making in various collaborative settings to solve complex challenges through collective intelligence.\n·Finally,in Part IV: Building Safe and Beneficial AI, we provide a comprehensive analysis of the security landscape for LLM-based agents. We introduce a framework categorizing threats as intrinsic or extrinsic. Intrinsic vulnerabilities arise from within the agent's architecture: the core LLM“brain\", and the perception and action modules that enable interactions with the world. Extrinsic risks stem from the agent's engagement with memory systems, other agents,and the broader environment. This part not only formalizes and analyzes these vulnerabilities,detailing specific atack vectors like jailbreaking and prompt injection, but alsoreviews a range of defense mechanisms. Moreover, we explore future directions,including superalignment techniques and the scaling law of AI safety-the interplay between capability and risk.",
              "index": 2,
              "part": 0,
              "translated_content": "该调查分为四个关键部分：\n\n· 第一部分：智能代理的模块化设计，介绍了代理的核心模块，包括认知模块，作为代理的“大脑”；用于解释感官输入的感知系统；以及用于与外部世界交互的行动系统。在认知系统中，我们进一步讨论了记忆、世界建模、情感、目标和奖励系统，分析它们的当前进展、局限性和研究挑战。\n\n· 第二部分：智能代理的自我增强，我们将焦点转向代理体自我进化和优化的能力。我们探讨了像自适应学习、自我反思和基于反馈的改进这样的机制，受到人类随时间增长和完善技能的能力的启发。本部分还讨论了动态记忆系统和持续知识整合的重要性，以使代理在不断变化的环境中保持相关和有效。\n\n· 第三部分：协作和进化智能系统，我们研究代理如何与彼此和环境互动以解决复杂的大规模问题。我们讨论多代理系统，在机器人、医疗系统和科学发现等领域的应用，突出它们的应用。本部分探讨了多代理系统的拓扑结构和代理协议，追溯了从静态到动态框架的沟通和协作的演变。我们将代理与人类协作范式相结合，研究互动模式如何塑造智能的共同进化，以及多代理系统如何在各种协作环境中调整其决策，通过集体智慧解决复杂挑战。\n\n· 最后，在第四部分：构建安全和有益的人工智能中，我们全面分析了基于LLM的代理的安全格局。我们引入了一个将威胁分类为内在或外在的框架。内在的脆弱性源于代理体系结构内部：核心LLM“大脑”，以及使代理与世界互动的感知和行动模块。外在风险源自代理与记忆系统、其他代理和更广泛环境的互动。本部分不仅形式化和分析了这些脆弱性，详细说明了诸如越狱和提示注入等具体攻击向量，还审查了一系列防御机制。此外，我们探讨了未来的发展方向，包括超对齐技术和人工智能安全的扩展法则——能力和风险之间的相互作用。"
            },
            {
              "type": "text",
              "content": "By weaving togetherthese threads,our survey aims to provide aholistic perspectiveonthecurrnt state of intelligent agents andaforward-lookingroadmapfortheir development.Ourunique focus onintegratingcognitive science insights with computational design principlespositions this survey asa foundationalresource forresearchers seeking to design agents that are notonly powerfulandeffcient butalsoadaptive,ethical,anddeplyalignedwiththecomplexitiesof human society.",
              "index": 3,
              "part": 0,
              "translated_content": "通过将这些线索编织在一起，我们的调查旨在提供对智能代理当前状态的整体视角，并为它们的发展提供展望性路线图。我们独特地关注将认知科学见解与计算设计原则相结合，将这项调查定位为研究人员设计代理的基础资源，这些代理不仅强大高效，而且适应性强、具有道德性，并且与人类社会的复杂性深度契合。"
            }
          ],
          "raw_title": "Navigating This Survey",
          "type": null,
          "children": [],
          "translated_title": "1.4 浏览本调查"
        }
      ],
      "translated_title": "1 引言 12"
    },
    {
      "title": "2Cognition 25",
      "number": "2",
      "level": 1,
      "content": [],
      "raw_title": "Cognition 25",
      "type": null,
      "children": [
        {
          "title": "2.1  Learning 25",
          "number": "2.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "2.1.1 Learning Space 27\n2.1.2 Learning Objective 29\n2.2 Reasoning 31\n2.2.1 Structured Reasoning 32\n2.2.2 Unstructured Reasoning 34\n2.2.3 Planning 36",
              "index": 0,
              "part": 0,
              "translated_content": "2.1.1 学习空间 27\n2.1.2 学习目标 29\n2.2 推理 31\n2.2.1 结构化推理 32\n2.2.2 非结构化推理 34\n2.2.3 规划 36"
            }
          ],
          "raw_title": "Learning 25",
          "type": null,
          "children": [],
          "translated_title": "2.1 学习 25"
        },
        {
          "title": "2.1 Learning",
          "number": "2.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Learning represents the fundamental process through which intelligent agents transform experiences into knowledge within their mental states.This transformation occurs across different cognitive spaces,fom holistic updates across the full mental state torefinement of specificcognitivecomponents.The scope oflearning encompasses remarkable capacities that serve diferent objectives:enhancing perceptual understanding,improving reasoning capabilities, and developing richer world understanding.",
              "index": 0,
              "part": 0,
              "translated_content": "学习代表着智能代理在其心智状态中将经验转化为知识的基本过程。这种转化跨越不同的认知空间，从对整个心智状态的全面更新到特定认知组件的细化。学习的范围涵盖了为不同目标提供支持的显著能力：增强感知理解、改进推理能力和发展更丰富的世界理解。"
            },
            {
              "type": "figure",
              "src": "images/d3e62f0be6d4fce59bf96b53373cb15cde8f7403963feb75602992f077d742e6.jpg",
              "alt": "",
              "caption": "Figure 2.1: Illustrative Taxonomy of Cognition system, including learning and reasoning paradigm.",
              "index": 1,
              "part": 0,
              "translated_caption": "图2.1：认知系统的分类示意图，包括学习和推理范式。"
            },
            {
              "type": "text",
              "content": "Humanlearning operates across multiple spaces and objectives through the brain's adaptable neural networks. The brain coordinates learming acrossits entire network through integrated systems:the hippocampus facilitates rapid encoding ofepisodicexperiences, thecerebellum supports supervisedlearningforprecisemotorskill,thebasalganglia enable reinforcement learning through dopaminergic reward signals,andcorticalregions facilitate unsupervised pattern extraction[99].Atmore focusedlevels,specific neuralcircuitscan undergotargetedadaptation,allowingforspecialized skill development and knowledge acquisition. These systems work together on different timescales,ranging from immediate responses tolifelong development, while being influenced byfactors like attention,emotions,and social environment [27].",
              "index": 2,
              "part": 0,
              "translated_content": "人类学习通过大脑适应性神经网络在多个空间和目标上运作。大脑通过综合系统协调整个网络上的学习：海马体促进情节经验的快速编码，小脑支持精确运动技能的监督学习，基底神经节通过多巴胺奖励信号实现强化学习，大脑皮层区域促进无监督模式提取。在更专注的层面上，特定神经回路可以经历定向适应，从而实现专业化技能发展和知识获取。这些系统在不同时间尺度上协同工作，范围从即时响应到终身发展，同时受到注意力、情绪和社会环境等因素的影响。"
            },
            {
              "type": "text",
              "content": "LLM agents, while fundamentally diffrent in architecture, implement analogous learning processes across their mental state spaces.At thecomprehensive level,they acquire broad knowledge through pre-training on massive datasets,demonstrating a form of unsupervised learning.At more focused levels, theyrefine specific capabilities through parameter-updating mechanisms like supervised fine-tuning and reinforcementlearning. Uniquely, they also demonstrate in-contextlearning capabilities,adapting to noveltaskswithout parameter changes by leveraging context within their atention window:a capability that mirrors aspects of human working memory but operates through fundamentally different mechanisms.",
              "index": 3,
              "part": 0,
              "translated_content": "虽然在体系结构上存在根本差异，但LLM代理在其心智状态空间中实现类似的学习过程。在综合层面上，它们通过在大规模数据集上进行预训练来获取广泛知识，展示一种形式的无监督学习。在更专注的层面上，它们通过参数更新机制（如监督微调和强化学习）来完善特定能力。独特的是，它们还展示了在上下文中学习的能力，通过利用其注意力窗口内的上下文来适应新任务，而无需参数更改：这种能力反映了人类工作记忆的某些方面，但通过根本不同的机制运作。"
            },
            {
              "type": "text",
              "content": "The comparison between human and artificial learning systems provides valuable insights for developing more capable,adaptive agents.Humanlearning demonstrates notable characteristics in eficiency,contextualization, and integration with emotional systems,while LLM-basedapproaches show distinct capabilities in processng large datasets, representing formal knowledge,and synthesizing information acrossdomains.These complementary strengths suggest productive directions for research.As we explorethe foundations of learning,we first examine the spaces where learning occurs within mental states,folowed byan analysisof the specificobjectives thatdrivelearning proceses.",
              "index": 4,
              "part": 0,
              "translated_content": "人类和人工学习系统之间的比较为开发更具能力和适应性的代理提供了宝贵的见解。人类学习在效率、情境化和与情感系统的整合方面表现出显著特征，而基于LLM的方法则在处理大规模数据集、表示形式知识和跨领域综合信息方面具有独特能力。这些互补的优势为研究提供了富有成效的方向。在探索学习的基础时，我们首先研究学习发生的心智状态空间，然后分析推动学习过程的具体目标。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td>Method</td><td>Model</td><td>Perception</td><td>Reasoning</td><td>Memory</td><td>Reward</td><td>World Model</td></tr><tr><td>Voyager [47]</td><td>0</td><td>0</td><td></td><td>·</td><td></td><td>0</td></tr><tr><td>Generative Agents [50]</td><td>0</td><td></td><td>0</td><td>·</td><td>0</td><td>0</td></tr><tr><td>Learn-by-interact [102]</td><td>·</td><td>O</td><td></td><td>·</td><td></td><td></td></tr><tr><td>RAGEN[63]</td><td>·</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td>DigiRL[103]</td><td>·</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td>R1-Searcher [45]</td><td>·</td><td>·</td><td>·</td><td></td><td>·</td><td>0</td></tr><tr><td>RewardAgent [104]</td><td>·</td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td></tr><tr><td>Text2Reward [105]</td><td></td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td></tr><tr><td>ARAMP [106]</td><td>·</td><td></td><td>0</td><td>0</td><td>·</td><td>0</td></tr><tr><td>ActRe [49]</td><td>·</td><td></td><td>·</td><td>0</td><td>0</td><td>·</td></tr><tr><td>WebDreamer [107]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>·</td></tr><tr><td>RAP [74]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>·</td></tr><tr><td>AutoManual [108]</td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td></tr></table></body></html>",
              "caption": "Table 2.1: Summary of Learning Methods with Different State Modifications. $\\bullet$ indicates primary impact while $\\scriptscriptstyle\\mathrm{~o~}$ indicates secondary or no direct impact.",
              "index": 5,
              "part": 0,
              "translated_caption": "表2.1：不同状态修改学习方法总结。$\\bullet$表示主要影响，而$\\scriptscriptstyle\\mathrm{~o~}$表示次要或无直接影响。"
            }
          ],
          "raw_title": "Learning",
          "type": null,
          "children": [
            {
              "title": "2.1.1 Learning Space",
              "number": "2.1.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The learning approaches in LLMagents represent astructured, data-driven paradigm in contrast to the exploratory, emotionally-driven learningobserved in humans.While human learning often involves active curiosity,motivation,and emotionalreinforcement, LLM-based agents typicall learn through more formalized proceses,such as parameter updates during training or structured memory formation during exploration.Current agent architectures attemptto bridge this gapbyimplementing mechanisms that simulate aspects ofhuman learning while leveraging the strengths of computational systems.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "LLM代理中的学习方法代表了一种结构化的、数据驱动的范式，与人类观察到的探索性、情感驱动的学习形成对比。虽然人类学习通常涉及积极的好奇心、动机和情感强化，但基于LLM的代理通常通过更正式的过程学习，例如在训练期间的参数更新或在探索过程中的结构化记忆形成。当前的代理架构尝试通过实施模拟人类学习方面的机制，同时利用计算系统的优势来弥合这一差距。"
                },
                {
                  "type": "text",
                  "content": "Leaning within an intellgent agent occurs across different spaces, encompassing both the underlying model $\\theta$ and mental states $M$ ,where the former fundamentally supports the capabilities and limitations of the latter. Formally, we define an intelligent agent's internal state as a tuple ${\\mathcal{T}}=(\\theta,M)$ that includes both the model parameters and mental state components.The mental state can be further decomposed intodifferent structures as we illustrated in 1.2:",
                  "index": 1,
                  "part": 0,
                  "translated_content": "智能代理内部的学习发生在不同的空间中，涵盖了基本模型 $\\theta$ 和心智状态 $M$，前者从根本上支持后者的能力和局限性。形式上，我们将智能代理的内部状态定义为一个元组 ${\\mathcal{T}}=(\\theta,M)$，其中包括模型参数和心智状态组件。如我们在1.2节中所示，心智状态可以进一步分解为不同的结构："
                },
                {
                  "type": "formula",
                  "content": "$$ \nM=\\{M^{m e m},M^{w m},M^{e m o},M^{g o a l},M^{r e w}\\}\n $$",
                  "index": 2,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "where $M^{m e m}$ represents memory, $M^{w m}$ denotes world model, $M^{e m o}$ indicates emotional state, $M^{g o a l}$ represents goals, and $M^{r e w}$ represents reward signals.\n\nModifications to the underlying modelcan beviewed as fullmental state learning,as they fundamentally alter the agent's capabilities.While model-level modifications can affect diferent mental states to varying degrees,changes to the model'scontext window orexternal structures tend tofocus on specific mental state components.For instance, leaning experiences andskill fromthe environment primarilyinfluencememory,while leveraging the LLM's inherent predictive capabilities enhances the world model.\n\nFull Mental State Learning Fullmental state learming enhances thecapabilities of an agent through comprehensive modifications to the underlying model $\\theta$ , which in turn affects all components of the mental state $M$ . This process begins with pre-training, which establishes the foundation of language models by acquiring vast world knowledge, analogous to how human babies absorb environmental information during development,though in a more structured and extensive manner.\n\nPost-training techniques represent the cornerstone for advancing agent capabilities.Similar tohow human brains are shaped byeducation,these techniques whileaffcting theentire model,can emphasize diffrent aspects ofcognitive development.Specificaly,various forms of tuning-basedlearning enable agents toacquiredomain-specificknowledge and logical reasoning capabilities. Supervised Fine-Tuning (SFT)[41] serves as the fundamental approach where models learnfrom human-labeled examples,encoding knowledge directly intothe model's weights.For computational effciency, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged.Adapter-BERT[42]introduced modular designs that adapt models to downstream tasks without modifying allparameters, while Low-Rank Adaptation (LoRA) [109] achieves similarresults by decomposing weight updates into low-rank matrices,adjusting only a smallsubsetof effective parameters.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "其中，$M^{mem}$ 代表记忆（Memory），$M^{wm}$ 表示世界模型（World Model），$M^{emo}$ 表示情感状态（Emotional State），$M^{goal}$ 代表目标（Goals），$M^{rew}$ 表示奖励信号（Reward Signals）。\n\n对基础模型的修改可以被视为全面的心智状态学习，因为它们从根本上改变了代理的能力。虽然模型级别的修改可能以不同程度影响不同的心智状态，但对模型的上下文窗口或外部结构的更改往往集中在特定的心智状态组件上。例如，从环境中学习经验和技能主要影响记忆，而利用LLM固有的预测能力则增强了世界模型。\n\n全面的心智状态学习通过对基础模型 $\\theta$ 进行全面修改来增强代理的能力，从而影响心智状态 $M$ 的所有组件。这个过程始于预训练，通过获取广泛的世界知识来建立语言模型的基础，类似于人类婴儿在发育过程中吸收环境信息，尽管以更有结构和广泛的方式进行。\n\n后续训练技术代表了提升代理能力的基石。类似于人类大脑受教育的方式塑造，这些技术虽然影响整个模型，但可以强调认知发展的不同方面。具体而言，各种形式的基于调整的学习使代理能够获得领域特定知识和逻辑推理能力。监督微调（SFT）[41] 是一种基本方法，模型从人类标记的示例中学习，直接将知识编码到模型的权重中。为了提高计算效率，出现了参数高效微调（PEFT）方法。Adapter-BERT[42]引入了模块化设计，使模型适应下游任务而无需修改所有参数，而低秩调整（LoRA）[109] 通过将权重更新分解为低秩矩阵，仅调整一小部分有效参数，实现了类似的结果。"
                },
                {
                  "type": "text",
                  "content": "Some agent capabilities are closelyconnectedto how wellthey align with human preferences, with alignment-based learning approaches modifying the modeltoreshape aspects of the agent's underlying representations.Reinforcement learning from human feedback (RLHF)[110] aligns models with human values by training a reward model on comparative judgments and usingthis to guide policy optimization.InstructGPT[43]demonstrated how this approach could dramatically improve consistency with user intent across diverse tasks.Direct Preference Optimization (DPO) [111] has further simplified this process byreformulating it as direct preference learning without explicit reward modeling, maintaining alignment quality while reducing computational complexity.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "一些代理能力与其与人类偏好的一致性密切相关，基于一致性的学习方法修改模型以重塑代理基础表示的方面。从人类反馈中强化学习（RLHF）[110] 通过在比较判断上训练奖励模型并将其用于指导策略优化，将模型与人类价值观对齐。InstructGPT[43]展示了这种方法如何显著提高在各种任务中与用户意图的一致性。直接偏好优化（DPO）[111] 进一步简化了这一过程，将其重新构造为无需明确奖励建模的直接偏好学习，保持了一致性质量同时降低了计算复杂性。"
                },
                {
                  "type": "text",
                  "content": "Reinforcement learning (RL)presents a promising pathway for specialized learning in specific environments. RL has shown particular promise in enhancing reasoningcapabilities,essentially enabling the agent's underlying model to learn within the space of thought.Foundational works such as Reinforcement Fine-Tuning (ReFT)[44] enhance reasoning throughfine-tuning with automatically sampledreasoning paths under online reinforcement learning rewards. DeepSeek-R1[89]advances this approachthrough rule-based rewards and Group Relative Policy Optimization (GRPO) [112],while Kimik1.5[13]combinescontextualreinforcementlearning withoptimizedchain-of-thoughttechniques to improve both planning processes and inference eficiency. In specificenvironments, modifying models to enhance agentsunderstanding of actions and externalenvironments has proven effective,as demonstrated by DigiRL[103], which implements a two-stage reinforcement learning approach enabling agents to perform diverse commands on real-world Android device simulators.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "强化学习（RL）为特定环境中专业学习提供了一条有前途的途径。RL在增强推理能力方面表现出特别的潜力，基本上使代理的基础模型能够在思维空间内学习。基础作品如强化微调（ReFT）[44]通过在线强化学习奖励下的自动采样推理路径进行微调，增强了推理能力。DeepSeek-R1[89]通过基于规则的奖励和群体相对策略优化（GRPO）[112]推进了这一方法，而Kimik1.5[13]将上下文强化学习与优化的思维链技术相结合，以改善规划过程和推理效率。在特定环境中，修改模型以增强代理对行为和外部环境的理解已被证明是有效的，正如DigiRL[103]所展示的，该方法实现了一个两阶段强化学习方法，使代理能够在真实的Android设备模拟器上执行各种命令。"
                },
                {
                  "type": "text",
                  "content": "Recent works have attemptedto integrate agent action spaces directlyintomodeltraining[45,55],enabling learning of appropriate actions for diferent states through RL or SFT methods.This integration fundamentally affcts the agent's memory,reward understanding,and world modelcomprehension, pointing toward a promising direction forthe emergence of agentic models.\n\nPartial Mental State Learning While full mental state learning through model modifications provides comprehensive capability updates, learning focused on particular components of an agent's mental state $M$ represents another essential and oftenmoreeffcient approach.Such partial mental state learning can be achieved eitherthrough targeted model updates or through in-context adaptation without parameter changes.\n\nIn-Context Learning (ICL)ilustrates how agents can effectively modify specific mental state components without modifying theentire model.This mechanism alows agents toadapt tonew tasks byleveraging examples orinstructions within their context window, paraleling human working memory's role in rapid task adaptation. Chain-of-Thought (CoT)[46] demonstrates the efectivenessof this approach, showing how agents can enhance specific cognitive capabilities while maintaining their base model parameters unchanged.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "最近的研究尝试将代理的行动空间直接整合到模型训练中，通过强化学习（RL）或SFT方法学习不同状态下的适当行动。这种整合从根本上影响了代理的记忆、奖励理解和世界模型理解，指向了一个有前途的方向，即代理模型的出现。\n\n部分心智状态学习，虽然通过模型修改实现了全面的能力更新，但专注于代理心智状态 $M$ 的特定组件的学习代表了另一种重要且通常更高效的方法。这种部分心智状态学习可以通过有针对性的模型更新或无需参数更改的情境适应来实现。\n\n情境学习（ICL）说明了代理如何能够有效地修改特定心智状态组件，而无需修改整个模型。这种机制使代理能够通过利用上下文窗口内的示例或指令，适应新任务，类似于人类工作记忆在快速任务适应中的作用。思维链（CoT）展示了这种方法的有效性，展示了代理如何在保持基本模型参数不变的同时增强特定的认知能力。"
                },
                {
                  "type": "text",
                  "content": "The feasibilityofpartial mentalstate learning is evidenced throughvarious approaches targeting diffrent components such as memory $(M^{m e m})$ , reward $(M^{r e w})$ , and world model $(M^{w m})$ . Through normal communication and social interaction, Generative Agents [50] demonstrate how agents can accumulate andreplay memories,extracting highlevel insights to guide dynamic behavior planning. In environmentalinteraction scenarios, Voyager [47]showcases how agents cancontinuously update their skill library through direct engagement with the Minecraft environment, accumulating procedural knowledge without modelretraining.Learn-by-Interact[102]further extends this approach by synthesizing experientialdatathrough direct environmentalinteraction,eliminating the need for manual annotation or reinforcementlearning frameworks.Aditionally,agents can learn from their mistakes and improve throughreflection, as demonstratedbyReflexion[48],whichguides agents'futurethinkingandactions byobtainingtextualfeedback from repeated trial and error experiences.",
                  "index": 7,
                  "part": 0,
                  "translated_content": "部分心智状态学习的可行性通过针对不同组件（如记忆$M^{mem}$、奖励$M^{rew}$和世界模型$M^{wm}$）的各种方法得到证明。通过正常的交流和社交互动，生成式代理[50]展示了代理如何积累并重现记忆，提取高层次见解以指导动态行为规划。在环境交互场景中，Voyager[47]展示了代理如何通过与Minecraft环境的直接互动持续更新其技能库，积累程序化知识而无需重新训练模型。通过直接环境交互综合经验数据，Learn-by-Interact[102]进一步扩展了这种方法，消除了手动注释或强化学习框架的需求。此外，代理可以通过反思从错误中学习并改进，如Reflexion[48]所示，通过从反复试验中获得文本反馈来引导代理的未来思考和行动。"
                },
                {
                  "type": "text",
                  "content": "Modifications to reward and world models provide another example of partial mental state learning.ARMAP[106] refinesenvironmentalreward models bydistilling themfrom agent action trajectories,providing afoundation for further learning.AutoMC[114]constructs dense reward models through environmentalexploration tosupport agent behavior. Meanwhile,[107] explicitly leverages LLMs as world models to predict the impact of future actions,efectively modifying the agent's world understanding $(M^{w m})$ . ActRe[49] builds upon the language model's inherent world understanding to construct tasks from trajectories,enhancing the agent'scapabilities as botha world model and reasoning engine through iterative training.",
                  "index": 8,
                  "part": 0,
                  "translated_content": "对奖励和世界模型的修改提供了部分心智状态学习的另一个例子。ARMAP[106]通过从代理行动轨迹中提炼环境奖励模型，对其进行改进，为进一步学习奠定了基础。AutoMC[114]通过环境探索构建密集奖励模型，以支持代理行为。同时，[107]明确利用LLMs作为世界模型，预测未来行动的影响，有效地修改代理的世界理解$(M^{wm})$。ActRe[49]利用语言模型固有的世界理解，从轨迹中构建任务，通过迭代训练提升代理作为世界模型和推理引擎的能力。"
                }
              ],
              "raw_title": "Learning Space",
              "type": null,
              "children": [],
              "translated_title": "2.1.1 学习空间"
            },
            {
              "title": "2.1.2 Learning Objective",
              "number": "2.1.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The learning processof intelligent agents manifests acrossallaspects of their interaction withthe environment.At the input level, agents lean tobeterperceive and parseenvironmentalinformation; attheprocessing level,agents lean how to conduct effctive reasoning based on existing knowledge orreasoningcapabilities; atthe comprehensionlevel, agents form and optimize their understanding of the world through continuous interaction.This multi-level learning objective framework enables agents toevolvecontinuouslyacross different dimensions,allowingthem tobetter handle complex and dynamic task environments.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "智能代理的学习过程在其与环境的互动的各个方面都有体现。在输入层面，代理倾向于更好地感知和解析环境信息；在处理层面，代理学会如何基于现有知识或推理能力进行有效推理；在理解层面，代理通过持续互动形成和优化对世界的理解。这种多层次学习目标框架使代理能够在不同维度上持续演化，从而使它们能够更好地处理复杂和动态的任务环境。"
                },
                {
                  "type": "text",
                  "content": "Learning for Better Perception The abilityto effectively perceive and processinformation fromthe environment is fundamentalto agent intelligence.Toenhance perceptualcapabilities,agents employtwo primary learning approaches: expanding multimodal perception and leveraging retrieval mechanisms.\n\nMultimodal perception learning enables agents to process and integrate diverse sensory inputs,similar to human multi-sensory integration butunconstrained bybiologicallimitations.Thiscapabilityhas evolved significantlythrough advances like CLIP[51],which pioneered the alignmentof visualandlinguistic representations in sharedembedding spaces.Building on this foundation, models like LLaVA [52] enhanced visual perception by training specialized projectors on image-text pairs, while CogVLM[53] advanced visual reasoning through unified representational architectures.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "为了更好地感知 学习如何有效感知和处理来自环境的信息是智能代理的基础。为了增强感知能力，代理采用了两种主要的学习方法：扩展多模态感知和利用检索机制。\n\n多模态感知学习使代理能够处理和整合不同的感官输入，类似于人类的多感官整合，但不受生物限制。这种能力通过像CLIP[51]这样的进展显著发展，CLIP首创了在共享嵌入空间中对视觉和语言表示进行对齐。在此基础上，像LLaVA[52]这样的模型通过在图像-文本对上训练专门的投影仪来增强视觉感知，而CogVLM[53]通过统一的表征架构推进了视觉推理。"
                },
                {
                  "type": "text",
                  "content": "The expansion of perceptual modalities continues across multiple sensory domains. In audio processing, QwenAudio[54] demonstrates theunified encoding of diverse acoustic information,from speech to environmental sounds. Recent work by[l15] has even ventured into tactile perception, developing datasets that align touch, vision, and languagerepresentations.These advancesenable agents toengage more comprehensively with both physicaland digital environments.\n\nAgents alsolearn to enhance their observationalcapabilities through retrieval mechanisms.Unlike human perception, which is constrained by immediate sensory input, agents can learn to accessand integrate information from vast external knowledge repositories.Retrieval-augmented approaches like RAG[116] enhance perceptual understanding by connecting immediate observations with relevant stored knowledge.\n\nRecent work on retrieval-based agents demonstrates the potential for enhancing active information acquisition capabilities.Search-ol[117] guides reasoning models to learn active retrievalthrough prompting,thereby expanding their knowledge boundaries.Taking this further,R1-Searcher[45]and Search-R1[55]directly incorporate retrieval capabilities into the model,enabling autonomous informationretrieval during the reasoning process.These advances suggest a promising direction for improving agent perception:enhancing model-level active perception capabilities to enrich the foundation for decision-making. This approach may represent a significant avenue for future agent development.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "感知模态的扩展在多个感官领域持续进行。在音频处理方面，QwenAudio[54]展示了对各种声学信息的统一编码，从语音到环境声音。最近的工作[l15]甚至涉及触觉知觉，开发了将触觉、视觉和语言表示进行对齐的数据集。这些进展使代理能够更全面地参与物理和数字环境。\n\n代理还通过检索机制学习增强其观察能力。与受限于即时感官输入的人类感知不同，代理可以学习访问并整合来自庞大外部知识库的信息。类似于RAG[116]这样的检索增强方法通过将即时观察与相关存储知识连接起来，增强了感知理解能力。\n\n关于基于检索的代理的最新工作展示了增强主动信息获取能力的潜力。Search-ol[117]引导推理模型通过提示学习主动检索，从而扩展其知识边界。更进一步，R1-Searcher[45]和Search-R1[55]直接将检索能力纳入模型，使其能够在推理过程中进行自主信息检索。这些进展为改进代理感知提供了一个有前途的方向：增强模型级主动感知能力，丰富决策基础。这种方法可能代表了未来代理发展的一个重要途径。"
                },
                {
                  "type": "text",
                  "content": "Learning for Better Reasoning Reasoning serves as acritical bridge between an agent's mental state andits actions, making the abilityto reason effectively andthe development of reasoningcapabilitiesessentialfor intellgent agents. The foundation of reasoning in modern agents stems from two key elements: the rich world knowledge embedded in their underlying models, andtherobust logical frameworks supportedeither internallyorthroughcontext structuring. This makes learning for better reasoning a vital objective in agent development.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "学习以改进推理 推理作为代理的心智状态与行动之间的关键桥梁，有效推理能力和推理能力的发展对于智能代理至关重要。现代代理推理的基础源自两个关键要素：嵌入在其基础模型中的丰富世界知识，以及通过内部支持或上下文结构化支持的健壮逻辑框架。这使得学习以改进推理成为代理发展中的一个至关重要的目标。"
                },
                {
                  "type": "text",
                  "content": "The development ofreasoningcapabilities isdemonstratedthrough severalkeyphenomena.First,high-quality reasoning data directly enhances modelreasoning ability; second,suchhigh-qualitydata often requires verification orreward models foreffectivecuration; andthird,directreinforcementlearningonfoundation modelscanspontaneouslymanifest reasoning capabilities.\n\nThe importance of reasoning in agent developmenthas been re-emphasized following therelease of theol series.A common approach involves collecting and distilling data from open/closed-source reasoning models. For instance, SKY-32B [56] distilled data from QWQ-32B [118] to train a 32B reasoning model at a cost of $\\$450$ . Similarly, Open Thoughts[57]trainedBespoke-Stratos-32Batalowcost bydistiling and synthesizing datasets from R1.These studies demonstrate that even without complex algorithmic design,using reasoning data to perform Supervised Fine-Tuning (SFT) on base models can effectively activate reasoning capabilities.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "推理能力的发展通过几个关键现象来展示。首先，高质量的推理数据直接增强了模型的推理能力；其次，这种高质量数据通常需要验证或奖励模型以进行有效的整理；第三，对基础模型进行直接强化学习可以自发地表现出推理能力。随着ol系列的发布，推理在代理发展中的重要性再次被强调。一种常见的方法涉及收集和提炼来自开源/闭源推理模型的数据。例如，SKY-32B[56]从QWQ-32B[118]提炼数据以$\\$450$的成本训练了一个32B推理模型。类似地，Open Thoughts[57]通过提炼和合成来自R1的数据集，以较低成本训练了Bespoke-Stratos-32B。这些研究表明，即使没有复杂的算法设计，使用推理数据对基础模型进行监督微调(SFT)可以有效激活推理能力。"
                },
                {
                  "type": "text",
                  "content": "Another crucialinsight regarding data quality isthathighly structuredreasoningdata more effectively enables agents and language models tolearn reasoning proceses. Notably,LIMO[58]demonstrated that powerfulreasoning models could be built with extremely few data samples byconstructing long and efective reasoning chains for complex reasoning tasks.This insight stems fromtheirobservationthatlanguage models inherently possessufficient knowledge for reasoning but require high-quality reasoning paths to activate these capabilities.Supportingthis view, Liet al.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "关于数据质量的另一个关键见解是，高度结构化的推理数据更有效地使代理和语言模型学习推理过程。值得注意的是，LIMO[58]表明，通过为复杂推理任务构建长而有效的推理链，可以利用极少的数据样本构建强大的推理模型。这一见解源自他们的观察，即语言模型固有地具有进行推理所需的知识，但需要高质量的推理路径来激活这些能力。支持这一观点，Li等人。"
                },
                {
                  "type": "text",
                  "content": "[9]revealed thatboth Long CoTand Short CoTfundamentaly teach models to learn reasoning structures rather than specific content,suggesting that automated selection of high-qualityreasoning data may become an important future direction.\n\nOne viable exploration approach involves firstconducting extensive searches,and then using verifiable environments or trainablereward models to provide feedback on reasoning trajectories,thereby filtering out high-qualityreasoning data.This approachhas ledtoseveralfamiliesof techniques thatleverage different feedbackmechanisms toimprove reasoning capabilities.\n\nThe firstcategoryfollows thebootstrap paradigmexemplifiedbySTaR[59]anditsariantswhichimplement techniques where models generate step-by-step rationales and iteratively improve through fine-tuning on successful reasoning paths.This familyincludesQuiet-TaR[91],-STaR[120],andStar-Math[11],withthelaterspecificallenancing mathematicalreasoning through reinforcement learning principles.Byiteratively selecting correct reasoning paths for training, these methods achieve self-improvement through successive refinement cycles.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "[9]揭示了长CoT和短CoT两者基本上都是教导模型学习推理结构而非特定内容，这表明自动选择高质量推理数据可能成为未来重要的方向。\n\n一种可行的探索方法首先涉及进行广泛搜索，然后利用可验证的环境或可训练的奖励模型对推理轨迹提供反馈，从而过滤出高质量的推理数据。这种方法导致了几种利用不同反馈机制来提高推理能力的技术系列的出现。\n\n第一类技术遵循了由STaR[59]及其变体所示范的引导范式，这些技术使模型生成逐步的推理过程并通过对成功推理路径的微调来逐步改进。这个系列包括Quiet-TaR[91]，-STaR[120]和Star-Math[11]，后者通过强化学习原则专门增强了数学推理。通过迭代地选择正确的推理路径进行训练，这些方法通过连续的改进循环实现了自我改进。"
                },
                {
                  "type": "text",
                  "content": "The second category extends this paradigm by more explicitly incorporating reinforcement learning principles. The ReST family,beginning with the original ReST[60] introducing reinforced self-training,performs multiple attempts (typically 10) per sample and creates new training datasets from successful reasoning instances. ReST-EM[22] enhances the approach with expectation maximization, while ReST-MCTS[122]further integrates Monte Carlo Tree Search to enable improved reasoning capabilities through more sophisticated exploration strategies.",
                  "index": 7,
                  "part": 0,
                  "translated_content": "第二类技术通过更明确地融入强化学习原则来扩展这一范式。ReST家族以最初的ReST[60]引入了强化自我训练开始，对每个样本进行多次尝试（通常为10次），并从成功的推理实例中创建新的训练数据集。ReST-EM[22]通过期望最大化增强了这一方法，而ReST-MCTS[122]进一步整合了蒙特卡洛树搜索，通过更复杂的探索策略实现了改进的推理能力。"
                },
                {
                  "type": "text",
                  "content": "Several approaches have introduced Policy Reward Models (PRMs)to provide quality feedback on reasoning paths. Methods like OpenR [61] and LLaMA-Berry [62] model reasoning tasks as Markov Decision Processes (MDPs) and leverage tree search to explore diverse reasoning paths while using PRMs for quality assessment.In domain-specific applications, methods like rStar-Math [121] and DeepSeekMath [112]have demonstrated success in mathematical problem-solvingthroughmulti-round self-iterationandbalancedexploration-exploitationstrategies.Forcode generation, ol-Coder[13]leveragesMCTStogeneratecode withreasoning proceses,whileMarco-ol[123]extends this approach to open-ended tasks.These implementations highlight how the synergy between MCTS and PRM achieves effective reasoning path exploration while maintaining solution quality through fine-grained supervision.",
                  "index": 8,
                  "part": 0,
                  "translated_content": "一些方法引入了策略奖励模型（PRMs）来提供关于推理路径的质量反馈。像OpenR[61]和LLaMA-Berry[62]这样的方法将推理任务建模为马尔可夫决策过程（MDPs），利用树搜索探索各种推理路径，同时利用PRMs进行质量评估。在特定领域的应用中，像rStar-Math[121]和DeepSeekMath[112]这样的方法通过多轮自我迭代和平衡的探索-利用策略，在数学问题解决中取得了成功。对于代码生成，ol-Coder[13]利用MCTS生成带有推理过程的代码，而Marco-ol[123]将这种方法扩展到开放式任务。这些实现突显了MCTS和PRM之间的协同作用如何通过细粒度监督实现了有效的推理路径探索，同时保持解决方案质量。"
                },
                {
                  "type": "text",
                  "content": "Beyond data-driven approaches, reinforcement learning (RL) has demonstrated remarkable success in enhancing language modelsreasoning capabilities,as evidenced byrecent breakthroughs like DeepSeek R1[89]and Kimi-K-1.5 [113].The foundation of RL for LLMs can be traced to several pioneering frameworks: ReFT [44] introduced a combination of supervised fine-tuning and online reinforcement learning, while VeRL[124]established an opensourceframework supporting various RL algorithms for large-scale models up to 70B parameters.RFT[125] further demonstrated the effectiveness of reward-guided optimization in specific reasoning tasks.",
                  "index": 9,
                  "part": 0,
                  "translated_content": "除了数据驱动方法之外，强化学习（RL）在增强语言模型推理能力方面取得了显著成功，正如最近的突破所证实的，例如DeepSeek R1和Kimi-K-1.5。强化学习在LLM中的基础可以追溯到几个开创性框架：ReFT引入了监督微调和在线强化学习的组合，而VeRL建立了一个开源框架，支持各种RL算法用于规模达到70B参数的大型模型。RFT进一步证明了奖励引导优化在特定推理任务中的有效性。"
                },
                {
                  "type": "text",
                  "content": "Building upon these foundations,subsequent works have explored diverse applications and improvements.OpenR1[64] and RAGEN[63]extended RL techniques to enhance generalreasoning capabilities,while specializedimplementations like SWE-Gym[126]demonstrated success in software engineering tasks.Notably,DigiRL[103] introduced novel approaches for digital-world agent enhancement.\n\nRecent advances have further integrated RL with tool usage and reasoning. Qwen-QwQ-32B[118] employs reinforcement learning and a general reward mechanism to incorporate toolcalling intothe reasoning processenabling the seamlessuse ofarbitrary tools during reasoning and achieving agent-like capabilities directly within the model. Similarly,RAGEN[63]focuses on multi-step agentic scenarios,establishing a framework for agent reinforcement learning in complex environments.These developments suggest an increasing convergence between model training and agent development, potentiallyleading to more integrated andcapable intelligent systems.These implementations highlight how RL can effectively improve model performance while reducing dependence on large-scale annotated datasets, particularly in complex reasoning scenarios.",
                  "index": 10,
                  "part": 0,
                  "translated_content": "在这些基础上，后续的研究探索了各种应用和改进。OpenR1和RAGEN将强化学习技术扩展到增强通用推理能力，而像SWE-Gym这样的专门实现在软件工程任务中取得了成功。值得注意的是，DigiRL引入了数字世界代理增强的新方法。\n\n最近的进展进一步将强化学习与工具使用和推理相结合。Qwen-QwQ-32B利用强化学习和通用奖励机制将工具调用融入推理过程，实现在推理过程中无缝使用任意工具，并在模型内部直接实现类似代理的功能。类似地，RAGEN专注于多步骤代理场景，建立了一个在复杂环境中进行代理强化学习的框架。这些发展表明模型训练和代理开发之间的趋同性日益增强，可能导致更加综合和功能强大的智能系统。这些实现突显了强化学习如何有效提高模型性能，同时减少对大规模注释数据集的依赖，特别是在复杂推理场景中。"
                },
                {
                  "type": "text",
                  "content": "Learning for World Understanding Acriticalaspectof agent intellgence is the ability tounderstand how the world operates through direct interaction and experience accumulation.This understanding encompasss how the environment responds to different actions and theconsequences these actions bring.Through continuous interaction with their environment,agentscan build andrefine their memory,reward understanding,and world model,learning from both successes and failures to develop a more comprehensive grasp of their operational domain.",
                  "index": 11,
                  "part": 0,
                  "translated_content": "对于世界理解的学习\n智能体智能的一个关键方面是通过直接互动和经验积累理解世界运作的能力。这种理解包括环境如何对不同行为做出响应以及这些行为带来的后果。通过与环境的持续互动，智能体可以构建和完善它们的记忆、奖励理解和世界模型，从成功和失败中学习，以发展对其操作领域更全面的理解。"
                },
                {
                  "type": "text",
                  "content": "Recent research has revealeddiverse approaches to experientiallearning for world understanding.At the foundational level, Inner Monologue[65] demonstrates how agents can accumulate basic environmental knowledge through continuous interaction.Similarly,Learn-by-Interact [102]shows that meaningful understanding can emerge from direct environmental engagement without explicit reward mechanisms.More sophisticated approaches are exemplified by DESP[66] and Voyager[47] intheMinecraftenvironment, where agentsnot only gather experiences butalso actively process them: DESP through outcome analysis and Voyager through dynamic skill library expansion.",
                  "index": 12,
                  "part": 0,
                  "translated_content": "最近的研究揭示了各种各样的用于理解世界的经验学习方法。在基础层面，内部独白（Inner Monologue）展示了智能体如何通过持续互动积累基本的环境知识。类似地，“通过互动学习”（Learn-by-Interact）表明，有意义的理解可以在没有明确奖励机制的情况下从直接的环境参与中产生。更复杂的方法由DESP和Voyager在Minecraft环境中展示，代理不仅收集经验，还积极加以处理：DESP通过结果分析，Voyager通过动态技能库扩展。"
                },
                {
                  "type": "text",
                  "content": "The processing and utilization ofaccumulated experiences have beenfurther systematized through advancedframeworks. Generative Agents[50] introduces sophisticated memory replay mechanisms,enabling agents to extract high-level insights from past interactions.This systematic approach is enhanced by Self-refine[67] and Critic[68],which implement structured cycles of experience evaluation and refinement.\n\nThe optimization of reward understanding through environmentalinteraction has emerged as another crucial aspect of world understanding.Text2Reward [105] demonstrates how agents can continuously refine reward functions through human feedback, bettr aligning them with task objectives and environmental characteristics.Similarly, AutoManual[108]builds behavioral guidelines through sustained interaction,developing reward-verified protocols that provide a foundation for understanding environmentalrewards and decision-making.These interaction-based optimization mechanisms enable agents to bettrcomprehend environmental dynamics and generate more precise reward signals,ultimately enhancing their adaptability and decision-making capabilities in complex,dynamic environments.",
                  "index": 13,
                  "part": 0,
                  "translated_content": "通过先进的框架，对积累经验的处理和利用进一步系统化。《生成式智能体》（Generative Agents）引入了复杂的记忆重放机制，使智能体能够从过去的互动中提取高层次的见解。这种系统化方法得到了《自我完善》（Self-refine）和《评论家》（Critic）的增强，它们实施了结构化的经验评估和完善循环。\n\n通过环境互动优化奖励理解已经成为世界理解的另一个关键方面。《文本到奖励》（Text2Reward）展示了智能体如何通过人类反馈不断完善奖励函数，更好地使其与任务目标和环境特征保持一致。类似地，《自动手动》（AutoManual）通过持续互动建立行为指南，制定经过奖励验证的协议，为理解环境奖励和决策制定了基础。这些基于互动的优化机制使智能体能够更好地理解环境动态，并产生更精确的奖励信号，最终增强它们在复杂、动态环境中的适应性和决策能力。"
                },
                {
                  "type": "text",
                  "content": "Building on these foundations, RAP[74]represents asignificant advancement byconceptualizingreasoning as planning with a world model.By repurposing LLMs as both reasoning agents and world models,RAP enables agents tosimulate theoutcomes of potentialactions before committingtothem,facilitating moreeffctive planning through Monte Carlo Tree Search.This approach allows agents to strategically explorethereasoning space witha proper balance between exploration and exploitation.\n\nFurther innovations in leveraging world models for agent learning include ActRe[127],which reverses the typical reasoning-action sequence by first performing actions and then generating post-hoc explanations.This capability to rationalize actions demonstrates LLMs'inherent understanding of world dynamics,enabling autonomous trajectory annotation and facilitating contrastive self-training.\n\nThe importanceofcognitive maps inworld understandingis highlighted by[128],whoshow that structured mentalrepresentations inspired by human cognition significantly enhance LLMs'extrapolation capabilities in novelenvironments. These cognitive maps not only improve planning but also exhibit human-like characteristicssuchas structured mental simulation and rapid adaptation.\n\nIn web-based environments,recent work by[107]and[129]demonstrates thatLLMscan function asefective world models for anticipating the outcomes of web interactions. By simulating potential state changes before executing actions,these approaches enablesafer and more eficient decision-making,particularlyin environments where actions may be irreversible.\n\nThrough systems like Reflexion [48]and ExpeL[69],agents have advanced experientiallearning by autonomously managing thefullcycleofexperiencecoection, analysis,andapplication,enablingthem tolearneffectivelyfromboth successes and failures.\n\nThese developments collectively illustrate how world models are becoming increasingly central to agent learning systems, providing a foundation for understanding environmental dynamics and enabling more efective planning, reasoning, and decision-making in complex, interactive environments.",
                  "index": 14,
                  "part": 0,
                  "translated_content": "在这些基础上，RAP[74]作为一个重要进展，将推理概念化为具有世界模型的规划。通过将LLMs重新定位为推理代理和世界模型，RAP使智能体能够在承诺之前模拟潜在行动的结果，通过蒙特卡罗树搜索促进更有效的规划。这种方法使智能体能够在探索和利用之间找到适当的平衡，有策略地探索推理空间。\n\n利用世界模型进行代理学习的进一步创新包括ActRe[127]，它通过首先执行行动，然后生成事后解释，颠倒了典型的推理-行动顺序。这种理性化行动的能力展示了LLMs对世界动态的固有理解，实现了自主轨迹注释，并促进了对比自我训练。\n\n[128]强调了认知地图在理解世界中的重要性，他们表明，受人类认知启发的结构化心理表征显著增强了LLMs在新环境中的外推能力。这些认知地图不仅改善了规划，还表现出类似于人类的特征，如结构化心理模拟和快速适应。\n\n在基于网络的环境中，[107]和[129]的最新工作表明，LLMs可以作为有效的世界模型，用于预测网络交互的结果。通过在执行行动之前模拟潜在状态变化，这些方法使决策更加安全和高效，特别是在行动可能是不可逆的环境中。\n\n通过像Reflexion[48]和ExpeL[69]这样的系统，智能体通过自主管理完整的经验循环（包括经验收集、分析和应用），使它们能够有效地从成功和失败中学习。\n\n这些发展共同展示了世界模型如何越来越成为代理学习系统的核心，为理解环境动态提供基础，并在复杂的互动环境中实现更有效的规划、推理和决策。"
                }
              ],
              "raw_title": "Learning Objective",
              "type": null,
              "children": [],
              "translated_title": "2.1.2 学习目标"
            }
          ],
          "translated_title": "2.1 学习"
        },
        {
          "title": "2.2 Reasoning",
          "number": "2.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Reasoningrepresents thekey tointellgentbehavior,ransformingraw information intoactionable knowledgethat drives problem-solving and decision-making.For both humans and artificial agents,it enables logicalinference,hypothesis generation,and purposeful interaction with the world. In human cognition, reasoning emerges through multiple strategies:deductive reasoning applies generalrules to specificcases, inductive reasoning builds generalizations from particula instances,and abductive reasoningconstructs plausible explanations from incompletedata[130,131].These processes are augmented byheuristics—mental shortcuts that streamline decision-making under uncertainty-and are continuouslyrefinedthrough environmental feedback, ensuring that reasoning remains grounded inreality and adaptive to change.",
              "index": 0,
              "part": 0,
              "translated_content": "推理代表着智能行为的关键，将原始信息转化为可操作的知识，推动问题解决和决策制定。对于人类和人工代理来说，推理使逻辑推理、假设生成和有目的地与世界互动成为可能。在人类认知中，推理通过多种策略出现：演绎推理将一般规则应用于特定情况，归纳推理从具体实例中建立概括，而诱导推理从不完整数据中构建合理解释。这些过程通过启发式得到增强——这是在不确定性下简化决策的心理快捷方式，并通过环境反馈不断完善，确保推理保持现实基础并适应变化。"
            },
            {
              "type": "text",
              "content": "For LLM-based agents,reasoning serves a parallelrole,elevating them beyond reactive systems to proactive entities capable of sophisticated cognition. Through reasoning, these agents process multimodal inputs,integrate diverse knowledge sources,and formulatecoherentstrategies to achieve objectives.The environment playsa dual function: supplying informationthatfuelsreasoning andserving astheproving groundwherereasonedactions aretested,creating a feedback loop that enables agents to validate inferences and learn from errors.",
              "index": 1,
              "part": 0,
              "translated_content": "对于基于LLM的代理，推理发挥着并行作用，将它们提升至超越反应性系统的主动实体，能够进行复杂认知。通过推理，这些代理处理多模态输入，整合多样化知识源，并制定一致的策略以实现目标。环境发挥着双重功能：提供推动推理的信息，并作为验证推理行为的实践场所，创造一个反馈循环，使代理能够验证推断并从错误中学习。"
            },
            {
              "type": "text",
              "content": "In LLM-based agents,reasoning can be formally defined as the processof action selection based on mental states, representing acrucialbridgebetween perceptionandaction.More precisely,givenamentalstate Mt at timet,easoning can be formalized as a function $\\mathrm{R}(\\mathrm{Mt})\\rightarrow\\mathrm{at}$ ， where at represents the selected action. This process operates across various environments—extual,digital,andphysicalworlds—wherecompletingatask typicallyrequires either a single reasoning step or a composition of multiple reasoning actions.",
              "index": 2,
              "part": 0,
              "translated_content": "在基于LLM的代理中，推理可以被正式定义为基于心智状态的行动选择过程，代表了知觉和行动之间的关键桥梁。更准确地说，在时间t给定心智状态Mt，推理可以被形式化为一个函数$\\mathrm{R}(\\mathrm{Mt})\\rightarrow\\mathrm{at}$，其中at代表所选的行动。这一过程跨越各种环境——文本、数字和物理世界——在这些环境中完成任务通常要求进行单一推理步骤或多个推理行动的组合。"
            },
            {
              "type": "figure",
              "src": "images/f0c6ddab0ba65d0f4b90000c85c15384b215f712c1c72846af09d27c433031df.jpg",
              "alt": "",
              "caption": "Figure 2.2: Comparison of reasoning paradigms in LLM-based agents.",
              "index": 3,
              "part": 0,
              "translated_caption": "图2.2：基于LLM的代理中推理范式的比较。"
            },
            {
              "type": "text",
              "content": "The composition of reasoning actions naturallleads totwodistinct approaches:structured and unstructured reasoning Structured reasoning $(R_{s})$ can be formalized as an explicit composition $R_{s}=R_{1}\\circ R_{2}\\circ...\\circ R_{n}$ ，where each $R_{i}$ represents a discrete reasoning step with clear logical dependencies. In contrast, unstructured reasoning $(R_{u})$ takes a more holistic form $R_{u}=f(M_{t})$ , where the composition remains implicit and flexible, allowing for dynamic adaptation to context.This dual framework mirrors human cognition,where structured reasoning parallels our explicit logical deduction processes,while unstructured reasoning reflects our capacity for intuitive problem-solving and pattern recognition.",
              "index": 4,
              "part": 0,
              "translated_content": "推理行动的组成自然地导致了两种不同的方法：结构化推理（$R_{s}$）和非结构化推理（$R_{u}$）。结构化推理可以被形式化为显式的组合$R_{s}=R_{1}\\circ R_{2}\\circ...\\circ R_{n}$，其中每个$R_{i}$代表一个具有明确逻辑依赖关系的离散推理步骤。相反，非结构化推理采用更全面的形式$R_{u}=f(M_{t})$，其中组合保持隐式和灵活，允许动态适应上下文。这种双重框架反映了人类认知，其中结构化推理类似于我们的显式逻辑推理过程，而非结构化推理反映了我们在直觉问题解决和模式识别方面的能力。"
            },
            {
              "type": "text",
              "content": "The environment plays a crucial role in this formalization, serving both as a source of observations $o_{t}$ that influence mental state updates $(M_{t}=L(M_{t-1},a_{t-1},o_{t}))$ and as a testing ground for reasoning outcomes. This creates a continuous feedbackloopwhere reasoning not only drives action selection but also inffuences how the agent's mental state evolves, enabling iterative refinement of reasoning strategies through experience.\n\nIn this section, we willexamine how these reasoning approaches manifest in practice.We begin with structured reasoning,which emphasizes systematic problem decomposition and multi-step logical chains.We then explore unstructured reasoning, which allows for flexible response patterns and parall solution exploration. Finally, we investigate planning asa specializedform ofreasoning thatcombines both structuredand unstructured approaches for tackling complex, long-horizon tasks.",
              "index": 5,
              "part": 0,
              "translated_content": "环境在这种形式化过程中发挥着至关重要的作用，既作为影响心智状态更新的观察结果$o_{t}$的来源$(M_{t}=L(M_{t-1},a_{t-1},o_{t}))$，也作为推理结果的测试场所。这创造了一个连续的反馈循环，推理不仅推动行动选择，还影响代理的心智状态如何演变，通过经验实现推理策略的迭代改进。\n\n在本节中，我们将探讨这些推理方法在实践中的体现。我们首先从结构化推理开始，强调系统性问题分解和多步逻辑链。然后我们探讨非结构化推理，它允许灵活的响应模式和并行解决方案探索。最后，我们研究规划作为一种专门形式的推理，结合了结构化和非结构化方法，用于解决复杂的、长期的任务。"
            }
          ],
          "raw_title": "Reasoning",
          "type": null,
          "children": [
            {
              "title": "2.2.1 Structured Reasoning",
              "number": "2.2.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Structured reasoning represents a methodical approach to problem-solving that employs explicit organizational frameworks to guide thereasoning process.Unlike unstructured approaches,structured reasoning makes the compositionof reasoning steps explicit, which can be formalized as $R_{s}=R_{1}\\circ R_{2}\\circ...\\circ R_{n}$ ，where each $R_{i}$ represents a discrete reasoning step with clear logical dependencies.In this formulation,each reasoning node is an explicitly executed computational unit, and the connections between nodes represent definite information flow paths. This approach enables more systematic explorationof solution spaces and facilitates more robust decision-making through deliberate step-by-step analysis, providing high interpretability and traceability throughout the reasoning process.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "结构化推理代表了一种系统化的问题解决方法，采用明确的组织框架来引导推理过程。与非结构化方法不同，结构化推理使得推理步骤的组成变得明确，可以形式化表示为$R_{s}=R_{1}\\circ R_{2}\\circ...\\circ R_{n}$，其中每个$R_{i}$代表一个具有清晰逻辑依赖关系的离散推理步骤。在这种表述中，每个推理节点都是一个明确定义的计算单元，节点之间的连接代表着明确的信息流路径。这种方法使得更系统地探索解决空间成为可能，并通过深思熟虑的逐步分析促进更健壮的决策制定，提供了在整个推理过程中的高可解释性和可追溯性。"
                }
              ],
              "raw_title": "Structured Reasoning",
              "type": null,
              "children": [
                {
                  "title": "2.2.1.1 Dynamic Reasoning Structures",
                  "number": "2.2.1.1",
                  "level": 4,
                  "content": [
                    {
                      "type": "text",
                      "content": "Dynamic reasoning structures allow for the adaptive construction ofreasoning pathsduring problem-solving,creating versatile frameworks that can adjust based on intermediate results and insights.\n\nLinear SequentialReasoning Linear structures frame reasoning asa series of sequential steps,where eachstep builds on the one before.ReAct[70]illustrates thisbycombining reasoningtraceswithtask-specific actions inanalternating fashion.This combination allows for reasoning traces to guide and modify action plans while actions can access external sources for further information.This mutualinteraction improves both reasoning integrity and environmental adaptation.\n\nReasoning via Planning (RAP)[74]extends the linear reasoning paradigm by formulating LLM reasoning as a Markov decision proces,though it was limited by states specifically designed for particular problems.TheMarkov Chain of Thought (MCoT)[71]extended this paradigm byconceptualizing each reasoningstepas aMarkovianstate accompanied by executable code.This approach enables effcient next-step inference withoutrequiring alengthy contextwindow by compressing previous reasoning into a simplified mathquestion.Atom of Thoughts[132]explicitly defined problems as state representations and designed a general decomposition-contraction two-phase state transition mechanism to construct Markovian reasoning processes, transforming complex problems into a series of atomic questions.",
                      "index": 0,
                      "part": 0,
                      "translated_content": "动态推理结构允许在解决问题过程中自适应地构建推理路径，创造出可以根据中间结果和见解进行调整的多功能框架。\n\n线性顺序推理将推理框架构建为一系列顺序步骤，其中每一步都建立在前一步之上。ReAct通过将推理轨迹与特定任务的动作以交替方式结合来说明这一点。这种组合使得推理轨迹可以指导和修改行动计划，同时行动可以访问外部来源获取更多信息。这种相互作用提高了推理的完整性和环境适应性。\n\n通过规划进行推理（RAP）扩展了线性推理范式，将LLM推理构建为马尔可夫决策过程，尽管受限于专门设计用于特定问题的状态。思维马尔可夫链（MCoT）通过将每个推理步骤概念化为一个马尔可夫状态并附带可执行代码来扩展了这一范式。这种方法通过将先前推理压缩为简化的数学问题，实现了高效的下一步推理，而无需长篇上下文窗口。思维原子明确将问题定义为状态表示，并设计了通用的分解-收缩两阶段状态转换机制来构建马尔可夫推理过程，将复杂问题转化为一系列原子问题。"
                    },
                    {
                      "type": "text",
                      "content": "Tree-Based Exploration Tree-based approaches expand beyond linear structures by organizing reasoning into hierarchicalframeworks that support branching exploration.Tree of Thoughts(ToT)[72]introduces a structured approach where complexproblems are decomposed into intermediate steps,enabling breadth-first or depth-first search through the solution space.This alows the model to consider multiple reasoning paths simultaneously and systematically explore alternatives.\nLanguage Agent Tree Search (LATS)[73] advances this paradigm by integrating Monte Carlo TreeSearch (MCTS) with LLMs,using the environment as an external feedback mechanism.This approach enables more deliberate and adaptive problem-solving bybalancing exploration and exploitation through asophisticated search process guided by LLM-powered value functions and self-reflection.\nReasoning via Planning (RAP)[74] further enhances tree-based reasoning by repurposing LLMs as both reasoning agents and world models.Throughthisdualrole,RAP enables agents to simulate theoutcomes of potentialreasoning path before committing tothem,creating a principled planning framework thatbalances exploration with exploitation in the reasoning space.\nGraph-Based Reasoning Graph structures offer even greater flexibility by allowing non-hierarchical relationships between reasoning steps.Graph of Thoughts (GoT)[75] extends tree-based approaches to arbitrary graph structures, enabling more complex reasoning paterns thatcan capture interdependencies between diffrent steps.This approach allows forconnections between seemingly disparate reasoning branches,facilitating more nuanced exploration of the solution space.",
                      "index": 1,
                      "part": 0,
                      "translated_content": "基于树的探索方法通过将推理组织成支持分支探索的分层框架，扩展了超越线性结构的范式。思维树（ToT）引入了一种结构化方法，将复杂问题分解为中间步骤，实现对解决空间的广度优先或深度优先搜索。这使模型能够同时考虑多条推理路径，并系统地探索替代方案。\n\n语言代理树搜索（LATS）推进了这一范式，通过将蒙特卡洛树搜索（MCTS）与LLMs集成，利用环境作为外部反馈机制。这种方法通过LLM驱动的价值函数和自我反思，通过一个复杂的搜索过程，在探索和开发之间取得更加深思熟虑和适应性的问题解决平衡。\n\n通过规划进行推理（RAP）通过将LLMs重新用作推理代理和世界模型进一步增强了基于树的推理。通过这种双重角色，RAP使代理能够在承诺之前模拟潜在推理路径的结果，创建一个在推理空间中平衡探索和开发的原则性规划框架。\n\n基于图的推理图结构提供了更大的灵活性，允许推理步骤之间存在非层次关系。思维图（GoT）将基于树的方法扩展到任意图结构，实现能够捕捉不同步骤之间相互依赖关系的更复杂推理模式。这种方法允许看似不相关的推理分支之间建立连接，促进解决空间的更加微妙的探索。"
                    },
                    {
                      "type": "text",
                      "content": "Pathof Thoughts (PoT)[76]addresses relation reasoning challenges by decomposing problems into three key stages: graph extraction, path identification, and reasoning.Byexplicitlyextracting atask-agnostic graphthat identifies entities,relationsand attributes withinthe problemcontext,PoTcreatesastructured representation thatfacilitates the identification ofrelevant reasoning chains,significantly improving performance on tasksrequiring longreasoning chains.\nDiagram of Thought (DoT)[77] models iterative reasoning as the construction of a directed acyclic graph (DAG), organizing propositions,critiques,refinements, and verifications intoaunified structure.This approach preserves logical consistency while enabling theexploration of complexreasoning pathways, providing atheoretically sound framework grounded in Topos Theory.",
                      "index": 1,
                      "part": 1,
                      "translated_content": "思维路径（PoT）[76]通过将问题分解为三个关键阶段：图提取、路径识别和推理，来解决关系推理挑战。通过明确提取任务无关的图，识别问题背景中的实体、关系和属性，PoT创建了一个结构化表示，有助于识别相关推理链，显著提高了需要长推理链的任务的性能。\n\n思维图（DoT）[77]将迭代推理建模为有向无环图（DAG）的构建，将命题、批评、改进和验证组织成统一结构。这种方法保持逻辑一致性，同时能够探索复杂的推理路径，提供了一个基于拓扑理论的理论上合理的框架。"
                    }
                  ],
                  "raw_title": "Dynamic Reasoning Structures",
                  "type": null,
                  "children": [],
                  "translated_title": "2.2.1.1 动态推理结构"
                },
                {
                  "title": "2.2.1.2 Static Reasoning Structures",
                  "number": "2.2.1.2",
                  "level": 4,
                  "content": [
                    {
                      "type": "text",
                      "content": "Static reasoning structuresemploy fixedframeworks that guide thereasoning process without dynamicall adjusting the structure itself, focusing instead on improving the content within the established framework.\nEnsemble Methods. Ensemble approaches leverage multiple independent reasoning atempts to improve overall performance through aggregation.Self-Consistency[78] pioneered this approach by sampling multiple reasoning paths rather than relying on single greedy decoding,significantly improving performance through majority votingamong the generated solutions.\nMedPrompt[133] demonstrates how domain-specific ensemble techniques can enhance performance by carefully crafting prompts that elicit diverse reasoning approaches,achieving state-of-the-art results on medical benchmarks through systematic composition of prompting strategies.\nLLM-Blender[134] introduces a sophisticated ensembling framework that leverages thediverse strengths of multiple LLMs through pairwise comparison (PairRanker)andfusion (GenFuser)of candidate outputs.This approach enables the system to select theoptimal modeloutputforeach specific example,creating responsesthatexceed thecapabilities of any individual model.\nProgressive Improvement. Progressive improvement frameworks focus on iteratively refining reasoning through structured feedback loops.Self-Refine[67]implements an iterative approach where the model generates initial output, provides self-feedback,anduses thatfeedback torefineitself.This mimics humanrevision processes without requiring additional training or reinforcement learning, resulting in significant improvements across diverse tasks.",
                      "index": 0,
                      "part": 0,
                      "translated_content": "静态推理结构采用固定的框架来指导推理过程，而不会动态调整结构本身，而是专注于改进已建立框架内的内容。\n\n集成方法。集成方法利用多个独立的推理尝试来通过聚合提高整体性能。Self-Consistency[78]开创了这一方法，通过采样多个推理路径而不是依赖于单一贪婪解码，通过在生成的解决方案中采用多数投票显著提高性能。\n\nMedPrompt[133]展示了领域特定的集成技术如何通过精心设计引发多样化推理方法的提示来提高性能，通过系统构成提示策略在医学基准测试中取得了最先进的结果。\n\nLLM-Blender[134]引入了一个复杂的集成框架，通过成对比较（PairRanker）和融合（GenFuser）候选输出，利用多个LLM的各种优势。这种方法使系统能够为每个特定示例选择最佳模型输出，创造出超越任何单个模型能力的响应。\n\n渐进改进。渐进改进框架专注于通过结构化的反馈循环逐步完善推理。Self-Refine[67]实现了一种迭代方法，模型生成初始输出，提供自我反馈，并利用该反馈来自我完善。这模仿了人类的修订过程，而无需额外的训练或强化学习，从而在各种任务中取得显著的改进。"
                    },
                    {
                      "type": "text",
                      "content": "Reflexion[48]extends this concept by integrating environmental feedback,enabling agents toverballyreflect on task feedback signals and maintainreflective textinan episodic memory buffr.This approach guidesfuture decision-making by incorporating insights from previous attempts,significantly enhancing performance in sequential decision-making, coding, and reasoning tasks.\nProgressive-Hint Prompting (PHP)[79]further develops this paradigm by using previously generated answers as hints to progressvely guide the model toward correct solutions. This approach enables automatic multiple interactions between users and LLMs,resulting in significant accuracy improvements while maintaining high efficiency.\nError Correction. Error correction frameworks focus specifically on identifying and addressing mistakes in the reasoning process.Self-Verification [80] introduces a self-critique system that enables models to backward-verify their conclusions bytaking thederivedanswer asaconditionforsolving theoriginalproblem,producing interpretable validation scores that guide answer selection.",
                      "index": 0,
                      "part": 1,
                      "translated_content": "Reflexion[48]通过整合环境反馈扩展了这一概念，使代理能够口头反思任务反馈信号，并将反思性文本保留在一个情节性记忆缓冲区中。这种方法通过吸收以前尝试的见解来指导未来的决策，显著增强了在顺序决策、编码和推理任务中的表现。\n\n渐进提示引导（PHP）[79]进一步发展了这一范式，利用先前生成的答案作为提示逐步引导模型朝向正确解决方案。这种方法使用户和LLMs之间能够进行自动多次交互，显著提高准确性同时保持高效性。\n\n错误校正。错误校正框架专注于识别和解决推理过程中的错误。Self-Verification[80]引入了一个自我批判系统，使模型能够通过将得出的答案作为解决原始问题的条件来向后验证其结论，产生可解释的验证分数，指导答案选择。"
                    },
                    {
                      "type": "text",
                      "content": "Refiner[135]addresses the challenge of scatteredkey information by adaptivelyextracting query-relevantcontent and restructuring it based on interconnectedness,highlighting information distinction andeffctivelyaligning downstream LLMs with the original context.\nChain-of-Verification(CoVe)[81]tacklesfactual hallucinations througha structured process where the model drafts an initialresponse,plansverifcationquestions,independentlyanswers those questions,andgeneratesafinal verified response. This deliberate verification process significantly reduces hallucinations across a variety of tasks.\nRecursive Criticism and Improvement (RCI)[128]enables LLMs to execute computer tasks byrecursivelycriticizing and improving their outputs, outperforming existing methods on the MiniWoB $^{++}$ benchmark with only a handful of demonstrations per task and without task-specific reward functions.\nCritic [68]extends thisapproachbyintegratingexternal tools forvalidation,enablingLLMs toevaluate and progressively amend their outputs likehuman interactionwith tools.This fameworkalows initiall“black box\"models toengage in a continuous cycle of evaluation and refinement, consistently enhancing performance across diverse tasks.",
                      "index": 0,
                      "part": 2,
                      "translated_content": "Refiner[135]通过自适应提取与查询相关内容并基于相互关联性重组这些内容来解决分散关键信息的挑战，突出信息区分并有效地将下游LLMs与原始上下文对齐。\n\nChain-of-Verification(CoVe)[81]通过一个结构化过程来解决事实幻觉问题，模型起草初始响应，规划验证问题，独立回答这些问题，并生成最终经过验证的响应。这种刻意的验证过程显著降低了在各种任务中出现的幻觉。\n\nRecursive Criticism and Improvement (RCI)[128]使LLMs能够通过递归批评和改进其输出来执行计算机任务，在MiniWoB++基准测试中表现优异，每个任务只需少量演示，而无需特定于任务的奖励函数。\n\nCritic [68]通过集成外部工具进行验证扩展了这种方法，使LLMs能够评估和逐步修改其输出，就像人类与工具进行交互一样。这个框架允许最初的“黑盒”模型参与持续的评估和改进循环，持续增强在各种任务中的性能。"
                    }
                  ],
                  "raw_title": "Static Reasoning Structures",
                  "type": null,
                  "children": [],
                  "translated_title": "2.2.1.2 静态推理结构"
                },
                {
                  "title": "2.2.1.3 Domain-Specific Reasoning Frameworks",
                  "number": "2.2.1.3",
                  "level": 4,
                  "content": [
                    {
                      "type": "text",
                      "content": "Domain-specificreasoning frameworks adapt structured reasoning approaches to the unique requirements of particular domains, leveraging specialized knowledge and techniques to enhance performance in specific contexts.\n\nMathPrompter[82]addresses arithmetic reasoning challenges by generating multiple algebraic expressions or Python functions tosolvethe same math problem indiffrent ways.Thisapproach improvesconfidence inthe outputresults by providing multiple verification paths,significantly outperforming state-of-the-art methods on arithmetic benchmarks.\n\nPhysicsReasoner[84]addresses the unique challenges ofphysics problems through aknowledge-augmentedframework that constructs acomprehensive formula set and employs detailedchecklists to guide efective knowledge application. This three-stage approach—problem analysis,formularetrieval, and guided reasoning—significantly improves performance on physics benchmarks by mitigating issues of insuffcient knowledge and incorrect application.\n\nPedagogical Chain-of-Thought (PedCoT)[83]leverages educationaltheory,particularlytheBloomCognitive Model, to guide the identification of reasoning mistakes in mathematical contexts.Thisapproachcombines pedagogical principles for prompt design with atwo-stage interaction process, providing a foundation for reliable mathematical mistake identification and automatic answer grading.\n\nThe evolution of structured reasoning in LLM agents reflects a growing understanding ofhow to enhance reasoning capabilities through explicit organizational frameworks.From linear sequences to complex graphs,and ensemble methods to specialized domain frameworks,these approaches demonstrate the powerof structuralguidance inimproving reasoning performance across diverse tasks and domains.",
                      "index": 0,
                      "part": 0,
                      "translated_content": "领域特定推理框架将结构化推理方法调整到特定领域的独特需求，利用专业知识和技术来增强特定情境下的性能。\n\nMathPrompter[82]通过生成多个代数表达式或Python函数来解决相同数学问题的不同方式，解决了算术推理挑战。该方法通过提供多条验证路径，显著优于现有方法，增强了结果的可信度，在算术基准测试中表现出色。\n\nPhysicsReasoner[84]通过知识增强框架解决物理问题的独特挑战，构建了一个全面的公式集，并采用详细的检查清单来指导有效的知识应用。这种三阶段方法——问题分析、公式检索和引导推理——通过减轻知识不足和错误应用问题，在物理基准测试中显著提高了性能。\n\n教学思维链（PedCoT）[83]利用教育理论，特别是布鲁姆认知模型，指导在数学背景下识别推理错误。该方法将提示设计的教学原则与两阶段交互过程相结合，为可靠的数学错误识别和自动答案评分奠定基础。\n\nLLM代理中结构化推理的演进反映了如何通过明确的组织框架增强推理能力的日益增长的理解。从线性序列到复杂图表，从集成方法到专门的领域框架，这些方法展示了结构指导在改善跨不同任务和领域的推理性能中的强大作用。"
                    }
                  ],
                  "raw_title": "Domain-Specific Reasoning Frameworks",
                  "type": null,
                  "children": [],
                  "translated_title": "2.2.1.3 领域特定推理框架"
                }
              ],
              "translated_title": "2.2.1 结构化推理"
            },
            {
              "title": "2.2.2 Unstructured Reasoning",
              "number": "2.2.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In contrast to structured reasoning approaches that explicitly organize reasoning steps, unstructured reasoning $(R_{u})$ takes a holistic form ${\\cal R}_{u}=f(M_{t})$ , where the composition remains implicit and flexible. In this mode, the reasoning process is encapsulated withinasinglefunction mapping,without explicitlydefining intermediate stepsorstatetransitions.This approach leverages the inherent capabilties of language models to generatecoherent reasoning without enforcing rigid structural constraints, with intermediate reasoning processes occurring explicitlyin the language space or implicitly in the latent space.Unstructured reasoning methods havedemonstratedremarkable effectivenessacross diverse tasks while maintaining simplicity and effciency in implementation.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "与明确组织推理步骤的结构化推理方法相比，非结构化推理（$R_{u}$）采用了整体形式${\\cal R}_{u}=f(M_{t})$，其中组成部分保持隐式和灵活。在这种模式下，推理过程封装在单个函数映射中，没有明确定义中间步骤或状态转换。这种方法利用语言模型的固有能力生成连贯的推理，而不强加严格的结构约束，其中中间推理过程明确发生在语言空间中或隐式发生在潜在空间中。非结构化推理方法在各种任务中展现出显著的有效性，同时在实施上保持简单和高效。"
                }
              ],
              "raw_title": "Unstructured Reasoning",
              "type": null,
              "children": [
                {
                  "title": "2.2.2.1 Prompting-Based Reasoning",
                  "number": "2.2.2.1",
                  "level": 4,
                  "content": [
                    {
                      "type": "text",
                      "content": "The most accessible way toelicit reasoning in LMagents lies incarefullycrafted prompts.By providing appropriate reasoning demonstrations or instructing LLMs to perform inferential steps,agents canleverage theirlogical deduction capabilities to solve problems through flexible reasoning processes.\n\nChain-of-Thought Variants.The cornerstone of prompting-based reasoning is Chain-of-Thought (CoT)prompting[46],whichoperationalizesreasoning throughfew-shotexamples with explicit generationof intermediaterationalization steps.Thisfoundational technique has inspired severalevolutionary variants that enhanceits basic approach. Zero-shot CoT[136] eliminates the need for demonstration examples through strategic prompting(e.g.,“Let's think step by step\"),making the approach more accessble while maintaining effectivenessAuto-CoT[137]automates the creation of effectivedemonstrations byclustering diverse questions and generating reasoning chains forrepresentative examples from each cluster. Least-to-Most Prompting [138]addresses complex reasoning by decomposing problems into sequential sub-problems,enabling a progressive planning processthat facilitates easy-to-hard generalization. Complex CoT[139]further enhances reasoning depth by specifically selecting high-complexityexemplars as prompting templates, better-equipping models to tackle intricate problems.",
                      "index": 0,
                      "part": 0,
                      "translated_content": "在LM代理中引发推理的最直接方式是精心设计的提示。通过提供适当的推理演示或指导LLM执行推理步骤，代理可以利用其逻辑推断能力通过灵活的推理过程解决问题。\n\n思维链变体。基于提示的推理的基石是思维链（CoT）提示，通过明确生成中间理性化步骤的少样本示例来实现推理。这一基础技术已经激发了几种进化变体，增强了其基本方法。零样本CoT通过战略提示（例如，“让我们一步一步地思考”）消除了演示示例的需求，使方法更易接触同时保持有效性。Auto-CoT自动化创建有效演示，通过对不同问题进行聚类并为每个聚类的代表性示例生成推理链。从最少到最多提示通过将问题分解为顺序子问题来解决复杂推理，实现了逐步规划过程，促进了易到难的泛化。复杂CoT通过专门选择高复杂度的示例作为提示模板进一步增强了推理深度，更好地装备模型来解决复杂问题。"
                    },
                    {
                      "type": "text",
                      "content": "Problem Reformulation Strategies.Advanced prompting strategies demonstrate architectural innovations in reasoning guidance by reformulating the original problem. Step-Back Prompting [85] implements abstraction-first reasoning throughconceptual elevation,enabling models to derive high-level concepts and first principles before addressing specific details.Experimental results demonstrate substantial performance gains on various reasoning-intensive tasks, with improvements of $7.27\\%$ across physics, chemistry, and multi-hop reasoning benchmarks. Rephrase and Respond[140]employ semantic expansion to transform original questions into more tractable forms, allowing models to approach problems from multiple linguistic angles and identify the most efective problem formulation.",
                      "index": 1,
                      "part": 0,
                      "translated_content": "问题重构策略。先进的提示策略通过重新构造原始问题展示了推理指导中的架构创新。Step-Back Prompting [85] 通过概念提升实现了抽象优先的推理，使模型能够在处理具体细节之前得出高层概念和首要原则。实验结果表明，在各种需要推理的任务中，该方法取得了显著的性能提升，物理、化学和多跳推理基准测试的提升达到了$7.27\\%$。Rephrase and Respond[140] 利用语义扩展将原始问题转化为更易处理的形式，使模型能够从多个语言角度解决问题并确定最有效的问题表述。"
                    },
                    {
                      "type": "text",
                      "content": "Abstraction-of-Thought[141] introduces a novel structuredreasoning format that explicitlyrequires varying levels of abstraction within thereasoning process.This approachelicitslanguage models tofirstcontemplate attheabstractlevel before incorporating concrete details,aconsideration overlooked by step-by-step CoT methods.By aligning models with the AoTformat through finetuning on high-quality samples,the approach demonstrates substantial performance improvements across a wide range of reasoning tasks compared to CoT-aligned models.",
                      "index": 2,
                      "part": 0,
                      "translated_content": "思维抽象化[141]引入了一种新颖的结构化推理格式，明确要求推理过程中采用不同层次的抽象化。这种方法促使语言模型在吸收具体细节之前首先在抽象层面上思考，这是逐步CoT方法所忽视的考虑。通过在高质量样本上微调，将模型与AoT格式对齐，该方法在各种推理任务中表现出明显的性能提升，相较于与CoT对齐的模型。"
                    },
                    {
                      "type": "text",
                      "content": "Enhanced Prompting Frameworks. Several frameworks extend the basic prompting paradigm to create more sophisticated reasoning environments. Ask Me Anything[86]constrains open-ended generation by reformulating tasks into structured question-answersequences,enforcing focusedreasoning trajectories.This approachrecursively usesthe LLM itself totransformtask inputs to the effectiveQAformat,enabling open-source GPT-J-6B to matchorexceedthe performance of few-shot GPT3-175B on 15 of 20 popular benchmarks.",
                      "index": 3,
                      "part": 0,
                      "translated_content": "增强提示框架。一些框架扩展了基本提示范式，以创建更复杂的推理环境。\"问我任何事\"[86]通过将任务重新构建为结构化的问答序列，强制执行专注的推理轨迹，限制了开放式生成。这种方法通过递归地使用LLM本身将任务输入转换为有效的问答格式，使得开源的GPT-J-6B能够在20个常见基准测试中的15个上达到或超过少样本的GPT3-175B的性能水平。"
                    },
                    {
                      "type": "text",
                      "content": "Algorithm of Thoughts[142] proposes a novel strategy that propels LLMsthrough algorithmic reasoning pathways by employing algorithmic examples full in context.This approach exploits the innate recurrence dynamics of LLMs, expanding theiridea exploration with merely one or a few queries.The technique outperforms earlier single-query methods and even morerecentmulti-query strategies whileusing significantlyfewer tokens,suggesting that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself.",
                      "index": 4,
                      "part": 0,
                      "translated_content": "《思维算法》提出了一种新颖的策略，通过利用完整上下文中的算法示例，推动LLM（大型语言模型）通过算法推理路径。这种方法利用LLM的固有循环动力学，仅通过一个或几个查询扩展其思想探索。该技术胜过先前的单查询方法，甚至胜过最近的多查询策略，同时使用的记号数量显著较少，表明利用算法指导LLM可以实现超越算法本身性能的效果。"
                    },
                    {
                      "type": "text",
                      "content": "Chain-of-Knowledge (CoK)[87] augments LLMs by dynamically incorporating grounded information from heterogeneous sources,resulting in more factualrationales andreduced hallucination.CoKconsists of threestages:reasoning preparation, dynamic knowledge adapting,and answer consolidation, leveraging both unstructured and structured knowledge sources through an adaptive query generator.This approachcorrects rationales progressively using preceding corrected rationales, minimizing error propagation between reasoning steps.",
                      "index": 5,
                      "part": 0,
                      "translated_content": "知识链（CoK）[87]通过动态地整合来自异构来源的基础信息，增强了LLM，从而产生了更多的事实依据和减少的虚构。CoK包括三个阶段：推理准备、动态知识调整和答案整合，通过自适应查询生成器利用非结构化和结构化知识源。该方法逐步使用先前校正的依据来纠正依据，从而最小化推理步骤之间的错误传播。"
                    },
                    {
                      "type": "text",
                      "content": "Self-Explained Keywords (SEK)[88]addresses thechallengeof low-frequency terms incode generationby extracting and explaining key terms in problem descriptions with the LLM itself and ranking them based on frequency. This approach significantly improves code generation performance across multiple benchmarks,enabling models to shift attention from low-frequency keywords to their corresponding high-frequency counterparts.",
                      "index": 6,
                      "part": 0,
                      "translated_content": "自解释关键词（SEK）[88]通过在问题描述中提取和解释关键术语，并根据其频率对其进行排序，解决了代码生成中低频术语的挑战。该方法显著提高了跨多个基准测试的代码生成性能，使模型能够将注意力从低频关键词转移到其对应的高频关键词上。"
                    }
                  ],
                  "raw_title": "Prompting-Based Reasoning",
                  "type": null,
                  "children": [],
                  "translated_title": "2.2.2.1 基于提示的推理"
                },
                {
                  "title": "2.2.2.2 Reasoning Models",
                  "number": "2.2.2.2",
                  "level": 4,
                  "content": [
                    {
                      "type": "text",
                      "content": "Recent advances inlanguage models have ledto the developmentof specialized reasoning models designed explicitly for complex inferential tasks.These models are fine-tuned or specialy trained to optimize reasoning capabilities, incorporating architecturalandtraining innovations that enhancetheir performance ontasks requiring multi-steplogical inference.\n\nReasoning models like DeepSeek's R1 [89], Anthropic's Claude 3.7 Sonnet[9],and OpenAl's o series models [90] represent the frontier of reasoning capabilities,demonstratingremarkable proficiency across diverse reasoning benchmarks.These models aretrained with specializedmethodologies that emphasizereasoning patterns,often incorporating significant amounts of human feedback and reinforcement learning to enhance their inferential abilities.\n\nThe emergence of dedicated reasoning models reflects a growing understanding of the importance of reasoning capabilities in language models andthe potential benefits of specialized training forthese tasks. Byconcentrating on reasoning-focused training data and objectives,these models achieve performance levels that significantly exceed those of general-purpose language models,particularlyontasks thatrequirecomplex logicalinference, mathematical reasoning, and multi-step problem-solving.",
                      "index": 0,
                      "part": 0,
                      "translated_content": "最近在语言模型领域的进展推动了专门针对复杂推理任务设计的专用推理模型的发展。这些模型经过微调或专门训练以优化推理能力，融合了架构和训练创新，提升它们在需要多步逻辑推理的任务中的性能。\n\nDeepSeek的R1 [89]、Anthropic的Claude 3.7 Sonnet [9]和OpenAI的o系列模型 [90]等推理模型代表了推理能力的前沿，展示了在各种推理基准测试中出色的熟练度。这些模型经过专门的方法训练，强调推理模式，通常融入大量人类反馈和强化学习以增强其推理能力。\n\n专用推理模型的出现反映了对语言模型中推理能力重要性的日益认识，以及专门训练这些任务的潜在好处。通过专注于以推理为中心的训练数据和目标，这些模型在需要复杂逻辑推理、数学推理和多步问题解决的任务上实现了显著超越通用语言模型的性能水平。"
                    }
                  ],
                  "raw_title": "Reasoning Models",
                  "type": null,
                  "children": [],
                  "translated_title": "2.2.2.2 推理模型"
                },
                {
                  "title": "2.2.2.3 Implicit Reasoning",
                  "number": "2.2.2.3",
                  "level": 4,
                  "content": [
                    {
                      "type": "text",
                      "content": "Beyond explicit reasoning approaches,recent research has exploredthe potential of implicit reasoning method that operate without overtly exposing thereasoning process.These approaches aim to improve effciency by reducing the number of tokens generated while maintaining or enhancing reasoning performance.\n\nQuiet-STaR [91] generalizes the Self-Taught Reasoner approach by teaching LMs to generate rationales at each token to explain thefuture text improving their predictions.This approach addresses key chalenges including computationalcost,theinitialunfamiliaritywith generating internalthoughtsandthe need to predict beyond individual tokens. Experimental results demonstrate zero-shot improvements in mathematical reasoning $(5.9\\%\\rightarrow10.9\\%)$ and commonsense reasoning $(36.3\\%\\rightarrow47.2\\%)$ ） after continued pretraining, marking a step toward LMs that learn to reason in a more general and scalable way.",
                      "index": 0,
                      "part": 0,
                      "translated_content": "除了显式推理方法之外，最近的研究探索了隐式推理方法的潜力，这些方法在不公开推理过程的情况下运行。这些方法旨在通过减少生成的标记数量来提高效率，同时保持或增强推理性能。\n\nQuiet-STaR [91] 泛化了自学习推理器方法，通过教导LLMs在每个标记处生成理由来解释未来文本，从而改善它们的预测。这种方法解决了包括计算成本、生成内部思想的初始陌生感以及需要预测超出单个标记之外的关键挑战。实验结果表明，在持续预训练后，在数学推理（从5.9%提高到10.9%）和常识推理（从36.3%提高到47.2%）方面实现了零-shot改进，标志着LLMs朝着以更通用和可扩展的方式学会推理迈出了一步。"
                    },
                    {
                      "type": "text",
                      "content": "Chain of Continuous Thought (Coconut)[92] introduces a paradigm that enables LLM reasoning in an unrestricted latent space instead of using naturallanguage.Byutilizing the last hidden state of the LLM as arepresentation of the reasoning state and feeding it back as the subsequent input embedding directly inthe continuous space,Coconut demonstrates improved performance on reasoning tasks with fewer thinking tokens during inference.This approach leads to emergent advancedreasoning pattens,icludingtheabilitytoencodemultiplealternative nextreasoning steps, allowing the model to perform a breadth-first search rather than commiting to a single deterministic path.",
                      "index": 1,
                      "part": 0,
                      "translated_content": "连续思维链（Coconut）[92] 提出了一种范式，使LLM能够在无限制的潜在空间中进行推理，而不是使用自然语言。通过利用LLM的最后隐藏状态作为推理状态的表示，并将其直接作为连续空间中的后续输入嵌入反馈，Coconut 在推理任务中表现出更好的性能，且在推理过程中需要更少的思考标记。这种方法导致了新兴的高级推理模式，包括编码多个备选的下一推理步骤的能力，使模型能够进行广度优先搜索，而不是致力于单一确定性路径。"
                    },
                    {
                      "type": "text",
                      "content": "Recent analysis[143] of implicit reasoning in transformers reveals important insights into its limitations.While language modelscan perform step-by-stepreasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning when trained onfixed-pattern data, implicit reasoning abilities emerging from training on unfixed-patterndatatendtooverfit specificpatterns andfailto generalizefurther.These findings suggest thatlanguage models acquire implicitreasoning through shortcutlearning,enabling strong performance ontaskswith similar patterns while lacking broader generalization capabilities.",
                      "index": 2,
                      "part": 0,
                      "translated_content": "最近对变压器中的隐式推理进行的分析揭示了其局限性的重要见解。虽然语言模型可以通过隐式推理进行逐步推理，并在固定模式数据训练时在领域内和领域外测试中实现高准确性，但从在非固定模式数据上训练中出现的隐式推理能力往往会过度拟合特定模式，并且无法进一步泛化。这些发现表明，语言模型通过快捷学习获得了隐式推理能力，在类似模式的任务上表现出色，但缺乏更广泛的泛化能力。"
                    },
                    {
                      "type": "text",
                      "content": "The evolution of unstructured reasoning approaches demonstrates the remarkable adaptability of language models to different reasoning paradigms.From simple prompting techniques to sophisticated implicit reasoning methods, these approaches leverage the inherent capabilities of LLMs to perform complex logicalinferences without requiring explicit structuralconstraints.Thisfexibilityenables more intuitive problem-solving while maintaining efficiency and effectiveness across diverse reasoning tasks.",
                      "index": 3,
                      "part": 0,
                      "translated_content": "非结构化推理方法的演变展示了语言模型对不同推理范式的显著适应性。从简单的提示技术到复杂的隐式推理方法，这些方法利用LLM的固有能力进行复杂的逻辑推理，而无需显式的结构约束。这种灵活性使得在各种推理任务中更直观地解决问题，同时保持效率和有效性。"
                    }
                  ],
                  "raw_title": "Implicit Reasoning",
                  "type": null,
                  "children": [],
                  "translated_title": "2.2.2.3 隐性推理"
                }
              ],
              "translated_title": "2.2.2 非结构化推理"
            },
            {
              "title": "2.2.3 Planning",
              "number": "2.2.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Planning is afundamentalaspect of human cognition,enabling individuals toorganize actions,anticipate outcomes, and achieve goals in complex, dynamic environments[144].Formally, planningcan bedescribed as the process of constructing potential pathways from an initial state to a desired goal state, represented as $P:S_{0}\\rightarrow\\{a_{1},a_{2},...,a_{n}\\}\\rightarrow$ $S_{g}$ , where $S_{0}$ is the starting state, $\\{a_{1},a_{2},\\ldots,a_{n}\\}$ denotes a sequence of possible actions, and $S_{g}$ is the goal state. Unlike direct reasoning,planning involves generating hypotheticalaction sequences before execution,functioning as computational nodes that remain inactive until deployed.Thiscognitive abilityemerges fromthe interplayof specialized neuralcircuits,including the prefrontalcortex,whichgovernsexecutivecontrol,andthehippocampus,whichsuppots episodic foresight and spatial mapping.Insights from decisiontheory,psychology,and cybernetics—such as rational frameworks,prospect theory,and feedback loops-demonstrate how planning allows humans totranscend reactive behavior, actively shaping their futures through deliberate intent and adaptive strategies.Thiscapacity not only underpins intellgent behavior but also serves as a modelfor developing LLM-basedagents that seek to replicate and enhance these abilities computationally [145, 146]",
                  "index": 0,
                  "part": 0,
                  "translated_content": "规划是人类认知的一个基本方面，使个体能够在复杂、动态环境中组织行动、预测结果并实现目标。形式上，规划可以描述为从初始状态到期望目标状态构建潜在路径的过程，表示为 $P:S_{0}\\rightarrow\\{a_{1},a_{2},...,a_{n}\\}\\rightarrow$ $S_{g}$ ，其中 $S_{0}$ 是起始状态，$\\{a_{1},a_{2},\\ldots,a_{n}\\}$ 表示一系列可能的行动，$S_{g}$ 是目标状态。与直接推理不同，规划涉及在执行之前生成假设的行动序列，作为计算节点，直到部署前保持不活跃。这种认知能力源自专门的神经回路的相互作用，包括负责执行控制的前额叶皮层和支持情景预测和空间映射的海马。决策理论、心理学和控制论等领域的见解，如理性框架、前景理论和反馈环路，展示了规划如何使人类超越反应性行为，通过深思熟虑的意图和适应性策略积极塑造未来。这种能力不仅支撑着智能行为，还可作为基于LLM的代理开发的模型，这些代理旨在在计算上复制和增强这些能力。"
                },
                {
                  "type": "text",
                  "content": "In human cognition, planning operates as a hierarchical process,integrating immediate decisions with long-term objectives.This reflects the brain's modular architecture,where neural systemscolaborate to balance short-term demands withfuture possibilities-a dynamic informed by control theory's principles of stability and optimization. Similarly, LLM-based agents employ planning by leveraging their extensive linguistic knowledge and contextual reasoning totransform inputs into actionable steps. Whether addressing structured tasksor unpredictablechallenges, these agents emulate human planning bydecomposing objectives,evaluating potentialoutcomes,andrefining their strategiesblending biologicalinspiration with artificialintelligence.This section examines thetheoreticalfoundations and practical techniques of planning,fromsequentialapproaches toparallelexploration,highlighting itscriticalrole in intelligent systems.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "在人类认知中，规划作为一个分层过程运作，将即时决策与长期目标整合在一起。这反映了大脑的模块化结构，神经系统协同工作以平衡短期需求和未来可能性，这种动态受控制理论的稳定性和优化原则的影响。同样，基于LLM的代理通过利用其丰富的语言知识和情境推理，将输入转化为可行步骤来进行规划。无论是处理结构化任务还是不可预测的挑战，这些代理通过分解目标、评估潜在结果和完善策略来模拟人类规划，将生物灵感与人工智能相结合。本节探讨了规划的理论基础和实用技术，从顺序方法到并行探索，突出了规划在智能系统中的关键作用。"
                },
                {
                  "type": "text",
                  "content": "Despite the potential of LLMs in automated planning, their performance faces limitations due to gaps in world knowledge[147]. LLMs often lack deepcomprehension of world dynamics,relying on pattern recognition rather than genuine causalreasoning,which hinders theirabilityto manage sub-goalinteractions and environmentalchanges[148]. Additionally,their reliance onstatic pre-training data restricts adaptability in real-time scenarios, limiting their generalization in dynamic planning tasks [149].The absence of an intrinsic System 2 reasoning mechanism further complicatestheirabilityto independently generate structured,optimal plans[10].However,esearchers have proposed strategies such as task decomposition, search optimization,and external knowledge integration to mitigate these challenges.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "尽管LLM在自动规划中具有潜力，但由于世界知识的差距，它们的性能存在局限性。LLM经常缺乏对世界动态的深刻理解，依赖模式识别而非真正的因果推理，这妨碍了它们管理子目标交互和环境变化的能力。此外，它们依赖静态的预训练数据限制了它们在实时场景中的适应性，限制了它们在动态规划任务中的泛化能力。缺乏内在的系统2推理机制进一步复杂化了它们独立生成结构化、最优计划的能力。然而，研究人员已经提出了诸如任务分解、搜索优化和外部知识整合等策略来缓解这些挑战。"
                },
                {
                  "type": "text",
                  "content": "Task Decomposition Task decomposition enhances LLM planning by breaking complex goals into smaller, manageable subtasks,reducing problem complexity and improving systematic reasoning.The Least-to-Most Prompting method [138] exemplifies this approach, guiding LLMs to solve subproblems incrementally.ADaPT[151] further refines this strategy bydynamically adjustingtask decomposition based on complexity and modelcapability,particularly in interactivedecision-making scenarios.These methods alsofacilitate parallelsubtask processingbackward error tracing, and independence determination [132], providing a structured framework for reasoning.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "任务分解通过将复杂目标分解为更小、可管理的子任务，提高了LLM规划的效果，降低了问题复杂性，并改善了系统推理。最小到最多提示方法[138]就是这种方法的典范，引导LLM逐步解决子问题。ADaPT[151]通过根据复杂性和模型能力动态调整任务分解，特别是在互动决策场景中，进一步完善了这一策略。这些方法还促进了并行子任务处理、反向错误跟踪和独立性确定[132]，为推理提供了结构化框架。"
                },
                {
                  "type": "text",
                  "content": "In LLM planning, tasks function as executable units-distinct from static state descriptions in formal models—emphasizing structured sequences that achieve intended outcomes [66].These tasks vary in nature: some are subproblems requiring specificsolutions (e.g.solving equations within broaderchallenges),while others involve tool invocation (e.g.,querying APIs for weather data in travel planning)[152,153].Alternatively,tasks may be represented as graph nodes mapping dependencies,such as prioritizing objectives in project management [154].By defining clear, modular goals,these formulations enhance reasoning and action eficiency, guiding agents through complex problem spaces with greater precision [93].",
                  "index": 4,
                  "part": 0,
                  "translated_content": "在LLM规划中，任务作为可执行单元，与形式模型中的静态状态描述不同，强调实现预期结果的结构化序列[66]。这些任务的性质各不相同：有些是需要具体解决方案的子问题（例如在更广泛的挑战中解方程），而其他一些涉及工具调用（例如在旅行规划中查询API获取天气数据）[152,153]。另外，任务也可以表示为映射依赖关系的图节点，比如在项目管理中优先考虑的目标[154]。通过定义清晰的模块化目标，这些表述增强了推理和行动的效率，以更高的精度引导代理在复杂问题空间中前进[93]。"
                },
                {
                  "type": "text",
                  "content": "Searching Given the stochastic nature of LLMs[155],paralelsampling combined with aggregated reasoning can improve inference performance.Task decompositionstructures individualsolution trajectories,enablingtheconstruction of a solutionspace that includes multiple pathways toa goal andtheir interelationships[72,156].This space allows sampling diverse potential solutions[157],facilitating exploration through techniques likereflection,review,and parallel sampling informed by existing knowledge [158].",
                  "index": 5,
                  "part": 0,
                  "translated_content": "鉴于LLM的随机性质[155]，结合并行抽样和聚合推理可以提高推理性能。任务分解构建了个体解决方案轨迹，使得可以构建包括多条通往目标及其相互关系的解空间[72,156]。这个空间允许抽样各种潜在解决方案[157]，通过反思、审查和并行抽样等技术促进探索，这些技术受到现有知识的启发[158]。"
                },
                {
                  "type": "text",
                  "content": "Computationalconstraints often preclude exhaustive evaluation,making eficient navigation of the solution space essential.Methods include tree searchalgorithms like LATS[159],heuristic approaches such as PlanCritic's genetic algorithms[160], and CoT-SC,which identifies recurring solutions via self-consistency checks[78].Reward-based models like ARMAP assessintermediate and final outcomes to optimize planning [106].This iterative exploration and refinement process enhances adaptability, ensuring robust strategies for complex problems.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "由于计算约束通常限制了详尽评估的可能性，因此高效地导航解空间至关重要。方法包括像LATS这样的树搜索算法[159]，像PlanCritic的遗传算法这样的启发式方法[160]，以及通过自一致性检查识别重复解决方案的CoT-SC[78]。基于奖励的模型如ARMAP评估中间和最终结果以优化规划[106]。这种迭代的探索和完善过程增强了适应性，确保了复杂问题的强大策略。"
                },
                {
                  "type": "text",
                  "content": "World Knowledge Efective planning requires agents to navigate dynamic environments, anticipate changes, and predict outcomes, underscoring the importance of world knowledge. RAP[74] examines the interplay between LLMs, agent systems, and world models,positioning LLMs as dual-purpose entities:as world models,they predict state changes following actions[107,161]; asagents,they select actions based on states and goals[70].This framework mirrors human cognition-simulating action consequences before selecting optimal paths—and unifies language models, agent models, and world models as pillars of machine reasoning [162].",
                  "index": 7,
                  "part": 0,
                  "translated_content": "世界知识。有效的规划要求智能体在动态环境中进行导航，预测变化并预测结果，突显了世界知识的重要性。RAP[74]研究LLMs、智能体系统和世界模型之间的相互作用，将LLMs定位为具有双重目的的实体：作为世界模型，它们预测行动后的状态变化[107,161]；作为智能体，它们基于状态和目标选择行动[70]。这一框架模拟了人类认知——在选择最佳路径之前模拟行动后果——并将语言模型、智能体模型和世界模型统一为机器推理的支柱[162]。"
                },
                {
                  "type": "text",
                  "content": "Agents augment LLM capabilities by integrating external knowledge,addressing gaps in world understanding. ReAct employs an action-observation loop to gather environmental feedback,combining real-time data with linguistic knowledge to improve decision-making in complex scenarios[7o]. This enables LLMs to iterativelyrefine their world models during action execution, supporting adaptive planning. Conversely, $\\mathsf{L L M+P}$ [163] integrates LLMs with the PDDL planning language,converting naturallanguage inputs intoformalizedrepresentations solved by classical planners [164,165].This hybrid approachcompensates for LLMs\"limitations in structured planning,merging their linguistic flexibility with the reliability of traditional systems.",
                  "index": 8,
                  "part": 0,
                  "translated_content": "智能体通过整合外部知识增强了LLM的能力，填补了对世界理解的空白。ReAct采用动作-观察循环来收集环境反馈，将实时数据与语言知识相结合，以改善在复杂场景中的决策制定。这使得LLMs能够在执行动作过程中迭代地完善其世界模型，支持自适应规划。相反，LLM+P将LLMs与PDDL规划语言相结合，将自然语言输入转换为由经典规划器解决的形式化表示，从而弥补了LLMs在结构化规划方面的局限性，将它们的语言灵活性与传统系统的可靠性融合在一起。"
                },
                {
                  "type": "text",
                  "content": "Further advancements enhance LLM planning through world knowledge integration. CodePlan[166] uses code-form plans—pseudocode outlining logical steps—to guide LLMs through complex tasks,achieving notable performance improvements across benchmarks[167]. The World Knowledge Model(WKM)equips LLMs with prior task knowledge and dynamic state awareness,reducing trial-and-error and hallucinations in simulated environments[168].A neurosymbolic approach combining Linear Temporal Logic with Natural Language (LTL-NL) integrates formal logic with",
                  "index": 9,
                  "part": 0,
                  "translated_content": "进一步的进展通过整合世界知识提升了LLM的规划能力。CodePlan采用代码形式的计划——概述逻辑步骤的伪代码——来引导LLMs完成复杂任务，在各项基准测试中取得了显著的性能改进。世界知识模型（WKM）为LLMs提供了先前任务知识和动态状态意识，减少了在模拟环境中的试错和幻觉。一种神经符号方法将线性时间逻辑与自然语言（LTL-NL）相结合，将形式逻辑与"
                },
                {
                  "type": "text",
                  "content": "LLMs,leveraging implicit world knowledge to ensure reliable,adaptive planning [169]. Together,these methods illustrate how structured frameworks and environmental understanding can transform LLMs into effective planners.",
                  "index": 10,
                  "part": 0,
                  "translated_content": "利用隐含的世界知识确保可靠、自适应规划的LLMs[169]。这些方法共同展示了结构化框架和环境理解如何将LLMs转变为有效的规划者。"
                }
              ],
              "raw_title": "Planning",
              "type": null,
              "children": [],
              "translated_title": "2.2.3 规划"
            }
          ],
          "translated_title": "2.2 推理"
        }
      ],
      "translated_title": "2认知 25"
    },
    {
      "title": "3.1  Overview of Human Memory. 39",
      "number": "3.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "3.1.1  Types of Human Memory 39\n3.1.2 Models of Human Memory . 41\n3.2 From Human Memory to Agent Memory 42\n3.3 Representation of Agent Memory 44\n3.3.1 Sensory Memory 44\n3.3.2 Short-Term Memory 46\n3.3.3 Long-Term Memory 46\n\n3.4 The Memory Lifecycle 47",
          "index": 0,
          "part": 0,
          "translated_content": "3.1.1  人类记忆类型 39\n3.1.2  人类记忆模型 41\n3.2  从人类记忆到智能代理记忆 42\n3.3  智能代理记忆的表征 44\n3.3.1  感觉记忆 44\n3.3.2  短期记忆 46\n3.3.3  长期记忆 46\n\n3.4  记忆生命周期 47"
        }
      ],
      "raw_title": "Overview of Human Memory. 39",
      "type": null,
      "children": [],
      "translated_title": "3.1 人类记忆概述。"
    },
    {
      "title": "3.4.1 Memory Acquisition 47",
      "number": "3.4.1",
      "level": 3,
      "content": [
        {
          "type": "text",
          "content": "3.4.2 Memory Encoding 48\n3.4.3 Memory Derivation 49\n3.4.4 Memory Retrieval and Matching 50\n3.4.5 Neural Memory Networks 51\n3.4.6 Memory Utilization . 52\n3.5 Summary and Discussion 53",
          "index": 0,
          "part": 0,
          "translated_content": "3.4.2 记忆编码 48\n3.4.3 记忆推导 49\n3.4.4 记忆检索和匹配 50\n3.4.5 神经记忆网络 51\n3.4.6 记忆利用 52\n3.5 总结与讨论 53"
        }
      ],
      "raw_title": "Memory Acquisition 47",
      "type": null,
      "children": [],
      "translated_title": "3.4.1 记忆获取 47"
    },
    {
      "title": "4 World Model 54",
      "number": "4",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "4.1 The Human World Model 55\n4.2 Translating Human World Models to AI 55\n4.3 Paradigms of AI World Models 56\n4.3.1 Overview of World Model Paradigms 56\n4.3.2 Implicit Paradigm . 57\n4.3.3 Explicit Paradigm . 57\n4.3.4 Simulator-Based Paradigm 58\n4.3.5 Hybrid and Instruction-Driven Paradigms 58\n4.3.6 Comparative Summary of Paradigms 58\n4.4 Relationships to Other Modules 58\n4.4.1 Memory and the World Model 59\n4.4.2 Perception and the World Model 60\n4.4.3 Action and the World Model 60\n4.4.4 Cross-Module Integration 61\n4.5 Summary and Discussion 61",
          "index": 0,
          "part": 0,
          "translated_content": "4.1 人类世界模型 55\n4.2 将人类世界模型转化为人工智能 55\n4.3 人工智能世界模型的范式 56\n4.3.1 世界模型范式概述 56\n4.3.2 隐式范式 57\n4.3.3 显式范式 57\n4.3.4 基于模拟器的范式 58\n4.3.5 混合和指导式范式 58\n4.3.6 范式的比较总结 58\n4.4 与其他模块的关系 58\n4.4.1 记忆与世界模型 59\n4.4.2 感知与世界模型 60\n4.4.3 行动与世界模型 60\n4.4.4 跨模块整合 61\n4.5 总结与讨论 61"
        }
      ],
      "raw_title": "World Model 54",
      "type": null,
      "children": [],
      "translated_title": "4 世界模型 54"
    },
    {
      "title": "5 Reward 63",
      "number": "5",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "5.1 The Human Reward Pathway 64\n5.2 From Human Rewards to Agent Rewards 65\n5.3AI Reward Paradigms 65\n5.3.1 Definitions and Overview 65\n5.3.2 Extrinsic Rewards 67\n5.3.3 Intrinsic Rewards 67\n5.3.4 Hybrid Rewards 68\n5.3.5 Hierarchical Rewards 68\n5.4 Summary and Discussion 69\n5.4.1 Interaction with Other Modules 69\n5.4.2 Challenges and Directions 69",
          "index": 0,
          "part": 0,
          "translated_content": "5.1 人类奖励通路 64\n5.2 从人类奖励到智能代理奖励 65\n5.3 人工智能奖励范式 65\n5.3.1 定义和概述 65\n5.3.2 外在奖励 67\n5.3.3 内在奖励 67\n5.3.4 混合奖励 68\n5.3.5 分层奖励 68\n5.4 总结与讨论 69\n5.4.1 与其他模块的互动 69\n5.4.2 挑战和方向 69"
        }
      ],
      "raw_title": "Reward 63",
      "type": null,
      "children": [
        {
          "title": "5.1 The Human Reward Pathway",
          "number": "5.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "The brain's reward system is broadly organized into two major anatomical pathways.The first is the medial forebrain bundle,whichoriginates inthebasalforebrainandprojects throughthe midbrain,ultimatelyterminating inbrainstem regions.The second is the dorsaldiencephalicconduction system,whicharises from therostralportion of the medial forebrain bundle,traverses the habenula,andprojects toward midbrain structures[407].The feedback mechanisms and substances inthehuman brain arecomplex,involvingavarietyof neurotransmiters,hormones,andother molecules, which regulate brainfunction,emotions,cognition,andbehaviorthroughfeedback mechanismssuch as neurotransmiter systems and reward circuits.Feedback mechanismscan be positive(such as feedbackinthe reward system)or negative (such asinhibiting excessive neural activity).Wellknown feedback substances [41l]includedopamine,neuropeptides, endorphins, glutamate, etc.",
              "index": 0,
              "part": 0,
              "translated_content": "大脑的奖励系统广泛分为两大解剖途径。第一个是内侧前脑束，起源于基底前脑，通过中脑并最终终止于脑干区域。第二个是背侧间脑传导系统，起源于内侧前脑束的前部，穿过腔隙核，并投射至中脑结构。人脑中的反馈机制和物质非常复杂，涉及多种神经递质、激素和其他分子，通过神经递质系统和奖励回路等反馈机制调节大脑功能、情绪、认知和行为。反馈机制可以是积极的（如奖励系统中的反馈）或消极的（如抑制过度神经活动）。众所周知的反馈物质包括多巴胺、神经肽、内啡肽、谷氨酸等。"
            },
            {
              "type": "text",
              "content": "Dopamine is a signaling molecule that plays an important role in the brain, afecting our emotions, motivation, movement,andother aspects[412].Thisneurotransmiteriscriticalforreward-based learning,butthis functioncanbe disrupted in many psychiatricconditions,such as mood disorders and addiction.The mesolimbic pathway[406],akey dopaminergic system,originates from dopamine-producing neurons in the ventral tegmentalarea (VTA)and projects to multiple limbic andcorticalregions,including the striatum, prefrontalcortex,amygdala,and hippocampus.This pathway plays acentralrole inreward processing, motivation,andreinforcementlearning,and is widelyrecognized as acore component of the brain's reward system. Neuropeptides are another important classof signaling molecules in the nervous system, involved inavariety offunctions from mood regulation to metabolic control, and are slow-acting signaling molecules.Unlike neurotransmitters,which arelimitedtosynapses,neuropeptide signalscan affect a wider range of neural networks and provide broader physiological regulation.There is a significant cortical-subcortical gradientin thedistributionofdiffrent neuropeptidereceptors inthe brain.Inaddition,neuropeptide signaling ha been shown to significantly enhance the structure-function coupling of brain regions and exhibita specializedgradient from sensory-cognitive toreward-physicalfunction[413].Table 5lists thecommon reward pathways inthehuman brain,the neurotransmitters they transmit, and the corresponding mechanisms of action,describing the basic framework of the human brain reward system.",
              "index": 1,
              "part": 0,
              "translated_content": "多巴胺是一种在大脑中发挥重要作用的信号分子，影响我们的情绪、动机、运动等多个方面。这种神经递质对基于奖励的学习至关重要，但在许多精神疾病中，如情绪障碍和成瘾，这种功能可能会受到干扰。多巴胺能通路是一个关键的多巴胺系统，起源于产生多巴胺的神经元位于腹侧被盖区（VTA），并投射至多个边缘和皮层区域，包括纹状体、前额叶皮层、杏仁核和海马等。这一通路在奖励加工、动机和强化学习中发挥着核心作用，并被广泛认为是大脑奖励系统的核心组成部分。神经肽是神经系统中另一类重要的信号分子，参与从情绪调节到代谢控制等各种功能，并且是一种作用缓慢的信号分子。与神经递质不同，神经肽信号可以影响更广泛的神经网络，并提供更广泛的生理调节。在大脑中，不同神经肽受体的分布存在显著的皮质-皮质下梯度。此外，已经显示神经肽信号显著增强了大脑区域的结构-功能耦合，并展现出从感觉-认知到奖励-生理功能的专门梯度。表5列出了人类大脑中常见的奖励通路、它们传递的神经递质以及相应的作用机制，描述了人类大脑奖励系统的基本框架。"
            }
          ],
          "raw_title": "The Human Reward Pathway",
          "type": null,
          "children": [],
          "translated_title": "5.1 人类奖赏通路"
        },
        {
          "title": "5.2 From Human Rewards to Agent Rewards",
          "number": "5.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Having examined the foundations of human reward pathways, we now turn tohow artificialagents learn andoptimize behavior through reward signals.While biological systems rely on complex neurochemical and psychological feedback loops, artificial agents operate using formalized reward functionsdesigned to guide learning and decision-making. Though inspired byhuman cognition,agent reward mechanisms arestructurallyandfunctionally distinct.Understanding the analogies anddisanalogies between these systems iscrucialfor aligning artificialbehavior with human preferences.",
              "index": 0,
              "part": 0,
              "translated_content": "在审查了人类奖励路径的基础之后，我们现在转向研究人工智能代理如何通过奖励信号学习和优化行为。尽管生物系统依赖于复杂的神经化学和心理反馈回路，人工代理则通过旨在引导学习和决策的形式化奖励函数来运作。尽管受到人类认知的启发，代理奖励机制在结构和功能上是有区别的。理解这些系统之间的类比和非类比对于使人工行为与人类偏好保持一致至关重要。"
            },
            {
              "type": "text",
              "content": "In humans,rewards are deeplyembedded inarichweb ofemotional,social, and physiologicalcontexts.They emerge through evolutionarily tuned mechanisms involving neurotransmitters like dopamine and are shaped by experiences, culture, and individual psychology. Incontrast, artificialagents relyon mathematically defined reward functions that are externally specified and precisely quantified.These functions assgn scalar or probabilisticfeedback to actionsor states, providing a signal for optimization algorithms such as reinforcement learning [3, 414].",
              "index": 1,
              "part": 0,
              "translated_content": "在人类中，奖励深深植根于丰富的情感、社会和生理背景中。它们通过涉及多巴胺等神经递质的进化调谐机制而出现，并受到经验、文化和个体心理的塑造。相比之下，人工代理依赖于数学定义的奖励函数，这些函数是外部指定并精确量化的。这些函数为行动或状态分配标量或概率反馈，为强化学习等优化算法提供信号。"
            },
            {
              "type": "text",
              "content": "One key distinction lies in the programmability and plasticity of agent rewards.Unlike human reward systems, which are constrained by biological architecture and evolutionary inertia, agent reward functions are full customizable and can berapidly redefined or adjusted based on task requirements.This flexibility enables targeted learning but also introduces design challenges—specifying a reward function that accurately captures nuanced human values is notoriously difficult.\n\nAnother important disanalogy concerns interpretability and generalization.Human rewards are often implicit and context-dependent, whereas agent rewards tend to be explicit and task-specific.Agents lack emotional intuition and instinctualdrives; theirlearning dependsentirelyontheform andfidelityof the reward signal. While frameworks like reinforcement learning from human feedback (RLHF) attempt to bridge this gap by using preference data to shape agent behavior[12], suchmethods stillstruggle withcapturing the fullcomplexity of human goals,especially when preferences are intransitive, cyclical, or context-sensitive [321].",
              "index": 2,
              "part": 0,
              "translated_content": "一个关键区别在于代理奖励的可编程性和可塑性。与受生物结构和进化惯性限制的人类奖励系统不同，代理奖励函数是完全可定制的，并且可以根据任务要求迅速重新定义或调整。这种灵活性实现了有针对性的学习，但也带来了设计挑战——准确捕捉微妙的人类价值的奖励函数设计是极其困难的。\n\n另一个重要的不同之处在于可解释性和泛化性。人类奖励通常是隐含的且依赖于上下文，而代理奖励往往是明确的且特定于任务。代理缺乏情感直觉和本能驱动；它们的学习完全依赖于奖励信号的形式和准确性。虽然诸如从人类反馈中强化学习（RLHF）这样的框架试图通过使用偏好数据来塑造代理行为来弥合这一差距，但这些方法仍然难以捕捉人类目标的全部复杂性，特别是当偏好是不传递的、循环的或依赖于上下文时。"
            },
            {
              "type": "text",
              "content": "Moreover, atempts to borrow from human reward mechanisms—such as modeling intrinsic motivation or social approval—face limitations due tothe absence ofconsciousness,embodiment, and subjective experience in artificial agents.Consequently,whilehuman reward systems ofer valuable inspiration, thedesign of agentreward functions must addressfundamentally different constraints,including robustnessto misspecification, adversarial manipulation, and misalignment with long-term human interests.\n\nThe following section willdelve deeper into agent reward models,focusing on their design principles,evolution,and how these models selectively incorporatehuman-inspired insights tooptimize artificialbehavior within formal systems.",
              "index": 3,
              "part": 0,
              "translated_content": "此外，试图借鉴人类奖励机制——比如建模内在动机或社会认可——会面临一些限制，因为人工智能代理缺乏意识、具身性和主观体验。因此，尽管人类奖励系统提供了宝贵的启发，但代理奖励函数的设计必须应对根本不同的约束，包括对错误规范的鲁棒性、对对抗性操纵的防范以及与长期人类利益的不一致性。\n\n接下来的部分将更深入地探讨代理奖励模型，重点关注它们的设计原则、演化以及这些模型如何有选择性地融入受人类启发的见解，以优化在形式系统内的人工行为。"
            }
          ],
          "raw_title": "From Human Rewards to Agent Rewards",
          "type": null,
          "children": [],
          "translated_title": "5.2 从人类奖励到智能体奖励"
        },
        {
          "title": "5.3 AI Reward Paradigms",
          "number": "5.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Rewards also exist in intellgent agents,especially inreinforcement learning scenarios. Rewards are the core signal used to guide how intelligent agents act intheenvironment.Theyexpressfeedback onthebehaviorofintelligent agents and are used to evaluate anaction'squality inacertainstate, thereby aecting thedecision-makingof subsequent actions.Throughcontinuous trialanderrorandadjustment, intelligent agents learn tochoose behavioral strategies that can obtain high rewards in different states.",
              "index": 0,
              "part": 0,
              "translated_content": "奖励也存在于智能代理中，特别是在强化学习场景中。奖励是指导智能代理如何在环境中行动的核心信号。它们表达了对智能代理行为的反馈，并用于评估某一状态下行动的质量，从而影响后续行动的决策。通过持续的试错和调整，智能代理学会选择可以在不同状态下获得高奖励的行为策略。"
            }
          ],
          "raw_title": "AI Reward Paradigms",
          "type": null,
          "children": [
            {
              "title": "5.3.1 Definitions and Overview",
              "number": "5.3.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In reinforcement learning,thereward modeldictateshow an agent is provided with feedback according tothe actions it performs within its environment.This model plays acrucialrole in guiding the agent's behavior by quantifying the desirability of actions in a given state, thus influencing its decision-making.\n\nFormalDefinition. The agent's interaction with its environment can be framed within the formalism of a Markov Decision Process (MDP) [415], which is represented as:",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在强化学习中，奖励模型规定了代理根据其在环境中执行的动作而获得反馈的方式。该模型通过量化给定状态下动作的可取性，从而影响其决策过程，对引导代理的行为起着至关重要的作用。\n\n形式化定义。代理与环境的交互可以在马尔可夫决策过程（MDP）[415]的形式化框架内加以描述，如下所示："
                },
                {
                  "type": "formula",
                  "content": "$$ \n\\boldsymbol{\\mathcal{M}}=(\\boldsymbol{\\mathcal{S}},\\boldsymbol{\\mathcal{A}},\\boldsymbol{P},\\boldsymbol{r},\\gamma),\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "where:\n\n· $s$ denotes the state space, encompassing all possible states in the environment.\n· $\\mathcal{A}$ denotes the action space, which encompasses all actions available to the agent at any given state.\n· $P(s^{\\prime}|s,a)$ defines the state transition probability. It represents the likelihood of transitioning to state $s^{\\prime}$ after the agent takes action $a$ in state $s$ .\n· $r(s,a)$ specifies the reward function, which assigns an immediate scalar reward received by the agent for executing action $a$ in state $s$ .\n· $\\gamma\\in[0,1]$ is the discount factor, which controll the agent's preference for immediate versus future rewards by weighting the contribution of future rewards to the overall return.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "其中：\n\n- $s$ 表示状态空间，包括环境中所有可能的状态。\n- $\\mathcal{A}$ 表示动作空间，包括代理在任何给定状态下可以执行的所有动作。\n- $P(s^{\\prime}|s,a)$ 定义了状态转移概率。它表示在代理在状态 $s$ 中执行动作 $a$ 后转移到状态 $s^{\\prime}$ 的可能性。\n- $r(s,a)$ 指定了奖励函数，为代理在状态 $s$ 中执行动作 $a$ 后获得的即时标量奖励。\n- $\\gamma\\in[0,1]$ 是折扣因子，控制代理对即时奖励与未来奖励的偏好，通过对未来奖励对总回报的贡献进行加权。"
                },
                {
                  "type": "text",
                  "content": "The reward function $r(s,a)$ serves as a fundamental component in the formulation of the Agent Reward Model. It is mathematically represented as:",
                  "index": 3,
                  "part": 0,
                  "translated_content": "奖励函数$r(s,a)$在制定代理奖励模型中起着基础性作用。其数学表示如下："
                },
                {
                  "type": "formula",
                  "content": "$$ \nr(s,a):S\\times\\mathcal{A}\\to\\mathbb{R}\n $$",
                  "index": 4,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "This function returns a scalar reward based on the agent's current state $s$ and the action $a$ it selects. The scalar value $r(s,a)$ is a feedback signal that indicates the immediate benefit(or cost)ofthe chosen action in the given state.This reward signal guides the agent's learning processas it helps evaluate the quality of actions takenwithin specific contexts.\n\nObjective of the Agent Reward Model.The agent's primary objective is to maximize itsoverallcumulative reward over time.This is typicallachieved by selecting actions that yieldhigherlong-termrewards,which are captured in the form of the return $G_{t}$ at time step $t$ ,defined as the sum of future discounted rewards:",
                  "index": 5,
                  "part": 0,
                  "translated_content": "该函数根据代理的当前状态$s$和选择的动作$a$返回一个标量奖励。标量值$r(s,a)$是一个反馈信号，指示在给定状态下选择的动作的即时收益（或成本）。这个奖励信号指导代理的学习过程，因为它有助于评估在特定环境中采取的行动的质量。\n\n代理奖励模型的目标。代理的主要目标是随着时间的推移最大化其累积奖励。通常通过选择产生更高长期奖励的动作来实现这一目标，这些长期奖励以时间步$t$处的回报$G_{t}$的形式捕获，定义为未来折现奖励的总和："
                },
                {
                  "type": "formula",
                  "content": "$$ \nG_{t}=\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k},\n $$",
                  "index": 6,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "where $r_{t+k}$ denotes the reward received at time step $t+k$ , and $\\gamma^{k}$ is the discount factor applied to rewards received at time step $t+k$ . The agent aims to optimize its policy by maximizing the expected return over time.\n\nAta higher level,thereward modelcanbeclassifiedintothreecategories basedontheoriginof thefeedback signal:i) extrinsic reward,ii) intrinsic reward,ii)hybridrewardand iv)hierarchicalmodel.Each of thesecategoriescanbe furthersubdivided intosmallersubclasses.Figure 5.2illustrates different typesofrewards.Next, we willexplore these different types of reward in more detail, outlining the distinct features and applications of each type.",
                  "index": 7,
                  "part": 0,
                  "translated_content": "其中，$r_{t+k}$表示在时间步$t+k$收到的奖励，$\\gamma^{k}$是应用于在时间步$t+k$收到的奖励的折现因子。代理的目标是通过最大化随时间推移的预期回报来优化其策略。\n\n在更高的层次上，奖励模型可以根据反馈信号的来源分为三类：i) 外部奖励，ii) 内在奖励，iii) 混合奖励和iv) 分层模型。每个类别可以进一步细分为更小的子类。图5.2展示了不同类型的奖励。接下来，我们将更详细地探讨这些不同类型的奖励，概述每种类型的独特特征和应用。"
                },
                {
                  "type": "figure",
                  "src": "images/adffbb163aa483a68509e5d7aabe5e1e25de662d418b9d9a3c1ee3c70025a9f0.jpg",
                  "alt": "",
                  "caption": "Figure 5.2: Illustration of different types of reward.",
                  "index": 8,
                  "part": 0,
                  "translated_caption": "图5.2：不同类型奖励的示意图。"
                }
              ],
              "raw_title": "Definitions and Overview",
              "type": null,
              "children": [],
              "translated_title": "5.3.1 定义和概述"
            },
            {
              "title": "5.3.2 Extrinsic Rewards",
              "number": "5.3.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Extrinsic rewards are externally defined signals that guide an agent's behavior toward specific goals. In artificial learning systems,especiallyreinforcement learning,these signalsserve as aproxy forsuccessthat shape the policy throughmeasurableoutcomes.However,the structure anddelivery of theserewards significantlyinfuence the leaning dynamics, which present different trade-offs depending on how feedback is distributed.\n\nDense Reward.Dense reward signals provide high-frequency feedback,typically at every steporaftereach action.This frequent guidance accelerates learning by allowing agents to immediately associate actions with outcomes.However, dense feedbackcan sometimes incentivize short-sighted behavior or overfit toeasily measurable proxies rather than deeper alignment.\n\nFor example, InstructGPT[43] uses human rankings of model outputs to provide continuous preference signals throughout fine-tuning,enabling efficient behavior shaping.Similarly,Cringe Loss[416]and its extensions [374] transform pairwise human preferences into dense training objectives,offering immediate signal ateach comparison Direct Reward Optimization (DRO)[367] further simplifes this paradigm by avoiding pairwise comparisons entirely, associating each response with a scalar score—making the reward signal more scalable and cost-effective. These methods exemplify how dense feedback facilitates fine-grained optimization but must be carefully designed to avoid superficial alignment.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "外在奖励是外部定义的信号，指导代理的行为朝向特定目标。在人工学习系统中，特别是在强化学习中，这些信号充当成功的代理，通过可测量的结果塑造政策。然而，这些奖励的结构和传递方式显著影响学习动态，根据反馈的分布方式，会产生不同的权衡。\n\n密集奖励。密集奖励信号提供高频反馈，通常在每一步或每个动作之后。这种频繁的指导通过允许代理立即将动作与结果联系起来，加速了学习过程。然而，密集反馈有时可能激励短视行为，或过度拟合于易于测量的代理，而非更深层次的一致性。\n\n例如，InstructGPT使用模型输出的人类排名，在微调过程中提供连续的偏好信号，实现高效的行为塑造。类似地，Cringe Loss及其延伸将人类偏好转化为密集训练目标，为每次比较提供即时信号。直接奖励优化（DRO）进一步简化了这一范式，完全避免了成对比较，将每个响应与一个标量分数关联起来，使奖励信号更具可扩展性和成本效益。这些方法展示了密集反馈如何促进细粒度优化，但必须经过精心设计，以避免表面一致性。"
                },
                {
                  "type": "text",
                  "content": "Sparse Reward. Sparse rewards are infrequent and typically only triggered by major milestones or task completions. While they often reflect more meaningful or holistic successcriteria, their delayed nature can make credit assignment more difficult, especially in complex environments.\n\nPAFT[376] exemplifies this challenge by decoupling supervised learning and preference alignment, with feedback appliedonly at select decision points.This sparsityreflectsa moreglobal notion of successbut increases theburdenon optimization.Similarly, SimPO[377] uses log-probability-based implicit rewards without dense comparisons.The sparsitysimplifiesthe training pipeline butcanlimitresponsiveness to subtle preference shifts.Sparse reward systems thus tend to be more robust but demand stronger modeling assumptions or more strategic exploration.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "稀疏奖励。稀疏奖励是不经常发生的，通常只在重大里程碑或任务完成时触发。虽然它们通常反映了更有意义或更全面的成功标准，但由于延迟的特性，可能会使学分分配变得更加困难，特别是在复杂环境中。\n\nPAFT在解决这一挑战时提供了示例，通过在选择的决策点应用反馈，将监督学习和偏好对齐分离开来。这种稀疏性反映了更全局的成功概念，但增加了优化的负担。类似地，SimPO使用基于对数概率的隐式奖励，而不进行密集比较。这种稀疏性简化了训练流程，但可能限制对微小偏好变化的响应。因此，稀疏奖励系统往往更加健壮，但需要更强的建模假设或更有策略性的探索。"
                },
                {
                  "type": "text",
                  "content": "Delayed Reward. Delayed rewardsdefer feedback untilaftera sequence of actions,requiring agents toreason about long-term consequences.This setup is essentialfor tasks where intermediate steps may be misleading oronly make sense in retrospect.The challenge lies in atributing outcomes toearlier decisions,which complicates learning but encourages planning and abstraction.\n\nContrastive Preference Optimization(CPO)[384]trains models bycomparing sets of translations rather than evaluating each one in isolation.The reward signal arises only after generating multiple candidates,reinforcing patterns across iterations. Nash Learning from Human Feedback [385] similarly delays feedback until the model identifies stable strategies through competitive comparisons.These methods leverage delayed rewards to push beyond surface-level optimization,aligning more withlong-termgoalsatthecostofslowerconvergence and morecomplextraining dynamics.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "延迟奖励。延迟奖励推迟反馈直到一系列动作之后，要求代理人考虑长期后果。这种设置对于中间步骤可能具有误导性或仅在事后才有意义的任务至关重要。挑战在于将结果归因于先前的决策，这使学习变得复杂，但鼓励规划和抽象。\n\n对比偏好优化（CPO）[384]通过比较翻译集合而不是单独评估每个翻译来训练模型。奖励信号仅在生成多个候选项之后产生，强化迭代中的模式。Nash从人类反馈中学习[385]同样推迟反馈，直到模型通过竞争性比较确定稳定策略。这些方法利用延迟奖励超越表面层优化，更符合长期目标，但以较慢的收敛速度和更复杂的训练动态为代价。"
                },
                {
                  "type": "text",
                  "content": "Adaptive Reward. Adaptive rewards evolve dynamicall in response to the agent's behavioror learning progress.By modulating the reward functionsuch as increasing task difficulty or shifting reward targets,this approach supports continualimprovement,especially in non-stationary or ambiguous environments.However,it introduces additional complexity in reward design and evaluation.\n\nSelf-Play Preference Optimization (SPO)[386]adapts rewards based on self-play outcomes,using socialchoice theory to aggregate preferences and guidelearning.This approach alows the system torefine itself by evolving internal standards.f-DPO[373]buildsonthis ideabyintroducing divergenceconstraintsthatadapttherewardlandscape during training.Bytuning alignment-diversitytrade-offs dynamically, these methodsenable robust preference modeling under uncertainty, though they require careful calibration to avoid instability or unintended bias.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "自适应奖励。自适应奖励根据代理的行为或学习进展动态演化。通过调节奖励函数，如增加任务难度或转移奖励目标，这种方法支持持续改进，特别是在非稳态或模糊环境中。然而，这也在奖励设计和评估中引入了额外的复杂性。\n\n自我对弈偏好优化（SPO）[386]根据自我对弈结果调整奖励，利用社会选择理论来汇总偏好并指导学习。这种方法允许系统通过演变内部标准来不断完善自身。f-DPO[373]在此基础上引入了发散约束，通过在训练过程中调整奖励景观。通过动态调整对齐-多样性权衡，这些方法能够在不确定性下实现稳健的偏好建模，尽管需要仔细校准以避免不稳定性或意外偏见。"
                }
              ],
              "raw_title": "Extrinsic Rewards",
              "type": null,
              "children": [],
              "translated_title": "5.3.2 外在奖励"
            },
            {
              "title": "5.3.3 Intrinsic Rewards",
              "number": "5.3.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Intrinsic rewards serve asinternalgenerated signalsthatmotivate agents toexplore,learn,andimprove,independent of external task-specific outcomes.These rewards are often structured to promote generalization,adaptability,and self-directed skillacquisition—qualitiescriticalforlong-term performance in complex or sparse-reward environments. Different intrinsic reward paradigms focus on fostering distinct behavioral tendencies within agents.\n\nCuriosity-Driven Reward. This reward encourages agents to reduce uncertainty by seeking novel or surprising experiences.Thekeyconcept is to incentivizethe agent to explore novel states where prediction errors aresignificant. This paradigm excels insparse-reward settings by promoting information acquisitionwhen external guidance is limited. For example, Pathak et al.[387]leverage an inverse dynamicsmodel to predict the outcome of actions,creating a feedback loop that rewards novelty.Plan2Explore[389] extends this further by incorporating forward planning to actively target areas of high epistemic uncertainty,thereby enabling faster adaptation to unseen environments.While effective at discovery,curiosity-driven methods can be sensitive to noise or deceptive novelty without safeguards.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "内在奖励作为内部生成的信号，激励代理探索、学习和改进，独立于外部任务特定结果。这些奖励通常被构建为促进泛化、适应性和自主技能获取，这些特质对于在复杂或稀疏奖励环境中的长期表现至关重要。不同的内在奖励范式侧重于在代理中培养不同的行为倾向。\n\n好奇驱动奖励。这种奖励鼓励代理通过寻找新颖或令人惊讶的经验来减少不确定性。关键概念是激励代理探索预测误差显著的新颖状态。在稀疏奖励设置中，这种范式通过在外部指导有限时促进信息获取而表现出色。例如，Pathak等人利用逆动力学模型预测行动结果，创造了一个奖励新奇性的反馈循环。Plan2Explore进一步扩展了这一点，通过整合前向规划来主动瞄准高认知不确定性区域，从而实现对未见环境的更快适应。虽然在发现方面效果显著，好奇驱动方法可能对噪音或具有欺骗性的新奇性敏感，如果没有保护措施的话。"
                },
                {
                  "type": "text",
                  "content": "Diversity Reward. Diversity reward shifts focus from novelty to behavioral heterogeneity,encouraging agents to explore a wide range of strategies ratherthan converging prematurely on suboptimal solutions.This approach is particularly usefulin multi-agent or multimodal settings,where strategic variety enhances robustness andcollective performance.LIR[390]exemplifies this by assgning personalized intrinsic signals todifferent agents,driving them toward distinct roles while maintaining sharedobjectives.Diversity-driven exploration fosters broader policycoverage but may require careful balancing to avoid destabilizing coordination or goal pursuit.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "多样性奖励。多样性奖励将焦点从新颖性转移至行为多样性，鼓励代理探索各种策略，而不是过早收敛于次优解决方案。这种方法在多智能体或多模态环境中特别有用，战略多样性增强了稳健性和集体表现。LIR通过为不同代理分配个性化的内在信号，推动它们朝着不同的角色发展，同时保持共享目标，充分展示了这一点。基于多样性的探索促进了更广泛的策略覆盖，但可能需要仔细平衡，以避免破坏协调或目标追求。"
                },
                {
                  "type": "text",
                  "content": "Competence-Based Reward. Competence-based reward aims to foster learning progressby rewarding improvements in the agent's task proficiency.This reward adapts dynamically as the agent grows morecapable,which creates a self-curriculum that supportscontinual skillacquisition.Skew-Fit[392]facilitates this through entropy-based goal sampling,encouraging agents to reach diverse states while maintaining challenge.CURIOUS [391] further automates curriculum generation by selecting goals that maximize learning progress over time.Competence-based methods are well-suited for open-ended environments,though they often require sophisticated estimation of progress and goal difficulty.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "基于能力的奖励。基于能力的奖励旨在通过奖励代理任务熟练度的提高来促进学习进展。这种奖励会随着代理变得更有能力而动态调整，从而形成一个支持持续技能习得的自我课程。Skew-Fit通过基于熵的目标采样来促使代理达到多样的状态，同时保持挑战性。CURIOUS通过选择随时间最大化学习进展的目标进一步自动化课程生成。基于能力的方法非常适用于开放式环境，尽管它们通常需要对进展和目标难度进行复杂的估计。"
                },
                {
                  "type": "text",
                  "content": "Exploration Reward.Exploration reward directly incentivizes the agent toengage with under-exploredstatesor actions, which emphasize breadth over depth in environment interaction. Unlike curiosity,which focuseson unpredictability, exploration rewardoften targets coverage or noveltyrelative to the agent's visitationhistory. RND[394]exemplifies this by rewarding the prediction error of arandomly initialized network,pushingthe agent towardunfamiliar states. This approach helps preventpremature convergence and encourages robustnessthough it maylack focus if not paired with meaningful learning objectives.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "探索奖励。探索奖励直接激励代理与未充分探索的状态或动作进行互动，强调在环境互动中广度优先于深度。与好奇心关注的不确定性不同，探索奖励通常针对相对于代理访问历史的覆盖范围或新颖性。RND通过奖励随机初始化网络的预测误差来体现这一点，推动代理进入不熟悉的状态。这种方法有助于防止过早收敛，并鼓励鲁棒性，尽管如果没有与有意义的学习目标相结合，可能会缺乏重点。"
                },
                {
                  "type": "text",
                  "content": "Information Gain Reward. Information gain reward formalizes exploration as a processof uncertainty reduction, which guides agents totake actions that yield the highest expectedlearning.This reward is grounded ininformation theory andis especiallypowerfulinmodel-basedorreasoning-intensive tasks.CoT-Info[397]appliesthistolanguage models by quantifying knowledge gain at each reasoning step,optimizing sub-task decomposition.VIME[398] similarly employs Bayesian inference to rewardbeliefupdates about environmental dynamics.Byexplicitly targeting informationalvalue,these methods ofer principled exploration strategies,thoughtheyoften incur highcomputational cost and require accurate uncertainty modeling.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "信息增益奖励。信息增益奖励将探索形式化为不确定性减少的过程，引导代理采取能够产生最高期望学习的行动。这种奖励根植于信息理论，在基于模型或需要推理的任务中尤为强大。CoT-Info将此应用于语言模型，通过量化每个推理步骤的知识增益来优化子任务分解。VIME类似地利用贝叶斯推断奖励关于环境动态的信念更新。通过明确地针对信息价值，这些方法提供了基于原则的探索策略，尽管它们通常需要高计算成本并要求准确的不确定性建模。"
                }
              ],
              "raw_title": "Intrinsic Rewards",
              "type": null,
              "children": [],
              "translated_title": "5.3.3 内在奖励"
            },
            {
              "title": "5.3.4 Hybrid Rewards",
              "number": "5.3.4",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Hybrid reward frameworks integrate multiple sources of feedback, most commonly intrinsic and extrinsic rewards, to enable more balanced and adaptive learning.By combining the exploratory drive of intrinsic rewards with the goal-directed structure ofextrinsic rewards,these systems aim to improve both sample effciency and generalization. This paradigm is especialy beneficial in complex environments or open-ended tasks,where pure reliance on either feedback type may be insufficient.\n\nA core advantage ofhybridrewards is theircapacity toresolve the exploration-exploitation trade-offdynamically.For instance,Xiong et al.[403]combine intrinsic exploration with extrinsic human feedback within thecontextof RLHF. Using areverse-KLregularized contextual bandit framework,they facilitate strategic exploration while aligning the agent's actions with human preferences.The method integrates intrinsic and extrinsic rewards through an iterative DPO algorithm and multi-steprejection sampling,optimizing exploration and alignment without compromising efficiency.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "混合奖励框架整合了多个反馈来源，最常见的是内在和外在奖励，以实现更平衡和适应性学习。通过结合内在奖励的探索驱动和外在奖励的目标导向结构，这些系统旨在提高样本效率和泛化能力。在复杂环境或开放式任务中，这种范式特别有益，纯粹依赖任一类型反馈可能不足够。\n\n混合奖励的一个核心优势在于其动态解决勘探与开发之间的折衷。例如，Xiong等人将内在勘探与外在人类反馈结合在RLHF的背景下。他们利用一种反向-KL正则化的上下文臂框架，在优化探索的同时将代理的行为与人类偏好相一致。该方法通过迭代的DPO算法和多步拒绝抽样，将内在和外在奖励整合起来，优化探索和一致性而不损害效率。"
                }
              ],
              "raw_title": "Hybrid Rewards",
              "type": null,
              "children": [],
              "translated_title": "5.3.4 混合奖励"
            },
            {
              "title": "5.3.5 Hierarchical Rewards",
              "number": "5.3.5",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Hierarchicalrewardarchitectures decompose complexobjectives into layered subgoals,each associated with distinct reward signals.This structure mirrors the hierarchical organization of many real-world tasks, allowing agents to coordinate short-term decisions with long-term planning.By assigning lower-levelrewards to immediate actions and higher-levelrewards toabstract goals,agentscan learn compositionalbehaviors that scale more effectively tocomplex environments.\n\nIn language modeling,Token-level Direct PreferenceOptimization(TDPO)[405]ilustrates this principle by aligning LLMs through fine-grainedtoken-levelrewards derived from preference modeling. Using forward KL divergence and the Bradley-Terry model, TDPO simultaneouslyrefines localchoices and globalcoherence, improving alignment with nuanced human preferences.Thehierarchicalreward process here is not merely a structuraldesign but afunctionalone: reinforcing both micro-decisions and macro-outcomes in a coordinated fashion.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "层次奖励架构将复杂目标分解为分层子目标，每个子目标都与不同的奖励信号相关联。这种结构反映了许多现实世界任务的分层组织，使代理能够将短期决策与长期规划相协调。通过将较低级别奖励分配给即时行动，将较高级别奖励分配给抽象目标，代理可以学习组合行为，更有效地适应复杂环境。\n\n在语言建模中，基于标记级别的直接偏好优化（TDPO）[405]通过从偏好建模中获得的细粒度标记级别奖励来说明这一原则。使用前向KL散度和Bradley-Terry模型，TDPO同时优化本地选择和全局连贯性，提高与微妙人类偏好的一致性。这里的层次奖励过程不仅仅是一种结构设计，而是一种功能：以协调的方式加强微观决策和宏观结果。"
                },
                {
                  "type": "text",
                  "content": "More generally,hierarchicalrewardscan serve as scafolding for currculum learning,where agents progressively lean from simpler subtasks before tackling the overarching objective. In LLMagents,this might mean structuring rewards for subcomponents like tool-use,reasoning chains,orinteraction flows,each of whichcontributes tobroader task success.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "更一般地说，层次奖励可以作为课程学习的支架，代理可以逐渐从更简单的子任务中学习，然后再解决全局目标。在LLM代理中，这可能意味着为子组件（如工具使用、推理链或交互流程）设计奖励结构，每个子组件都有助于更广泛的任务成功。"
                }
              ],
              "raw_title": "Hierarchical Rewards",
              "type": null,
              "children": [],
              "translated_title": "5.3.5 层次化奖励"
            }
          ],
          "translated_title": "5.3 人工智能奖励范式"
        },
        {
          "title": "5.4 Summary and Discussion",
          "number": "5.4",
          "level": 2,
          "content": [],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": [
            {
              "title": "5.4.1 Interaction with Other Modules",
              "number": "5.4.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In intelligent systems,reward signals function not only as outcome-driven feedback but as centralregulators that interface with core cognitive modulessuchas perception,emotion,and memory. Inthe context of LLM-based agents, these interactions become particularlysalient,as modules likeatention,generation style,andretrieval memorycan be directly influenced through reward shaping, preference modeling, or fine-tuning objectives.\n\nPerception. In LLM agents, perception is often realizedthrough attention mechanisms that prioritize certain tokens, inputs, or modalities. Reward signalscan modulate these atention weights implicitly during training,reinforcing patterns that correlate with positive outcomes. For example, during reinforcement fine-tuning,reward models may upweight specific linguistic features—suchas informativeness,factuality,orpoliteness—causingthe model to attend more to tokens thatalign with these traits.This parallels how biological perception prioritizes salient stimuli via reward-linkedatentional modulation[417].Over time,the agent internalizesa perception policy:not merely“what is said,\" but“what is worth paying attention to\" in task-specific contexts.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在智能系统中，奖励信号不仅作为基于结果的反馈，而且作为与核心认知模块（如感知、情感和记忆）进行接口的中央调节器。在基于LLM的代理环境中，这些相互作用变得尤为显著，因为诸如注意力、生成风格和检索记忆等模块可以通过奖励塑造、偏好建模或微调目标直接受到影响。\n\n感知。在LLM代理中，感知通常通过注意机制实现，该机制优先考虑特定的标记、输入或模态。奖励信号可以在训练过程中隐式调节这些注意权重，强化与积极结果相关的模式。例如，在强化微调期间，奖励模型可以增加特定的语言特征权重，如信息量、事实性或礼貌性，导致模型更多地关注与这些特征相一致的标记。这类似于生物感知如何通过与奖励相关的注意调节优先考虑显著刺激。随着时间的推移，代理内化了一种感知策略：不仅仅是“说了什么”，而是“在特定任务背景下值得关注的是什么”。"
                },
                {
                  "type": "text",
                  "content": "Emotion.Though LLMs do not possess emotions in the biological sense,reward signalscan guide the emergence of emotion-like expressons andregulate dialogue style.In human alignment settings, models are often rewarded for generating responses that are empathetic,polite,or cooperativeleading to stylistic patterns that simulate emotional sensitivity.Positive feedback mayreinforceafriendlyorsupportivetone,whilenegativefeedback suppresses dismissive or incoherent behavior.This process mirrors affect-driven behavior regulation in humans [418],and allows agents to adapt their interaction style based onuser expectations,afective context, or application domain.Inmulti-turn settings, reward-modulated style persistence can give rise to coherent personas or conversational moods.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "情感。虽然LLM在生物学意义上不具备情感，但奖励信号可以引导类似情感表达的出现，并调节对话风格。在人类对齐设置中，模型通常会因生成具有共情、礼貌或合作性的回应而获得奖励，从而导致模拟情感敏感性的风格模式。积极的反馈可以强化友好或支持性的语气，而负面反馈则抑制轻蔑或不连贯的行为。这个过程反映了人类情感驱动的行为调节，并允许代理根据用户期望、情感背景或应用领域调整其互动风格。在多轮设置中，奖励调节的风格持久性可以形成连贯的人设或对话情绪。"
                },
                {
                  "type": "text",
                  "content": "Memory. Memory in LLM agents spans short-term context(e.g.,chat history) and long-term memory modules such as retrieval-augmented generation (RAG)or episodic memory bufers.Reward signals shape how knowledge is encoded, reused,ordiscarded.Forinstance,fine-tuningonpreference-labeleddatacanreinforcecertainreasoningpathsorfactual paterns,efectively consolidating them into the model's internal knowledge representation. Moreover, mechanisms like experience replay orself-reflection—where agents evaluate past outputs with learned rewardestimators-enable selective memory reinforcement, akin to dopamine-driven memory consolidation in biological systems [419]. This allows LLM agents to generalize from prior successful strategies and avoid repeating costly errors.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "记忆。LLM代理的记忆涵盖了短期上下文（例如，聊天记录）和长期记忆模块，如检索增强生成（RAG）或叙事性记忆缓冲区。奖励信号塑造了知识如何被编码、重复使用或丢弃。例如，对偏好标记数据的微调可以强化某些推理路径或事实模式，有效地将它们巩固到模型的内部知识表示中。此外，诸如经验重放或自我反思之类的机制——代理评估以学习的奖励估计器为基础的过去输出——使得选择性记忆强化成为可能，类似于生物系统中由多巴胺驱动的记忆巩固【419】。这使得LLM代理能够从先前成功的策略中泛化，并避免重复昂贵的错误。"
                },
                {
                  "type": "text",
                  "content": "In general,reward in LLM-based agents is nota passive scalar signal butan active agent of behavioral shaping.It modulatesatentiontopromote salientfeatures,guidesstylisticandaffectiveexpressiontoalignwithhuman preferences, and structures memory to prioritize useful knowledge.As agents evolve toward greater autonomy and interactivity, understanding these cross-modulereward interactions willbeessentialforbuilding systems that are notonlyintelligent, but also interpretable, controllable, and aligned with human values.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "一般而言，LLM代理中的奖励并非是一种被动的标量信号，而是行为塑造的主动因素。它调节注意力以促进显著特征，引导风格和情感表达以与人类偏好一致，并结构化记忆以优先考虑有用的知识。随着代理向更大的自治性和互动性发展，理解这些跨模块奖励交互将对构建系统至关重要，这些系统不仅智能，而且可解释、可控，并与人类价值观保持一致。"
                }
              ],
              "raw_title": "Interaction with Other Modules",
              "type": null,
              "children": [],
              "translated_title": "5.4.1 与其他模块的交互"
            },
            {
              "title": "5.4.2 Challenges and Directions",
              "number": "5.4.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Although extensive research has been conducted on various reward mechanisms,severalpersistent challenges remain. One fundamental issue is reward sparsity and delay. In manyreal-world scenarios,reward signals are often infrequent anddelayed,making itdiffcultforanagenttoaccuratelyatributecedit tospecificactions.This,intu,incrasesthe complexity of exploration and slows down the learning process.\nAnother significant challnge is the potential for reward hacking.Agents,in their pursuit of maximizing rewards, sometimes exploit unintended loopholes in the reward function. This can lead to behaviors that diverge from the intendeddesign goals,particularly in complex environments where optimization objectives may not always align with the true task requirements.\nMoreover,the processof reward shaping presents adelicatebalance.While shaping rewardscan accelerateleaming by guiding anagent toward desired behaviors,excessve or poorlydesigned shaping may lead tolocaloptima,trapping the agent in suboptimalbehaviors.Insomecases,itmayevenalterthefundamentalstructureoftheoriginaltask,making it difficult for the agent to generalize to other scenarios.\nMany real-world problems are inherentlymulti-objective in nature,requiring agents tobalance competing goals.Under a single reward function framework, finding theright trade-offs between these objectives remains an open problem. Ideally,ahierarchicalreward mechanismcould bedesigned to guide learning ina structured,step-by-step manner. However, constructing such mechanisms effectively is still a challenge.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "尽管对各种奖励机制进行了广泛研究，但仍存在一些持久性挑战。一个基本问题是奖励稀疏性和延迟。在许多现实场景中，奖励信号往往不经常且延迟，这使得代理很难准确地将其归因于特定的行为。这增加了探索的复杂性并减缓了学习过程的速度。另一个重要挑战是奖励欺骗的潜在可能性。代理在追求最大化奖励时，有时会利用奖励函数中意外的漏洞。这可能导致行为偏离预期的设计目标，特别是在复杂环境中，优化目标不总是与真实任务需求一致的情况下。此外，奖励塑造的过程呈现出微妙的平衡。虽然塑造奖励可以通过引导代理朝向期望的行为加速学习，但过度或设计不当的塑造可能导致局部最优解，将代理困在次优行为中。在某些情况下，甚至可能改变原始任务的基本结构，使代理难以推广到其他情景。许多现实问题本质上是多目标的，需要代理平衡竞争目标。在单一奖励函数框架下，找到这些目标之间的正确权衡仍然是一个悬而未决的问题。理想情况下，可以设计分层奖励机制以指导结构化、逐步学习。然而，有效构建这样的机制仍然是一个挑战。"
                },
                {
                  "type": "text",
                  "content": "Finally,reward misspecificationintroduces further uncertainty andlimits generalization.Often,areward function does not fullycapture the true task goal,leading to misalignment between the agent's learning objective andreal-world success.Additionally,manyrewardfunctions aretailored to specificenvironments andfailto generalize whenconditions change or tasks shift, highlighting the need for more robust reward models.\nAddressing these challenges requires novel approaches.One promising direction is to derive implicit rewards from standard examples or outcome-based evaluations, which can help mitigate reward sparsity issues.Additionally, decomposing complex tasks into hierarchical structures and designing rewards from the botom up can oer a more systmatic approach,even in multi-objective settings.Furthermore,leveraging techniques such as meta-learning and meta-reinforcementleaming can enhance the adaptability of reward models,allowing agents to transfer knowledge across tasks andperform efectively in diverseenvironments.By exploring these avenues,we can move toward more reliable and scalable reward mechanisms that better align with real-world objectives.",
                  "index": 0,
                  "part": 1,
                  "translated_content": "最后，奖励误设引入了进一步的不确定性并限制了泛化能力。通常，奖励函数并不能完全捕捉真实任务目标，导致代理的学习目标与真实世界的成功之间存在错位。此外，许多奖励函数都是针对特定环境量身定制的，当条件发生变化或任务转移时无法泛化，凸显了需要更健壮奖励模型的必要性。\n\n解决这些挑战需要新颖的方法。一个有前途的方向是从标准示例或基于结果的评估中推导出隐性奖励，有助于缓解奖励稀疏问题。此外，将复杂任务分解为层次结构，并从底层设计奖励，可以提供更系统化的方法，即使在多目标设置中也是如此。此外，利用元学习和元强化学习等技术可以增强奖励模型的适应性，使代理能够在任务之间传递知识，并在不同环境中有效执行。通过探索这些途径，我们可以朝着更可靠和可扩展的奖励机制迈进，更好地与真实世界的目标相一致。"
                }
              ],
              "raw_title": "Challenges and Directions",
              "type": null,
              "children": [],
              "translated_title": "5.4.2 挑战与方向"
            }
          ],
          "translated_title": "5.4 总结与讨论"
        }
      ],
      "translated_title": "5 奖励 63"
    },
    {
      "title": "6 Emotion Modeling 71",
      "number": "6",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "6.1 Psychological Foundations of Emotion 71\n6.2 Incorporating Emotions in AI Agents . 74\n6.3 Understanding Human Emotions through AI . 74\n6.4 Analyzing AI Emotions and Personality 74\n6.5 Manipulating AI Emotional Responses . 75\n6.6 Summary and Discussion 75",
          "index": 0,
          "part": 0,
          "translated_content": "6.1 情感的心理基础 71\n6.2 在人工智能代理中融入情感 74\n6.3 通过人工智能理解人类情感 74\n6.4 分析人工智能的情感和个性 74\n6.5 操控人工智能的情感反应 75\n6.6 总结与讨论 75"
        }
      ],
      "raw_title": "Emotion Modeling 71",
      "type": null,
      "children": [
        {
          "title": "6.1 Psychological Foundations of Emotion",
          "number": "6.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Psychological and neuroscientific theories of emotion provide essential frameworks for developing emotionally intelligent LLM agents.These theories can be categorized into several major approaches,each offering unique perspectives on how emotions function and how they might be implemented in AI systems.\n\nCategoricalTheories.These models posit that emotions exist as discrete,universalcategories with distinct physiological and behavioral signatures.Ekman's theory of basic emotions [421]identifies six fundamental emotions (anger, disgust,fear, happiness,sadness, and surprise that are recognized across cultures and expressed through specific facial configurations.This discrete approach has significantly infuenced affective computing, with many emotion classification systems in AI adopting these labels for training [422, 423]. For LLMagents,categorical frameworks provide clear taxonomies for classifying user emotions and generating appropriate responses. However, they face criticismforoversimplifying the complex,blended nature of human emotional experience[424]and may not capture cultural variations in emotional expression [425].",
              "index": 0,
              "part": 0,
              "translated_content": "情感的心理学和神经科学理论为开发具有情感智能的LLM代理提供了重要框架。这些理论可以分为几种主要方法，每种方法都提供了独特的视角，说明情感的功能以及它们如何在人工智能系统中实现。\n\n分类理论。这些模型认为情感存在为离散的、普遍的类别，具有明显的生理和行为特征。埃克曼的基本情感理论确定了六种基本情感（愤怒、厌恶、恐惧、快乐、悲伤和惊讶），这些情感在各种文化中得到认可，并通过特定的面部表情表达。这种离散方法在情感计算领域产生了重大影响，许多情感分类系统在AI中采用这些标签进行训练。对于LLM代理，分类框架为对用户情感进行分类和生成适当回应提供了清晰的分类法。然而，它们受到批评，因为它们过于简化了人类情感体验的复杂、混合性质，并且可能无法捕捉情感表达中的文化差异。"
            },
            {
              "type": "text",
              "content": "Dimensional Models. Rather than discrete categories, dimensional approaches represent emotions as points in a continuous space defined by fundamental dimensions. Russells Circumplex Model[426] maps emotions onto two primary dimensions:valence (pleasure-displeasure)and arousal(activation-deactivation).This framework enables more nuanced trackingof emotionalstates.It distinguishes between high-arousal panic and low-arousalanxiety despite both having negative valence.The PAD (Pleasure-Arousal-Dominance) model[427] extends this by adding a dominance dimension,capturing the sense ofcontrolor power associated with emotionalstates.Thesecontinuous representations have proven valuable forLLMsystems that need to generate emotionally gradedresponses ortrack subtle shifts inuser affect over time[428,429,430].Dimensionalmodels allow forfine-grainedcontrolover generatedcontent,enabling humansor agents to modulate tone along continuous scalesrather than switching between discrete emotional states.",
              "index": 1,
              "part": 0,
              "translated_content": "维度模型。与离散类别不同，维度方法将情绪表示为在由基本维度定义的连续空间中的点。Russell的圆环模型将情绪映射到两个主要维度：价值（愉悦-不愉悦）和唤醒（激活-去激活）。这一框架能够更细致地跟踪情绪状态，它能够区分高唤醒的恐慌和低唤醒的焦虑，尽管两者都具有负价值。PAD（愉悦-唤醒-支配）模型通过添加一个支配维度扩展了这一概念，捕捉了与情绪状态相关的控制或权力感。这些连续表示对需要生成情感分级响应或随时间跟踪用户情感细微变化的LLM系统非常有价值。维度模型允许对生成的内容进行精细的控制，使人类或代理能够沿着连续的尺度调节语气，而不是在离散的情绪状态之间切换。"
            },
            {
              "type": "text",
              "content": "Hybrid and Componential Frameworks. Recognizing limitations in pure categorical or dimensional approaches, severaltheories integrate aspects of both.Plutchik's Wheel of Emotions[431] arranges eight primary emotions in a wheel structure with intensity gradients and dimensional properties, allowing for the representation of complex emotional blends (e.g.,love as a mixture of joyand trust).Meanwhile,componential models like Scherer's Component Process Model (CPM)[432]conceptualize emotions as emerging from synchronized components including cognitive appraisal,physiologicalarousalaction tendencies,andsubjectivefelings.ParticularlyinfluentialinAIreseachisthe OCC (Ortony-Clore-Collins) model[433], which defines 22 emotion types based on how events, agents, or objects are evaluatedrelative to goals and standards.These appraisal-based frameworks havebeen implementedin dialogue systems that generate emotional responses through rule-based evaluation of situations [434,435]. For LLM agents, such models provide computational structuresforevaluating text input and selectingcontextuallappropriateemotional responses, improving both coherence and perceived empathy [436, 437].",
              "index": 2,
              "part": 0,
              "translated_content": "混合和成分框架。鉴于纯粹的分类或维度方法存在局限性，一些理论融合了两者的特点。普鲁切克的情绪轮[431]将八种主要情绪排列在一个具有强度梯度和维度属性的轮状结构中，允许表示复杂的情绪混合（例如，将爱描述为喜悦和信任的混合）。同时，像谢勒的成分过程模型（CPM）[432]这样的成分模型将情绪概念化为由认知评估、生理激活、行为倾向和主观感受等同步组成部分产生。在人工智能研究中具有特殊影响力的是OCC（奥尔托尼-克洛尔-科林斯）模型[433]，该模型基于事件、代理或对象相对于目标和标准的评估定义了22种情绪类型。这些基于评估的框架已经在通过基于规则的情境评估生成情感响应的对话系统中得到应用[434,435]。对于LLM代理，这些模型提供了计算结构，用于评估文本输入并选择上下文适当的情感响应，从而提高了连贯性和感知到的共情[436, 437]。"
            },
            {
              "type": "text",
              "content": "Neurocognitive Perspectives.The neuroscience of emotion ofers additionalinsights forLLMarchitectures.Damasio's somatic marker hypothesis[25]emphasizes how emotions,implemented through body-brain interactions,guide decisionmaking by associating physiological states with anticipated outcomes.This interaction between thelimbic system and the cortexshows atwo-processarchitecture:fast“alarm\"signals in the limbic system,like those processed by the amygdala, work alongside slower, more deliberate reasoning in the cortex. Contemporary LLM systems have begun implementing analogous architectures, where fast sentiment detection modules work in parallel with more thorough chain-of-thought reasoning[436,437].Recent evidence further suggests thatopponentcircuitryinthe striatum enables distributionalreinforcement learning byencodingnot just meanrewards but entireprobability distributions,offring a neural basis foremotion-influenced decision-making under uncertainty[438].Similarly,LeDoux's distinction between “low road\"（quick, automatic)and“high road\"(slower,cognitive)fear processing [24]suggests design patterns for systems that need both immediate safetyresponses and nuancedemotional understanding.Minsky'sframing of emotions as“way to think\"[420]thatreorganize cognitive processes has influenced frameworks like EmotionPrompt[428]and Emotion-LLaMA [423], where emotional context dynamically reshapes LLM reasoning.",
              "index": 3,
              "part": 0,
              "translated_content": "神经认知视角。情绪的神经科学为LLM架构提供了额外的见解。达马西奥的体验标记假设强调了通过身体和大脑相互作用实现的情绪如何通过将生理状态与预期结果相关联来引导决策。这种边缘系统与皮层之间的相互作用展示了一个双过程架构：边缘系统中的快速“警报”信号，如杏仁核处理的信号，与皮层中更为缓慢、更深思熟虑的推理一起工作。当代LLM系统已经开始实现类似的架构，其中快速情绪检测模块与更彻底的思维链推理并行工作。最近的证据进一步表明，纹状体中的对手回路通过编码整个概率分布而不仅仅是平均奖励，为在不确定性下受情绪影响的决策制定提供了神经基础。同样，勒杜克对“低路”（快速、自动）和“高路”（较慢、认知）恐惧处理的区分提出了对需要即时安全响应和细致情绪理解的系统的设计模式。明斯基将情绪框架化为“思考方式”，重新组织认知过程的观点影响了EmotionPrompt和Emotion-LLaMA等框架，其中情绪背景动态地重塑了LLM推理。"
            },
            {
              "type": "text",
              "content": "These theoretical frameworks increasingly inform the development of emotionally intellgent LLM agents.Categorical models provide clear labels for emotionclassification tasks [423,429],while dimensional embeddings enable continuous controlover generated text[428].Hybrid approaches help systems handle mixed emotions and emotional intensity.Appraisal-based methods,particularly those derived from theOCC model,allowLLMs to evaluate narrative events oruser statementscontextualy,selecting appropriateemotionalresponses thatfosterrapport andtrust [439]. Neuroscientifically-inspired dual-process architectures combine“fast\"sentiment detection with“slow\"deliberative reasoning,enabling both quick safety responses and deeper emotional understanding [436, 437].While explicit neurocognitive mechanisms (like dedicated“amygdala-like\"pathways)remainrare in current LLM pipelines,emerging research explores biologically-inspired modules tohandle urgent emotional signals and maintain consistent emotional states across extended interactions [440, 441].",
              "index": 4,
              "part": 0,
              "translated_content": "这些理论框架越来越多地影响着情绪智能LLM代理的发展。分类模型为情绪分类任务提供了明确的标签，而维度嵌入则实现了对生成文本的连续控制。混合方法帮助系统处理混合情绪和情绪强度。基于评估的方法，特别是源自OCC模型的方法，使LLM能够在情境中评估叙述事件或用户陈述，选择适当的情绪响应，促进融洽和信任。神经科学启发的双过程架构将“快速”情绪检测与“缓慢”的深思熟虑推理相结合，使系统能够既快速做出安全响应，又深入理解情绪。虽然当前LLM管道中明确的神经认知机制（如专门的“杏仁核样”途径）仍然很少见，但新兴研究探索了受生物启发的模块，以处理紧急情绪信号，并在长时间交互中保持一致的情绪状态。"
            },
            {
              "type": "text",
              "content": "Emotionisakeypartofhuman intellgence,anditwillikelybecomeone ofthekeycomponentsordesignconsiderations of LLM agents.Onekey futuredirection is systematicallytranslating these psychologicaland neuroscience theories into an LLM agent's internal processes. Techniques for translating might include using dimensional models (e.g., valence/arousal/dominance)aslatent states that influence generation or adopting explicit rule-basedappraisals (OCC)to labeluser messages and shape the agent's subsequent moves.Hybrid approaches offer acompelling balance: an LLM could first recognizeadiscretecategory(e.g.“fear\")butalsogauge itsintensityandcontroldimensionforfinergrained conversation.Such emotion-infused architectures might yield more coherent“moods\"over time,analogous to how humans sustain afective statesrather than resetting at every turn.Explicit alignment with psychological theories also enhances interpretability:designers can debug orrefine the agent's responses bycomparing them to well-established emotion constructs, rather than dealing with opaque emergent behaviors.",
              "index": 5,
              "part": 0,
              "translated_content": "情绪是人类智能的关键组成部分，很可能会成为LLM代理的设计考虑中的一个关键组件。一个关键的未来方向是将这些心理学和神经科学理论系统地转化为LLM代理的内部流程。翻译技术可能包括将情绪维度模型（例如，valence/arousal/dominance）作为影响生成的潜在状态，或采用明确的基于规则的评估（OCC）来标记用户消息并塑造代理的后续动作。混合方法提供了一个引人注目的平衡：LLM可以首先识别出一个离散的类别（例如“恐惧”），但也可以评估其强度和控制维度，以进行更细粒度的对话。这种注入情感的架构可能会随着时间产生更连贯的“情绪”，类似于人类如何维持情感状态而不是在每个转折点都重置。与心理学理论明确对齐也增强了可解释性：设计者可以通过将代理的响应与成熟的情感构建进行比较来调试或完善代理的响应，而不是处理不透明的新兴行为。"
            },
            {
              "type": "text",
              "content": "A second directionis harnessing these theories to improveaffectionateor supportive interactions,oftenreferredto as emotional alignment.For example,circumplex or PAD-based tracking can help an LLM detect negative valence and higharousalinauser's text andrespond soothingly (e.g.,lowering arousal,ofering empathetic reappraisals). In mental health or counseling scenarios,an appraisal-informed method could let the agent validate the user's feelings and understand their situation in terms of goal incongruence or perceived blame, which helps craftresponses that convey genuine empathy.Grounding emotional outputs incognitive theories (like“relief\"if a negativeoutcome is avoided,or“gratitude\"whenauserhelpsthesystem)likewisemakes interactions feelmore naturalandethically aligned. These enhancements are particularly salient as LLMs migrate intoreal-worldapplications like customer service,elder care,and tutoring, where emotional sensitivity can improve outcomes and user well-being.By incorporating robust psychological and limbic-system insights,developers can design LLM agents that notonly reason more efectively but also provide sincere emotional support, bridging the gap between computational precision and human-centric care.",
              "index": 6,
              "part": 0,
              "translated_content": "第二个方向是利用这些理论来改进亲情或支持性互动，通常被称为情感调整。例如，圆形模型或PAD（愉悦度/唤醒度/支配度）跟踪可以帮助LLM检测用户文本中的负面愉悦度和高唤醒度，并做出抚慰性回应（例如，降低唤醒度，提供共情的重新评估）。在心理健康或咨询情境中，一种基于评估的方法可以让代理人验证用户的感受，并从目标不一致或感知责任的角度理解他们的情况，这有助于制定传达真诚同情的回应。将情感输出基于认知理论（例如，如果避免了负面结果，则为“宽慰”，或者当用户帮助系统时为“感激”）同样使互动感觉更加自然和符合道德。这些增强特别重要，因为LLM正在转向真实世界应用，如客户服务、老年护理和辅导，情感敏感性可以改善结果和用户幸福感。通过融入强大的心理和边缘系统见解，开发人员可以设计出不仅推理更有效，而且提供真诚情感支持的LLM代理，弥合计算精度与以人为本关怀之间的鸿沟。"
            },
            {
              "type": "figure",
              "src": "images/bb9199c9a3bfcc8e7419924437d827169aa6e0f1d25b80d8f48e2afcd35afbdb.jpg",
              "alt": "",
              "caption": "Figure 61: Visualization and examples of major emotion theory categories.(a) Categorical Theories: Ekman's six basic emotions [421] showing discrete emotional states. (b)Dimensional Models: Russells Circumplex[426] representing emotions as coordinates in continuous space.(c)Hybrid/Componential Frameworks: Plutchik's Wheel [431] combining intensity gradients with categorical emotions.(d) Neurocognitive Perspectives: LeDoux's AmygdalaCentered Model[24] showing dual-pathway procesing of emotional stimuli.These psychological foundations inform diferent approaches to emotion modeling in AI systems,from discrete classification to dimensionalrepresentations, appraisal-based reasoning, and multi-pathway information processing.",
              "index": 7,
              "part": 0,
              "translated_caption": "图61：主要情绪理论类别的可视化和示例。(a) 分类理论：Ekman的六种基本情绪[421]展示了离散的情绪状态。(b) 维度模型：Russell的圆环模型[426]将情绪表示为连续空间中的坐标。(c) 混合/成分框架：Plutchik的情绪轮[431]将强度梯度与分类情绪结合在一起。(d) 神经认知视角：LeDoux的杏仁核中心模型[24]展示了情绪刺激的双通路处理。这些心理基础为人工智能系统中情绪建模的不同方法提供了参考，从离散分类到维度表示、评估为基础的推理，以及多通路信息处理。"
            }
          ],
          "raw_title": "Psychological Foundations of Emotion",
          "type": null,
          "children": [],
          "translated_title": "6.1 情绪的心理基础"
        },
        {
          "title": "6.2 Incorporating Emotions in AI Agents",
          "number": "6.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "The integration ofemotionalintelligence into large language models (LLMs)hasemerged asatransformative approach to enhancingtheir performance and adaptability. Recent studies,such as those of EmotionPrompt[422],highlight how emotional stimuli embedded in prompts can significantly improve outcomes across various tasks,including a notable $10.9\\%$ improvement in generative task metrics such as truthfulness and responsibility. By influencing the attention mechanisms of LLMs,emotionall enriched prompts enrich representation layers andresultin more nuanced outputs[42].These advancements bridge AI with emotionalintelligence,offering afoundation fortraining paradigms that better simulate human cognition and decision-making,particularly in contexts requiring social reasoning and empathy.",
              "index": 0,
              "part": 0,
              "translated_content": "将情感智能整合到大型语言模型（LLMs）中已经成为增强它们性能和适应性的一种变革性方法。最近的研究，比如EmotionPrompt的研究，突出了通过嵌入在提示中的情感刺激如何显著改善各种任务的结果，包括生成任务指标的显著提高，例如真实性和责任感方面的10.9%。通过影响LLMs的注意机制，情感丰富的提示丰富了表示层，并导致更加细致的输出。这些进展将人工智能与情感智能联系起来，为更好模拟人类认知和决策的训练范式奠定了基础，特别是在需要社交推理和移情的情境中。"
            },
            {
              "type": "text",
              "content": "Multimodal approaches further elevate the impact of emotional integration.Models like Emotion-LLaMA [440] demonstrate how combining audio, visual,and textual data enables better recognition and reasoning of emotions. Using datasets such as MERR[44o],these models align multimodal inputs into shared representations,facilitating improved emotional understanding and generation.This innovation extends beyondlinguistic improvements,ofering applications in human-computer interaction and adaptive learning.Together,these methods underscore thecritical role of emotions in bridging technicalrobustness with human-centric AIdevelopment,paving the wayfor systems that are both intelligent and empathetic.",
              "index": 1,
              "part": 0,
              "translated_content": "多模态方法进一步提升了情感整合的影响。Emotion-LLaMA等模型展示了如何结合音频、视觉和文本数据实现更好的情绪识别和推理。利用诸如MERR的数据集，这些模型将多模态输入对齐到共享表示中，促进了情感理解和生成的改进。这种创新不仅扩展到了语言改进，还在人机交互和自适应学习等应用中发挥作用。总的来说，这些方法强调了情感在技术稳健性与以人为中心的人工智能发展之间的关键作用，为既具有智能又具有移情能力的系统铺平了道路。"
            }
          ],
          "raw_title": "Incorporating Emotions in AI Agents",
          "type": null,
          "children": [],
          "translated_title": "6.2 AI代理中融入情感"
        },
        {
          "title": "6.3 Understanding Human Emotions through AI",
          "number": "6.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Textual Approaches.Recent workhighlights theability of LLMs to perform detailedreasoning aboutlatent sentiment andemotion. Using step-by-stepprompting strategies,such aschain of thought reasoning,researchers enable LLMs to infer sentiment even when explicit cues are absent[436].Beyond single-turn inference,negotiation-based frameworks further refine emotional judgments byleveraging multiple LLMs thatcross-evaluateeach other's outputs,effectively mimicking amore deliberative human reasoning process[437].These techniques underscore the importance of iterative, context-aware strategies to capture subtle emotional signals from purely textual input.",
              "index": 0,
              "part": 0,
              "translated_content": "文本方法。最近的研究突出了LLMs进行关于潜在情感和情绪的详细推理的能力。使用逐步提示策略，例如思维链推理，研究人员使LLMs能够推断情感，即使明确线索不存在[436]。在单轮推理之外，基于谈判的框架通过利用相互交叉评估彼此输出的多个LLMs，进一步细化情感判断，有效地模拟更具审慎性的人类推理过程[437]。这些技术强调了迭代的、上下文感知的策略的重要性，以捕捉纯文本输入中的微妙情感信号。"
            },
            {
              "type": "text",
              "content": "Multimodal Approaches.LLMs have alsobeen extended to integrate signals from audio,video,and images.Recent efforts show howadditionalcontextualor worldknowledge can befused with visualand textual information to capture deeper affective states[442].Moreover,frameworks that convert speech signals into textual promptsdemonstrate that vocal nuances can be embedded in LLM reasoning without changing the underlying model architecture[443]. This multimodalintegration,combined with explainableapproaches,allows forricher and moretransparent representations of emotional content [444].",
              "index": 1,
              "part": 0,
              "translated_content": "多模态方法。LLMs还被扩展为整合来自音频、视频和图像的信号。最近的努力展示了如何将额外的上下文或世界知识与视觉和文本信息融合，以捕捉更深层次的情感状态。此外，将语音信号转换为文本提示的框架表明，语音细微差异可以嵌入LLM推理中，而无需改变基础模型架构。这种多模态整合，结合可解释方法，可以实现更丰富和更透明的情感内容表示。"
            },
            {
              "type": "text",
              "content": "Specialized Frameworks.Beyond generic techniques,specialized systems addresstasks in which emotion recognition requires higherlevels ofawareness of ambiguity[439],context sensitivity,and generative adaptability[445].These approaches emphasize the inherent complexity of human emotion,treating it as dynamic and probabilistic rather than strictly categorical. Using flexible LLM instruction paradigms,they offer pathways to better interpret ambiguous emotional expressions and integrate contextual cues (e.g., dialogue history), moving LLM closer to human-like emotional comprehension.",
              "index": 2,
              "part": 0,
              "translated_content": "专门化框架。在超越通用技术的基础上，专门化系统解决了情感识别需要更高层次模糊意识、上下文敏感性和生成适应性的任务。这些方法强调人类情感的固有复杂性，将其视为动态和概率性，而非严格的分类。利用灵活的LLM指导范式，它们提供了更好解释模糊情感表达和整合上下文线索（例如，对话历史）的途径，使LLM更接近类人的情感理解。"
            },
            {
              "type": "text",
              "content": "Evaluation and Benchmarks.To holistically assessthe emotional intellgence of LM,researchers have proposed various benchmark suites.Some focus on generalized emotion recognition acrossdiferent modalities and social contexts[446,447],whileotherscomparethe performance andeffciencyofmodelsofvarying sizes[448].There are also specializedbenchmarks that evaluate multilingualcapabilities[449],annotationquality[450],orempathetic dialogue systems [451].Furthermore,frameworks such as EMOBENCH[441] and MEMO-Bench [452] test nuanced emotional understanding andexpression inboth text and images,whileMERBench[453]and wide-scale evaluations[454]address standardization concerns in multimodal emotion recognition.Together,these benchmarks reveal the growing,yet tll imperfect grasp of human emotion by LLMs,highlighting ongoing challenges such as implicit sentiment detection, cultural adaptation, and context-dependent empathy [455].",
              "index": 3,
              "part": 0,
              "translated_content": "评估和基准测试。为了全面评估LM的情感智能，研究人员提出了各种基准套件。一些基准关注跨不同模态和社交背景的广义情感识别[446,447]，而其他人则比较不同规模模型的性能和效率[448]。还有一些专门的基准评估多语言能力[449]、注释质量[450]或共情对话系统[451]。此外，诸如EMOBENCH[441]和MEMO-Bench [452]之类的框架测试文本和图像中微妙的情感理解和表达，而MERBench[453]和广泛的评估[454]解决了多模态情感识别中的标准化问题。总的来说，这些基准揭示了LLM对人类情感的认识日益增长但仍不完善的情况，突出了诸如隐含情感检测、文化适应和依赖于上下文的共情等持续挑战[455]。"
            }
          ],
          "raw_title": "Understanding Human Emotions through AI",
          "type": null,
          "children": [],
          "translated_title": "6.3 通过人工智能理解人类情绪"
        },
        {
          "title": "6.4 Analyzing AI Emotions and Personality",
          "number": "6.4",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Reliability of Personality Scales for LLMs.Large language models (LLMs)show conflicting evidence when evaluated through human-centered personality tests. On one hand, some studies challnge the validity of common metrics, reporting biases such as\\*agreebias\"and inconsistentfactor structures,raising doubts about whetherthese instruments capture genuine traits [456,457]. On the other hand, systematic experiments reveal that LLMs can exhibit stable, human-like traitpatterns and evenadapt todiffrent personas under specific prompts[458,459].Yet,concerns persist about actionconsistency,alignmentof self-knowledge,andwhetherrole-playingagents truly maintain fidelityto their assigned characters [460, 461].",
              "index": 0,
              "part": 0,
              "translated_content": "大型语言模型（LLMs）在人类中心的人格测试中的可靠性。在通过人类中心的人格测试评估时，大型语言模型（LLMs）显示出矛盾的证据。一方面，一些研究挑战常见指标的有效性，报告诸如“同意偏见”等偏见以及不一致的因素结构，对这些工具是否捕捉到真实特质表示怀疑[456,457]。另一方面，系统性实验显示，LLMs可以展现稳定的、类似人类的特质模式，甚至可以根据特定提示适应不同的人设[458,459]。然而，对于行动一致性、自我认知的一致性以及角色扮演代理是否真正保持忠实于其分配的角色仍然存在疑虑[460,461]。"
            },
            {
              "type": "text",
              "content": "Psychometric Methods & Cognitive Modeling Approaches. Recent work applies rigorous psychometric testing, cognitive tasks, and population-based analyses to uncover how LLM processes and represents mental constructs [462. 463,464].Fine-tuning on human behavioral data can align models with decision patterns that mirror individual-level cognition,while population-based sampling techniques expose variability in neural responses[465,466]. By merging psychologicaltheories with advanced prompting and embedding methods,researchers illuminate latent representations of constructs like anxiety or risk-taking, showing how LLMs can approximate human reasoning across tasks.",
              "index": 1,
              "part": 0,
              "translated_content": "心理测量方法与认知建模方法。最近的研究应用严格的心理测量测试、认知任务和基于人口的分析，揭示LLM如何处理和表征心理结构[462,463,464]。在人类行为数据上进行微调可以使模型与反映个体认知的决策模式相一致，而基于人口的抽样技术则揭示了神经反应的变异性[465,466]。通过将心理理论与先进的提示和嵌入方法相结合，研究人员阐明了诸如焦虑或冒险行为这样的结构的潜在表征，展示了LLM如何在各种任务中模拟人类推理。"
            },
            {
              "type": "text",
              "content": "Emotion Modeling. Studies on LM-based emotionalintellgence reveal notable abilities to interpret nuanced affct and predict emotion-laden outcomes,often surpassing average human baselines in standard tests[423,429].However, these models do not necessarily emulate human-like emotional processes; they rely on high-dimensional pattern matching that sometimes fails under changing contexts, negative input,or conflicting cues[467,468].However, hierarchical emotion structures,coping strategies, and empathy-like behaviorscan emerge in larger-scale models, underscoring boththe promise of emotional alignment andtheethicalchallenges increating AI systems thatappear and occasionally function as affective agents.",
              "index": 2,
              "part": 0,
              "translated_content": "情感建模。基于LM的情感智能研究揭示了解释微妙情感和预测带情感结果的显著能力，通常在标准测试中超越了平均人类基线[423,429]。然而，这些模型并不一定模拟类人的情感过程；它们依赖于高维模式匹配，有时在不断变化的环境、负面输入或矛盾线索下会失败[467,468]。然而，在更大规模的模型中，情感层次结构、应对策略和类似于移情行为的特征可能会出现，突显了情感调整的潜力，以及在创造出看似并且偶尔作为情感代理的AI系统时所面临的伦理挑战。"
            }
          ],
          "raw_title": "Analyzing AI Emotions and Personality",
          "type": null,
          "children": [],
          "translated_title": "6.4 分析人工智能的情感和个性"
        },
        {
          "title": "6.5 Manipulating AI Emotional Responses",
          "number": "6.5",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Prompt-based Methods. Recent research shows that adopting specific personas or roles through well-engineered prompts can bias LLM cognition, alowing targeted emotional or personality outcomes [469, 470, 471,472].By inserting instructions suchas“If you were a[persona]\",LLMs adapt not only their thematicstyle,but also their underlying emotional stance.This approach is powerfulforreal-time manipulation,though itcanbe inconsistent across tasks and model variants, highlighting the need for more systematic methods.",
              "index": 0,
              "part": 0,
              "translated_content": "基于提示的方法。最近的研究表明，通过精心设计的提示采用特定角色或人设可以偏向LLM认知，从而实现针对性的情感或个性结果。通过插入“If you were a[persona]”等指令，LLMs不仅调整其主题风格，还调整其潜在的情感立场。这种方法对于实时操纵非常有效，尽管在不同任务和模型变体之间可能不一致，突显了对更系统化方法的需求。"
            },
            {
              "type": "text",
              "content": "Training-based Methods.Fine-tuning and parameter-effcient strategies offer deeper, more stable ways to induce or alter LLM emotions [473,428, 474]. Quantized Low-Rank Adaptation (QLoRA)and specialized datasets can embed nuanced traits suchas theBig Fiveor MBTI profiles directly into the model's learnedweights.These methods enable LLMs to spontaneously exhibit trait-specific behaviors (including emoji use)and sustain their emotional states over longer dialogues, while also offering interpretability through neuron-level activation patterns.",
              "index": 1,
              "part": 0,
              "translated_content": "基于训练的方法。微调和参数高效策略提供了更深入、更稳定的方式来诱导或改变LLM的情绪[473, 428, 474]。量化低秩调整（QLoRA）和专门的数据集可以直接将诸如大五人格或MBTI档案等微妙特征嵌入模型学习的权重中。这些方法使LLMs能够自发展现特定特质行为（包括使用表情符号），并在更长的对话中维持他们的情绪状态，同时通过神经元级别的激活模式提供可解释性。"
            },
            {
              "type": "text",
              "content": "Neuron-based Methods. A recent advance isolates personality-specific neurons and manipulates them directly to evoke or suppress emotional traits[475].Bytoggling neuronactivations pinpointed through psychologicall grounded benchmarks (e.g.,PersonalityBench), LLMs can embody targeted emotional dimensions without retraining the entire network.This neuron-centric approach provides fine-grained,dynamiccontrolover modelbehaviors,representing a leap in precision and efficiency for emotional manipulation in LLMs.",
              "index": 2,
              "part": 0,
              "translated_content": "基于神经元的方法。最近的一个进展是隔离出特定于个性的神经元，并直接操纵它们以唤起或抑制情绪特征。通过切换通过心理学基准（例如人格基准）确定的神经元激活，LLMs可以在不重新训练整个网络的情况下具有特定的情绪维度。这种以神经元为中心的方法提供了对模型行为的精细、动态控制，代表了在LLMs中情绪操纵的精确性和效率方面的飞跃。"
            }
          ],
          "raw_title": "Manipulating AI Emotional Responses",
          "type": null,
          "children": [],
          "translated_title": "6.5 操控人工智能情感反应"
        },
        {
          "title": "6.6 Summary and Discussion",
          "number": "6.6",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Manipulation and Privacy Concerns.The rapidadoption of Emotional Al in advertising and politics raises significant manipulation and privacy risks [476, 477].Emotional AI often collcts sensitive biometric data,such as facial expressions and voice tones,to infer emotional states,enabling targeted advertising or politicalinfluence.However, these systemscan exploit human emotions for profit or political gain, infringing on fundamentalrights andfostering over-surveillance in public spaces[478,477]. Regulatory frameworks like GDPR and the EU AI Act arecritical to mitigating these risks responsibly.",
              "index": 0,
              "part": 0,
              "translated_content": "情绪操纵和隐私问题。情感人工智能在广告和政治中的迅速采用引发了重大的操纵和隐私风险[476, 477]。情感人工智能通常收集敏感的生物特征数据，如面部表情和语调，推断情绪状态，从而实现定向广告或政治影响。然而，这些系统可能利用人类情绪谋取利润或政治收益，侵犯基本权利，并在公共空间中促进过度监视[478, 477]。《通用数据保护条例》（GDPR）和欧盟人工智能法案等监管框架对于负责任地缓解这些风险至关重要。"
            },
            {
              "type": "text",
              "content": "Alignment Issues.Emotional AI'scapacity todetect and interpret emotions isoften misaligned with intendedoutcomes, leading to inaccuracies and biases.Anxiety-inducing prompts,for instance,have been shown toexacerbate biases in large language models (LLMs),affcting outputs inhigh-stakes domains such as healthcare and education[479,480]. Misinterpretationofemotionalcues byAIsystems,as seen in workplace applications,can exacerbate discrimination and power imbalances [481].Techniques like reinforcement learning from human feedback (RLHF)have proven effective in mitigating these issues but require further development to ensure robust alignment in diverse contexts[479,423].",
              "index": 1,
              "part": 0,
              "translated_content": "对齐问题。情感人工智能（Emotional AI）的能力检测和解释情绪常常与预期结果不一致，导致不准确性和偏见。例如，引发焦虑的提示已被证明会加剧大型语言模型（LLMs）中的偏见，影响医疗保健和教育等高风险领域的输出。AI系统对情绪线索的误解，如在工作场景应用中所见，可能加剧歧视和权力失衡。强化学习从人类反馈中学习（RLHF）等技术已被证明在减轻这些问题方面有效，但需要进一步发展以确保在不同环境中的稳健对齐。"
            },
            {
              "type": "text",
              "content": "Ethical Implications.Trust andacceptance ofAIsystems are significantlyinfluencedbytheirabilitytoexhibit empathy and maintain socially appropriate behavior[482, 483].However, the commodification of emotions in workplace management and customer service has raised concerns about ethicallabor practices and Al-human relationships[481]. Moreover,Emotional AI's reliance on anthropomorphic characteristics without suffcient empathycan undermine user trust [482]. Frameworks like SafeguardGPT, which incorporate psychotherapy techniques,demonstrate promising approaches tofostering trust and aligning AI behavior with societal norms[484]. Nonetheless,challenges remain in ensuring privacy, fairness, and cultural sensitivity [484, 483].",
              "index": 2,
              "part": 0,
              "translated_content": "伦理影响。信任和接受AI系统在很大程度上受其展现共情和保持社会适当行为能力的影响[482, 483]。然而，在工作场所管理和客户服务中情感商品化引发了对伦理劳动实践和人工智能-人类关系的担忧[481]。此外，情感人工智能对拟人特征的依赖而缺乏足够的共情能力可能会破坏用户信任[482]。像SafeguardGPT这样融入心理治疗技术的框架展示了促进信任建立和使AI行为与社会规范对齐的有希望的途径[484]。然而，在确保隐私、公平和文化敏感性方面仍然存在挑战[484, 483]。"
            },
            {
              "type": "text",
              "content": "Distinguishing AI Emotional Mimicry from Human Experience. Despite advances in emotion modeling for LLM agents,afundamentaldistinction remains:these systems donot actuall“feel\"emotions as humans do butonly show human-emotion-like patterns via probabilistic modeling.While LMs can convincingly simulate emotionalresponses, recognize emotionalpatterns,and generateaffctionaloutputs,they lacktheembodied, phenomenologicalexperience that defines human emotions.This simulation-reality gapcreates both technical andethicalchallenges.Users frequently anthropomorphize AI systems that display emotion-like behaviors[482], potentiall leading to misplaced trust or expectations.Thisdistinction needs tobecarefullythought inbothresearch anddeploymentcontexts,as the perceived emotional capabilities of LLMs influence human-AI relationships,ethical frameworks, and regulatory approaches. Future work should balance enhancing LLMs'emotional intelligence while maintaining transparency about their fundamental limitations as non-sentient systems.",
              "index": 3,
              "part": 0,
              "translated_content": "区分AI情感模仿与人类体验。尽管在LLM代理的情感建模方面取得了进展，但仍存在一个基本区别：这些系统并不像人类那样实际“感受”情绪，而是仅通过概率建模展现类似人类情绪的模式。虽然LM可以令人信服地模拟情感反应、识别情感模式并生成情感输出，但它们缺乏定义人类情绪的具身化、现象学体验。这种模拟与现实之间的差距造成了技术和伦理挑战。用户经常会将展现类似情感行为的AI系统拟人化，这可能导致信任或期望的误解。在研究和部署环境中都需要仔细思考这种区别，因为LLM的被认知情感能力影响人工智能与人类关系、伦理框架和监管方法。未来的工作应在增强LLM情感智能的同时保持透明，让人们了解其作为非感知系统的基本局限性。"
            }
          ],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": [],
          "translated_title": "6.6 总结与讨论"
        }
      ],
      "translated_title": "6 情感建模 71"
    },
    {
      "title": "7Perception 77",
      "number": "7",
      "level": 1,
      "content": [],
      "raw_title": "Perception 77",
      "type": null,
      "children": [
        {
          "title": "7.1  Human versus AI Perception 77",
          "number": "7.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "7.2Types of Perception Representation 79\n7.2.1 Unimodal Models 79\n7.2.2 Cross-modal Models 80\n7.2.3 Multimodal Models . 81\n7.3 Optimizing Perception Systems 83\n7.3.1 Model-Level Enhancements 83\n7.3.2 System-Level Optimizations 84\n7.3.3External Feedback and Control . 84\n7.4 Perception Applications . 84\n7.5 Summary and Discussion 85",
              "index": 0,
              "part": 0,
              "translated_content": "7.2 感知表征的类型 79\n7.2.1 单模型 79\n7.2.2 跨模型 80\n7.2.3 多模型 81\n7.3 优化感知系统 83\n7.3.1 模型级增强 83\n7.3.2 系统级优化 84\n7.3.3 外部反馈和控制 84\n7.4 感知应用 84\n7.5 总结与讨论 85"
            }
          ],
          "raw_title": "Human versus AI Perception 77",
          "type": null,
          "children": [],
          "translated_title": "7.1  人类与人工智能的感知对比"
        },
        {
          "title": "7.1 Human versus AI Perception",
          "number": "7.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Perception is fundamental to intelligence,serving as the interface throughwhich both humans and artificial agents interactwith the world.Although humans commonly thinkof perception in terms of thefiveclassical senses-vision, hearing,taste,smellandtouchmodern neuroscienceidentifies aricher sensorylandscape.Conservatively,humans are described as havingaround10senses; more comprehensive views listapproximately 21,while some researchers propose up to 33 distinct sensory modalities[546,547].Beyond the familiar senses,humans possesssophisticated intenal perceptions,such as vestibular(balance),proprioception(awarenessof body position),thermoception (temperature), and nociception (pain), enabling nuanced interaction with their environment.",
              "index": 0,
              "part": 0,
              "translated_content": "感知是智能的基础，是人类和人工智能代理与世界互动的接口。尽管人类通常以五种经典感官——视觉、听觉、味觉、嗅觉和触觉来思考感知，现代神经科学确定了更丰富的感官景观。保守地说，人类被描述为拥有大约10种感官；更全面的观点列出了大约21种，而一些研究人员认为存在多达33种独特的感觉模式。除了熟悉的感官之外，人类还拥有复杂的内部感知，如前庭（平衡）、本体感知（意识到身体位置）、热感知（温度）和痛觉（疼痛），使其能够与环境进行微妙的互动。"
            },
            {
              "type": "text",
              "content": "Human senses are finely tuned to specific physical signals:forexample,human vision detects electromagnetic waves with wavelengths between approximately $380{-}780\\mathrm{nm}$ , whereas hearing perceives sound frequencies from about 20 $\\mathrm{Hz}$ to $20\\mathrm{kHz}$ [548]. These sensory modalities allow humans to effortlessly engage in complex tasks like language communication,objectrecognition,socialinteraction,and spatialnavigation.Additionally,humans naturally perceive continuous changes overtime,seamlessy integrating motion perception and temporalawareness,abilities essential for coordinated movement and decision-making [549].Animals in the natural world exhibit even more diverse perceptual capabilities.Birds andcertain marine organisms,for instance,utilize magnetoreception to navigate using Earth's magnetic fields, whilesharksandelectriceels exploit electroreception to sense electrical signals emitted by other organisms—abilities humans do not possess [550].",
              "index": 1,
              "part": 0,
              "translated_content": "人类的感官对特定的物理信号进行了精细调节：例如，人类视觉可以检测波长约为$380{-}780\\mathrm{nm}$之间的电磁波，而听觉可以感知大约$20\\mathrm{Hz}$到$20\\mathrm{kHz}$的声音频率。这些感觉模式使人类能够轻松地参与复杂任务，如语言交流、物体识别、社交互动和空间导航。此外，人类自然而然地感知随时间持续变化，无缝地整合运动感知和时间意识，这些能力对于协调运动和决策至关重要。自然界的动物展示了更加多样化的感知能力。例如，鸟类和某些海洋生物利用磁感知来利用地球的磁场导航，而鲨鱼和电鳗则利用电感知来感知其他生物发出的电信号——这些是人类所不具备的能力。"
            },
            {
              "type": "text",
              "content": "In contrast to biologicalperceptionartificialagents relyupon engineered sensors designed totransformenvironmental stimuli into digital signalsthat algorithms can interpret.Common sensor modalitiesforAIagents include visual sensors (camras),auditorysensors(microphones),tactilesensors,andinertialmeasurement units.AIagents typicallexcelat processing visual, auditory, and textual data,leveraging advances in deep learning and signal processing. However, certain human sensory abilities—particularly taste and smell- -remain challenging for machines to emulate accurately. For example,the advanced bio-inspired olfactorychipdeveloped by researchers[551]curentlydistinguishes around 24 diferent odors,acapability significantlylesssensitive thanthehuman olfactory system,which discriminates among more than 4,000 distinct smells [552].",
              "index": 2,
              "part": 0,
              "translated_content": "与生物感知相反，人工智能代理依赖于设计用于将环境刺激转化为算法可以解释的数字信号的工程传感器。人工智能代理的常见传感器类型包括视觉传感器（摄像头）、听觉传感器（麦克风）、触觉传感器和惯性测量单元。人工智能代理通常擅长处理视觉、听觉和文本数据，利用深度学习和信号处理的进展。然而，某些人类感知能力——尤其是味觉和嗅觉——对机器准确模拟仍然具有挑战性。例如，研究人员开发的先进生物启发嗅觉芯片目前可以区分大约24种不同的气味，这一能力明显低于人类嗅觉系统，后者可以区分超过4,000种不同的气味。"
            },
            {
              "type": "figure",
              "src": "images/2afbdcea632e567b2c3d8f6b2c6a3a30ae589780255adc29af8f74fd6878167a.jpg",
              "alt": "",
              "caption": "Figure 7.1: Ilustrative Taxonomy of Perception System",
              "index": 3,
              "part": 0,
              "translated_caption": "图7.1：感知系统示意分类"
            },
            {
              "type": "text",
              "content": "Anothercrucial distinction lies in perceptual processing effciencyHuman perceptionislimited bybiologicalconstraints such as nerve conduction speeds,typically in therange of millseconds.Conversely,AIsystemscan process sensory inputs at speeds of microseconds or evennanoseconds,constrained primarily bycomputationalhardware performance rather than biological limitations.Neverthelesshuman perception naturally integrates information from multiple sensory modalities—known as multimodal perception-into coherent experiences effortlesslyFor AIagents, achieving this multimodalintegration requirescarefully designedfusion algorithms thatexplicitly combine inputs from diverse sensors to build unified environmental representations [553].",
              "index": 4,
              "part": 0,
              "translated_content": "另一个关键区别在于感知处理效率。人类感知受到生物限制的影响，比如神经传导速度，通常在毫秒级范围内。相反，人工智能系统可以以微秒甚至纳秒的速度处理感觉输入，主要受到计算硬件性能的限制，而不是生物限制。然而，人类感知自然地将来自多种感官模式的信息整合在一起，称为多模感知，轻松地将其融合为连贯的体验。对于人工智能代理来说，实现这种多模态整合需要精心设计的融合算法，明确地将来自不同传感器的输入结合在一起，构建统一的环境表示。"
            },
            {
              "type": "text",
              "content": "Further differences arise in the way humans and artificial agents handle temporal and spatial information. Human perceptionis inherentlycontinuousandfluid,smoothlyexperiencing the passage of timeand spatial motion without explicit temporaldiscretization.Incontrast,AIagents typicallrelyondiscrete sampling of sensordata,usingtimestamps or sequential processing tosimulate continuity.Spatialawareness inhumans effortlessy mergesvisual, auditory,and vestibular information to achieve intuitive spatial positioning.Forartificialagents,spatial perception usuallinvolves algorithmic processes such as simultaneous localization and mapping (SLAM)or 3D scene reconstruction from visual lata sequences [554].",
              "index": 5,
              "part": 0,
              "translated_content": "人类和人工智能代理处理时间和空间信息的方式存在进一步差异。人类感知是连续而流畅的，可以无需明确的时间离散化就能顺畅地体验时间流逝和空间运动。相比之下，人工智能代理通常依赖于对传感器数据的离散采样，使用时间戳或顺序处理来模拟连续性。在空间感知方面，人类能够轻松地将视觉、听觉和前庭信息融合在一起，实现直观的空间定位。而对于人工智能代理来说，空间感知通常涉及算法过程，比如同时定位与地图构建（SLAM）或从视觉数据序列中进行三维场景重建。"
            },
            {
              "type": "text",
              "content": "Physical orchemicalstimulitransmittedfrom theexternalenvironment tohuman sensoryorgans willbereceived by the sensory system(suchaseyes,ears,skin,etc.)andconverted into neural signals,whicharefinally processed bythe brain to produce perception of theenvironment.Similarly,toallow the intelligent agent toconnect withtheenvironment, it is alsocrucialtoobtain these perceptioncontents.Currently,various sensors are mainlyused toconvert electrical signals into processable digitalsignals. Inthis section,We distinguishbetween Unimodal models,Cross-modal models, and Multimodal models based on the number of modalities involved in the input and whether unified fusion modeling operations are performed.Unimodal Models specifically process and analyze data from a single modality or type of input (such as text,image,oraudio),while Cross-modal Models establish relationships and enable translations between different modalities throughdedicated mapping mechanisms, and Multimodal Models holistically integrate and process multiple modalities simultaneously to leverage complementary information for comprehensive understanding and decision-making.",
              "index": 6,
              "part": 0,
              "translated_content": "人类感知系统（如眼睛、耳朵、皮肤等）接收到从外部环境传递过来的物理或化学刺激，并将其转化为神经信号，最终由大脑加工处理以产生对环境的感知。类似地，为了使智能代理能够与环境连接，获取这些感知内容也至关重要。目前，各种传感器主要用于将电信号转换为可处理的数字信号。在本节中，我们根据输入中涉及的感知模态数量以及是否执行统一融合建模操作，区分了单模型、跨模型和多模型。单模型专门处理和分析来自单一模态或输入类型（如文本、图像或音频）的数据，而跨模型通过专用映射机制建立关系并实现不同模态之间的转换，多模型则全面整合和同时处理多个模态，以利用互补信息进行全面理解和决策制定。"
            },
            {
              "type": "figure",
              "src": "images/58c9f2130bd1717afa65171572961ef3023f844c3d87112206247236bdb67c4c.jpg",
              "alt": "",
              "caption": "Figure 7.2: Comparison of common perceptual types between human and agent.",
              "index": 7,
              "part": 0,
              "translated_caption": "图7.2：人类和代理之间常见感知类型的比较。"
            }
          ],
          "raw_title": "Human versus AI Perception",
          "type": null,
          "children": [],
          "translated_title": "7.1 人类与人工智能感知"
        },
        {
          "title": "7.2 Types of Perception Representation",
          "number": "7.2",
          "level": 2,
          "content": [],
          "raw_title": "Types of Perception Representation",
          "type": null,
          "children": [
            {
              "title": "7.2.1 Unimodal Models",
              "number": "7.2.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "When humans are in an environment,they can listen to beautiful music,look at sunrise and sunset, orexperience a wonderfulaudiovisualfeast onstage.These perception contents can be either a single image or audio,or a fusion of multiple perception contents.Regarding the types of perception input of intellgent agents, we will start with single-modal and multimodal inputs, and introduce their implementation and differences.\n\nText As an important means of communication,text carries a wealthof information,thoughts,emotions and culture. Humans indirectlybtain thecontentoftextthrough vision,hearingandtouch,which isoneofthe most important ways for humans tointeract with theenvironment.But forintelligentagents,textcandirectlyserve asabridge toconnect with the environment,taking text asdirect input andoutputting responsecontent.Inaddition to theliteral meaning, text alsocontains rich semantic information and emotional color. In the early days,the bag-of-words model[55] was usedtocount textcontentand was widely used in textclasificationscenarios,but semantic expressoncould not be obtained.BERT [485]uses a bidirectional Transformer architecture for language modeling and captures the deep semantic information of textthroughlarge-scaleunsupervised pre-training.[486,487]furtheroptimizedthe training efficiency of BERT.The autoregressive modelrepresented by GPT3.5[556] opened the prelude toLM and further unified the tasksof text understanding and text generation,whiletechnologies such as LoRA[109]greatlyreduced the application cost of LLM and improved the agent's perception ability of complex real-world scenario tasks.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "当人类置身于环境中时，他们可以聆听美妙的音乐，观赏日出和日落，或在舞台上体验精彩的视听盛宴。这些感知内容可以是单一的图像或音频，也可以是多种感知内容的融合。关于智能代理的感知输入类型，我们将从单模态和多模态输入开始，介绍它们的实现和区别。\n\n文本作为重要的交流方式，承载着丰富的信息、思想、情感和文化。人类通过视觉、听觉和触觉间接获取文本内容，这是人类与环境互动的重要方式之一。但对于智能代理，文本可以直接作为连接环境的桥梁，将文本作为直接输入，并输出响应内容。除了字面意义外，文本还包含丰富的语义信息和情感色彩。在早期，使用词袋模型来计算文本内容并广泛应用于文本分类场景，但无法获得语义表达。BERT使用双向Transformer架构进行语言建模，通过大规模无监督预训练捕获文本的深层语义信息。进一步优化了BERT的训练效率。以GPT3.5为代表的自回归模型开启了语言模型的序幕，进一步统一了文本理解和生成任务，而诸如LoRA之类的技术大大降低了LLM的应用成本，提高了代理在复杂现实场景任务中的感知能力。"
                },
                {
                  "type": "text",
                  "content": "Image Image is another important way for humans to interactwith the environment which inherently encode spatial information,encompassing crucialatributes such as morphologicalcharacteristics,spatialpositioning,dimensional relationships,and kinematic properties ofobjects.The evolution ofcomputer vision architectures hasdemonstrated significant advancement in processing these spatialatributes.The seminal ResNet architecture[488]established foundational principles fordeepvisual feature extraction,while subsequentYOLOseries[557,558]demonstrated the capability to simultaneously determine object localization and clasification with remarkable effciency.A paradigm shift occurred withthe introduction of DETR[489],which revolutionizedobject detection by implementing parallel prediction throughglobalcontextreasoning,effectivelyeliminating traditionalcomputationaloverhead assciated with non-maximum suppression and anchor point generation. More recently,DINO 1.5[490]has extended these capabilities to open-set scenariosthrougharchitecturalinnovations,enhanced backbone networks,andexpanded training paradigms, substantially improving open-set detection performance and advancing the perceptual generalization capabilities of artificial agents in unconstrained environments.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "图像是人类与环境互动的另一种重要方式，固有地编码了空间信息，包括形态特征、空间位置、维度关系和物体的运动特性等关键属性。计算机视觉架构的发展在处理这些空间属性方面取得了重大进展。开创性的ResNet架构奠定了深度视觉特征提取的基本原则，而随后的YOLO系列展示了同时确定对象定位和分类的显著效率。DETR的引入引发了范式转变，通过全局上下文推理实现并行预测，有效消除了与非极大值抑制和锚点生成相关的传统计算开销。最近，DINO 1.5通过架构创新、增强的骨干网络和扩展的训练范式，将这些能力扩展到开放场景，显著改善了开放式检测性能，并推进了人工代理在不受限制的环境中的感知泛化能力。"
                },
                {
                  "type": "text",
                  "content": "Video Video is an expression ofcontinuous image frames, which includes the time dimension and displays dynamic information that changes over time throughcontinuous image frames.The intelligent agent uses video as input and obtains richer perceptualcontent through continuous frames. ViViT[49] extracts spatiotemporal markers from videos, effctively decomposing the spatial and temporal dimensions of the input.VideoMAE [492] learns general video feature representations through self-supervised pre-training and hasstrong generalization capabilitiesonout-of-domain data. It lays a solid foundation for intelligent agents to acquire perceptual capabilities in new scenarios.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "视频是连续图像帧的表达，包括时间维度，并通过连续图像帧展示随时间变化的动态信息。智能代理使用视频作为输入，并通过连续帧获取更丰富的感知内容。ViViT从视频中提取时空标记，有效地分解了输入的空间和时间维度。VideoMAE通过自监督预训练学习通用视频特征表示，并具有强大的泛化能力，适用于领域外的数据。它为智能代理在新场景中获得感知能力奠定了坚实基础。"
                },
                {
                  "type": "text",
                  "content": "Audio Inaddition totext andvision,anotherimportant wayforhumans tointeract withtheenvironment isthrough audio. Audio notonlycontains direct text content,but alsocontains the speaker's tone and emotion[559].Wav2Vec2[495] defines the contrast task by quantizing the potential representation of joint learning,achieving speech recognition effctiveness with1/100labeled data volume.FastSpeech 2[493] directly introduces voice change information (pitch, energy,duration,etc.)and uses realtargets to train the modelto achieve more realistic text-to-speech conversion. Seamless [494] generates low-latencytarget translations through streaming and using an effcient monotonic multi-head atention mechanism,while maintainingthehuman voice style,toachieve synchronousspeech-to-speech/text translation frommultiple sourcelanguages totargetlanguages.Basedonthese means,the intelligent agentcanachievethe ability to listen and speak.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "除了文本和视觉之外，人类与环境互动的另一重要方式是通过音频。音频不仅包含直接的文本内容，还包含说话者的语调和情感。Wav2Vec2通过量化联合学习的潜在表示来定义对比任务，用1/100标记数据量实现了语音识别的有效性。FastSpeech 2直接引入声音变化信息（音高、能量、持续时间等），并使用真实目标训练模型，实现更逼真的文本转语音转换。Seamless通过流式生成低延迟目标翻译，并使用高效的单调多头注意力机制，同时保持人类语音风格，实现了多源语言到目标语言的同步语音/文本翻译。基于这些手段，智能代理可以实现听和说的能力。"
                },
                {
                  "type": "text",
                  "content": "Others At present, most oftheresearchon intellgent agents focuses on the above-mentioned common sensory input types.However, just as humans have more than 20types of perception,intellgent agents have also made progress in achieving corresponding perception capabilities through other sensors. The bionic olfactory chip developed by Hong Kong University of Science and Technology[551] integrates a nanotube sensor array on a nanoporous substrate, withupto 10,oO independently addressable gas sensors on eachchip,which is similarto theconfiguration of the olfactory system of humans and other animals, andcan accurately distinguish between mixed gases and 24different odors. In terms oftaste,Tongji University[56o] combines fluorescence and phosphorescence signals to develop an intelligent taste sensor with multi-mode lightresponse,whichcanefectively identify umami,sourness and biterness In orderto achieve human-like perception and grasping capabilities,New York University[561]launcheda low-cost magnetic tactile sensor AnySkin, which can be quickly asembled and replaced. Even in the perception of pain, the Chinese Academy of Sciences uses the unique electrical properties of liquid metal particle films whenthey are “injured\"(mechanically scratched)to imitate the perception and positioning of“wound.\"Someother works,including HuggingGPT [152], LLaVA-Plus [500], and ViperGPT [498],integrate these single-modal perception capabilities within theframework,select and applythemaccording totask requirements,and achieve the goalof achieving more complex tasks.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "目前，大多数智能代理的研究集中在上述常见感知输入类型上。然而，就像人类有20多种感知方式一样，智能代理也通过其他传感器在实现相应感知能力方面取得了进展。香港科技大学研发的仿生嗅觉芯片整合了纳米管传感器阵列在纳米多孔基板上，每个芯片上有多达10,000个可独立寻址的气体传感器，类似于人类和其他动物嗅觉系统的配置，能够准确区分混合气体和24种不同气味。在味觉方面，同济大学结合荧光和磷光信号开发了具有多模光响应的智能味觉传感器，可以有效识别鲜味、酸味和苦味。为了实现类人感知和抓握能力，纽约大学推出了低成本磁性触觉传感器AnySkin，可以快速组装和更换。甚至在疼痛感知方面，中国科学院利用液态金属颗粒膜的独特电性能在“受伤”（机械划伤）时模拟“伤口”的感知和定位。一些其他作品，包括HuggingGPT，LLaVA-Plus和ViperGPT，在框架内整合这些单模感知能力，根据任务需求选择并应用它们，实现更复杂任务的目标。"
                }
              ],
              "raw_title": "Unimodal Models",
              "type": null,
              "children": [],
              "translated_title": "7.2.1 单峰模型"
            },
            {
              "title": "7.2.2 Cross-modal Models",
              "number": "7.2.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Text-Image Cross-modal models integrating text and images have witnessed significant advancements in recent years,leading to improved alignment, retrieval, and generation between the two modalities.These models can be categorized based ontheir primaryobjectives,includingcross-modal alignment andretrieval,text-to-image generation, and image-to-text generation.\n\nOne of the primary focuses in cross-modal research is the alignment and retrieval of text and images.CLIP [51], introduced by OpenAI in 2021,employs contrastive learming to align textual and visual representations,enabling zero-shot crossmodalretrieval and clasification. Similarly, ALIGN[501]developed by Google in the same year, leverages large-scale noisy webdata tooptimizetext-imageembedding alignment.In 2022,CyCLIP[562]introduceda cyclic consistencyloss tofurtherenhancetherobustnessofcross-modalalignment,improving thereliabilityofretreval tasks.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "文本-图像跨模态模型在近年来取得了显著进展，整合了文本和图像，促进了两种模态之间的改进对齐、检索和生成。这些模型可以根据它们的主要目标进行分类，包括跨模态对齐和检索、文本到图像生成以及图像到文本生成。\n\n跨模态研究的一个主要焦点是文本和图像的对齐和检索。OpenAI于2021年推出的CLIP [51]采用对比学习来对齐文本和视觉表示，实现了零-shot跨模态检索和分类。类似地，Google在同一年开发的ALIGN [501]利用大规模嘈杂网络数据优化文本-图像嵌入对齐。2022年，CyCLIP [562]引入循环一致性损失，进一步增强了跨模态对齐的鲁棒性，提高了检索任务的可靠性。"
                },
                {
                  "type": "text",
                  "content": "Another major areaof progressinvolves text-to-image generation, where models aim tosynthesize high-quality images based on textual descriptions. OpenAl's DALL-E series[563, 564,502],spanning from 2021 to 2023,has made substantialcontributions inthis domain, withDALL·E3offring fine-grained semanticcontrolover generated images. Stable Diffusion[565],ntroducedbyStabilityAIin2022,employsadiffusion-based generativeappoachthatsuppts open-domain text-to-image synthesis and cross-modal editing.\n\nA third significantresearch direction is image-to-text generation, where models aim to generate high-quality textual descriptions based on image inputs.Typicalrepresentative work isthe BLIP[566] and BLIP-2[567]models, introduced by Salesforce between 2022and 2023,which utilize lightweight bridging modules to enhance vision-language model integration, enabling tasks such as image captioning and question answering.\n\nText-Video The key research here involves video text alignment, generation and retrieval.VideoCLIP[504]employs a videoencoder—typically based on temporal convolution ora transformer structure—to extract sequential features from video frames.These features are subsequently aligned with textualrepresentations generated by a language encoder,facilitating robustvideo-text asociation.In the domain of text-to-video generation,Meta's Make-A-Video model [06]extends spatial-temporal dimensions using diffusion-based techniques,allowing forhigh-quality video synthesis fromtextualdescriptions.Additionally,Google'sPhenaki[505]addresses thechallenge of generating long, temporallycoherent video sequences,demonstrating significantadvancements invideo synthesis throughcross-modal learning.DeepMind's Frozen in Time[568] adopts contrastive learning for video-text matching,thereby enabling efcient cross-modalretrieval.This approach enhances thecapacity to search andretrieve relevant video segments based on textual queries,further improving the integration of vision and language understanding.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "另一个取得重大进展的主要领域涉及文本到图像生成，其中模型旨在根据文本描述合成高质量图像。OpenAI的DALL-E系列[563, 564, 502]，跨越了从2021年到2023年，在这一领域做出了实质性贡献，其中DALL·E3提供了对生成图像的细粒度语义控制。StabilityAI于2022年推出的Stable Diffusion[565]采用基于扩散的生成方法，支持开放域文本到图像合成和跨模态编辑。\n\n第三个重要的研究方向是图像到文本生成，其中模型旨在根据图像输入生成高质量的文本描述。Salesforce在2022年到2023年间推出的典型代表作品是BLIP[566]和BLIP-2[567]模型，利用轻量级桥接模块增强视觉-语言模型集成，实现诸如图像字幕和问题回答等任务。\n\n文本-视频方面的关键研究涉及视频文本对齐、生成和检索。VideoCLIP[504]采用视频编码器（通常基于时间卷积或变压器结构）从视频帧中提取序列特征。随后，这些特征与语言编码器生成的文本表示进行对齐，促进稳健的视频-文本关联。在文本到视频生成领域，Meta的Make-A-Video模型[06]利用基于扩散的技术扩展了时空维度，从文本描述中实现高质量视频合成。此外，Google的Phenaki[505]解决了生成长、时间连贯视频序列的挑战，通过跨模态学习在视频合成方面取得了显著进展。DeepMind的Frozen in Time[568]采用对比学习进行视频-文本匹配，从而实现高效的跨模态检索。这种方法增强了根据文本查询搜索和检索相关视频片段的能力，进一步提高了视觉和语言理解的整合。"
                },
                {
                  "type": "text",
                  "content": "Text-Audio Cross-modal models connecting text and audio have made significant improvements inrelated tasks such as modalrepresentation, generation, and conversion, and enhanced the perception ability under a single modality.\n\nAudioCLIP[509],ntroduced in 2021,extends the CLIP framework to theaudiodomain,enabling trimodalretrieval across audio,text, and images.By incorporating audio as an additional modality,AudioCLIP utilizes multi-task learning tounifimage,text,andaudio representations intoashared embedding space.This advancement enhances the capability of cross-modalretrievaland interaction. Ina similar vein, VATT[508]adopts a unified Transformer-based architecture to process video,audio,andtextthrough independent encoding branches.These branches are subsequently fused into a shared multimodal space,facilitating tasks such as cross-modal retrieval and multi-task learning.This design allows for greater adaptability across diverse multimodal scenarios.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "文本-音频跨模态模型在连接文本和音频方面取得了显著进展，改进了相关任务，如模态表示、生成和转换，并增强了单一模态下的感知能力。\n\n2021年推出的AudioCLIP[509]将CLIP框架扩展到音频领域，实现了跨音频、文本和图像的三模态检索。通过将音频作为额外模态，AudioCLIP利用多任务学习将图像、文本和音频表示统一到共享的嵌入空间中。这一进展增强了跨模态检索和交互的能力。类似地，VATT[508]采用统一的基于Transformer的架构，通过独立的编码分支处理视频、音频和文本。这些分支随后融合到共享的多模态空间中，促进了跨模态检索和多任务学习等任务。这种设计使得在各种多模态场景下具有更大的适应性。"
                },
                {
                  "type": "text",
                  "content": "For text-to-audio generation, Meta introduced AudioGen[569] in 2023,which enables the synthesis of audio, such as environmental sounds and music fragments,directly from textual descriptions.This modelexemplifies the growing capabilities of AIin generating high-fidelity audio based on linguistic input,expanding applications in media, entertainment, and accessibility.\n\nAdditionally,inthedomainofspeech-to-text andtext-to-speechconversion,Microsoftdeveloped SpeechT[57o].This model unifies speech and text generation,supporting both speech synthesis and recognition withina single framework. By leveraging a sharedarchitecture forthese dualfunctionalities,SpeechT5 contributes tothe seamlessintegration of speech andtextprocessing,therebyenhancingapplications inautomatedtranscription,voiceasistantsandacceibility tools.\n\nOthers In some other scenarios and domains, cross-modal modeling also plays an important role.\n\nCLIP-Forge [510] presents a novel method for generating 3D shapes from textual descriptions. By leveraging the capabilities ofContrastive Language-Image Pre-training (CLIP),this approach enables the synthesis of high-quality 3D objects conditioned on naturallanguage inputs,bridging the gapbetween textand 3D geometry.Point-E[51] extends this concept by generating 3D point clouds from text descriptions.Unlike traditional 3D reconstruction techniques, Point-Efocuseson pointcloudrepresentations,facilitating effcient and scalable3Dcontent creationwhile maintaining high fidelity to textual prompts.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "对于文本到音频生成，Meta在2023年推出了AudioGen[569]，该模型能够根据文本描述直接合成环境声音和音乐片段等音频。这一模型展示了人工智能在基于语言输入生成高保真音频方面不断增强的能力，扩大了在媒体、娱乐和无障碍领域的应用。\n\n此外，在语音到文本和文本到语音转换领域，微软开发了SpeechT[570]。该模型统一了语音和文本生成，在单一框架内支持语音合成和识别。通过利用这些双重功能的共享架构，SpeechT有助于实现语音和文本处理的无缝集成，从而增强了自动转录、语音助手和辅助工具等应用。\n\n其他情景和领域中，跨模态建模也发挥着重要作用。\n\nCLIP-Forge[510]提出了一种从文本描述生成3D形状的新方法。通过利用对比语言-图像预训练（CLIP）的能力，这种方法能够在自然语言输入的条件下合成高质量的3D物体，弥合了文本和3D几何之间的差距。Point-E[51]扩展了这一概念，通过从文本描述生成3D点云。与传统的3D重建技术不同，Point-E专注于点云表示，促进了高效可扩展的3D内容创建，同时保持对文本提示的高保真度。"
                },
                {
                  "type": "text",
                  "content": "In the field of medicalimaging,MoCoCLIP[571]introduces an approach that enhances zero-shot learning capabilities. By integrating CLIP with Momentum Contrast (MoCo),this method improves the generalization of deep learning models in medicalimaging applications,addressing thechallenges associated with limited annotated data and domain adaptation.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "在医学影像领域，MoCoCLIP[571]提出了一种增强零样本学习能力的方法。通过将CLIP与动量对比（MoCo）相结合，该方法提高了深度学习模型在医学影像应用中的泛化能力，解决了有限标注数据和领域适应性所带来的挑战。"
                }
              ],
              "raw_title": "Cross-modal Models",
              "type": null,
              "children": [],
              "translated_title": "7.2.2 跨模态模型"
            },
            {
              "title": "7.2.3 Multimodal Models",
              "number": "7.2.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The cross-modal modeldescribed above mainly aligns and maps between modalities through contrastive learning and other mthods to achieve information complementarity and conversion between modalities.Furthermore, the work of multimodalmodels focuses onhow to integrate the features of multipledata(suchasvision,text, audio,etc.)to improve the performance of the overall model.\n\nVision Language Model Vision Language Model(VLM) is broadly defined as multimodal model that can learn from images(or videos)andtext.Humans live inaworld fullofmultimodalinformation.Visualinformation(such as images and videos)andlanguage information (such as text)oftenneed to be combined tofully expressmeaning.The same is true forintellgent agents.LLaVA[513]first tried touse gpt-4to generate a multimodallanguage image instruction dataset.Through end-to-end training,alarge multimodal model was obtained and excelent multimodalchat capabilities were demonstrated. LLaVA-NeXT [513] uses dynamic high-resolution and mixed data to show amazing zero-shot capabilities even inpureEnglish modaldata, and the computational/training datacost is 0O-10OO times smaer than other methods.Emu2[516]changes thetraditional way of using image tokenizer toconvert images intodiscretetokens, and directly uses image encoders to convert images into continuous embeddings and provide them to Transformer, enhancing multimodalcontext learning capabilities.MiniGPT-v2[512] employs unique identifiers for various tasks during training.These identifiers helpthemodeldifferentiate task instructions moreefectivelyenhancing its learning effciencyforeach task.Qwen2-VL[515],DeepSeek-VL2[572]use dynamicencoding strategies on visualcomponents, aiming to process images with different resolutions and generate more efficient and accurate visualrepresentations. At the same time, DeepSeek-VL2[572]also uses the MoE model with a multi-head potential attention mechanism to compress the key-value cache into a latent vector to achieve efficient reasoning.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "上述跨模态模型主要通过对比学习和其他方法在模态之间进行对齐和映射，以实现信息的互补和转换。此外，多模态模型的工作重点是如何整合多种数据的特征（如视觉、文本、音频等）以提高整体模型的性能。\n\n视觉语言模型（VLM）被广泛定义为可以从图像（或视频）和文本中学习的多模态模型。人类生活在充满多模态信息的世界中。视觉信息（如图像和视频）和语言信息（如文本）经常需要结合起来才能充分表达含义。智能代理也是如此。LLaVA首次尝试使用gpt-4生成多模态语言图像指令数据集。通过端到端训练，获得了一个大型多模态模型，并展示了出色的多模态聊天能力。LLaVA-NeXT利用动态高分辨率和混合数据展示了惊人的零-shot能力，即使在纯英文模态数据中，计算/训练数据成本也比其他方法小0-1000倍。Emu2改变了传统的使用图像标记器将图像转换为离散标记的方式，直接使用图像编码器将图像转换为连续的嵌入，并提供给Transformer，增强了多模态上下文学习能力。MiniGPT-v2在训练过程中为各种任务使用了独特的标识符。这些标识符帮助模型更有效地区分任务指令，增强了模型对每个任务的学习效率。Qwen2-VL、DeepSeek-VL2在视觉组件上使用动态编码策略，旨在处理具有不同分辨率的图像并生成更高效和准确的视觉表示。同时，DeepSeek-VL2还使用了带有多头潜在注意机制的MoE模型，将关键-值缓存压缩为潜在向量以实现高效的推理。"
                },
                {
                  "type": "text",
                  "content": "Previous work mainly uses image fusion text for training.Video-ChatGPT [573] extends the input to video and directly uses a video adaptive visualencoder combined with LLMfor training tocapture the temporal dynamics and inter-frame consistency relationships in video data,thereby enabling open conversations about video content in a coherent manner.To solvethelack ofunifiedtokenizationfor images andvideos,Video-LLaVA[574]unifies the visual representations of image and video encoding into the language feature space, making thetwo mutually reinforcing. Similarly,Chat-UniVi[575]employs asetof dynamic visual tokens to integrate images and videos, while utilizing multi-scale representations toallow the modelto grasp bothhigh-levelsemanticconcepts and low-level visual details. Youku-mPLUG[576]has made in-depth research in specific scenarios.Based on thehigh-quality Chinese video-text pairs in the Youku videosharing platform,itenhances theabilitytounderstand overalland detailed visualsemantics and recognize scene text. Unlike the previous method that requires training,SlowFast-LLaVA [577]caneffctively capture the detailed spatial semantics and long-term temporal context in the video through atwo-stream SlowFast designwithout anyaditionalfine-tuningofthevideodataachieving thesameorevenbetterresults thanthefine-tuning method.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "以往的工作主要是利用图像融合文本进行训练。Video-ChatGPT扩展了输入到视频，并直接使用视频自适应视觉编码器与LLM结合进行训练，以捕捉视频数据中的时间动态和帧间一致性关系，从而使关于视频内容的开放式对话以连贯的方式进行。为了解决图像和视频的统一标记化的缺乏，Video-LLaVA将图像和视频编码的视觉表示统一到语言特征空间中，使二者相互加强。类似地，Chat-UniVi采用一组动态视觉标记来整合图像和视频，同时利用多尺度表示，使模型能够把握高级语义概念和低级视觉细节。Youku-mPLUG在特定场景进行了深入研究。基于优酷视频分享平台中高质量的中文视频文本对，增强了对整体和详细视觉语义的理解，并识别场景文本。与之前需要训练的方法不同，SlowFast-LLaVA通过双流SlowFast设计，在不需要额外微调视频数据的情况下有效捕捉视频中的详细空间语义和长期时间上下文，达到了与微调方法相同甚至更好的结果。"
                },
                {
                  "type": "text",
                  "content": "As the parameters of large models gradually decrease and the computing power of the end-side increases, highperformance end-side models are gaining momentum.Smart terminal devices such as mobile phones and PCs have strong demands for image visual processng,which puts forward higher multimodalrecognition effcts and reasoning performance requirements forthe deployment of AImodels on theend-side.TinyGPT-V[517]is built based on the Phi-2[578] smallbackbone combined with BLIP-2[567],only 8G video memory or CPU is needed for reasoning, and solving the computational effciency problems of LLaVA[513] and MiniGPT-4[579].MiniCPM-V[519] mainly provides powerful OCR capabilities forlong and diffcult images,and has alow hallucination rate, providing reliable percptionoutput.Megrez-3B-Omni[580] ensures that allstructural parameters are highly compatible with mainstream hardware through coordinated optimization of software and hardware. Its inference speed is up to $300\\%$ faster than that of models with the same precision, improving its adaptability to different end-side hardware.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "随着大型模型参数逐渐减少和端侧计算能力增强，高性能端侧模型正逐渐受到关注。智能终端设备如手机和个人电脑对图像视觉处理有着强烈需求，这对在端侧部署AI模型提出了更高的多模态识别效果和推理性能要求。TinyGPT-V建立在Phi-2小型骨干和BLIP-2的基础上，仅需8G视频内存或CPU进行推理，解决了LLaVA和MiniGPT-4的计算效率问题。MiniCPM-V主要为长且复杂的图像提供强大的OCR能力，并具有较低的错觉率，提供可靠的感知输出。Megrez-3B-Omni通过软硬件协同优化，确保所有结构参数与主流硬件高度兼容，推理速度比同等精度模型快高达300%，提高了其适应不同端侧硬件的能力。"
                },
                {
                  "type": "text",
                  "content": "Similarly,there are more GUI-related works focusing onautomatic task execution on mobile phones and PCs.OmniParser[520]uses popular web page and icon description datasets for fine-tuning,significantly enhancing thedetection and functional semantic expresion capabilities of icons in screenshots. GUICourse[581] and OS-ATLAS[582]also built across-platform GUIgroundingcorpus,which brought significant performance improvements inthe understanding of GUI screenshots and enriching the interactive knowledge of GUI components.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "类似地，还有更多与图形用户界面(GUI)相关的工作集中于在手机和个人电脑上自动执行任务。OmniParser利用流行的网页和图标描述数据集进行微调，显著增强了对截屏中图标的检测和功能语义表达能力。GUI课程(GUICourse)和OS-ATLAS也构建了跨平台GUI基础语料库，显著提升了对GUI截图的理解，并丰富了GUI组件的交互知识。"
                },
                {
                  "type": "text",
                  "content": "Vision Language Action Model Vision-Language-Action (VLA) model, whichtakes vision and language as inputs and generates robotic actions asoutputs,representsan importantresearch direction in the fieldofembodied intellgence. The selection of vision and language encoders in VLA models has undergone diverse development,evolving from early CNNs to Transformer architectures,and further integrating 3D vision and large language models.Early models such as CLIPort[521] used ResNet[488] to process visual inputs and combined language embeddings to generate actions,layingthefoundation for multimodalfusion.RT-1[522]introducedtheTransformer architecture,emploing EficientNet as the visualencoder and USE as the language encoder, andfused visual and language information via FiLM mechanisms, significantlyenhancing the model's generalization ability.VIMA[523]further adopted multimodal prompts,combining the ViT visual encoder andthe T5 language model to support more complex tasks.PerAct [524] innovatively used 3D point clouds as visual inputs and processed multi-view information through Perceiver IO, providing richer spatial perception for robotic manipulation. Diffusion Policy[525] combined ResNet visual encoders and Transformerlanguage models,generating actions throughdiffusion models toimprove the diversityand accuracy of action generation. SayCan [583] integrated the PaLMlanguage model with visual inputs, using the CLIP visual encoder for task decomposition. PaLM-E [526] combined the ViT visual encoder and the PaLM language model, guiding low-level action execution through text planning.MultiPLY[527]further integrated 3D information into LLMs,combining the EVA visual encoder and the LLaMA language model to provide more comprehensive planning capabilities for complex tasks.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "视觉语言行为模型(Vision Language Action Model，VLA)将视觉和语言作为输入，并生成机器人动作作为输出，代表了体现智能领域中的重要研究方向。VLA模型中视觉和语言编码器的选择经历了多样化的发展，从早期的CNNs发展到Transformer架构，并进一步整合了3D视觉和大型语言模型。早期模型如CLIPort使用ResNet处理视觉输入，并结合语言嵌入生成动作，为多模态融合奠定了基础。RT-1引入了Transformer架构，采用EfficientNet作为视觉编码器，采用USE作为语言编码器，并通过FiLM机制融合视觉和语言信息，显著增强了模型的泛化能力。VIMA进一步采用了多模态提示，将ViT视觉编码器和T5语言模型结合起来，支持更复杂的任务。PerAct创新性地将3D点云作为视觉输入，并通过Perceiver IO处理多视角信息，为机器人操作提供更丰富的空间感知。Diffusion Policy结合了ResNet视觉编码器和Transformer语言模型，通过扩散模型生成动作，以提高动作生成的多样性和准确性。SayCan将PaLM语言模型与视觉输入整合，使用CLIP视觉编码器进行任务分解。PaLM-E结合了ViT视觉编码器和PaLM语言模型，通过文本规划指导低层次动作执行。MultiPLY进一步将3D信息整合到LLMs中，将EVA视觉编码器和LLaMA语言模型结合起来，为复杂任务提供更全面的规划能力。"
                },
                {
                  "type": "text",
                  "content": "Audio Language Model Audio Language Model(ALM) uses the audio and text to build multimodal model. Speechgpt[533]built alarge-scale cross-modal speech instruction dataset SpeechInstruct and trained discrete speech representations,achieving cross-modal speech dialogue capabilities beyond expectations.LauraGPT[584],unlikethe previous sampling ofdiscrete audio tokens to represent input and output audio, proposed a novel datarepresentation thatcombines thecontinuous anddiscrete featuresofaudio,anddemonstratedexcellnt performanceonawiderangeof audio tasks throughsupervised multi-task learning.[529,585,531]convertsaudiodata into embedded representations and thenfine-tunes instructions,so that excellent performance can be achieved on various speech processing tasks through naturallanguage instructions. In order to reduce the cost of fine-tuning training, AudioFlamingo[528] quickly enhances the ability to adapt to unseen tasks through contextuallearning and retrieval based onthe audio language model.UniAudio 1.5[530]uses words orsubwords inthe text vocabulary asaudio tokens,learns these audio representations through a smallnumber of samples,and achieves cross-modaloutput without fine-tuning. In order to make the output more realistic and in line with human expectations,Qwen2-Audio[54]introducedthe DPO training method to achieve human preference alignment.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "音频语言模型(Audio Language Model，ALM)利用音频和文本构建多模态模型。Speechgpt构建了大规模跨模态语音指令数据集SpeechInstruct，并训练了离散语音表示，实现了超出预期的跨模态语音对话能力。与以往采样离散音频标记来表示输入和输出音频不同，LauraGPT提出了一种结合音频的连续和离散特征的新型数据表示，并通过监督多任务学习在各种音频任务上展现出优异性能。将音频数据转换为嵌入表示，然后微调指令，通过自然语言指令在各种语音处理任务上取得出色表现。为了降低微调训练成本，AudioFlamingo通过基于音频语言模型的上下文学习和检索快速增强适应未见任务的能力。UniAudio 1.5使用文本词汇中的单词或子词作为音频标记，通过少量样本学习这些音频表示，并实现了无需微调的跨模态输出。为了使输出更加逼真符合人类期望，Qwen2-Audio引入了DPO训练方法以实现人类偏好对齐。"
                },
                {
                  "type": "text",
                  "content": "Audio Vision Language Model Audio Vision Language Model (AVLM) ultilizes audio, vision, and text to unify multimodal models.Previously, we introduced some work on building multimodal models using information from two modalities.Inthe pursuitof AGI,theobstacletoachieving this goallies inthediversityandheterogeneityoftasksand modalities.A suitable approach is toallow more modalcapabilities tobe supported within aunifiedframework.Some closed-source work[586,587]has achieved excellentcapabilities across modalities such as textvision,and audio. ImageBind[588] implements jointembedding acrossix diferent modes (image,text, audio,depth, thermal,and IMU data).Panda-GPT[535]combines ImageBind's multi-modalencoder and Vicuna[589], showing zero-shot cross-modal performance in addition to images andtext.Similar work includes[539,539,536],which achieves alignment and training throughthe encoding information of vision,audio andtext.Multimodal models often require more resources to train, and UniVAL [538] trained a model with only $\\sim{0.25B}$ parameters based on task balance and multimodal curriculum learning,and used weight interpolation to merge multimodal models, maintaining generalization under out-of-distribution. NExT-GPT [542] connects LLM with multimodal adapters and diffrent diffusion decoders,and only trains a small number of parameters $(1\\%)$ of certain projection layers.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "音频视觉语言模型（Audio Vision Language Model，AVLM）利用音频、视觉和文本来统一多模态模型。先前，我们介绍了一些利用两种模态信息构建多模态模型的工作。在追求通用人工智能（AGI）的过程中，实现这一目标的障碍在于任务和模态的多样性和异质性。一个合适的方法是在统一框架内支持更多的模态能力。一些闭源工作[586,587]已经在文本、视觉和音频等模态之间取得了出色的能力。ImageBind[588]实现了跨六种不同模态（图像、文本、音频、深度、热像和IMU数据）的联合嵌入。Panda-GPT[535]结合了ImageBind的多模态编码器和Vicuna[589]，展示了除图像和文本外的零样本跨模态性能。类似的工作包括[539,539,536]，通过编码视觉、音频和文本信息实现了对齐和训练。多模态模型通常需要更多的资源进行训练，UniVAL [538]基于任务平衡和多模态课程学习训练了一个仅有约0.25B参数的模型，并使用权重插值来合并多模态模型，在分布外保持泛化能力。NExT-GPT [542]将语言模型连接到多模态适配器和不同的扩散解码器，仅训练了某些投影层的少量参数（1%）。"
                },
                {
                  "type": "text",
                  "content": "Other works [543,590,544,545] have achieved input-output conversion between arbitrary modalities. Unified-IO 2[543]isthe frst autoregressive multimodalmodelthatcan understand and generate images,textaudio,and actions. It tokenizes diferent modalinputs into a shared semantic space and processes them using an encoder-decoder model. AnyGPT[59o]builds the firstlarge-scale any-to-any multimodalinstruction dataset, using discreterepresentations to uniformly processvarious modal inputs.Modaverse[545] directly aligns the output oftheLLM with the input of the generative modeltosolvethe problem that previous workrelies heavilyonthealignmentofthelatent spaceof text and non-text features,avoidingthecomplexityassociated withthe alignment of latent features.CoDi-2[44]outperforms earlier domain-specific models in tasks ike topic-based image generation,visual transformation,and audio editing.",
                  "index": 7,
                  "part": 0,
                  "translated_content": "其他工作[543,590,544,545]已经实现了任意模态之间的输入输出转换。Unified-IO 2[543]是第一个能够理解和生成图像、文本、音频和动作的自回归多模态模型。它将不同的模态输入标记化为共享的语义空间，并使用编码器-解码器模型进行处理。AnyGPT[590]构建了第一个大规模任意到任意多模态指令数据集，使用离散表示来统一处理各种模态输入。Modaverse[545]直接将LLM的输出与生成模型的输入对齐，以解决先前工作严重依赖文本和非文本特征的潜在空间对齐问题，避免了与潜在特征对齐相关的复杂性。CoDi-2[544]在主题为基础的图像生成、视觉转换和音频编辑等任务中胜过了早期的领域特定模型。"
                },
                {
                  "type": "text",
                  "content": "Others Humanshave explored the2D world more than the 3D world, but 3Dcan more accurately describe the shape and texture information of objects and provide richer perceptual information. PointLLM[540] uses a point cloud encoder to express geometric and appearance features, and integrates language features for two-stage training of complex point-text instructions, achieving excellent 3Dobject description and classification capabilities.Since 3D contains richer information than 2D,it alsobrings greatertraining costs.[541,591]reduces the training cost here, and MiniGPT-3D [541] uses 2D priors from 2D-LLM to align 3D point clouds with LLMs. Modal alignment is performed inacascade manner, and query expert modules are mixed to efficiently and adaptively aggregate features, achieving efficient training with smallparameter updates.LLaVA-3D[591] connects 2D CLIP patch features with their corresponding positions in 3D space, integrates 3D Patches into 2D LMM and uses joint 2D and 3D visuallanguage command adjustment to achieve a 3.5-fold acceleration in convergence speed.",
                  "index": 8,
                  "part": 0,
                  "translated_content": "人类对二维世界的探索比对三维世界的探索更多，但三维可以更准确地描述物体的形状和纹理信息，并提供更丰富的感知信息。PointLLM使用点云编码器来表达几何和外观特征，并整合语言特征进行复杂点-文本指令的两阶段训练，实现出色的三维物体描述和分类能力。由于三维包含比二维更丰富的信息，它也带来更大的训练成本。在这里降低了训练成本，而MiniGPT-3D利用2D-LLM的二维先验将三维点云与LLM对齐。模态对齐以级联方式进行，查询专家模块混合以高效自适应地聚合特征，实现了小参数更新的高效训练。LLaVA-3D将2D CLIP补丁特征与其在三维空间中的对应位置相连，将3D补丁整合到2D LMM中，并使用联合的二维和三维视觉语言命令调整，实现了收敛速度提升3.5倍。"
                },
                {
                  "type": "text",
                  "content": "In order to enable intellgent agents to accurately perceive and manipulate unknown objects, Meta[592] developed NeuralFels technology,which combines vision and touch to continuously model unknown objects in 3D, more accurately estimate the posture and shapeofobjects in handheldoperations,and improve the accuracyof ignorant object operations by $94\\%$ ，",
                  "index": 9,
                  "part": 0,
                  "translated_content": "为了使智能代理能够准确感知和操作未知物体，Meta[592]开发了NeuralFels技术，该技术将视觉和触觉结合起来，不断对3D中的未知物体进行建模，更准确地估计手持操作中物体的姿势和形状，并将对未知物体操作的准确性提高了94%。"
                }
              ],
              "raw_title": "Multimodal Models",
              "type": null,
              "children": [],
              "translated_title": "7.2.3 多模态模型"
            }
          ],
          "translated_title": "7.2 感知表征类型"
        },
        {
          "title": "7.3 Optimizing Perception Systems",
          "number": "7.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Perception errors,including inaccuracies, misinterpretations,and“halucinations\"(generation offalse information), pose substantial challenges to the reliability and effectiveness of LLM-based agents.Optimizing perception thus requires minimizing these errors using various strategies across model, system, and external levels.",
              "index": 0,
              "part": 0,
              "translated_content": "感知误差，包括不准确、误解和“幻觉”（生成虚假信息），对基于LLM的代理的可靠性和有效性构成重大挑战。因此，优化感知需要通过跨模型、系统和外部层面采用各种策略来最小化这些错误。"
            }
          ],
          "raw_title": "Optimizing Perception Systems",
          "type": null,
          "children": [
            {
              "title": "7.3.1 Model-Level Enhancements",
              "number": "7.3.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Fine-tuning.Fine-tuning pre-trained LLMson domain-specificdata significantly improves their ability to accurately perceive and interpret relevant information. For example,fine-tuning models such as LLaVA on specific landmarks has been shown to enhance their recognition accuracy, particularly in urban navigation tasks [513,593].Moreover, techniques such as Low-Rank Adaptation (LoRA)enable more effcientfine-tuning,avoiding a substantialincrease in model complexity while stillimproving performance[109,594]. Some LLM work combined with traditional vision is also widely used.IntegratingwithYOLOS[595]onthebasis of thetheLlama-Adapter[596]architecture significantly improves the detection and positioning capability.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "微调。在特定领域数据上微调预训练的LLMs显著提高了它们准确感知和解释相关信息的能力。例如，对于特定地标进行微调的模型，如LLaVA，已被证明可以提高它们的识别准确性，特别是在城市导航任务中[513,593]。此外，诸如低秩适应（LoRA）之类的技术可以实现更高效的微调，避免模型复杂度的显著增加，同时仍然提高性能[109,594]。一些LLM工作结合传统视觉也被广泛使用。在Llama-Adapter[596]架构的基础上与YOLOS[595]集成显著提高了检测和定位能力。"
                },
                {
                  "type": "text",
                  "content": "PromptEngineering.Thedesign ofefective prompts iscrucialtoensure LLMs generate outputs that are both accurate and aligned withthe desired goals.By providing clear instructions,contextual information,and specificformatting requirements, prompt engineering minimizes misinterpretation and hallucination [597]. System prompts define the agent's role, historical prompts to providecontext from past interactions, andcustomized prompts to ensure output consistency has been shown to reduce errors significantly [597].",
                  "index": 1,
                  "part": 0,
                  "translated_content": "提示工程。设计有效提示对于确保LLMs生成准确且符合期望目标的输出至关重要。通过提供清晰的指导、上下文信息和特定的格式要求，提示工程可以最大程度地减少误解和幻觉[597]。系统提示定义了代理的角色，历史提示提供了过去互动的上下文，定制提示可确保输出的一致性，已被证明可以显著减少错误[597]。"
                },
                {
                  "type": "text",
                  "content": "Retrieval-Augmented Generation. Supplementing LLMs with external knowledge sources through retrieval mechanisms helpsground their responses infactualinformation,reducing the likelihood of hallucinations and improving the accuracy of perceived information [334].",
                  "index": 2,
                  "part": 0,
                  "translated_content": "检索增强生成。通过检索机制将LLMs与外部知识源相结合，有助于将它们的回应基于事实信息，降低幻觉的可能性，提高感知信息的准确性。"
                }
              ],
              "raw_title": "Model-Level Enhancements",
              "type": null,
              "children": [],
              "translated_title": "7.3.1 模型层级增强"
            },
            {
              "title": "7.3.2 System-Level Optimizations",
              "number": "7.3.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Anticipation-Reevaluation Mechanism. In scenarios where agents face incomplete or ambiguous information, an anticipation-revaluation mechanism can enhance robustnessFor instance,in navigation tasks, agents can anticipate goal directions based on historicaldata andreevaluate their inferences when new information becomes available[598].\n\nMulti-Agent Collaboration. In multi-agent systems,structured communication and collaboration among agents can facilitate information sharing,error correction,and consensus-building,leading toa more accuratecollective perception of the environment[599].Diffrent communication topologies,such as fullyconnected,centralized, and hierarchical structures,offer varying trade-off in terms of efficiencyandrobustness[60o].InsightSee[601refines visual information through a multi-agent framework with description,reasoning, and decision-making,effectively enhancing visualinformation processngcapabilities.Similarly,HEV[602]integrates theglobalperspective information ofmultiple agents and endows RL agents with global reasoning capabilities through cooperative perception, thereby enhancing their decision-making capabilities.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "预期-重新评估机制。在代理面临信息不完整或模糊的情况下，预期-重新评估机制可以增强鲁棒性。例如，在导航任务中，代理可以基于历史数据预期目标方向，并在新信息可用时重新评估推断。\n\n多代理协作。在多代理系统中，代理之间的结构化沟通和协作可以促进信息共享、纠错和共识建立，从而实现对环境的更准确的集体感知。不同的通信拓扑结构，如全连接、集中式和分层结构，在效率和鲁棒性方面提供不同的权衡。InsightSee通过一个包含描述、推理和决策的多代理框架，对视觉信息进行精炼，有效增强了视觉信息处理能力。类似地，HEV整合了多个代理的全局视角信息，并通过合作感知赋予RL代理全局推理能力，从而增强了它们的决策能力。"
                },
                {
                  "type": "text",
                  "content": "Agent Specialization.Assigning distinct roles andcapabilities to individual agents within a multi-agent system allows fora divisionof labor in perception, with each agentfocusing on specificaspects of theenvironment ortask.This can enhance the overall accuracy and efficiency of perception [603].",
                  "index": 1,
                  "part": 0,
                  "translated_content": "代理专业化。在多代理系统中为各个个体代理分配不同的角色和能力，可以实现感知分工，使每个代理专注于环境或任务的特定方面。这可以增强感知的整体准确性和效率。"
                }
              ],
              "raw_title": "System-Level Optimizations",
              "type": null,
              "children": [],
              "translated_title": "7.3.2 系统级优化"
            },
            {
              "title": "7.3.3 External Feedback and Control",
              "number": "7.3.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "LossAgents for Optimization.Utilizing LLMs as lossagents, allows for thedynamic adjustment of loss function weights during training [604].This enables the optimization of image processing models based on complex,potentially non-differentiable objectives, including human feedback and evaluations from specialized models. This approach essentially externalizes the optimization objective,allowing the LLMto“perceive\"and adapt tocomplexcriteria[605].\n\nHuman-in-the-Loop Systems.Incorporating human feedback and oversight can helpcorrect errors,guide the agent's learning process, and ensure alignment with human values and expectations [43].\n\nContent and Output Mediation. Before presenting LLM outputs to users,content mediation filters and refines these outputs.This helps prevent unexpected or harmfulbehaviors,ensuring alignment with user expectations and safety guidelines [606].",
                  "index": 0,
                  "part": 0,
                  "translated_content": "LossAgents用于优化。利用LLMs作为LossAgents，在训练过程中允许动态调整损失函数权重[604]。这使得基于复杂、潜在不可微分目标的图像处理模型得以优化，包括来自人类反馈和专门模型评估。这种方法本质上是外部化了优化目标，使LLM能够“感知”并适应复杂标准[605]。\n\n人在环环系统。整合人类反馈和监督可以帮助纠正错误，指导代理的学习过程，并确保与人类价值观和期望保持一致[43]。\n\n内容和输出调解。在向用户呈现LLM输出之前，内容调解会过滤和完善这些输出。这有助于防止意外或有害行为，确保与用户期望和安全准则保持一致[606]。"
                }
              ],
              "raw_title": "External Feedback and Control",
              "type": null,
              "children": [],
              "translated_title": "7.3.3 外部反馈和控制"
            }
          ],
          "translated_title": "7.3 优化感知系统"
        },
        {
          "title": "7.4 Perception Applications",
          "number": "7.4",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Theoperational efficacy of intellgent agents ispredominantly inffuenced bythree criticalfactors:modelarchitecture dimensionality,hardware infrastructure specifications,and quantization optimization methodologies.The exponential progression in model parameters—from Bert-Base's modest 110M to GPT-3's substantial 175 billion, culminating in Llama 3's unprecedented 405 billion—has correspondingly escalated processing latency from milliseconds to hundreds ofmillseconds.Hardware performance variations are particularly noteworthy; empiricalevidencewith GPT-3 demonstrates that NVIDIA H100 exhibits a $50\\%$ improvement in token processing throughput compared to A100, while RTX 4090 achieves approximately double the processing capability.",
              "index": 0,
              "part": 0,
              "translated_content": "智能代理的运行效率主要受到三个关键因素的影响：模型架构维度、硬件基础设施规格和量化优化方法。从Bert-Base的110M到GPT-3的1750亿，再到Llama 3的前所未有的4050亿，模型参数的指数级增长导致处理延迟从毫秒级增加到数百毫秒。硬件性能的变化尤为引人注目；通过GPT-3的实证证据表明，与A100相比，NVIDIA H100的标记处理吞吐量提高了50%，而RTX 4090的处理能力大约是其两倍。"
            },
            {
              "type": "text",
              "content": "Contemporary intelligent agents have penetrated diverse domains,encompassing personalassistance systems, gaming environments, Robotic Process Automation (RPA),and multimediacontent generation,predominantly leveraging visual perception as their primary input modality. In the context of procedurally generated environments like Minecraft, STEVE [6o7] demonstrates remarkable performance improvements, achieving a $1.5\\mathrm{x}$ acceleration in technology tree progression and a $2.5\\mathrm{x}$ enhancement in block search effciency through visual information processing. Steve-Eye [608] advances this paradigm through end-to-end multimodal training, addresing environmental comprehension latency through integrated visual-textual input processing.",
              "index": 1,
              "part": 0,
              "translated_content": "当代智能代理已经渗透到各种领域，包括个人辅助系统、游戏环境、机器人流程自动化（RPA）和多媒体内容生成，主要利用视觉感知作为其主要输入方式。在像Minecraft这样的程序生成环境中，STEVE展示了显著的性能提升，通过视觉信息处理实现了科技树进展加速$1.5$倍和方块搜索效率提高$2.5$倍。Steve-Eye通过端到端多模态训练推进了这一范式，通过整合视觉-文本输入处理解决了环境理解延迟的问题。"
            },
            {
              "type": "text",
              "content": "In creative content generation, AssistEditor[609] exemplifies sophisticated multi-agent collaboration,facilitating professional video editing through style-driven content understanding. Similarly, Audio-Agent [610] implements cross-modalintegration between textual/visual inputs and audio outputs,enabling comprehensive audio manipulation capabilities [611, 612, 613].\n\nMobile and desktop platforms have witnessed significant advancements in agent applications. ExACT [614] has established new state-of-the-art benchmarks in VisualWebArena [615], achieving a $33.7\\%$ Success Rate through screenshot-based exploratory learning withcaption and Set of Mask integration.SPA-Bench [616]introduces acomprehensive mobile evaluationframeworkthat authenticalyreplicates real-world complexity.M3A [617] demonstrates superior performance with a $64.0\\%$ success rate in SPA-Bench through multimodal input processing. AgentStore [618] has markedly improved OsWorld PC benchmark performance to $23.85\\%$ through enhanced visual and accessibility tree processing.",
              "index": 2,
              "part": 0,
              "translated_content": "在创意内容生成领域，AssistEditor展示了复杂的多智能体协作，通过基于风格驱动的内容理解促进专业视频编辑。类似地，Audio-Agent实现了文本/视觉输入与音频输出之间的跨模态集成，实现了全面的音频处理能力。\n\n移动和桌面平台在智能体应用方面取得了显著进展。ExACT在VisualWebArena中建立了新的最先进基准，在带有标题和掩模集成的基于截图的探索性学习中实现了$33.7\\%$的成功率。SPA-Bench引入了一个全面的移动评估框架，真实地复制了现实世界的复杂性。M3A通过多模态输入处理在SPA-Bench中表现出色，成功率达到$64.0\\%$。AgentStore通过增强视觉和可访问性树处理，显著提高了OsWorld PC基准性能，达到$23.85\\%$。"
            },
            {
              "type": "text",
              "content": "Voice interactioncapabilities[619,586] in personal AIasistants have significantlyreduced interactionfrictionwhile enhancingoperationaleffciency.The integrationofemotionalprosody invoice interactions has demonstrated increased user engagement and retention.\n\nIn embodied intelligence applications,haptic andforce feedback mechanisms have emerged ascrucial modalities for environmentalinteraction, with enhanced sensory fidelityenabling increasingly precise operational capabilities[620].",
              "index": 3,
              "part": 0,
              "translated_content": "个人AI助手中的语音交互能力[619,586]显著降低了交互摩擦，同时提高了运营效率。在语音交互中整合情感音律已经证明增加了用户参与度和留存率。\n\n在具身智能应用中，触觉和力反馈机制已经成为环境交互中至关重要的模态，增强的感官保真度使操作能力变得越来越精确[620]。"
            }
          ],
          "raw_title": "Perception Applications",
          "type": null,
          "children": [],
          "translated_title": "7.4 知觉应用"
        },
        {
          "title": "7.5 Summary and Discussion",
          "number": "7.5",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Although more and more research works[543,590] focus on building unified multimodal models to support the input and outputofmultiple perceptioncapabilities.Agent perception,acomerstone ofautonomous systems,faces significant challenges in effectively interpreting and integrating multi-modal data.Current methodologies encounter persistent issues inrepresentation learning,alignment, and fusion,which hinder thedevelopment of robust and generalizable perception systems.\n\nOne of the primary issues lies in the representation methods employed, whichoften fail to capture the intricate nuances of multi-modal dataThis shortfallis particularly evident in scenarios where high-dimensional sensory inputs require asophisticatedabstraction thatpreserves criticalsemantic information.Furthermore,thealignmentof representations presents additional diffculties.Integrating heterogeneous datatypes intoacohesive feature space is not only computationally intensive butalso prone to inconsistencies,whichcan lead to misinterpretation of ambiguous signals.Thechallenge iscompounded whenattempting tofusethesediverserepresentations,asthe processof merging features from various sources frequently results in suboptimal integration and potentialloss of vital information.",
              "index": 0,
              "part": 0,
              "translated_content": "尽管越来越多的研究作品[543, 590]专注于构建统一的多模态模型，以支持多种感知能力的输入和输出。智能体感知作为自主系统的基石，在有效解释和整合多模态数据方面面临着重大挑战。当前的方法论在表示学习、对齐和融合方面遇到持续问题，这些问题阻碍了健壮且具有泛化能力的感知系统的发展。\n\n其中一个主要问题在于所采用的表示方法，通常未能捕捉到多模态数据的复杂微妙之处。这种不足在需要复杂抽象以保留关键语义信息的高维感官输入场景中尤为明显。此外，表示的对齐也带来额外的困难。将异构数据类型整合到一个连贯的特征空间不仅计算密集，而且容易出现不一致性，这可能导致对模糊信号的错误解读。当尝试融合这些不同的表示时，挑战更加严峻，因为从各个来源合并特征的过程通常会导致集成不佳和关键信息的潜在丢失。"
            },
            {
              "type": "text",
              "content": "Future research directions should prioritize adaptive representation learning through dynamic neural architectures capable ofautomatically adjusting their structurebased on environmentalcontext and taskdemands.This could involve meta-learned parameterization or graph-based representations that explicitly modelrelationships between perceptual entities.For cross-modal alignment, self-supervised spacetime synchronization mechanisms leveraging contrastive learning principles show promise in establishing dense correspondence withoutrequiring exhaustive labeled data.The integration ofcausalinference frameworks into alignment processes[621]could furtherenhancerobustnessagainst spurious correlations. Inrepresentation fusion,hierarchicalatention mechanisms with learnable gating functions merit deeper exploration to enable context-aware integration of complementary modalityfeatures.Emerging techniques in diferentiable memory networks may provide new pathways for maintaining and updating fused representations over extended temporal horizons.",
              "index": 1,
              "part": 0,
              "translated_content": "未来的研究方向应优先考虑通过动态神经结构实现自适应表示学习，这些结构能够根据环境背景和任务需求自动调整其结构。这可能涉及元学习参数化或基于图的表示，明确地建模感知实体之间的关系。对于跨模态对齐，利用对比学习原理的自监督时空同步机制显示出在建立密集对应关系方面的潜力，而无需耗费大量标记数据。将因果推断框架整合到对齐过程中[621]，可以进一步增强对抗虚假相关性的鲁棒性。在表示融合方面，具有可学习门控函数的分层注意力机制值得深入探索，以实现对互补模态特征的上下文感知整合。可微分记忆网络中的新技术可能为在较长时间范围内维护和更新融合表示提供新途径。"
            }
          ],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": [],
          "translated_title": "7.5 总结与讨论"
        }
      ],
      "translated_title": "7知觉 77"
    },
    {
      "title": "8.1 The Human Action System 86",
      "number": "8.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "8.2 From Human Action to Agentic Action 87\n8.3 Paradigms of Agentic Action System . 88\n8.3.1 Action Space Paradigm 88\n8.3.2 Action Learning Paradigm 91\n8.3.3 Tool-Based Action Paradigm 93\n8.4 Action and Perception: “Outside-In” or“Inside-out” 95\nSummary and Discussion 97",
          "index": 0,
          "part": 0,
          "translated_content": "8.2 从人类行为到主体行为 87\n8.3 主体行为系统的范式 88\n8.3.1 行为空间范式 88\n8.3.2 行为学习范式 91\n8.3.3 基于工具的行为范式 93\n8.4 行为与感知：“自外而内”还是“自内而外” 95\n总结与讨论 97"
        }
      ],
      "raw_title": "The Human Action System 86",
      "type": null,
      "children": [],
      "translated_title": "8.1 人类行动系统 86"
    },
    {
      "title": "[Self-Evolution in Intelligent Agents 101",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Optimization Spaces and Dimensions for Self-evolution 103\n9.1Overview of Agent Optimization 103\n9.2  Prompt Optimization 103\n9.2.1 Evaluation Functions 104\n9.2.2 Optimization Functions . 104\n9.2.3 Evaluation Metrics 105\n9.3Workflow Optimization 105\n9.3.1 Workflow Formulation 105\n9.3.2 Optimizing Workflow Edges 106\n9.3.3 Optimizing Workflow Nodes 106",
          "index": 0,
          "part": 0,
          "translated_content": "自我演化的优化空间和维度 103\n9.1 代理优化概述 103\n9.2 提示优化 103\n9.2.1 评估函数 104\n9.2.2 优化函数 104\n9.2.3 评估指标 105\n9.3 工作流优化 105\n9.3.1 工作流程制定 105\n9.3.2 优化工作流边缘 106\n9.3.3 优化工作流节点 106"
        }
      ],
      "raw_title": "[Self-Evolution in Intelligent Agents 101",
      "type": null,
      "children": [],
      "translated_title": "智能体中的自我进化 101"
    },
    {
      "title": "9.4 Tool Optimization 107",
      "number": "9.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "9.4.1 Learning to Use Tools 107\n9.4.2 Creation of New Tools 107\n9.4.3 Evaluation of Tool Effectiveness 108\nTowards Autonomous Agent Optimization 110",
          "index": 0,
          "part": 0,
          "translated_content": "9.4.1 学习使用工具 107\n9.4.2 创造新工具 107\n9.4.3 评估工具有效性 108\n走向自主代理优化 110"
        }
      ],
      "raw_title": "Tool Optimization 107",
      "type": null,
      "children": [],
      "translated_title": "9.4 工具优化 107"
    },
    {
      "title": "10 Large Language Models as Optimizers",
      "number": "10",
      "level": 1,
      "content": [],
      "raw_title": "Large Language Models as Optimizers",
      "type": null,
      "children": [
        {
          "title": "10.1 Optimization Paradigms",
          "number": "10.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Traditionaloptimization methods differintheir assumptions aboutobjective function accessibility.We categorize them into three broadclasses,each with an expanding levelof input space: gradient-based optimization, which relies on explicit function gradients; zeroth-orderoptimization, which operates without gradient information; and LLM-based optimization, which extends beyond numericalfunctions tooptimize over structured and high-dimensional input spaces.\n\n· Gradient-Based Optimization. These methods assume access to gradient information and iteratively refine parameters.Techniques such as stochastic gradient descent (SGD)and Newton's method[801] are widely used but require differentiability,limitingtheir applicability todiscrete problems like prompt tuning and structured decision workflows, often endowed with a graph structure.\n·Zeroth-Order Optimization.These methods bypassthe need for explicit gradients by estimating search directions from function evaluations [802]. Examples include Bayesian optimization [803], evolutionary strategies [804], and finite-difference methods[805],which are effctive when gradients are unavailable or expensive tocompute. However,they stillrelyon well-defined numericalobjectives and structured search spaces, which constrains their applicability to language-based tasks.\n·LLM-Based Optimization. LLMs optimize broader solution spaces by leveraging natural language as both the optimization domain and feedback mechanism.By incorporating structured reasoning and human-like iteration, LLMs excelinrefining prompts, generating adaptive workflows,and iteratively improving task performance based on user feedback.",
              "index": 0,
              "part": 0,
              "translated_content": "传统优化方法在对目标函数可访问性的假设上存在差异。我们将它们分为三类，每一类在输入空间的扩展级别上都有所不同：基于梯度的优化，依赖于显式函数梯度；零阶优化，无需梯度信息操作；以及基于LLM的优化，超越了对结构化和高维输入空间的数值函数进行优化。\n\n·基于梯度的优化。这些方法假设可以访问梯度信息，并通过迭代调整参数。诸如随机梯度下降（SGD）和牛顿法等技术被广泛使用，但需要可微性，这限制了它们在离散问题（如提示调整和结构化决策工作流）中的适用性，通常赋予了图结构。\n·零阶优化。这些方法通过从函数评估中估计搜索方向来绕过对显式梯度的需求。例如，贝叶斯优化、进化策略和有限差分方法等，当梯度不可用或计算昂贵时，这些方法是有效的。然而，它们仍依赖于明确定义的数值目标和结构化搜索空间，这限制了它们在基于语言的任务中的适用性。\n·基于LLM的优化。LLMs通过利用自然语言作为优化域和反馈机制来优化更广泛的解决方案空间。通过结构化推理和类人迭代，LLMs擅长于优化提示、生成自适应工作流，并根据用户反馈逐步改进任务性能。"
            },
            {
              "type": "text",
              "content": "While gradient-based and zeroth-order methods are typically applied to numerical objectives, their core principles, such as iterative refinement, searchheuristics,and adaptivelearning,alsounderlie LLM-based optimization strategies. Building on these insights,wehighlight arapidlyemerging classof LM-based optimization powered byreinforcement learning,whichhas become the backbone of slow thinking reasoning models[90,806,89].As these modelscontinue to evolve, we anticipate them driving the next wave of agentic applications,enabling LLMs to navigate complex environments with greater adaptability and strategic foresight.",
              "index": 1,
              "part": 0,
              "translated_content": "虽然基于梯度和零阶方法通常应用于数值目标，但它们的核心原则，如迭代细化、搜索启发和自适应学习，也支撑着基于LLM的优化策略。基于这些见解，我们强调了一类快速兴起的基于强化学习的LLM优化，已成为缓慢思考推理模型的支柱。随着这些模型不断发展，我们预计它们将推动下一波主动应用的浪潮，使LLM能够在复杂环境中具有更大的适应性和战略远见。"
            }
          ],
          "raw_title": "Optimization Paradigms",
          "type": null,
          "children": [],
          "translated_title": "10.1 优化范式"
        },
        {
          "title": "10.2 Iterative Approaches to LLM Optimization",
          "number": "10.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Some LLM-based optimization methods directly draw inspiration from classcal optimization theory by adapting key components toaddress discrete and structuredchallnges.Acentralcharacteristic of these approaches is the iterative update step,in which model-generated modifications are selectedfrom arange of possible improvements to refine the objective.Using the prompt optimization objective from Equation(9.1)as arunning example,a generaliterative",
              "index": 0,
              "part": 0,
              "translated_content": "一些基于LLM的优化方法直接从经典优化理论中汲取灵感，通过调整关键组件来解决离散和结构化挑战。这些方法的一个核心特征是迭代更新步骤，在这一步骤中，从各种可能的改进中选择模型生成的修改，以完善目标。以方程（9.1）中的提示优化目标作为运行示例，一个通用的迭代"
            },
            {
              "type": "figure",
              "src": "images/a504d5e4c2a1b73d57f402c35cd0fb6d1b71c8c4a357ede99bfcec139ea91e5c.jpg",
              "alt": "",
              "caption": "Figure 10.1: A taxonomy of LLM-based optimization methods,categorized into random search, gradient approximation, and surrogate modeling.We also highlight some theoreticalexplanations of in-context learning, which includes hypothesis learning,implicit Bayesian inference, and mechanistic interpretability, which underpin the optimization capabilities of LLMs.",
              "index": 1,
              "part": 0,
              "translated_caption": "图10.1：基于LLM的优化方法分类法，分为随机搜索、梯度逼近和代理建模。我们还强调了一些关于上下文学习的理论解释，其中包括假设学习、隐式贝叶斯推断和机械性可解释性，这些理论支撑了LLM的优化能力。"
            },
            {
              "type": "text",
              "content": "algorithm can be expressed as follows:\n\nSample: $T\\sim\\mathcal{D}$ Evaluation: $\\ensuremath{\\mathcal{L}}(T;T_{p})\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}),T)$ Update: $T_{p}^{\\prime}\\gets\\phi_{\\mathrm{opt}}\\left(\\mathcal{L}(T;T_{p})\\right)$\n\nHere, the Sample and Update steps aredefined based on the agent's task.Inthe simplest case,suchasoptimizing an instruction for binary classification of movie reviews, the objective $\\mathcal{L}$ is measured by classification accuracy. In more complex agentic workflows,the decision variable may include prompts at different workflow stages,tool selections, agent topologies,ora combination thereof.As discussed in Chapter 9,a common characteristic of these decision variables is their combinatorial nature-such as the set of all strings from an LLM's vocabulary $\\nu$ or all possible role assignments foragents ina workflow.Since enumerating allpossible solutions isoften intractable, this necessitates designing approximate update steps $\\phi_{\\mathrm{opt}}$ , which we discuss next.",
              "index": 2,
              "part": 0,
              "translated_content": "算法可以表达如下：\n\n抽样：$T\\sim\\mathcal{D}$ 评估：$\\ensuremath{\\mathcal{L}}(T;T_{p})\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}),T)$ 更新：$T_{p}^{\\prime}\\gets\\phi_{\\mathrm{opt}}\\left(\\mathcal{L}(T;T_{p})\\right)$\n\n这里，抽样和更新步骤是基于agent的任务定义的。在最简单的情况下，比如优化电影评论的二元分类指令，目标函数$\\mathcal{L}$ 通过分类准确度来衡量。在更复杂的agent工作流程中，决策变量可能包括不同工作流程阶段的提示、工具选择、agent拓扑结构，或者二者的组合。正如第9章所讨论的，这些决策变量的一个共同特征是它们的组合特性，比如来自LLM词汇$\\nu$的所有字符串集合或者工作流程中agent的所有可能角色分配。由于枚举所有可能解通常是不可行的，这就需要设计近似的更新步骤$\\phi_{\\mathrm{opt}}$，接下来我们将讨论这一点。"
            },
            {
              "type": "text",
              "content": "· Random Search. Early LLM-based optimization methods leveragedrandom search variants to optimize prompts in discrete naturallanguage spaces[774,807,651,732,808,809,810].These methods often resembleevolutionary algorithms that iteratively sample candidate decision variables and select the top-performing ones from each iteration. The general formulation follows:\n\nSample: $T\\sim\\mathcal{D}$ Evaluation: $\\mathcal{L}^{(i)}\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}^{(i)}),T),\\quad i=1,\\ldots,M$ Update: {T(k)}k=1 $\\{T_{p}^{(k)\\prime}\\}_{k=1}^{K}\\leftarrow\\mathrm{ArgTopK}_{i\\in[M]}\\mathcal{L}^{(i)},$ Replenishment (Optional): $\\{T_{p}^{(j)}\\}_{j=K+1}^{M}\\sim\\mathrm{Mutate}(\\{T_{p}^{(k)}\\}_{k=1}^{K}).$\n\nWe briefly override previous notations and let $M$ denote the total number of candidate prompts sampled per iteration, and $K$ (with $K<M$ ） control the number of top-performing candidates-selected with ArgTopK in our algorithm-retained for the next step.This algorithm can optionally incorporate a replenishment step to maintain diversity in the candidate pool acrossiterations.Random search methods are simple to implement, highly paralelizable, and particularly effective for single-prompt workflows.Beyond prompt optimization,they have also demonstrated strong performance in selecting in-context demonstrations [811,812].However, their efficiency comes at a cost—each iteration requires $O(M)$ parallel API queries, which can become prohibitively expensive for complex workflows involving multiple queries.",
              "index": 3,
              "part": 0,
              "translated_content": "· 随机搜索。早期基于LLM的优化方法利用随机搜索变体来优化离散自然语言空间中的提示[774,807,651,732,808,809,810]。这些方法通常类似于进化算法，通过迭代地抽样候选决策变量，并从每次迭代中选择表现最佳的候选解。一般的表达式如下：\n\n抽样：$T\\sim\\mathcal{D}$ 评估：$\\mathcal{L}^{(i)}\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}^{(i)}),T),\\quad i=1,\\ldots,M$ 更新：$\\{T_{p}^{(k)\\prime}\\}_{k=1}^{K}\\leftarrow\\mathrm{ArgTopK}_{i\\in[M]}\\mathcal{L}^{(i)},$ 补充（可选）：$\\{T_{p}^{(j)}\\}_{j=K+1}^{M}\\sim\\mathrm{Mutate}(\\{T_{p}^{(k)}\\}_{k=1}^{K}).$\n\n我们简要修改之前的符号，令$M$表示每次迭代中抽样的候选提示总数，$K$（其中$K<M$）控制通过ArgTopK在我们的算法中选择的表现最佳候选数量，保留到下一步。该算法可以选择性地包括一个补充步骤，以在迭代过程中保持候选集的多样性。随机搜索方法易于实现，高度可并行化，并且特别适用于单提示工作流程。除了提示优化，它们还在选择上下文演示中表现出色。然而，它们的效率是有代价的——每次迭代需要$O(M)$个并行API查询，这对于涉及多个查询的复杂工作流程可能会变得昂贵。"
            },
            {
              "type": "text",
              "content": "·Gradient Approximations.Several methods approximate gradient-based updates by iterativelyrefining solutions. Forinstance,[779,730,728] generate refinements at different workflow stages. StraGO[722]estimates descent directions using central-difference heuristics, while Trace [813]optimizes composed programs by modeling them as computation graphs, similar to backpropagation. The key analogy between gradient updates in continuous optimization and prompt-space refinement is the concept of a“descent direction\"-a systematic modification of the decision variable to improve the objective.In contrast, random search methods propose new decision variables independently at each step, without accessing past update trajectories. Gradient-based approaches, by contrast,exploit this historicalinformation,often leading tofasterconvergence.A generaliteration for gradient approximation methods is given below:",
              "index": 4,
              "part": 0,
              "translated_content": "· 梯度逼近。几种方法通过迭代地改进解来近似基于梯度的更新。例如，[779,730,728]在不同的工作流程阶段生成改进。StraGO[722]利用中心差异启发式方法估计下降方向，而Trace[813]通过将其建模为计算图来优化组合程序，类似于反向传播。连续优化中梯度更新和提示空间改进之间的关键类比是“下降方向”的概念，即系统性修改决策变量以改善目标。相比之下，随机搜索方法在每一步独立提出新的决策变量，而不访问过去的更新轨迹。相反，基于梯度的方法利用这些历史信息，通常导致更快的收敛。梯度逼近方法的一般迭代如下所示："
            },
            {
              "type": "formula",
              "content": "$$ \n\\begin{array}{r}{\\mathbf{Sample:}T^{(i)}\\sim\\mathcal{D},\\quad i=1,\\dots,M\\qquad}\\\\ {\\mathbf{Evaluation:}\\mathcal{L}^{(i)}\\leftarrow\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}),T^{(i)}),\\quad i=1,\\dots,M}\\\\ {\\mathbf{Gradient\\Approximation:}g\\leftarrow\\nabla_{\\mathrm{LLM}}\\mathbf{Agg}\\left(\\mathcal{L}^{(1)},\\dots,\\mathcal{L}^{(M)}\\right)}\\\\ {\\mathbf{Update:}T_{p}^{\\prime}\\leftarrow\\phi_{\\mathrm{opt}}(T_{p},g),\\qquad}\\end{array}\n $$",
              "index": 5,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $M$ is the minibatch size, $\\operatorname{Agg}({\\mathord{\\cdot}})$ is an aggregation function that combines feedback signals (e.g., in numerical optimization, Agg is typically the average operator), $\\nabla_{\\mathrm{LLM}}$ represents an abstract“LLM-gradient operator\" [728] that generates textualrefinement directions based onthe feedback signaland thecurrnt minibatch(e.g.,the agent should consider the edge case of ...). Additionaly, $\\phi_{\\mathrm{opt}}$ can be instantiated as an LLM query, allowing the agent to update its prompt based on $g$",
              "index": 6,
              "part": 0,
              "translated_content": "这里，$M$ 是小批量大小，$\\operatorname{Agg}({\\mathord{\\cdot}})$ 是一个聚合函数，用于结合反馈信号（例如，在数值优化中，Agg 通常是平均算子），$\\nabla_{\\mathrm{LLM}}$ 表示一个抽象的“LLM-梯度算子”[728]，它基于反馈信号和当前小批量生成文本细化方向（例如，代理应考虑边缘情况...）。此外，$\\phi_{\\mathrm{opt}}$ 可以被实例化为一个LLM查询，使代理能够根据 $g$ 更新其提示。"
            },
            {
              "type": "text",
              "content": "Compared to random search methods, gradient-based approaches offer two key advantages: they enable the incorporation of past refinement directions into $\\phi_{\\mathrm{opt}}$ , analogous to momentum-based techniques in first-order optimization algorithms[814,815],andthey facilitate backpropagation-liketechniques for optimizing computation graphs [651,813,780],making them particularly effective for multi-stage workflows with interdependent optimizable modules.However, this flexibility comes at the cost of increased design overhead,such as the need for meta-prompts to aggregate feedback and apply refinement directions. We further discuss the feasibility of using LLMs to optimize these hyperparameters below.Some approaches also explored direct gradient-based optimization of soft prompts[816,817,818].While effective forsimple input-output sequence learning,these methods struggle with multi-step workflows and sequential decision-making [630, 300].",
              "index": 7,
              "part": 0,
              "translated_content": "相对于随机搜索方法，基于梯度的方法具有两个关键优势：它们能够将过去的细化方向纳入 $\\phi_{\\mathrm{opt}}$ 中，类似于一阶优化算法中的动量技术[814,815]，并且它们促进类似反向传播的技术来优化计算图[651,813,780]，使它们特别适用于具有相互依赖的可优化模块的多阶段工作流程。然而，这种灵活性是以增加设计开销为代价的，例如需要元提示来聚合反馈并应用细化方向。我们将在下文进一步讨论使用LLMs来优化这些超参数的可行性。一些方法还探讨了直接基于梯度优化软提示[816,817,818]。虽然对于简单的输入-输出序列学习有效，但这些方法在多步工作流程和顺序决策方面存在困难[630, 300]。"
            },
            {
              "type": "text",
              "content": "Finally,while these methods leverage first-order optimization insights,the extension of second-order techniques (e.g.quasi-Newton methods)to LLM-based optimization remains largely unexplored.Fortunately,recent works such as Revolve[780]have taken a step in this direction by introducing a structured approach for second-order optimization, modeling the evolution ofresponse patterns over multiple iterations.Byincorporating higher-order refinements,Revolve enables more stable and informed optimization, efectively mitigating stagnation in complex tasks.We are also excited by emerging trends in leveraging inference-time compute [90, 89] to incorporate historical refinement directions and investigate the benefits of momentum.",
              "index": 8,
              "part": 0,
              "translated_content": "最后，尽管这些方法利用了一阶优化的见解，但将二阶技术（例如拟牛顿方法）扩展到基于LLM的优化仍然是一个相对未被探索的领域。幸运的是，最近的一些工作，如Revolve，已经朝着这个方向迈出了一步，通过引入结构化方法进行二阶优化，对多次迭代中响应模式的演化进行建模。通过整合更高阶的细化，Revolve实现了更稳定和明智的优化，有效地缓解了复杂任务中的停滞现象。我们也对利用推断时计算来整合历史细化方向并探讨动量的好处的新趋势感到兴奋。"
            },
            {
              "type": "text",
              "content": "·Bayesian Optimization and Surrogate Modeling. While the aforementioned approaches achieved significant progress in LLM-based optimization,they often entail substantial financial and environmentalcosts due to the high number of required LLM interactions. Moreover, these methods can be sensitive to noise, and the optimization landscape of discrete prompts,among other decision variables,remains poorly understood[819,820].Under these constraints, Bayesian Optimization (BO)emerges as acompelling alternative, as it buildsa noise-resilient surrogate model of the optimization objective:",
              "index": 9,
              "part": 0,
              "translated_content": "·贝叶斯优化和代理模型。虽然前述方法在基于LLM的优化中取得了显著进展，但由于所需LLM交互次数较高，通常会带来相当大的财务和环境成本。此外，这些方法对噪声敏感，而离散提示的优化景观，以及其他决策变量，仍然知之甚少。在这些限制条件下，贝叶斯优化（BO）作为一个引人注目的替代方案出现，因为它构建了一个抗噪声的优化目标的代理模型："
            },
            {
              "type": "text",
              "content": "Sample: T \\~ D Proposal: $\\{T_{p}^{(i)}\\}_{i=1}^{M}\\sim S$ Propose Evaluation: $\\mathcal{L}^{(i)}\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}^{(i)}),T),\\quad i=1,\\hdots,M$ Update $:S\\gets S.\\mathrm{UpdatePrior}(\\{\\mathcal{L}^{(i)}\\}_{i=1}^{M},\\{T_{p}^{(i)}\\}_{i=1}^{M}),$ where $S$ represents a probabilistic surrogate model of the optimization objective,equipped with a proposal operator (e.g.,posterior sampling from a Gausian Process BO procedure[803])and an update mechanism based on observed evidence from prompt evaluations. For instance, MIPRO[821] employs a Tree-Structured Parzen Estimator as its surrogate[822],while PROMST[823] trains a score-prediction model to guide prompt tuning. Leveraging a surrogate model for LLM-based optimization aligns with the emerging trend ofamortized optimization for nondiffrentiable objectives [824].For instance,[825] trains a prompt-generator LLM to amortize the computational cost of instantiating a beam search problem for discovering jailbreak attack prefixes.",
              "index": 10,
              "part": 0,
              "translated_content": "样本：T \\~ D 提议：$\\{T_{p}^{(i)}\\}_{i=1}^{M}\\sim S$ 提议评估：$\\mathcal{L}^{(i)}\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}^{(i)}),T),\\quad i=1,\\hdots,M$ 更新：$S\\gets S.\\mathrm{UpdatePrior}(\\{\\mathcal{L}^{(i)}\\}_{i=1}^{M},\\{T_{p}^{(i)}\\}_{i=1}^{M}),$ 其中$S$表示优化目标的概率代理模型，配备有一个提议操作符（例如，从高斯过程BO过程中后验采样[803]）和一个基于提示评估观察证据的更新机制。例如，MIPRO[821]采用树结构帕森估计器作为其代理[822]，而PROMST[823]训练一个分数预测模型来指导提示调整。利用代理模型进行基于LLM的优化符合非可微目标的摊销优化的新兴趋势[824]。例如，[825]训练一个提示生成器LLM来摊销实例化用于发现越狱攻击前缀的波束搜索问题的计算成本。"
            },
            {
              "type": "text",
              "content": "Finaly,severalother works fit anadditional lightweight module-such as a Bayesian belief posterior or a utility function-from LLM outputs,to aid the optimization of domain-specific workflows,such as decision-making and multi-agent negotiations [826,827].This type of amortized methods-those that fit aparameterized model that is reusable for unseen inputs-have found increasing usage in LLM-based optimization,such as jailbreaking [828,825].",
              "index": 11,
              "part": 0,
              "translated_content": "最后，还有一些工作将额外的轻量级模块（例如贝叶斯信念后验或效用函数）与LLM输出相结合，以帮助优化特定领域工作流程，如决策制定和多智能体协商。这种摊销方法，即适应参数化模型以便用于未知输入的方法，在基于LLM的优化中越来越常见，例如越狱攻击。"
            }
          ],
          "raw_title": "Iterative Approaches to LLM Optimization",
          "type": null,
          "children": [],
          "translated_title": "10.2 LLM 优化的迭代方法"
        },
        {
          "title": "10.3 Optimization Hyperparameters",
          "number": "10.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Similar to traditional optimization, LLM-based methods are highlysensitive tohyperparameters that influence search effciency and generalization.A key consideration in gradient-based LLMoptimizers is thechoice of the aggregation function $\\operatorname{Agg}({\\mathord{\\cdot}})$ , which determines how textual feedback is synthesized to guide prompt updates. An improper choice can leadto loss of critical information or misalignment in iterative refinements.Additionally,[813]introduces a \"whiteboard\" approach, where an LLM program is decomposed into human-interpretable modules.However, design choices in structuring such modular workflowsremainlargely unexplored, which poses an openchallenge foroptimizing LLM-driven decision-making pipelines.",
              "index": 0,
              "part": 0,
              "translated_content": "类似于传统优化，基于LLM的方法对影响搜索效率和泛化的超参数非常敏感。在基于梯度的LLM优化器中，一个关键考虑因素是聚合函数 $\\operatorname{Agg}({\\mathord{\\cdot}})$ 的选择，该函数决定了如何综合文本反馈以指导提示更新。选择不当可能导致关键信息的丢失或在迭代改进中出现不对齐。此外，引入了一种“白板”方法，其中LLM程序被分解为人类可解释的模块。然而，在构建这种模块化工作流程方面的设计选择仍然很少被探讨，这对优化LLM驱动的决策流程构成了一个开放性挑战。"
            },
            {
              "type": "text",
              "content": "Hyperparameters in LLMoptimization often parallel those in numerical optimization.For example,batch size plays a crucialrole: justas minibatch updates enhance stability and effciency inclasscaloptimization,LLM-based approaches like TextGrad[728]aggregate feedback across multiple generated samples before making updates.Another key factor is momentum-while it stabilizes updates in gradient-based methods by incorporating past gradients, LLM-based optimizers similarlyleverage historicalrefinements to improve performance over time[728,813].Despite progressin numerical optimization,hyperparameter selection for LLM-based optimizers remains largelyheuristic,often relying on ad hoc, trial-and-error tuning.",
              "index": 1,
              "part": 0,
              "translated_content": "LLM 优化中的超参数通常与数值优化中的超参数相似。例如，批量大小起着至关重要的作用：就像小批量更新在经典优化中增强了稳定性和效率一样，基于LLM的方法（如TextGrad）在进行更新之前会在多个生成的样本中聚合反馈信息。另一个关键因素是动量-它通过结合过去的梯度来稳定梯度方法中的更新，在LLM优化器中同样利用历史的改进来随时间提高性能。尽管在数值优化方面取得了进展，针对基于LLM的优化器的超参数选择仍然主要是启发式的，通常依赖于临时的试错调整。"
            },
            {
              "type": "text",
              "content": "In agentic system design,hyperparameters proliferate across various components,including role assignments of agents, selection of in-context demonstrations,and scheduling of tool invocations.Each of these choices has a profound impact on downstream performance, yet principled methods for optimizing them remain underdeveloped. While traditionalhyperparameter tuning techniques,such as grid searchandBayesian optimization,can be appliedtodiscrete LLM-driven workflows,their computational cost scales poorly due to the high variance in language model outputs. Additionally,thecombinatorialnatureofthesehyperparameters,where agent configurations,prompting strategiesand reasoning structures interact in complex ways, makes an exhaustive search infeasible.Recent work has atempted to address thischallenge byembedding agenticworkflows into structuredframeworks such as finitestate machines [729], optimaldecisiontheory[826],andgametheory[827].However,these approachesoftenfailto generalize acrossdiverse environments. A promising direction for addressing these challenges is meta-optimization, where LLMs are used to optimize their own hyperparameters and decision-making strategies.For example, an LLM-based optimizer can iterativelyrefine itsown prompting strategies bytreatingpast decisions asexperience,akintolearnedoptimizers in deep learning [829].Moreover, amortized approaches train auxiliary models to predict effective hyperparameters, which canreduce thecomputationalcost ofexhaustive search[821,823].While thesetechniques offerexciting possibilities, they also introduce newchallnges,such as balancing exploration with exploitation in adaptivetuning and ensuring generalization across diverse optimization tasks.Investigating principled meta-optimization strategies tailored to LLM-driven workflows remains a critical area for future research.",
              "index": 2,
              "part": 0,
              "translated_content": "在代理系统设计中，超参数在各个组件中广泛存在，包括代理的角色分配、上下文演示的选择以及工具调用的调度。这些选择对下游性能产生深远影响，然而，针对它们的优化方法仍未得到系统发展。虽然传统的超参数调整技术，如网格搜索和贝叶斯优化，可以应用于离散的基于LLM的工作流程，但由于语言模型输出的高方差，它们的计算成本呈现不良的扩展性。此外，这些超参数的组合性质，即代理配置、提示策略和推理结构以复杂方式相互作用，使得穷举搜索变得不可行。最近的研究尝试通过将代理工作流嵌入到结构化框架中，如有限状态机、最优决策理论和博弈论，来解决这一挑战。然而，这些方法通常难以在不同环境中推广。解决这些挑战的一个有前途的方向是元优化，其中LLM被用于优化其自身的超参数和决策策略。例如，基于LLM的优化器可以通过将过去的决策视为经验来迭代地优化其提示策略，类似于深度学习中的学习优化器。此外，摊销方法训练辅助模型来预测有效的超参数，可以降低穷举搜索的计算成本。虽然这些技术提供了令人兴奋的可能性，但也引入了新的挑战，比如在自适应调整中平衡探索与利用，并确保在各种优化任务中实现泛化。研究针对LLM驱动工作流程量身定制的有原则的元优化策略，仍然是未来研究的一个关键领域。"
            }
          ],
          "raw_title": "Optimization Hyperparameters",
          "type": null,
          "children": [],
          "translated_title": "10.3 优化超参数"
        },
        {
          "title": "10.4 Optimization across Depth and Time",
          "number": "10.4",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Unlike conventional optimizers that update parameters ina static setting, LLMs optimize workflows dynamically, considering both depth (single-pass workflows)andtime(recurrentupdates).In terms ofdepth,LLMsfunction similarly to feedforward networks, sequentially optimizing workflows as they passthrough different modules—most existing LLM-based optimizers follow this paradigm.Beyond single-pass execution, LLMs can also optimize over time, akin to recurrent architectures such as RNNs or Universal Transformers [830], by iteratively refining decision-making. For instance, StateFlow [729] enhances workflows by incorporating feedback across multiple iterations, enabling dynamic refinement and adaptation over time.Whilethese analogies are compelling,many wellestablishedengineering optimization techniques—such as checkpointing [831] and truncated backpropagation[832]—remain underexplored in LLM-based optimization. We seethis as a promising avenue for future research,echoing previouscall for deeper investigation [813].",
              "index": 0,
              "part": 0,
              "translated_content": "与在静态环境中更新参数的传统优化器不同，LLMs在考虑深度（单次工作流）和时间（循环更新）时动态优化工作流程。在深度方面，LLMs的功能类似于前馈网络，按顺序优化工作流程，通过不同模块时进行优化，大多数现有基于LLMs的优化器都遵循这种范例。除了单次执行，LLMs还可以随时间进行优化，类似于循环架构，如RNN或Universal Transformers，通过迭代地改进决策制定。例如，StateFlow通过跨多次迭代结合反馈来增强工作流程，实现随时间的动态改进和适应。虽然这些类比令人信服，但许多成熟的工程优化技术，如检查点和截断反向传播，在基于LLMs的优化中仍未得到充分探讨。我们认为这是未来研究的一个有前途的方向，呼应先前对更深入研究的呼吁。"
            }
          ],
          "raw_title": "Optimization across Depth and Time",
          "type": null,
          "children": [],
          "translated_title": "10.4 深度和时间跨度优化"
        },
        {
          "title": "10.5 A Theoretical Perspective",
          "number": "10.5",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Recent studies suggest that transformers inherently perform optimization-like computations,supporting their potential as general-purpose optimizers forcomputational workflows.However,asignificant gapremains betweentheir empirical success and theoretical understanding.Here, we provide a brief overview of recent progressin bridging this gap.\n\n·In-Context Learning.A fundamental perspective on transformers as optimizers emerges from in-context learning, particularly in few-shot settings [2].[733] demonstrated that transformers can in-context learn diverse regression hypotheses,including regularized linear models,decision trees, and shallow neural networks. Building on this, later works[734,833,735] provided constructive proofs that transformers can implement iterative optimization algorithms,suchasgradientdescentandsecond-orderupdates.However, whilethese theoreticalmodelscharacterize transformersoptimization capabilities, they do not fully explain in-context learning in large-scale LLMs, which operate in discrete input-output spaces. Empirical analyses [819,834,820] instead sought to understand how pre-trained LLMs generalize in-context.[834] proposed that in-context learning resembles a hidden Markov model (HMM) performing implicit Bayesian inference, while [819,820] challenged the conventional view that in-context demonstrations serve as new test-time samples for hypothesis formation. In-context learning remains the central emergent ability[835] enabling self-improvement and optimization from context, yet it continues to elude comprehensive theoretical analysis.\n·Mechanistic Interpretability.Paraleltotheoreticalanalyses,mechanistic interpretability aims touncover interal transformer computations by identifying subgraphs, also known as circuits,responsible for specific behaviors. Early studies mapped circuits for stylized language tasks in pre-trained GPT-2 models [836,837,838],while more recent efforts have scaled up by identifying semantically meaningful features using sparse autoencoders [839, 736,840, 841].These methods have been largely successful in eliciting causal and controllable behavior from frontier-class LLMs,but they alsorevealanunintendedconsequence: in-context learning capabilitiesoften entangle beneficial generalization with harmful behaviors when conditioned on many-shot demonstrations[842].This raises challenges for optimizing LLM workflows safely and reliably.\n·Limitations Under Uncertainty. While LLMs demonstrate moderate capabilities in sequential decision-making when provided with in-context information, theystruggle to make optimalchoices under uncertainty[843,844,845. 846].In particular,[826] found that LLM-based optimizers exhibit diffculty in adapting to stochastic environments, often failing toexplore optimally.These findings serve asa cautionary notefor deploying LLM-based optimizers in dynamic or uncertain settings where exploration and robust decision-making are critical.\nLMs redefine optimization by integrating structured reasoning,naturallanguage processing, and in-context learning,",
              "index": 0,
              "part": 0,
              "translated_content": "最近的研究表明，变压器本质上执行类似优化的计算，支持它们作为计算工作流通用优化器的潜力。然而，在它们的经验成功和理论理解之间存在重大差距。在这里，我们简要概述了最近在弥合这一差距方面取得的进展。\n\n·上下文学习。将变压器视为优化器的一个基本视角源自于上下文学习，特别是在少样本设置中。研究表明，变压器可以在上下文中学习多样的回归假设，包括正则化线性模型、决策树和浅层神经网络。后续研究证明了变压器可以实现迭代优化算法，如梯度下降和二阶更新。然而，尽管这些理论模型表征了变压器的优化能力，它们并未完全解释大规模LLMs中的上下文学习，后者在离散的输入-输出空间中运行。相反，经验分析试图理解预训练LLMs在上下文中的泛化。提出上下文学习类似于执行隐马尔可夫模型（HMM）进行隐式贝叶斯推断，而挑战传统观点认为上下文演示为假设形成提供了新的测试样本。上下文学习仍然是使自我改进和优化从上下文中产生的核心新能力，然而它仍然难以进行全面的理论分析。\n·机制可解释性。与理论分析相对应，机制可解释性旨在通过识别负责特定行为的子图，也称为电路，揭示内部变压器计算。早期研究在预训练的GPT-2模型中为艺术语言任务映射了电路，而最近的工作通过使用稀疏自动编码器识别语义上有意义的特征进行了扩展。这些方法在从事前沿级LLMs中引出因果和可控行为方面取得了很大成功，但它们也揭示了一个意外后果：在很多样本演示的条件下，上下文学习能力常常将有益的泛化与有害行为纠缠在一起。这给安全可靠地优化LLM工作流程带来了挑战。\n·不确定性下的局限性。尽管LLMs在提供上下文信息时在顺序决策方面表现出中等能力，但它们在不确定性下做出最优选择时会遇到困难。特别是发现，基于LLMs的优化器在适应随机环境时存在困难，通常无法进行最佳探索。这些发现为在探索和稳健决策至关重要的动态或不确定环境中部署基于LLMs的优化器提出了警示。LLMs通过整合结构化推理、自然语言处理和上下文学习重新定义了优化。"
            },
            {
              "type": "text",
              "content": "expanding beyond traditional numerical methods.Despite strong empirical performance in structured search spaces, open questions remain about thetheoretical underpinnings of LLM-based optimization,particularlytheemergence of in-context learning from standard gradient-based training.",
              "index": 1,
              "part": 0,
              "translated_content": "超越传统的数值方法。尽管在结构化搜索空间中表现出强大的实证性能，但关于基于LLM的优化的理论基础仍存在一些未解之谜，特别是从标准梯度训练中出现上下文学习的问题。"
            }
          ],
          "raw_title": "A Theoretical Perspective",
          "type": null,
          "children": [],
          "translated_title": "10.5 理论视角"
        }
      ],
      "translated_title": "10大语言模型作为优化器"
    },
    {
      "title": "111",
      "number": "111",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "10.1 Optimization Paradigms 111\n10.2 Iterative Approaches to LLM Optimization 111\n10.3 Optimization Hyperparameters 114\n10.4 Optimization across Depth and Time 114\n10.5 A Theoretical Perspective. 115",
          "index": 0,
          "part": 0,
          "translated_content": "10.1 优化范式 111\n10.2 LLM优化的迭代方法 111\n10.3 优化超参数 114\n10.4 跨深度和时间的优化 114\n10.5 理论视角 115"
        }
      ],
      "raw_title": "",
      "type": null,
      "children": [],
      "translated_title": "111"
    },
    {
      "title": "Online and Offline Agent Self-Improvement 116",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "11.1 Online Agent Self-Improvement 116\n11.2 Offline Agent Self-Improvement 117\n11.3 Comparison of Online and Offline Improvement 118\n11.4 Hybrid Approaches 118",
          "index": 0,
          "part": 0,
          "translated_content": "11.1 在线代理自我完善 116\n11.2 离线代理自我完善 117\n11.3 在线和离线改进的比较 118\n11.4 混合方法 118"
        }
      ],
      "raw_title": "Online and Offline Agent Self-Improvement 116",
      "type": null,
      "children": [],
      "translated_title": "在线和离线代理自我改进 116"
    },
    {
      "title": "12 Scientific Discovery and Intelligent Evolution 120",
      "number": "12",
      "level": 1,
      "content": [],
      "raw_title": "Scientific Discovery and Intelligent Evolution 120",
      "type": null,
      "children": [
        {
          "title": "12.1 Agent's Intelligence for Scientific Knowledge Discovery 120",
          "number": "12.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "12.1.1 KL Divergence-based Intelligence Measure 120\n12.1.2 Statistical Nature of Intelligence Growth 122\n12.1.3 Intelligence Evolution Strategies . 123\n12.2 Agent-Knowledge Interactions 123\n12.2.1 Hypothesis Generation and Testing 124\n12.2.2 Protocol Planning and Tool Innovation . 126\n12.2.3 Data Analysis and Implication Derivation 126\n. Technological Readiness and Challenges . 127\n12.3.1 Real-World Interaction Challenges . 127\n12.3.2 Complex Reasoning Challenges 128\n12.3.3 Challenges in Integrating Prior Knowledge 129",
              "index": 0,
              "part": 0,
              "translated_content": "12.1.1 基于KL散度的智能度量 120\n12.1.2 智能增长的统计性质 122\n12.1.3 智能演化策略 123\n12.2 代理-知识交互 123\n12.2.1 假设生成与测试 124\n12.2.2 协议规划与工具创新 126\n12.2.3 数据分析与推论推导 126\n技术准备和挑战 127\n12.3.1 现实世界交互挑战 127\n12.3.2 复杂推理挑战 128\n12.3.3 整合先前知识的挑战 129"
            }
          ],
          "raw_title": "Agent's Intelligence for Scientific Knowledge Discovery 120",
          "type": null,
          "children": [],
          "translated_title": "12.1 代理人智能用于科学知识发现 120"
        },
        {
          "title": "12.1 Agent's Intelligence for Scientific Knowledge Discovery",
          "number": "12.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Knowledge,traditionally defined as justifiedtrue belief, traces back to Plato[860]and has beenfurtherrefined by Edmund Gettier [861], who argued that knowledge must be produced by a reliable cognitive process-though its precise definition remains debated[862].Inour discussion, we describe scientific knowledge discovery as the process of collecting data and information to either justifyorfalsify rationalhypotheses about target scientific problems.To discussthecapabilityof agents in scientificknowledge discovery,we firstexplore ageneralframework for measuring an agent's intelligence through the lens of information theory.",
              "index": 0,
              "part": 0,
              "translated_content": "传统上被定义为合理真实信念的知识可以追溯到柏拉图[860]，并由埃德蒙·盖特尔[861]进一步完善，他认为知识必须由可靠的认知过程产生，尽管其确切定义仍然存在争议[862]。在我们的讨论中，我们将科学知识发现描述为收集数据和信息的过程，以证明或证伪关于目标科学问题的理性假设。为了讨论智能体在科学知识发现中的能力，我们首先通过信息论的视角探讨衡量智能体智能的一般框架。"
            }
          ],
          "raw_title": "Agent's Intelligence for Scientific Knowledge Discovery",
          "type": null,
          "children": [
            {
              "title": "12.1.1 KL Divergence-based Intelligence Measure",
              "number": "12.1.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The agent's intellgence can be measured by the KL divergence between its predicted and real-world probability distributions of unknown information. A long-standing goal in both artificial intelligence and the philosophy of science istoformalizewhatit means for an agent to“understand\"theworld.From Jaynes'viewofprobability theory as extended logic forreasoning under uncertainty[863], toParret al.'s framing of intelligence as minimizing model-world divergence underthe freeenergy principle[864],many frameworksconverge onacommontheme: intelligent behavior arisesfrom making accurate predictions about an uncertain world. Clark[344],for instance, argues that intelligent agents constantly engage with the world through prediction and error correction to reduce surprise.Cholet[865] emphasizes that intelligence shouldreflectskillacquisitionefficiency,because of thedynamicnature oftaskadaptation. Together,these views suggest that inteligence involves building predictive and adaptable models—anidea formalized here through aprobabilistic framework thatlinks reasoning to knowledge acquisition and enablescomparison across agents in scientific discovery.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "智能代理的智能可以通过其对未知信息的预测和现实世界概率分布之间的KL散度来衡量。人工智能和科学哲学中长期以来的一个目标是形式化代理“理解”世界的含义。从Jaynes将概率论视为在不确定性下推理的扩展逻辑[863]，到Parret al.将智能框架化为在自由能原则下最小化模型-世界差异[864]，许多框架都聚焦于一个共同主题：智能行为源于对不确定世界进行准确预测。例如，Clark[344]认为智能代理通过预测和误差校正不断与世界互动以减少惊奇。Cholet[865]强调智能应反映技能习得效率，因为任务适应具有动态性质。总的来说，这些观点表明智能涉及建立预测性和可适应性模型——这里通过一个概率框架形式化，将推理与知识获取联系起来，并促进在科学发现中跨代理的比较。"
                },
                {
                  "type": "text",
                  "content": "Building onthis foundation,weconsider intellgence in the specificcontextof scientificknowledge discovery,where the agent's primaryobjective is to infer unknown aspects of the physical worldfrom limited data.From the agent's perspective in knowledge discovery, the world $\\boldsymbol{\\mathcal{W}}$ is characterized by an ensemble of datasets $\\mathbf{x}=\\{x_{1},x_{2},...,x_{n}\\}$ related to the scientific problem the agent aims to understand. During the agent's interaction with $\\mathcal{W}$ , each dataset appears in the experimental measurements or observations with a probability $P_{\\mathcal{W}}(\\mathbf{x})$ . Here we assume that individual data points $x_{i}$ may or may not be correlated. For example, in a task of text generation using a language model, $x_{i}$ represents a chunk of tokens forming a meaningful proposition, and $\\mathbf{x}$ is a coherent text constructed from known and inferred propositions. In this context, the“world\" is the ensemble of all propositions.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "在这个基础上，我们考虑智能在科学知识发现特定背景下的表现，其中代理的主要目标是从有限数据中推断物理世界的未知方面。从知识发现的角度看，世界 $\\boldsymbol{\\mathcal{W}}$ 被描述为与代理旨在理解的科学问题相关的数据集合 $\\mathbf{x}=\\{x_{1},x_{2},...,x_{n}\\}$ 。在代理与 $\\mathcal{W}$ 的互动过程中，每个数据集以概率 $P_{\\mathcal{W}}(\\mathbf{x})$ 出现在实验测量或观察中。在这里，我们假设个别数据点 $x_{i}$ 可能相关也可能不相关。例如，在使用语言模型生成文本的任务中，$x_{i}$ 代表形成有意义命题的一组标记，而 $\\mathbf{x}$ 是由已知和推断的命题构成的连贯文本。在这种情境下，“世界”是所有命题的集合。"
                },
                {
                  "type": "text",
                  "content": "Let $\\theta$ denote the parameter that parameterizes the agent's world model, $M_{t}^{\\mathrm{wm}}$ , as defined in Table 1.2. For instance, in a transformer model with a fixed architecture, $\\theta$ represents its weights. Given $\\theta$ and a dataset $\\mathbf{x}$ , the agent predicts a probability distribution $P_{\\theta}(\\mathbf{x})$ . In general, different AI agents could be optimized for different goals. For scientific knowledge discovery,we assumethatthe agent'sgoalis toproduceagooddescriptionofthe realworld,i.e.a world model that predicts yet-to-be-explored natural phenomena as accurately as posible.A more intellgent agent produces a better approximation of the real-world distribution $P_{\\mathcal{W}}(\\mathbf{x})$ . The agent's intelligence can thus be measured by the KL divergence, or relative entropy, between these two probability distributions:",
                  "index": 2,
                  "part": 0,
                  "translated_content": "设 $\\theta$ 表示参数，该参数对应代理的世界模型 $M_{t}^{\\mathrm{wm}}$，如表1.2所定义。例如，在具有固定架构的Transformer模型中，$\\theta$ 代表其权重。给定 $\\theta$ 和数据集 $\\mathbf{x}$，代理预测一个概率分布 $P_{\\theta}(\\mathbf{x})$。一般来说，不同的人工智能代理可能针对不同的目标进行优化。对于科学知识发现，我们假设代理的目标是产生对真实世界的良好描述，即一个能够尽可能准确预测尚未探索的自然现象的世界模型。一个更智能的代理会产生对真实世界分布 $P_{\\mathcal{W}}(\\mathbf{x})$ 更好的近似。因此，代理的智能可以通过这两个概率分布之间的KL散度或相对熵来衡量："
                },
                {
                  "type": "formula",
                  "content": "$$ \nD_{0}(\\theta)=\\sum_{{\\bf x}\\subseteq\\mathcal{W}}P_{\\mathcal{W}}({\\bf x})\\log\\frac{P_{\\mathcal{W}}({\\bf x})}{P_{\\theta}({\\bf x})}\n $$",
                  "index": 3,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "$D_{0}(\\theta)$ describes the difference between $P_{\\mathcal{W}}(\\mathbf{x})$ and $P_{\\theta}(\\mathbf{x})$ . More precisely, in the context of hypothesis testing, if we sample $P_{\\mathcal{W}}(\\mathbf{x})~N$ times and compare the results with the predictions from $P_{\\theta}(\\mathbf{x})$ , the probability of mistaking $P_{\\mathcal{W}}(\\mathbf{x})$ for $P_{\\theta}(\\mathbf{x})$ scales as $e^{-N D_{0}(\\theta)}$ [866]. Inotherordsanagentwithalower $D_{0}(\\theta)$ produces predictions thatalign more closely with reality.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "$D_{0}(\\theta)$描述了$P_{\\mathcal{W}}(\\mathbf{x})$和$P_{\\theta}(\\mathbf{x})$之间的差异。更准确地说，在假设检验的背景下，如果我们从$P_{\\mathcal{W}}(\\mathbf{x})$中抽样$N$次，并将结果与$P_{\\theta}(\\mathbf{x})$的预测进行比较，将$P_{\\mathcal{W}}(\\mathbf{x})$误认为$P_{\\theta}(\\mathbf{x})$的概率按$e^{-N D_{0}(\\theta)}$进行缩放。换句话说，具有较低$D_{0}(\\theta)$的代理会产生更接近现实的预测。"
                },
                {
                  "type": "text",
                  "content": "Forexample,considertwomaterials synthesis agentswhose goal, $M_{t}^{g o a l}$ ,is to understandwhetherornotaninoganic compound of interest, $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ , is synthesizable. The agents can predict either (1) ${\\bf x}_{\\mathrm{1}}{=}\\{\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ is synthesizable), and (2) $\\scriptstyle\\mathbf{x}_{2}=\\{\\mathbf{CaFe}_{2}(\\mathbf{PO}_{4})_{2}\\mathbf{O}$ is not synthesizable). In reality, since $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ is a natural mineral, $\\dot{P}_{\\mathcal{W}}(\\mathbf{x}_{1})=1$ and $P_{\\mathcal{W}}(\\mathbf{x}_{2})=0$ . However, this mineral was only recently reported on October 4, 2023[refl, after the knowledge cutoff of many LLMs; thus, the agents lacks that knowledge. Compare Agent 1, which guesses randomly $P_{\\theta_{1}}(\\mathbf{x}_{1}){\\bf{\\bar{\\alpha}}}=P_{\\theta_{1}}(\\mathbf{x}_{1})=0.5$ ，yielding $D_{0}(\\theta_{1}\\bar{)\\ =\\ }\\log2$ . In contrast, Agent 2 uses first-principles calculations and finds that $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ (assume structure is xx [cite: Materials Project ID]) is the lowest-energy phase among its competitors [ref], indicating stability. Thereby, Agent 2 predicts that $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ is likely synthesizable, suggesting $P_{\\theta_{2}}(\\mathbf{\\bar{x}}_{1})>0.5>P_{\\theta_{2}}(\\mathbf{x}_{2})$ . Consequently, $\\dot{D}_{0}(\\dot{\\theta}_{2})=-\\log P_{\\theta_{2}}(\\mathbf{x}_{1})<D_{0}(\\theta_{1})$ , meaning that Agent 2 has a more accurate understanding of the real world.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "例如，考虑两个材料合成代理，它们的目标是了解感兴趣的无机化合物$\\mathrm{CaFe_{2}(PO_{4})_{2}O}$是否可合成。这些代理可以预测(1)${\\bf x}_{1}=\\{\\mathrm{CaFe_{2}(PO_{4})_{2}O}$可合成)，以及(2)$\\scriptstyle\\mathbf{x}_{2}=\\{\\mathbf{CaFe}_{2}(\\mathbf{PO}_{4})_{2}\\mathbf{O}$不可合成)。实际上，由于$\\mathrm{CaFe_{2}(PO_{4})_{2}O}$是一种天然矿物，$\\dot{P}_{\\mathcal{W}}(\\mathbf{x}_{1})=1$且$P_{\\mathcal{W}}(\\mathbf{x}_{2})=0$。然而，这种矿物直到2023年10月4日才被报道[参考文献]，在许多LLM的知识截止日期之后；因此，这些代理缺乏这方面的知识。比较起随机猜测的代理1，$P_{\\theta_{1}}(\\mathbf{x}_{1})=\\bar{\\alpha}=P_{\\theta_{1}}(\\mathbf{x}_{1})=0.5$，从而得到$D_{0}(\\theta_{1})=\\log2$。相比之下，代理2使用第一性原理计算，并发现$\\mathrm{CaFe_{2}(PO_{4})_{2}O}$（假设结构为xx [引用：Materials Project ID]）是其竞争对手中能量最低的相[参考]，表明其稳定性。因此，代理2预测$\\mathrm{CaFe_{2}(PO_{4})_{2}O}$可能可合成，暗示$P_{\\theta_{2}}(\\bar{\\mathbf{x}}_{1})>0.5>P_{\\theta_{2}}(\\mathbf{x}_{2})$。因此，$\\dot{D}_{0}(\\dot{\\theta}_{2})=-\\log P_{\\theta_{2}}(\\mathbf{x}_{1})<D_{0}(\\theta_{1})$，这意味着代理2对真实世界有着更准确的理解。"
                },
                {
                  "type": "text",
                  "content": "Now,let us assume the agenthas conducted some measurements and determined specificvalues fora subset of data points $x_{i}$ . Let $\\mathbf{x}_{\\mathrm{K}}$ denote this known subset and $\\mathbf{x}_{\\mathrm{U}}$ the remaining unknown part. Correspondingly, we define the space of all existing knowledge as $\\kappa$ and the space of all unknown information as $\\mathcal{U}$ ，satisfying $\\mathbf{x}_{\\mathrm{K}}\\subseteq{\\mathcal{K}}$ $\\mathbf{x}_{\\mathrm{U}}\\subseteq\\mathcal{U}$ and $\\kappa\\cup\\mathcal{U}=\\mathcal{W}$ . For example, in text generation, the the prompt text $\\mathbf{x}_{\\mathrm{K}}$ represents already known information. The efficiency of the language model is then measured by its predictive accuracy for the generated text $\\mathbf{x}_{\\mathrm{U}}$ based on $\\mathbf{x}_{\\mathrm{K}}$ More generaly,the agent'sintelligence is measuredbytherelativeentropyof theconditional probabilitydistribution:",
                  "index": 6,
                  "part": 0,
                  "translated_content": "现在，让我们假设代理已经进行了一些测量，并确定了一些数据点$x_{i}$的具体数值。设$\\mathbf{x}_{\\mathrm{K}}$表示这些已知数据点的子集，$\\mathbf{x}_{\\mathrm{U}}$表示其余未知部分。相应地，我们定义所有已知知识的空间为$\\kappa$，所有未知信息的空间为$\\mathcal{U}$，满足$\\mathbf{x}_{\\mathrm{K}}\\subseteq{\\mathcal{K}}$，$\\mathbf{x}_{\\mathrm{U}}\\subseteq\\mathcal{U}$，以及$\\kappa\\cup\\mathcal{U}=\\mathcal{W}$。例如，在文本生成中，提示文本$\\mathbf{x}_{\\mathrm{K}}$代表已知信息。然后，语言模型的效率通过其基于$\\mathbf{x}_{\\mathrm{K}}$生成文本$\\mathbf{x}_{\\mathrm{U}}$的预测准确性来衡量。更一般地，代理的智能性可通过条件概率分布的相对熵来衡量："
                },
                {
                  "type": "formula",
                  "content": "$$ \nD_{\\mathrm{K}}(\\boldsymbol{\\theta},{\\bf x}_{\\mathrm{K}})=\\sum_{{\\bf x}\\subseteq\\mathcal{U}}P_{\\mathcal{W}}\\left({\\bf x}|{\\bf x}_{\\mathrm{K}}\\right)\\log\\frac{P_{\\mathcal{W}}\\left({\\bf x}|{\\bf x}_{\\mathrm{K}}\\right)}{P_{\\boldsymbol{\\theta}}\\left({\\bf x}|{\\bf x}_{\\mathrm{K}}\\right)}\n $$",
                  "index": 7,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "In practice, all of the agent's knowledge is stored in its memory $M_{t}^{\\mathrm{mem}}$ ,i.e., $\\mathbf{x}_{\\mathrm{K}}={\\boldsymbol{K}}=M_{t}^{\\mathrm{mem}}$ and $\\mathcal{U}=\\mathcal{W}\\setminus M_{t}^{\\mathrm{mem}}$ we define the agent's intelligence as:",
                  "index": 8,
                  "part": 0,
                  "translated_content": "在实践中，代理的所有知识都存储在其记忆$M_{t}^{\\mathrm{mem}}$中，即$\\mathbf{x}_{\\mathrm{K}}={\\boldsymbol{K}}=M_{t}^{\\mathrm{mem}}$，$\\mathcal{U}=\\mathcal{W}\\setminus M_{t}^{\\mathrm{mem}}$，我们将代理的智能定义为："
                },
                {
                  "type": "formula",
                  "content": "$$ \nI Q_{t}^{\\mathrm{agent}}\\equiv-D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})=-\\sum_{\\mathbf{x}\\subseteq\\mathcal{U}}P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}{P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}\n $$",
                  "index": 9,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "In other words,thethe agent'sintelligence $I Q_{t}^{\\mathrm{agent}}$ is determined by its memory $M_{t}^{\\mathrm{mem}}$ and the parameter $\\theta$ of its world model $M_{t}^{\\mathrm{wm}}$ . A schematic plot is shown in Figure 12.1. At time $t=0$ , when the $M_{t}^{\\mathrm{mem}}$ is very limited or lack relevant information toa new target scientific problem, $I Q_{t}^{\\mathrm{agent}}$ is primarily determined by the zero-shot predictive ability of $M_{t}^{\\mathrm{wm}}$ ,corresponding to fluid intelligence [867]. Over time, as more relevant knowledge is incorporated into $M_{t}^{\\mathrm{mem}}$ ， $I Q_{t}^{\\mathrm{agent}}$ becomesincreasinglydependentontheknowledge-augmentedpredictivecapabilityof $M_{t}^{\\mathrm{wm}}$ reflecting crystallized intelligence [868].",
                  "index": 10,
                  "part": 0,
                  "translated_content": "换句话说，代理的智能$I Q_{t}^{\\mathrm{agent}}$取决于其记忆$M_{t}^{\\mathrm{mem}}$和其世界模型$M_{t}^{\\mathrm{wm}}$的参数$\\theta$。如图12.1所示的示意图。在$t=0$时，当$M_{t}^{\\mathrm{mem}}$非常有限或缺乏与新目标科学问题相关的信息时，$I Q_{t}^{\\mathrm{agent}}$主要由$M_{t}^{\\mathrm{wm}}$的零样本预测能力决定，对应于流体智力。随着时间的推移，随着更多相关知识被纳入$M_{t}^{\\mathrm{mem}}$，$I Q_{t}^{\\mathrm{agent}}$越来越依赖于$M_{t}^{\\mathrm{wm}}$的知识增强预测能力，反映出结晶智力。"
                },
                {
                  "type": "figure",
                  "src": "images/47374d7245528b34b2d026b34dae42125569a091e0f59fd0421a26a4c6027972.jpg",
                  "alt": "",
                  "caption": "Figure 12.1: Schematic representation of agent intelligence and knowledge discovery.The agent's intelligence, measured by the KL divergence $D_{\\mathrm{K}}$ between predictions and real-world probability distributions, evolves from fluid intellgence (zero-shot predictions for new problems)tocrystallzed intelligence (knowledge-augmented predictions after learning) as it accumulates data in its memory $M_{t}^{\\mathrm{mem}}$ over time $t$ . Given $M_{t}^{\\mathrm{mem}}$ , the evolution of $D_{\\mathrm{K}}$ varies within the world model's parameter space $\\Theta$ , as illustrated by $\\theta_{1}$ and $\\theta_{2}$ in the solid lines. The expressive limitation of $\\Theta$ is characterizedbytheenvelope $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}$ . Given $\\Theta$ $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}$ is influenced bydifferentknowledgeexpansionstrategies, such as $^1M_{t}^{\\mathrm{mem}}$ and $^{2}M_{t}^{\\mathrm{mem}}$ , shown as dash lines.",
                  "index": 11,
                  "part": 0,
                  "translated_caption": "图12.1: 代理智能和知识发现的示意图。代理的智能，通过预测和真实世界概率分布之间的KL散度$D_{\\mathrm{K}}$来衡量，在时间$t$内随着数据在其记忆$M_{t}^{\\mathrm{mem}}$中的积累，从流体智能（对新问题的零猜测预测）逐渐演变为晶体智能（学习后知识增强的预测）。在给定$M_{t}^{\\mathrm{mem}}$的情况下，$D_{\\mathrm{K}}$在世界模型参数空间$\\Theta$内的演变因$\\Theta$的不同取值而异，如实线所示的$\\theta_{1}$和$\\theta_{2}$。参数空间$\\Theta$的表达限制由$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}$来描述。在给定$\\Theta$的情况下，$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}$受到不同知识扩展策略的影响，例如$^1M_{t}^{\\mathrm{mem}}$和$^{2}M_{t}^{\\mathrm{mem}}$，如虚线所示。"
                }
              ],
              "raw_title": "KL Divergence-based Intelligence Measure",
              "type": null,
              "children": [],
              "translated_title": "12.1.1 基于KL散度的智能度量"
            },
            {
              "title": "12.1.2 Statistical Nature of Intelligence Growth",
              "number": "12.1.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The agent's intelligence, in a statistical sense, is a non-decreasing function of acquired knowledge. Roughly speaking, $I Q_{t}^{\\mathrm{agent}}$ quantfiesbottheamountofknowledgeanagenthaacquiredandhoweffctivelytheagentcan apply that knowledge after learning from $M_{t}^{\\mathrm{mem}}$ . Intuitively, if the agent gains additional information at time $t{\\mathrm{.}}$ which corresponds to enlarging $M_{t}^{\\mathrm{mem}}$ and shrinking $\\mathcal{U}$ its intelligence should increase.\n\nTo understand this process, consider a small region $\\Delta\\subseteq{\\mathcal{U}}$ and examine the effect of adding a dataset $\\mathbf{x}_{\\Delta}$ from $\\Delta$ to $M_{t}^{\\mathrm{mem}}$ . Denote $\\mathcal{U}=\\mathcal{U}^{\\prime}\\cup\\Delta$ , where $\\mathcal{U}^{\\prime}$ represents theremaining unknown part oftheworld.The agent's intelligence at time $t+1$ is given by:",
                  "index": 0,
                  "part": 0,
                  "translated_content": "智能代理的智能在统计意义上是随着获取知识而不减的函数。粗略地说，$I Q_{t}^{\\mathrm{agent}}$ 量化了代理获取的知识量以及代理从$M_{t}^{\\mathrm{mem}}$中学习后能有效应用该知识的能力。直观地，如果代理在时间$t$获得额外信息，对应于扩大$M_{t}^{\\mathrm{mem}}$并缩小$\\mathcal{U}$，其智能应该增加。\n\n为了理解这一过程，考虑一个小区域$\\Delta\\subseteq{\\mathcal{U}}$，并研究将来自$\\Delta$的数据集$\\mathbf{x}_{\\Delta}$添加到$M_{t}^{\\mathrm{mem}}$的效果。记$\\mathcal{U}=\\mathcal{U}^{\\prime}\\cup\\Delta$，其中$\\mathcal{U}^{\\prime}$表示世界的剩余未知部分。时间$t+1$时代理的智能由以下公式给出："
                },
                {
                  "type": "formula",
                  "content": "$$ \nI Q_{t+1}^{\\mathrm{agent}}\\equiv-D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})=-\\sum_{\\mathbf{x}^{\\prime}\\subseteq\\boldsymbol{U}^{\\prime}}P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})}{P_{\\theta}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})}\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "Directly comparing $I Q_{t}^{\\mathrm{{agent}}}$ and $I Q_{t+1}^{\\mathrm{agent}}$ ischallengingf $I Q_{t+1}^{\\mathrm{agent}}$ averaging over $\\mathbf{x}_{\\Delta}$ with probability $P_{\\mathcal{W}}(\\mathbf{x}_{\\Delta}|M_{t}^{\\mathrm{mem}})$ Thisexpectationrepresentstheaverageamountofknowledge gained by measuring $\\Delta$ ， given prior knowledge in $M_{t}^{\\mathrm{mem}}$ . We obtain:",
                  "index": 2,
                  "part": 0,
                  "translated_content": "直接比较$I Q_{t}^{\\mathrm{agent}}$和$I Q_{t+1}^{\\mathrm{agent}}$是具有挑战性的，因为$I Q_{t+1}^{\\mathrm{agent}}$通过对$\\mathbf{x}_{\\Delta}$进行概率$P_{\\mathcal{W}}(\\mathbf{x}_{\\Delta}|M_{t}^{\\mathrm{mem}})$的平均化。这个期望代表了在给定$M_{t}^{\\mathrm{mem}}$中的先前知识的情况下，通过测量$\\Delta$所获得的平均知识量。我们得到："
                },
                {
                  "type": "formula",
                  "content": "$$ \n\\begin{array}{r}{\\displaystyle\\sum_{\\mathbf{x}\\subseteq\\Delta}P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})I Q_{t+1}^{\\mathrm{agent}}=-\\sum_{\\mathbf{x}^{\\prime}\\subseteq\\mathcal{U}^{\\prime},\\mathbf{x}\\subseteq\\Delta}P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}\\mathbf{x}|M_{t}^{\\mathrm{mem}})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x})}{P_{\\theta}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x})}}\\\\ {=I Q_{t}^{\\mathrm{agent}}+\\displaystyle\\sum_{\\mathbf{x}\\subseteq\\Delta}P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}{P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}}\\end{array}\n $$",
                  "index": 3,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "The second term is the relative entropy of the conditional probability distribution of $\\mathbf{x}_{\\Delta}$ conditioned on $M_{t}^{\\mathrm{mem}}$ ,which is always non-negative.Therefore,on average, $I Q_{t}^{\\mathrm{{agent}}}$ is non-decreasing as $M_{t}^{\\mathrm{mem}}$ acquires new knowledge over time. Note that $I Q_{t+1}^{\\mathrm{agent}}$ canbefurthersedbggtcqdede $\\theta$ within $M_{t}^{\\mathrm{wm}}$ .\n\nInterestingly, the expected gain in intelligence at time $t$ is determined by the discrepancy between the actual distribution $P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})$ and the model-predicted distribution $P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})$ . In other words, the rate of intelligence growth in Figure 12.1 is higher when the new measurement result is more unexpected.Thisobservationidentifies scientist agents [859]as aspecialtypeofcriosity-driven agent[869],prioritizingexplorationover exploitationtoexpandthe frontiers of knowledge fordeeper understanding ofnature.Unlikeagents that leverage existing knowledge toachieve predefined objectives,curiosity-drivenagentscanlearn withoutextrinsicrewards[387,870] (see Section 5.3fordetails)enabling discoveries beyond human-planned search spaces and revealing knowledge in unexplored domains.This potentialalso underscoresthe importance ofequipping curiosity-driven agents withfundamental perception and action tools that can be transferred to explore new knowledge domains.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "第二项是$\\mathbf{x}_{\\Delta}$在$M_{t}^{\\mathrm{mem}}$条件概率分布的相对熵，始终为非负。因此，平均而言，随着$M_{t}^{\\mathrm{mem}}$随时间获得新知识，$I Q_{t}^{\\mathrm{agent}}$是不减的。需要注意的是，$I Q_{t+1}^{\\mathrm{agent}}$可以在$M_{t}^{\\mathrm{wm}}$内通过$\\theta$进一步改进。\n\n有趣的是，时间$t$的智能预期增益取决于实际分布$P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})$与模型预测分布$P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})$之间的差异。换句话说，在图12.1中，智能增长的速率在新的测量结果更加意外时更高。这一观察将科学家代理（859）确定为一种特殊类型的好奇驱动代理（869），其优先考虑探索而不是开发，以拓展知识边界，深入理解自然。与利用现有知识实现预定义目标的代理不同，好奇驱动代理可以在没有外在奖励的情况下学习（详情见第5.3节），从而实现超出人类规划搜索空间的发现，并揭示未开发领域的知识。这一潜力也强调了装备好奇驱动代理基本感知和行动工具的重要性，这些工具可以被转移到探索新知识领域。"
                }
              ],
              "raw_title": "Statistical Nature of Intelligence Growth",
              "type": null,
              "children": [],
              "translated_title": "12.1.2 智力增长的统计特性"
            },
            {
              "title": "12.1.3 Intelligence Evolution Strategies",
              "number": "12.1.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The strategy for expanding known information determines how quickly an agent's inteligence evolves. For a given knowledge base $M_{t}^{\\mathrm{mem}}$ , the parameter $\\theta$ can be optimized over a space of world models $\\Theta$ characterized by the architecture of $M_{t}^{\\mathrm{wm}}$ . The optimal agent is the one that minimizes $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ , thereby maximizing $I Q_{t}^{\\mathrm{agent}}$ .．",
                  "index": 0,
                  "part": 0,
                  "translated_content": "扩展已知信息的策略决定了代理智能进化的速度。对于给定的知识库$M_{t}^{\\mathrm{mem}}$，参数$\\theta$可以在由$M_{t}^{\\mathrm{wm}}$的架构表征的世界模型$\\Theta$空间中进行优化。最优代理是使$D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$最小化，从而最大化$I Q_{t}^{\\mathrm{agent}}$的代理。"
                },
                {
                  "type": "formula",
                  "content": "$$ \n\\theta_{\\mathrm{K},t}^{*}\\equiv\\arg\\operatorname*{sup}_{\\theta}I Q_{t}^{\\mathrm{agent}}=\\arg\\operatorname*{inf}_{\\theta}D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "and",
                  "index": 2,
                  "part": 0,
                  "translated_content": "和"
                },
                {
                  "type": "formula",
                  "content": "$$ \nD_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})\\equiv D_{\\mathrm{K}}(\\theta_{\\mathrm{K},t}^{*},M_{t}^{\\mathrm{mem}})\n $$",
                  "index": 3,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "Here, $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ represents the minimum unknown after learning from $M_{t}^{\\mathrm{mem}}$ for this family of models, quantifying the expressive limitations of $\\Theta$ . As shown in Figure 12.1, $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ forms the envelopeof thefamilyof functions $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ ,where $\\theta$ ranges over $\\Theta$ ，\n\nFor a given model family $\\Theta$ $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ measures the best possible prediction of residual unknowns in addressing thetarget scientificproblem basedon $M_{t}^{\\mathrm{mem}}$ .Inotherwords,theknowledgecontent in $M_{t}^{\\mathrm{mem}}$ is captured by $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(\\bar{M}_{t}^{\\mathrm{mem}})$ . One can prove that $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ is monotonicallynon-increasing as $M_{t}^{\\mathrm{mem}}$ expands, since it forms the envelope of a family of non-increasing functions $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ .This expansion process is tied tohow theagent acts and gains information, driven by $M_{t}^{\\mathrm{wm}}$ ,which determines the optimal expansion and executes it through the action $a_{t}\\in\\mathcal A$ at time $t$ (see Table 1.2).",
                  "index": 4,
                  "part": 0,
                  "translated_content": "在这里，$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$代表了从$M_{t}^{\\mathrm{mem}}$学习后针对这类模型的最小未知量，量化了$\\Theta$的表达限制。如图12.1所示，$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$形成了函数族$D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$的包络线，其中$\\theta$在$\\Theta$范围内变化，\n\n对于给定的模型族$\\Theta$，$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$衡量了基于$M_{t}^{\\mathrm{mem}}$解决目标科学问题时残余未知量的最佳预测。换句话说，$M_{t}^{\\mathrm{mem}}$中的知识内容被$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(\\bar{M}_{t}^{\\mathrm{mem}})$所捕获。可以证明，$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$随着$M_{t}^{\\mathrm{mem}}$的扩展是单调非增的，因为它形成了一族单调非增函数$D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$的包络线。这一扩展过程与智能体如何行动和获取信息密切相关，由$M_{t}^{\\mathrm{wm}}$驱动，后者确定最优扩展并通过时间$t$的动作$a_{t}\\in\\mathcal A$执行它（见表1.2）。"
                },
                {
                  "type": "text",
                  "content": "During knowledge discovery,diffrent strategiescan beemployedtoexpand $M_{t}^{\\mathrm{mem}}$ . The optimal expansion strategy is theonethatresultsinthesteepestdecreaseof $D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{me\\bar{m}}})$ .Forinstance,ingueeilltatei for expanding $M_{t}^{\\mathrm{mem}}$ , denoted as $^1M_{t}^{\\mathrm{mem}}$ and $^{2}M_{t}^{\\mathrm{mem}}$ . The first strategy, $^1M_{t}^{\\mathrm{mem}}$ , represents random exploration, while the second, $^{2}M_{t}^{\\mathrm{mem}}$ ,follows ahypothesis-drivenapproach[871]in whichthe agent firstformulatesahypothesis about theunderlying mechanismofthetargetproblemandthendesigns anexperiment tojustifyorfalsifythis hypothesis 1749] , on $M_{t}^{\\mathrm{mem}}$ $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ ase This approach is generally more efficient than random exploration for expanding $M_{t}^{\\mathrm{mem}}$ , leading to $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(^{2}M_{t}^{\\mathrm{mem}})$ descending faster than $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(^{1}M_{t}^{\\mathrm{mem}})$ ，",
                  "index": 5,
                  "part": 0,
                  "translated_content": "在知识发现过程中，可以采用不同策略来扩展$M_{t}^{\\mathrm{mem}}$。最佳的扩展策略是导致$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$最陡下降的策略。例如，在扩展$M_{t}^{\\mathrm{mem}}$时，可以采用两种策略，分别表示为$^1M_{t}^{\\mathrm{mem}}$和$^{2}M_{t}^{\\mathrm{mem}}$。第一种策略$^1M_{t}^{\\mathrm{mem}}$代表随机探索，而第二种策略$^{2}M_{t}^{\\mathrm{mem}}$采用基于假设的方法，在这种方法中，智能体首先对目标问题的潜在机制提出假设，然后设计实验来验证或证伪这一假设。相对于随机探索，这种方法通常更有效率，能够使$M_{t}^{\\mathrm{mem}}$更快地扩展，导致$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(^{2}M_{t}^{\\mathrm{mem}})$的下降速度快于$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(^{1}M_{t}^{\\mathrm{mem}})$。"
                },
                {
                  "type": "text",
                  "content": "In general, the knowledge discovery processproceeds iteratively,repeatedlyoptimizing the world modelparameter $\\theta$ to approach $\\theta_{\\mathrm{K},t}^{*}$ and expanding $M_{t}^{\\mathrm{mem}}$ inarationalmannertoacceleratethedecreaseof $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ . The ideal state is achievingepistemiccompleteness,i.e., $D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})=0$ ,meaning zerodiscrepancybetweethe agent'spredition and the real-world phenomena.However, for a specific agent, a discovery bound may exist, where $D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ approaches zero butremains positive.These discrepancies arise from practicalconstraints andthe limitations of $\\Theta$ ， $\\mathcal{A}$ and other design spaces of the agent [872].Achieving a low discovery bound requires designing an adaptive world model architecture, an eficient knowledge expansion strategy, and a sufficient action space.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "通常，知识发现过程是迭代进行的，反复优化世界模型参数$\\theta$以接近$\\theta_{\\mathrm{K},t}^{*}$，并以理性的方式扩展$M_{t}^{\\mathrm{mem}}$，加速$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$的减少。理想状态是实现认识完备性，即$D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})=0$，表示智能体的预测与真实世界现象之间没有差异。然而，对于特定智能体，可能存在一个发现界限，即$D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$接近零但仍保持正值。这些差异源于实际约束条件、$\\Theta$、$\\mathcal{A}$以及智能体的其他设计空间的限制。实现较低的发现界限需要设计一种自适应世界模型架构、高效的知识扩展策略以及充分的行动空间。"
                }
              ],
              "raw_title": "Intelligence Evolution Strategies",
              "type": null,
              "children": [],
              "translated_title": "12.1.3 智能进化策略"
            }
          ],
          "translated_title": "12.1 代理人智能在科学知识发现中的应用"
        },
        {
          "title": "12.2 Agent-Knowledge Interactions",
          "number": "12.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Typical forms ofscientificknowledge includeobservational knowledge(e.g.experimentalmeasurements,computational results),methodological knowledge (e.gxperimentalmethods,omputationaltechniques, protocols)andtheical knowledge (e.g.theories,laws,predictive models).These forms ofknowledgecancontribute toscientificunderstanding as long as theyconsistofdataandinformation processedinaway that affects theprobabilitydistributionof unknown information $P_{\\theta}\\left(\\mathbf{x}_{\\mathrm{U}}|M_{t}^{\\mathrm{mem}}\\right)$ , reduces $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ , and facilitates decision-making.",
              "index": 0,
              "part": 0,
              "translated_content": "典型的科学知识形式包括观察知识（例如实验测量、计算结果）、方法论知识（例如实验方法、计算技术、协议）和伦理知识（例如理论、法则、预测模型）。只要这些知识形式包含以影响未知信息概率分布 $P_{\\theta}\\left(\\mathbf{x}_{\\mathrm{U}}|M_{t}^{\\mathrm{mem}}\\right)$、减少 $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$，并促进决策的方式处理的数据和信息，它们就可以促进科学理解。"
            },
            {
              "type": "text",
              "content": "In principle,external scientificknowledge has been shown to be usefulin improving agent performance in reasoning and decision-making[873,874].However, the scope of this survey lies in how agents can autonomously discover and utilizeknowledge toenhancethemselves.Scientificknowledge discovery workflows typically involve hypothesis generation,protocol planning,conducting experiments andcomputations, analyzing data,deriving implications,and revising hypotheses-often as part of an iterativecycle.An agent that can perceive,learn,reason,and act has the potential to drive such workflows in an autonomous manner, forexample by using application programming interfaces (APIs)to interact with physicalinstruments toacquire scientificknowledge and iterativelyenhance itsknowledge base (Figure 12.2). The agent will use the acquired knowledge to update its mental states $M_{t}$ to make better decisions when interacting with the world $\\mathcal{W}$ . We will now highlight three scenarios where agents discover scientific knowledge and enhance themselves.",
              "index": 1,
              "part": 0,
              "translated_content": "原则上，已经证明外部科学知识对改善推理和决策中的代理性能是有用的。然而，本调查的范围在于代理如何能够自主地发现和利用知识来增强自身。科学知识发现工作流通常涉及假设生成、协议规划、实验和计算、数据分析、推导含义，并修订假设，通常作为迭代循环的一部分。一个能够感知、学习、推理和行动的代理有潜力以自主方式推动这种工作流，例如通过使用应用程序编程接口（API）与物理仪器进行交互以获取科学知识并迭代增强其知识库（图12.2）。代理将利用获取的知识来更新其心智状态 $M_{t}$，以在与世界 $\\mathcal{W}$ 互动时做出更好的决策。接下来，我们将重点介绍代理发现科学知识并增强自身的三种情景。"
            },
            {
              "type": "figure",
              "src": "images/c5c82e5a7514fcfaf43052e142255ac41d198892842dfdf66a1c16c30746c063.jpg",
              "alt": "",
              "caption": "Figure 12.2:Closed-loop knowledge discovery for sustainable self-evolution.The agent aims toiteratively enhance its intelligence $I Q_{t}^{\\mathrm{{agent}}}$ through hypothesis generation and testing,as wellas through data analysis and implication derivation. When interacting with the physical world $W$ , the agent generates hypotheses as an explicitly or implicitly predicted distribution $(P_{\\theta})$ of unknown information, takes actions $(a_{t})$ for hypothesis testing, observes experimental results $(o_{t})$ , and updates beliefs based on perception of the real-world distribution $(P_{W})$ . When not interacting with $W$ the agent distills knowledge from existing data and premises, updating mental states $M_{t}$ directly. Inspired by Figures 2.3 and 2.5 in [864].",
              "index": 2,
              "part": 0,
              "translated_caption": "图12.2：可持续自我进化的闭环知识发现。代理通过假设生成和测试以及数据分析和推导来迭代地增强其智能$I Q_{t}^{\\mathrm{{agent}}} $。当与物理世界$W$交互时，代理生成假设作为未知信息的明示或隐含预测分布$(P_{\\theta})$，采取行动$(a_{t})$进行假设测试，观察实验结果$(o_{t})$，并基于对真实世界分布$(P_{W})$的感知更新信念。当未与$W$交互时，代理从现有数据和前提中提炼知识，直接更新心智状态$M_{t}$。受[864]中图2.3和2.5的启发。"
            }
          ],
          "raw_title": "Agent-Knowledge Interactions",
          "type": null,
          "children": [
            {
              "title": "12.2.1 Hypothesis Generation and Testing",
              "number": "12.2.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Hypothesis generation and testing (Figure 12.2)is acriticalapplicationof agents in autonomous scientific discovery, as it hasthepotentialtoenableoutside-the-box innovations[749].In essence,hypothesis generation istheformation of potentialrules that govern data distribution—ranging from single observations to large datasets—pertaining to unobserved scientific phenomena.According to Sir Karl Popper,a scientific hypothesis must be falsifable[875,876]; in this discusson,wedefineahypothesisthat survivesfalsificationasajustifedtruehypothesis[877860].Typically, scientists test hypothesesbyconducting experiments toeitherjustify orfalsifythem.Ahypothesis isconsidered more valuable if it is broad enough to explain a wide range of data and is highly likely to be true.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "假设生成和测试（图12.2）是代理在自主科学发现中的一个关键应用，因为它有潜力促进创新思维[749]。实质上，假设生成是形成潜在规则的过程，这些规则控制着与未观察到的科学现象相关的数据分布——从单个观察结果到大型数据集。根据卡尔·波普尔爵士的说法，科学假设必须是可证伪的[875,876]；在本讨论中，我们将幸存于证伪之中的假设定义为合理的真实假设[877860]。通常，科学家通过进行实验来测试假设，以证明或证伪它们。如果一个假设足够广泛以解释广泛范围的数据，并且极有可能是真实的，那么它被认为更有价值。"
                },
                {
                  "type": "text",
                  "content": "To tacklea scientific problem,theagentformulatesone ora smallnumber of high-value hypotheses basedonits mental state $M_{t}$ , which contains only incomplete information about the partially observable world $\\mathcal{W}$ . After testing through experiments orcomputations,ajustifiedtruehypothesis becomes instructiveknowledge,expanding $M_{t}^{\\mathrm{mem}}$ in a way that rapidly minimizes $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ Hence,generatingandtestinghighvaluehypothesescanquicklypote knowledgediscovery andincrease $I Q_{t}^{\\mathrm{agent}}$ .Inthisscenarioteagentemploystheleaing futn $\\mathrm{L}$ , to process observations from hypothesis testing, $o_{t}$ , into knowledge and update its mental states $M_{t}$ .",
                  "index": 1,
                  "part": 0,
                  "translated_content": "为解决科学问题，代理根据其关于部分可观测世界$\\mathcal{W}$的不完整信息所包含的心智状态$M_{t}$，提出一个或少数几个基于高价值的假设。经过实验或计算测试后，一个经证明为真的假设成为有益的知识，扩展$M_{t}^{\\mathrm{mem}}$，从而迅速减小$D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$。因此，生成和测试高价值假设可以快速促进知识发现并增加$I Q_{t}^{\\mathrm{agent}}$。在这种情况下，代理采用学习函数$\\mathrm{L}$，将假设测试的观察$o_{t}$处理为知识，并更新其心智状态$M_{t}$。"
                },
                {
                  "type": "text",
                  "content": "Generating physically meaningful hypotheses is a keystep.The agent typically uses LLMs along withcollaborative architectures and domain knowledge forhypothesis generation[878].Siet al.[742]conductedalarge-scalehumanstudy involving over $100{\\mathrm{NLP}}$ researchers, and found that LLM-generated ideas were rated as more novel $(p<0.05)$ than human expert ideas,albeitslightlyweakerinfeasibility.Ghafarollahiet al.[743]developed SciAgents,whichgeneates and refines materials science hypotheses to elucidate underlying mechanisms, design principles, and unexpected properties of biologicall inspired materials.Basedonlarge-scale ontological knowledge graphs,SciAgents samples a viable pathbetweenconceptsof interest,formulates apertinenthypothesis,andexpands it intoafullresearchproposal withdetailedhypothesis-testing methodsandcriteria.Itemploystwodedicatedagents toreview,critique,andimprove theproposedhypothesis,butdoes notinclude the stepof hypothesis testingthroughactualexperiments.Similarly,Suet al.[879]and Baek et al.[880]proposed leveraging teamwork—such as collaborative discussions and agent critics—to produce novel and effective scientific hypotheses. In addition, Gower et al. [881] introduced $\\mathrm{LGEM^{+}}$ , which utilizes a first-orderlogicframework todescribe biochemicalpathways and generate 2,094 unique candidate hypotheses for the automated abductive improvement of genome-scale metabolic models in the yeast S. cerevisiae.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "生成具有物理意义的假设是一个关键步骤。代理通常利用LLMs与协作架构和领域知识进行假设生成[878]。Siet al.[742]进行了一项涉及100多名NLP研究人员的大规模人类研究，发现LLM生成的想法在新颖性上评分更高（p<0.05）而在可行性上略微弱。Ghafarollahiet al.[743]开发了SciAgents，用于生成和完善材料科学假设，以阐明生物启发材料的基本机制、设计原则和意想不到的特性。基于大规模本体知识图，SciAgents在感兴趣的概念之间采样一个可行路径，构思一个相关的假设，并将其扩展为一个具有详细假设测试方法和标准的完整研究提案。它利用两个专门的代理人来审查、批评和改进所提出的假设，但不包括通过实际实验进行假设测试的步骤。类似地，Suet al.[879]和Baek等人[880]提出利用团队合作，如协作讨论和代理批评，以产生新颖且有效的科学假设。此外，Gower等人[881]引入了LGEM+，它利用一阶逻辑框架描述生物化学途径，并为酵母S. cerevisiae的基因组规模代谢模型的自动背推改进生成了2,094个独特的候选假设。"
                },
                {
                  "type": "text",
                  "content": "Hypotheses only become knowledge after being justified through computational or experimental observations. Lu et al.[745]introduced the AI Scientist,a system designed forfully automated scientificdiscovery.The AIScientist can conductresearch independentlyandcommunicate itsfindings,as demonstratedin threemachinelearning subfieldsdiffusion modeling,transformer-based language modeling,and leaning dynamics. It generates original research ideas,writescde,performscomputationalexperiments,visualizesresults,draftscomplete scientificpapers,and even simulates a peer review processfor evaluation. For instance, it proposed the hypothesis that“adaptive dual-scale denoising can improve diffusion models by balancing global structure and localdetails in generated samples\"which was justified through image generation tests on four 2D datasets. Similarly, Schmidgalletal.[746] developed the Agent Laboratory toautonomouslycarry out the entireresearch process,including literaturereview,computational experimentation, and report writing.They evaluated Agent Laboratory's capability for knowledge discovery by addressing five research questions in computer vision and naturallanguage processng,achieving an average humanevaluatedexperiment qualityscore of 3.2out of 5.Inadition,Tiukovaetal.[744]developed Genesis,anautomated system capable of controlling one thousand $\\mu$ -bioreactors, performing mass spectrometry characterization, accessing a structured domain information database, and applying experimental observations to improve systems biology models. Genesis can initiate and execute 1,O0o hypothesis-driven closed-loop experimentalcycles per day.Using a similar approach,the Genesis team has advanced the yeast(S.cerevisiae)diauxic shift model,outperformingthe previous best and expanding its knowledge by 92 genes $(+45\\%)$ and 1,048 interactions $(+147\\%)$ [882]. This knowledge also advances our understandingofcancer, the immune system,and aging.Similarly,Gottweis et al.[749]introducedthe AIco-scientist,whichautonomouslygenerates andrefines novelresearch hypotheses,within vitro validation in three biomedicalareas:drug repurposing,noveltarget discovery,and mechanisms of bacterialevolution and antimicrobial resistance.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "假设只有通过计算或实验观察得到证实后才能成为知识。Lu等人引入了AI科学家，这是一个旨在完全自动化科学发现的系统。AI科学家可以独立进行研究并传达其发现，如在扩散建模、基于Transformer的语言建模和学习动态领域所展示的。它生成原创研究思路，撰写代码，执行计算实验，可视化结果，起草完整的科学论文，甚至模拟同行评审过程以进行评估。例如，它提出了“自适应双尺度去噪可以通过在生成的样本中平衡全局结构和局部细节来改进扩散模型”的假设，通过对四个2D数据集进行图像生成测试得到证实。类似地，Schmidgal等人开发了Agent Laboratory，可以自主进行整个研究过程，包括文献回顾、计算实验和报告撰写。他们通过解决计算机视觉和自然语言处理领域的五个研究问题来评估Agent Laboratory在知识发现方面的能力，取得了平均人工评估实验质量得分为3.2（满分5分）。此外，Tiukova等人开发了Genesis，这是一个自动化系统，能够控制一千个微生物反应器，执行质谱表征，访问结构化领域信息数据库，并将实验观察应用于改进系统生物学模型。Genesis每天可以发起和执行1000个以假设为驱动的封闭循环实验周期。采用类似方法，Genesis团队已经推进了酵母（S.cerevisiae）二态转换模型，优于之前的最佳模型，并通过增加92个基因（+45%）和1048个相互作用（+147%）扩展了知识。这些知识也推进了我们对癌症、免疫系统和衰老的理解。类似地，Gottweis等人介绍了AI共同科学家，它可以自主生成和完善在药物再利用、新靶点发现以及细菌进化和抗微生物耐药机制等三个生物医学领域的新研究假设，并进行体外验证。"
                },
                {
                  "type": "text",
                  "content": "Discovered knowledge enhances the agent's mental states, such as $M_{t}^{\\mathrm{mem}}$ ， $M_{t}^{\\mathrm{wm}}$ , and $M_{t}^{\\mathrm{rew}}$ . Tang et al. [747] developed ChemAgent, which improves chemical reasoning through a dynamic, self-updating memory, $M_{t}^{\\mathrm{mem}}$ ChemAgent proposes hypothetical answers tochemistry questions ina development dataset,evaluates themagainst the ground truth,and simulatesthe hypothesis-testing processused inreal-worldresearch.Correctanswers arethenstored as knowledge in its memory to support future chemistry question answering.This self-updating memory resulted in performance gains of up to $46\\%$ (with GPT-4) when ChemAgent was applied to four chemical reasoning datasets from SciBench[883]. Wang et al.[884] introduced Molecular Language-Enhanced Evolutionary Optimization (MOLLEO), which iteratively proposes hypotheses for modifying candidate drug molecules in $M_{t}^{\\mathrm{mem}}$ , evaluates their drug-likeness and activity, and updates the candidates in $M_{t}^{\\mathrm{mem}}$ to enhance drug discovery.Similarly, Jiaet al.[885] developed LLMatDesign, which employs hypothesis-guided structure generation and a self-updating $M_{t}^{\\mathrm{mem}}$ to design inorganic photovoltaic materials,whose idealityisdefinedbymatching thetargetbandgapandhaving themost negativeformation energy.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "发现的知识增强了代理的心智状态，如$M_{t}^{\\mathrm{mem}}$，$M_{t}^{\\mathrm{wm}}$和$M_{t}^{\\mathrm{rew}}$。唐等人开发了ChemAgent，通过动态的、自我更新的记忆$M_{t}^{\\mathrm{mem}}$来提高化学推理能力。ChemAgent在一个开发数据集中针对化学问题提出假设答案，评估其与真实情况的一致性，并模拟现实世界研究中使用的假设检验过程。正确答案随后被存储为知识在其记忆中，以支持未来的化学问题回答。这种自我更新的记忆使ChemAgent在应用于SciBench的四个化学推理数据集时性能提升高达46%（使用GPT-4）。王等人引入了分子语言增强进化优化（MOLLEO），该方法迭代地提出修改候选药物分子的假设，评估其药物样性和活性，并在$M_{t}^{\\mathrm{mem}}$中更新候选物质以增强药物发现。类似地，贾等人开发了LLMatDesign，采用假设引导的结构生成和自我更新的$M_{t}^{\\mathrm{mem}}$来设计无机光伏材料，其理想性由匹配目标带隙和具有最负形成能确定。"
                },
                {
                  "type": "text",
                  "content": "Simet al.[748]introduced ChemOS 2.0,whichorchestratesclosed-loopoperations inchemical self-driving laboratories (SDLs).ChemOS 2.0integrates ab initiocalculations,experimentalorchestration, andstatisticalalgorithms forthe autonomous discovery ofhigh-performance materials.Acase study ondiscovering organic lasermolecules demonstrates its capabilities. It employs a Bayesian optimizer, Altas, as its world model $M_{t}^{\\mathrm{wm}}$ to predict the optical properties of hypothetical molecules—specifically Bis[(N-carbazole)styryl]biphenyl (BSBCz)derivatives-including gain cross section and spectral grain factor. Based on these predictions,ChemOS 2.0 recommends molecules with a higher probabilityof success inthe experimentalcampaign.Itthen utilizes anopticalcharacterization platformandthe AiiDA software package to measure and simulate the properties of test molecules. The results are used to update $M_{t}^{\\mathrm{wm}}$ improving the accuracy of future experimental predictions.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "Sim等人[748]引入了ChemOS 2.0，该系统在化学自动驾驶实验室（SDLs）中进行闭环操作的编排。ChemOS 2.0整合了从头计算、实验编排和统计算法，用于自主发现高性能材料。一个有机激光分子的发现案例展示了其能力。它采用贝叶斯优化器Altas作为其世界模型$M_{t}^{\\mathrm{wm}}$，用于预测假设分子的光学特性，具体包括双[(N-碳基)亚基]联苯（BSBCz）衍生物的增益截面和光谱增益因子。基于这些预测，ChemOS 2.0推荐在实验活动中成功概率较高的分子。然后利用光学表征平台和AiiDA软件包来测量和模拟测试分子的性质。结果用于更新$M_{t}^{\\mathrm{wm}}$，提高未来实验预测的准确性。"
                },
                {
                  "type": "text",
                  "content": "Hysmithet al.[886] published a perspective highlighting the crucial role of reward function design in developing forward-looking workflows for SDLs. Agents can be highly efective at solving POMDP problems in simulated environments,such ascomputer games or simulations,butoften struggle withreal-worldapplications.A welldefined reward function is essential for iterative self-evolution.However, in many real-world scientificresearch problems, reward functions areilldefinedorabsent attheendofexperimentalcampaignsdue tothelackofdirect measurements, thecomplexityofexperimentalresults,andthe needtobalance multipleobjectives.Thediscoveryofnewknowledge can serve as a valuable resource for refining $M_{t}^{\\mathrm{rew}}$ , guiding hypothesis exploration and experimental data collection.",
                  "index": 6,
                  "part": 0,
                  "translated_content": "Hysmith等人[886]发表了一篇观点文章，强调了奖励函数设计在发展面向未来的SDL工作流程中的关键作用。在模拟环境中，如电脑游戏或模拟中，代理可以高效解决POMDP问题，但往往难以应用于真实世界的场景。一个明确定义的奖励函数对于迭代式自我进化至关重要。然而，在许多真实世界的科学研究问题中，由于缺乏直接测量、实验结果复杂性以及需要平衡多个目标，奖励函数往往在实验活动结束时被定义不清晰或缺失。发现新知识可以作为改进$M_{t}^{\\mathrm{rew}}$、引导假设探索和实验数据收集的宝贵资源。"
                }
              ],
              "raw_title": "Hypothesis Generation and Testing",
              "type": null,
              "children": [],
              "translated_title": "12.2.1 假设生成与检验"
            },
            {
              "title": "12.2.2 Protocol Planning and Tool Innovation",
              "number": "12.2.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The capability to plan experimental protocols and optimize tool usage enablesthe agent to solve complex scientific puzzles withinthe autonomous discovery loop.As introducedin Section 9.4,the agentcan systematicallyevaluate and refine its approach toselecting,invoking,andintegratingavailabletools—andevendevelopnewtools tailored to specific task requirements. While optimized protocols and tool usage do not directly reduce $\\bar{D}_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ , they enhance execution efficiency and effectiveness in refining the probability distribution of unknown information, $\\dot{P}_{\\theta}\\left(\\mathbf{x}_{\\mathrm{U}}|M_{t}^{\\mathrm{mem}}\\right)$ ， thereby accelerating knowledge discovery. In this scenario,the agent leverages the reasoning function $\\mathrm{R}$ to translate its evolving mental states $M_{t}$ , continuously updated with new knowledge, into real-world actions $a_{t}$ for more effective and faster hypothesis testing (Figure 12.2).",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在能够规划实验方案并优化工具使用的能力下，代理程序能够在自主发现循环中解决复杂的科学难题。正如第9.4节中介绍的那样，代理程序可以系统地评估和完善其选择、调用和整合可用工具的方法，甚至开发出符合特定任务需求的新工具。虽然优化的方案和工具使用并不能直接降低 $\\bar{D}_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$，但它们提高了执行效率和效果，从而加速未知信息的概率分布 $\\dot{P}_{\\theta}\\left(\\mathbf{x}_{\\mathrm{U}}|M_{t}^{\\mathrm{mem}}\\right)$ 的精炼，进而加快知识发现。在这种情况下，代理程序利用推理功能 $\\mathrm{R}$ 将其不断更新的新知识反映在心智状态 $M_{t}$ 中，转化为更有效和更快的假设检验的现实行动 $a_{t}$（见图12.2）。"
                },
                {
                  "type": "text",
                  "content": "Scheduling and orchestrating theselection and recombination of existing tools is critical.Scientific experiments typically depend on diverse instruments for analyzing reaction products, with decisions rarely rely on just one measurement.Eectively utilizing necessary instruments without wasting resources and time requires the agent to learn to use tools in an integrated and adaptive manner.Dai etal.[75o]designed amodular workflow thatintegrates mobile robots, anautomated synthesis platform, and variouscharacterization instruments forautonomous discovery. They exemplified this system across three domains:structural diversification chemistry,supramolecular host-guest chemistry,and photochemical synthesis.The mobile robot followsa synthesis-analysis-decision cycle to mimic human experimental strategies,autonomously determining subsequent workflow steps.It selects appropriate instruments,such as the Chemspeed ISynth platform for synthesis, aliquid chromatography-massspectrometer(UPLC-MS)for measuring mass spectracorresponding tochemical peak signals,and a benchtop nuclear magnetic resonance spectrometer (NMR) for tracking chemical transformations from starting materials to products.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "安排和协调选择和重组现有工具是至关重要的。科学实验通常依赖于各种仪器来分析反应产物，决策很少仅依赖于单一测量。在不浪费资源和时间的情况下有效利用必要的仪器，需要代理程序学会以一种整合和适应的方式使用工具。Dai等人设计了一个模块化工作流程，集成了移动机器人、自动合成平台和各种表征仪器，用于自主发现。他们在结构多样化化学、超分子主客体化学和光化学合成三个领域展示了这一系统。移动机器人遵循合成-分析-决策循环，模仿人类实验策略，自主确定后续工作流程步骤。它选择适当的仪器，例如Chemspeed ISynth平台用于合成，液相色谱-质谱仪（UPLC-MS）用于测量与化学峰信号相对应的质谱，以及台式核磁共振谱仪（NMR）用于跟踪起始物到产物的化学转化过程。"
                },
                {
                  "type": "text",
                  "content": "Beyond individuallaboratories,toolorchestration is essntial for delocalized and asynchronous scientific discovery. Strieth-Kalthoff et al.[751] demonstrated a closed-loop integration of five materials science laboratories across three continents,advancing delocalized and democratizedscientific discovery.These fivelaboratories have varying strength—for example,the Universityof BritishColumbia specializes incontinuous preferentialcrystallzation,while Kyushu University excels inthin filmfabrication andcharacterization.Strieth-Kalthoff et al.employed acloud-based experiment planner tocontinuously learn from the incoming data and efectively prioritize informative experiments acro the fivelaboratories,resulting inthediscoveryof 2lnewstate-of-the-art materialsfororganic solid-statelasers.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "超越单个实验室，工具编排对于非集中和异步科学发现至关重要。Strieth-Kalthoff等人展示了跨越三大洲的五个材料科学实验室的闭环集成，推动了非集中化和民主化的科学发现。这五个实验室各有所长——例如，不列颠哥伦比亚大学专注于连续优先结晶，而九州大学擅长薄膜制备和表征。Strieth-Kalthoff等人采用基于云的实验计划器，持续从传入数据中学习并有效地优先考虑五个实验室中具有信息量的实验，从而发现了21种新的用于有机固态激光器的最新材料。"
                },
                {
                  "type": "text",
                  "content": "Moreover,the agent can optimize existing tools andeven create new ones to enhance its capabilities.Swanson et al.[752]developedtheVirtualLab,anAI-drivenresearch environment thatfacilitated the design and experimental validation of new SARS-CoV-2 nanobodies. Within the Virtual Lab, AIagents conduct scientific discusson in team meetingsand execute specializedtasks inindividualsessions.Onekey agendafortheagents was developing tools toaid in the design of nanobody binders [887],including:(1)asequence analysis toolthatrankscandidate point mutations using log-likelihoodratios fromthe ESM protein language model[88]; (2)a structure evaluation tool that extracts interface pLDDT scores fromAlphaFold-Multimer predictions [889],ofering a proxy for antibody-antigen binding affnity; and (3)anenergyestimationtolbuiltonRoseta[890]toquantifybinding strengthbetweennanobodyvariants and the spike protein.These agent-generatedtools enabled theVirtual Lab to discover two novelnanobodies with enhanced binding tothe JN.1or KP.3SARS-CoC-2variants,while preserving strong affnity for the ancestral viral spike protein.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "此外，代理还可以优化现有工具，甚至创建新工具以增强其功能。Swanson等人开发了Virtual Lab，这是一个由人工智能驱动的研究环境，促进了新型SARS-CoV-2纳米抗体的设计和实验验证。在Virtual Lab中，人工智能代理在团队会议中进行科学讨论，并在个别会话中执行专门任务。代理的一个重要议程是开发工具，以辅助设计纳米抗体结合物[887]，包括：(1) 一种序列分析工具，利用来自ESM蛋白语言模型的对数似然比对候选点突变进行排序；(2) 一种结构评估工具，从AlphaFold-Multimer预测中提取界面pLDDT分数，为抗体-抗原结合亲和力提供代理；以及(3) 一个基于Roseta的能量估算工具，用于量化纳米抗体变体与刺突蛋白之间的结合强度。这些代理生成的工具使Virtual Lab能够发现两种新型纳米抗体，其对JN.1或KP.3SARS-CoV-2变体具有增强结合能力，同时保持对祖先病毒刺突蛋白的强亲和力。"
                }
              ],
              "raw_title": "Protocol Planning and Tool Innovation",
              "type": null,
              "children": [],
              "translated_title": "12.2.2 协议规划与工具创新"
            },
            {
              "title": "12.2.3 Data Analysis and Implication Derivation",
              "number": "12.2.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Although most knowledge discovery processes relyon generating hypotheses andtesting themin the real world-where observations $o_{t}$ are essential—a significant portion of knowledge can be derived purely through internal actions such as iterative reasoning and deep thinking,whichare common in theoretical disciplines.For example,all theorems in Euclidean geometry can be deduced from just five axioms,butthese theorems do not explicitly exist in the mental state beforethey arederived. Givenallnecessary premises,such as Euclid's five postulates,the true probabilityofa hypothesis may remain elusive. However, using deductive and inductive reasoning to draw implications from known premises and data can help either justify or falsify hypotheses, thus reducing $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ and enhancing $I Q_{t}^{\\mathrm{agent}}$ (Figure 12.2). In this scenario, the agent employs the cognition function C to use prior mental states $M_{t-1}$ and internal actions $a_{t}$ to derive new knowledge and update mental states to $M_{t}$ ，",
                  "index": 0,
                  "part": 0,
                  "translated_content": "尽管大多数知识发现过程依赖于提出假设并在现实世界中进行测试，其中观察值 $o_{t}$ 至关重要，但是相当一部分知识可以纯粹通过内部行为（如迭代推理和深入思考）获取，这在理论学科中很常见。例如，欧几里得几何中的所有定理都可以从仅有的五条公设推导出来，但在推导之前，这些定理在心智状态中并不存在。在给定所有必要前提条件（如欧几里得的五条公设）的情况下，假设的真实概率可能仍然难以捉摸。然而，利用演绎和归纳推理从已知前提和数据中得出推论可以帮助证实或推翻假设，从而减少 $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ 并增强 $I Q_{t}^{\\mathrm{agent}}$（见图12.2）。在这种情况下，代理利用认知函数 C，利用先前的心智状态 $M_{t-1}$ 和内部行为 $a_{t}$ 推导新知识，并更新心智状态为 $M_{t}$。"
                },
                {
                  "type": "text",
                  "content": "Deductive reasoning enables knowledge derivation through logic. Trinh et al.[753] developed AlphaGeometry for the forward deduction of new mathematical theorems based on existing theorems in Euclidean plane geometry. AlphaGeometry employs a neural language model to construct auxiliary points in plane geometry problems and integratesspecialized symbolic engines toexhaustivelydeduce new true statements,therebyexpanding the jointclosure of known truths.Byleveraging this expanded closure, it altermates between auxiliary constructions and symbolic reasoning engines to uncover further implications.AlphaGeometry demonstratedremarkable performance ona test setof 30recent Olympiad-level problems,solving 25—more thandouble the10 problems solved by the previous best method-and coming close to the level of an average International Mathematical Olympiad (IMO) gold medalist.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "演绎推理通过逻辑实现知识推导。Trinh等人开发了AlphaGeometry[753]，用于基于欧几里得平面几何中现有定理的前向推导新数学定理。AlphaGeometry采用神经语言模型构建平面几何问题中的辅助点，并整合专门的符号引擎，通过详尽推导新的真命题，从而扩展已知真相的联合闭包。通过利用这一扩展闭包，它在辅助构造和符号推理引擎之间交替，揭示进一步的含义。AlphaGeometry在一个包含30个最新奥林匹克水平问题的测试集上表现出色，解决了25个问题，是之前最佳方法解决的10个问题的两倍以上，并接近平均国际数学奥林匹克（IMO）金牌得主的水平。"
                },
                {
                  "type": "text",
                  "content": "Inductive reasoning enables knowledge derivation through pattern recognition and statisticallearning.Liu et al. [754] introduced theTeam of AI-made Scientists(TAIS)to simulate therole of adata scientist for streamlined data analysis.TAIS decomposes a complex data analysis problem into different computational tasks, including coding, self-critique,andregresson analysis, toextract meaningfulinsights fromcomplexdatasets.When applied toidentifing disease-predictive genes, TAIS achieved an overall success rate of $45.73\\%$ on a benchmark dataset containing 457 genetic questions.Ideally,the extracted insights should be logicall sound; otherwise,they must bediscarded to ensure only accurate findings are safely integrated into mental states.However,limitation in datacoverage and the implementation of analysis algorithms may lead to hallcinated insights, underscoring the need for reliable data analyzers and reasoning tools to prevent over-analysis.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "归纳推理通过模式识别和统计学习实现知识推导。Liu等人[754]引入了人工智能科学家团队(TAIS)，以模拟数据科学家的角色，实现数据分析的流程化。TAIS将复杂的数据分析问题分解为不同的计算任务，包括编码、自我批判和回归分析，以从复杂数据集中提取有意义的见解。当应用于识别疾病预测基因时，TAIS在包含457个遗传问题的基准数据集上取得了总体成功率为45.73%。理想情况下，提取的见解应该在逻辑上合理；否则，它们必须被丢弃，以确保只有准确的发现能够安全地融入思维状态。然而，数据覆盖范围的限制和分析算法的实施可能导致虚构的见解，凸显了需要可靠的数据分析器和推理工具来防止过度分析。"
                }
              ],
              "raw_title": "Data Analysis and Implication Derivation",
              "type": null,
              "children": [],
              "translated_title": "12.2.3 数据分析与含义推导"
            }
          ],
          "translated_title": "12.2 代理-知识交互"
        },
        {
          "title": "12.3 Technological Readiness and Challenges",
          "number": "12.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "The self-evolution of agents,which in turndrives the advancement of human knowledge,is promised by their early success in the innovation cycle.This cycle involves generating meaningful hypotheses,designing real-time testing protocols,coordinating various experimental and computational tools, analyzing data,deriving implications,and engaging inself-reflection.However, achieving fullyautonomous self-evolutionremains a significant challenge,given the current technology readiness levels (TRLs)of three fundamentalcapabilities:real-world interaction,complex reasoning,and the integrationofpriorknowledge.Further technologicalprogress is required to improve the cycleof self-driven innovation.",
              "index": 0,
              "part": 0,
              "translated_content": "智能代理的自我演化，从而推动人类知识的进步，在创新周期的早期成功中得以实现。这一周期包括生成有意义的假设、设计实时测试协议、协调各种实验和计算工具、分析数据、得出结论，并进行自我反思。然而，实现完全自主的自我演化仍然是一个重大挑战，鉴于当前技术准备水平（TRL）在三个基本能力方面存在局限：现实世界互动、复杂推理和先验知识的整合。需要进一步的技术进步以改善自驱动创新的循环。"
            }
          ],
          "raw_title": "Technological Readiness and Challenges",
          "type": null,
          "children": [
            {
              "title": "12.3.1 Real-World Interaction Challenges",
              "number": "12.3.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Agents interact with the real world primarily through application programming interfaces (APIs).While numerous demonstrations[891]have showntheir strong capabilityto use various APIs,a significant bottleneck inautonomous knowledgediscovery remains:the lack of APIs that allow agents to directly execute tasks ina physicallaboratory. Physical APIs—interfaces that enable direct control of lab equipmentare far lessabundant than computational APIs due to the significant investment of time, expertise,and cost required to develop them.Although existing autonomous laboratories have shown promise,they remain in an early developmental stage(typically TRL 4-6),where straightforward replication or scale-up ischallenging.Consequently, building further systems or broadening their applicationacrossadditional scientificdomains stillrequires substantialcustomizationtoaddress domain-specific needs, along with specialized expertise.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "智能代理主要通过应用程序接口（APIs）与现实世界进行交互。尽管许多演示已经展示了它们利用各种API的强大能力，但在自主知识发现方面仍存在一个重要瓶颈：缺乏允许代理直接在物理实验室执行任务的APIs。物理APIs——能够直接控制实验室设备的接口相对较少，这是因为开发这些接口需要大量的时间、专业知识和成本投入。尽管现有的自主实验室显示出了潜力，它们仍处于早期发展阶段（通常是TRL 4-6），在这个阶段，直接复制或扩展规模是具有挑战性的。因此，进一步构建系统或将其应用扩展到额外的科学领域仍需要大量定制以解决特定领域需求，以及专门的专业知识。"
                },
                {
                  "type": "text",
                  "content": "Twokey tasks are essntialforenablingreal-worldinteraction:operating labdevices andtransferring samples between devices.Seamlessintegration of physical hardware and experimental samples is crucial to maintaining uninterrupted workflows.However, most experimental instruments are originally designed for human operation. Making them accessible to agents requires extensive efforts across multiple disciplines, including robotics,electricalenginering, mechanicalengineeing, and software programming.The rising prominence of SDLs is catalyzing the transformation of human-operated devices into agent-accessble systems through APIs. In autonomous labs conducting complex experiments,two parallel and often complementary approaches are commonly adopted to integrate hardware with agentic systems.Both approaches are modular,reconfigurable,and valuable, yet they require ongoing,dedicated development.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "实现现实世界交互的两个关键任务是操作实验室设备和在设备之间传输样本。无缝集成物理硬件和实验样本对于保持工作流程的连续性至关重要。然而，大多数实验仪器最初是为人类操作而设计的。使它们对代理可访问需要跨越多个学科的广泛努力，包括机器人技术、电气工程、机械工程和软件编程。自主实验室的日益突出促使人类操作的设备通过APIs转变为代理可访问的系统。在进行复杂实验的自主实验室中，通常采用两种并行且常常互补的方法来将硬件与代理系统集成。这两种方法都是模块化、可重构且有价值的，但它们需要持续的专门开发。"
                },
                {
                  "type": "text",
                  "content": "Approach 1: API Integration via Direct Device Adaptation.This approach involves equipping individual devices with dedicated mechanical adaptations and I/O controllrs,enabling them to receive and execute commands from a central controlPC.Forexample,toachieve solid-state synthesis and structuralcharacterization of inorganic materials, A-lab has implemented 16 types of devices to automate experimental tasks such as powder dosing, heating,and diffraction [892].This approach allows laboratories to function as fully integrated entities by maximizing device utilization,optimizing space andresources, and enabling bespoke tools.However,it is costly,time-consuming,and requiresexpert knowledge to prototype or retrofit devices for automation.Large language models (LLMs)have been applied to facilitate access to diverse tools,asillustrated byCACTUS,a Chemistry Agent Connecting Tool-Usage to Science [893].",
                  "index": 2,
                  "part": 0,
                  "translated_content": "方法一：通过直接设备适配实现API集成。这种方法涉及为单个设备配备专用的机械适配器和I/O控制器，使它们能够接收并执行来自中央控制PC的命令。例如，为了实现无机材料的固态合成和结构表征，A实验室已经实施了16种设备，用于自动化实验任务，如粉末投料、加热和衍射。这种方法允许实验室作为完全集成的实体运作，通过最大化设备利用率、优化空间和资源，并实现定制工具。然而，这种方法成本高、耗时长，并且需要专业知识来为自动化原型化或改装设备。大型语言模型(LLMs)已被应用于促进对各种工具的访问，如化学代理连接工具使用到科学中的CACTUS。"
                },
                {
                  "type": "text",
                  "content": "A more accessble alternative for smallteams is thecloud lab or science factory [894], where responsibility for device engineeing shifts from individuallaboratories todedicated userfacilities orcommercial service providers.For instance,Boikoet al.[895]demonstrated an autonomouschemicalresearch agent, Coscientist,capable ofcarrying out cross-coupling Suzuki and Sonogashira reactions using experimental setups at the Emerald Cloud Lab[896].However, cloud labs offer only afixed set of pre-built devices optimized forcommon procedures,posing potentialchallenges for researchers whose experiments require equipment customization,as integrating non-standard tools may involve a lengthy process of negotiation and development.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "对于小团队来说，更具可访问性的选择是云实验室或科学工厂[894]，在这里，设备工程的责任从单个实验室转移到专门的用户设施或商业服务提供商。例如，Boiko等人[895]展示了一种自主化学研究代理人Coscientist，能够在Emerald Cloud Lab的实验设置中执行交叉偶联Suzuki和Sonogashira反应。然而，云实验室仅提供针对常见程序优化的固定设备集，这对那些需要设备定制的研究人员可能构成潜在挑战，因为整合非标准工具可能涉及漫长的谈判和开发过程。"
                },
                {
                  "type": "text",
                  "content": "Approach 2: Robotic Operation of Experimental Devices. This approach involves using mobile robots or robotic arms to operate existing devices and transfer samples. In many cases,robots can interact with instruments without modification,apart from minoradjustmentssuch as adding specializedactuators,grippers,orholders.Forexample,Dai et al.[750]employed mobilerobots toexplore syntheticchemistry.Intheirautonomous laboratory, mobilerobots enable physicalinkages betweensynthesis andanalysis devices that arespatially separated,automating sampletransportation and handling. In principle,the robots can perform allactions human researchers require in thelaboratory.However, current robotic systems stillrely on human pre-programming tomapthe lab layout,define movement trajectories,and register device positions.Handling unexpected or adaptive situations remains achallenge, as pre-programming cannot anticipate every possible state ofan experimental setup.Real-timelearning andadaptive manipulation are active areas ofresearchthat require further technologicaladvancements.Inthelong term,embodied AI[897]is expectedtoenhance robotic learning, allowing agents to quickly adapt to new environments and tools.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "方法二：实验设备的机器人操作。这种方法涉及使用移动机器人或机械手来操作现有设备并转移样本。在许多情况下，机器人可以与仪器互动而无需修改，除了进行一些小的调整，例如添加专门的执行器、夹持器或支架。例如，Dai等人利用移动机器人探索合成化学。在他们的自主实验室中，移动机器人实现了合成和分析设备之间的物理连接，这些设备在空间上是分开的，自动化了样本的运输和处理。原则上，机器人可以执行实验室中人类研究人员需要的所有动作。然而，当前的机器人系统仍然依赖人类预先编程来映射实验室布局，定义移动轨迹，并注册设备位置。处理意外或自适应情况仍然是一个挑战，因为预先编程无法预料实验装置的每种可能状态。实时学习和自适应操作是需要进一步技术进步的活跃研究领域。从长远来看，体现式人工智能有望增强机器人学习能力，使代理能够快速适应新环境和工具。"
                },
                {
                  "type": "text",
                  "content": "The two approaches can be combined. For example, Vescovi et al.[894] define a modular laboratory robotics architecturethat allowsfortranslatinghigh-levelcommands into specificoperations fora varietyof diferentrobotic apparatus andlaboratoryequipment, and forlinking robotic apparatus with other elements of an Al-driven discovery architecture,such as high-performance computing [898].This architecture has been used to automate experiments in both the biological and physical sciences[899].Similarly,Fernando et al.[900] integrate a Robotic Operating System 2(ROS2)compatible robot intothe Bluesky experimentalorchestration framework. Loet al.[90l] argue for thedevelopment and integration oflow-cost“frugaltwins\"of more expensiveequipment tofacilitate experimentation and democratize access.",
                  "index": 5,
                  "part": 0,
                  "translated_content": "两种方法可以结合使用。例如，Vescovi等人定义了一种模块化实验室机器人架构，允许将高级命令转化为各种不同机器人设备和实验室设备的特定操作，并将机器人设备与高性能计算等人工智能驱动发现架构的其他元素连接起来。该架构已被用于在生物和物理科学中自动化实验。类似地，Fernando等人将一个与Robotic Operating System 2（ROS2）兼容的机器人集成到Bluesky实验编排框架中。Lo等人主张开发和整合低成本的“节俭双胞胎”设备，以促进实验并使更多人能够获得实验设备。"
                }
              ],
              "raw_title": "Real-World Interaction Challenges",
              "type": null,
              "children": [],
              "translated_title": "12.3.1 现实世界互动挑战"
            },
            {
              "title": "12.3.2 Complex Reasoning Challenges",
              "number": "12.3.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "A fundamental philosophical question is whether agents,often powered by LLMs,can truly perform reasoning.By definition,languages models generate outputs by predicting the next token, a mechanism fundamentally diffrent from human reasoning. From an outcome-driven perspective, these input-output systems exhibit reasoning ability phenomenologically,as they produce meaningfuloutputs compared toareference system generating arbitraryresponses [902].However,regardlessof the perspective taken, thiscapability remains imperfect—particularly whenhandling complex logical and numerical problems, which are crucial for scientific knowledge discovery.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "一个基本的哲学问题是，通常由LLMs驱动的代理是否真正可以进行推理。根据定义，语言模型通过预测下一个标记来生成输出，这一机制与人类推理根本不同。从结果驱动的角度来看，这些输入-输出系统在现象学上表现出推理能力，因为它们产生的输出与生成任意响应的参考系统相比具有意义[902]。然而，无论采取何种角度，这种能力仍然是不完美的，特别是在处理对科学知识发现至关重要的复杂逻辑和数值问题时。"
                },
                {
                  "type": "text",
                  "content": "Agents and LLMs struggle with hard reasoning tasks. Glazer et al.[903]introduced FrontierMath,abenchmark comprising hundreds of original and challenging mathematics problems covering most major branches of modern mathematics.Evaluation of state-of-the-art LLM-driven agents-including ol-preview (OpenAl),ol-mini (OpenAI), GPT-4o(OpenAI, 2024-08-06 version), Claude 3.5 Sonnet (Anthropic, 2024-10-22 version),Grok 2 Beta (XAI), and Gemini $1.5\\:\\mathrm{Pro}\\:002$ (Google DeepMind)- -revealed that no model achieved even a $2\\%$ success rate on the full benchmark.Chen et al.[873] presented ScienceAgentBench, a benchmark designed to evaluate language agents in data-driven scientific discovery.Among 102tasks derived from 44 peer-reviewed publications acrossfour disciplines, OpenAI ol successfully solved only $42.2\\%$ of them. Chollet [865] proposed the Abstraction and Reasoning Challenge (ARC) to assesssLLMs'ability to perform abstract inductive reasoning without relying on memorization or external knowledge. Even with careful prompting, GPT-4o correctly solved only $19\\%$ of the tasks, far below the $\\sim75\\%$ average human performance[94,905].Zhuet al.[96]suggesteda four-level classification of AIintellgence, including Ll (arbitrating isputes), L2(auditing a review),L3(reviewing a paper), and L4(authoring a paper).They classify the current state-of-the-art LLM-driven agents as approaching L2-levelcapabilities.Toenhance agentsreasoning abilities, researchers have introduced techniquessuchaschain-of-thought[907],tree-of-thoughts[72],and[70].Although new methods continue toemerge,as discussed in Section 2.2,further advancements inreasoning capacityremaincrucial for achieving reliable causal inference in scientific research.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "代理和LLMs在艰难的推理任务中遇到困难。Glazer等人[903]引入了FrontierMath，一个包含数百个原创且具有挑战性的数学问题的基准，涵盖了现代数学的大多数主要分支。对最先进的由LLMs驱动的代理进行评估，包括ol-preview（OpenAI）、ol-mini（OpenAI）、GPT-4o（OpenAI，2024年8月6日版本）、Claude 3.5 Sonnet（Anthropic，2024年10月22日版本）、Grok 2 Beta（XAI）和Gemini $1.5\\:\\mathrm{Pro}\\:002$（Google DeepMind），结果显示没有任何模型在完整基准测试中取得甚至$2\\%$的成功率。Chen等人[873]提出了ScienceAgentBench，这是一个旨在评估语言代理在数据驱动科学发现中的基准。在从四个学科领域的44篇同行评审出版物中提取的102个任务中，OpenAI ol只成功解决了其中的$42.2\\%$。Chollet [865]提出了抽象和推理挑战（ARC），以评估LLMs在执行抽象归纳推理时不依赖记忆或外部知识的能力。即使经过仔细提示，GPT-4o仅正确解决了$19\\%$的任务，远远低于人类平均表现的$\\sim75\\%$[94,905]。Zhu等人[96]提出了AI智能的四级分类，包括L1（仲裁争端）、L2（审计评论）、L3（审查论文）和L4（撰写论文）。他们将当前最先进的LLM驱动代理分类为接近L2级能力。为增强代理的推理能力，研究人员引入了诸如chain-of-thought[907]、tree-of-thoughts[72]和[70]等技术。尽管新方法不断涌现，但正如第2.2节讨论的那样，推理能力的进一步发展对于在科学研究中实现可靠的因果推断仍然至关重要。"
                },
                {
                  "type": "text",
                  "content": "Agents and LLMs also struggle with quantitative and symbolic problems.For example, GPT-4 and GPT-3.5 often struggle with reliably performing complex arithmetic such as multiplying 12, $345\\times98$ , 765, or translating IUPAC chemical names into accurate molecular graphs [908, 697]. A common approach to overcoming these limitations is to useexternal tools rather than relying on the LLM itself for reasoning. In mathematical problem-solving,for example,tools like symbolicsolvers are preferred over direct LLM inference[753].However, this mitigation does not resolve the intrinsicdeficiency in numerical understanding,which poses a potentialrisk to scientific reasoning. Moreover, Yu et al.[909]found that tool-augmented LLMs do notconsistently outperform base LLMs without tools in chemistry problem-solving.For instance,for specializedchemistry tasks,such as synthesis prediction, augmenting",
                  "index": 2,
                  "part": 0,
                  "translated_content": "代理和LLMs在处理定量和符号问题时也存在困难。例如，GPT-4和GPT-3.5经常难以可靠地执行复杂的算术运算，如$12, 345\\times98$，765，或将IUPAC化学名称翻译成准确的分子结构图。克服这些限制的常见方法是使用外部工具，而不是依赖LLM本身进行推理。在数学问题解决中，例如，像符号求解器这样的工具比直接LLM推理更可取。然而，这种缓解并不能解决数值理解的固有缺陷，这可能对科学推理构成潜在风险。此外，Yu等人发现，在化学问题解决中，使用工具增强的LLMs并不总是能够在化学问题解决中 consistently 超越没有工具的基础LLMs。例如，在专门的化学任务中，如合成预测，增强LLMs with tools 不能一直胜过基础LLMs。"
                },
                {
                  "type": "text",
                  "content": "LLMs with specialized toolscan boost the performance substantially;however,oolaugmentationis less effective for generalchemistry questions,suchas those in exams,where no specific toolscan directly solvea given question.In these scenarios,an agent'sability toreasoncorrectly byusing multiple pieces ofchemistryknowledge becomes more important.\n\nThe preceding discussion emphasizes the importance of developing robust methodologies for evaluating AI agents as scientific research assistants, a topic discussed at length by Cappello et al. [910].",
                  "index": 3,
                  "part": 0,
                  "translated_content": "使用专门工具增强的LLMs可以显著提升性能；然而，对于一般化学问题，如考试中那些没有特定工具可以直接解决的问题，工具增强效果较差。在这些情况下，代理通过利用多个化学知识片段进行正确推理的能力变得更加重要。\n\n前面的讨论强调了开发评估人工智能代理作为科学研究助手的稳健方法的重要性，这是Cappello等人（910）长篇讨论的主题。"
                }
              ],
              "raw_title": "Complex Reasoning Challenges",
              "type": null,
              "children": [],
              "translated_title": "12.3.2 复杂推理挑战"
            },
            {
              "title": "12.3.3 Challenges in Integrating Prior Knowledge",
              "number": "12.3.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Prior kowledge isacrucial factor forhigher intellgence.As discusses in Section12.1,the agent's prior knowledge, $M_{t}^{\\mathrm{mem}}$ , helps decrease $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ and increase the agent's intelligence, $I Q_{t}^{\\mathrm{agent}}$ . Human-led scientific discoveries frequentlyachieve breakthroughs withrelativelysmalldatasets,thanks tothevastpriorknowledge humans possess.The start-of-the-art LLMs that power autonomous agentsare trainedon nearly all publicly available textualdata,including websites,books, and other sources, thereby encompassng most common knowledge as wellas publicly accessible specialized knowledge.However, achieving an agent that can seamlessly integrate allexisting human knowledge remains a significant challenge.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "先前的知识是提升智能的关键因素。如在第12.1节中讨论的，代理的先前知识$M_{t}^{\\mathrm{mem}}$有助于减少$D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$并提高代理的智能$I Q_{t}^{\\mathrm{agent}}$。人类主导的科学发现通常可以在相对较小的数据集上取得突破，这要归功于人类拥有的广泛先前知识。支持自主代理的最先进的LLMs被训练在几乎所有公开可用的文本数据上，包括网站、书籍和其他来源，从而涵盖了大部分常见知识以及公开可访问的专业知识。然而，实现一个能够无缝整合所有现有人类知识的代理仍然是一个重大挑战。"
                },
                {
                  "type": "text",
                  "content": "At least three types of knowledge sources may not be included in LLM pre-training: (1)Paywalled or unpublished knowledge, including non-open-access publications,industry-specific data,andfailed experiments [91l].Theyare often not accesible to publicmodels despite their potential value in refining domain-specific insights. (2)Empirical knowlege.Heuristic decisions by experts are often effective, particularly in scenarios where no existing data is available foranew problem.However,large amounts ofexpertheuristics aretypicallynotaccessibleas textual data.(3) Contextual or situational knowledge.Knowledge relatedtoreal-worldconditions,suchas safety protocols in chemical reactionsor equipment handling,is often absent from pre-trained models but is essentialfor practicalapplications.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "至少有三种类型的知识来源可能未包含在大型语言模型（LLMs）的预训练中：（1）付费或未发表的知识，包括非开放获取出版物、行业特定数据和失败的实验。尽管这些知识在细化领域特定见解方面具有潜在价值，但它们通常无法被公共模型访问。 （2）经验知识。专家的经验决策通常是有效的，特别是在没有现有数据可供解决新问题的情况下。然而，大量的专家启发式通常作为文本数据不可访问。 （3）情境或局部知识。与现实世界条件相关的知识，例如化学反应中的安全协议或设备操作，通常未包含在预训练模型中，但对于实际应用至关重要。"
                },
                {
                  "type": "text",
                  "content": "Additionally, integrating diverse knowledge sources presents challenges in reconciling conflicting information.For example, OpenAI's Deep Research [912] actively gathers online information and performs multi-step reasoning, achieving state-of-the-art performance on Humanity's LastExam and the GAIA benchmark.However,it stillstruggles to distinguish between authoritative information andrumors and exhibits limitations in confidence calibration,often misrepresenting its levelofcertainty[912]Establishing asystem toassessthelevels of evidence[913]of different knowledge fragments—such as quantifying reliability and verifying references—may be necessary for effctive knowledge fusion.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "此外，整合多样化的知识来源在协调冲突信息方面存在挑战。例如，OpenAI的Deep Research积极收集在线信息并进行多步推理，在“人类最后一次考试”和GAIA基准测试中取得了最先进的性能。然而，它仍然难以区分权威信息和谣言，并在信心校准方面存在局限，经常错误地表达其确定性水平。建立一个评估不同知识片段证据级别的系统，比如量化可靠性和验证参考文献，可能对有效的知识融合至关重要。"
                }
              ],
              "raw_title": "Challenges in Integrating Prior Knowledge",
              "type": null,
              "children": [],
              "translated_title": "12.3.3 整合先验知识中的挑战"
            }
          ],
          "translated_title": "12.3 技术准备度与挑战"
        }
      ],
      "translated_title": "12 科学发现与智能演化 120"
    },
    {
      "title": ") Design of Multi-Agent Systems 133",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "13.1 Strategic Learning: Cooperation vs. Competition 133\n13.2 Modeling Real-World Dynamics 134\n13.3 Collaborative Task Solving with Workflow Generation 135\n13.4 Composing AI Agent Teams 135\n13.5 Agent Interaction Protocols . 137\n13.5.1 Message Types 137\n13.5.2 Communication Interface 138\n13.5.3 Next-Generation Communication Protocols 138",
          "index": 0,
          "part": 0,
          "translated_content": "13.1 战略学习：合作与竞争 133\n13.2 建模现实世界动态 134\n13.3 通过工作流生成进行协作任务解决 135\n13.4 组合AI代理团队 135\n13.5 代理交互协议 137\n13.5.1 消息类型 137\n13.5.2 通信接口 138\n13.5.3 下一代通信协议 138"
        }
      ],
      "raw_title": ") Design of Multi-Agent Systems 133",
      "type": null,
      "children": [],
      "translated_title": "多Agent系统的设计"
    },
    {
      "title": "4 Communication Topology 141",
      "number": "4",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "14.1 System Topologies 141\n14.1.1 Static Topologies 141\n14.1.2 Dynamic and Adaptive Topologies 142\n14.2 Scalability Considerations 144",
          "index": 0,
          "part": 0,
          "translated_content": "14.1 系统拓扑结构 141\n14.1.1 静态拓扑结构 141\n14.1.2 动态和自适应拓扑结构 142\n14.2 可伸缩性考虑 144"
        }
      ],
      "raw_title": "Communication Topology 141",
      "type": null,
      "children": [
        {
          "title": "4.1 The Human World Model",
          "number": "4.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Humans naturallconstruct internalrepresentations of the world,often referred toasmental models in psychology 341 342,343]. These models serve as compact and manipulable depictions of externalreality,enabling individuals to predict outcomes,planactions,and interpret novel scenarios with minimalreliance on direct trial-and-error.Early work on spatialnavigation,forinstance,showed thathumans andanimalsform“cognitive maps\"of their surroundings [341], suggesting an underlying ability to imagine potential paths before actually traversing them.",
              "index": 0,
              "part": 0,
              "translated_content": "人类自然而然地构建对世界的内部表征，通常在心理学中被称为心智模型。这些模型作为对外部现实的简洁且可操作的描绘，使个体能够在最小程度依赖直接试错的情况下预测结果、规划行动，并解释新颖情境。早期关于空间导航的研究表明，人类和动物形成了对周围环境的“认知地图”，暗示了一种在实际穿越之前就能够想象潜在路径的能力。"
            },
            {
              "type": "text",
              "content": "Craik's seminal argument was that the human mind runs internal“smal-scale models of reality\"[342] to simulate how events might unfoldand evaluate posible courses of action.Later studies proposedthatsuchsimulations stretch across modalities-vision,language, and motor control—and are dynamicaly updated by comparing predictions to new observations.This process merges memory recallwithforward projection,implying aclose interplay between stored knowledge andthe active generation of hypothetical future states[343].More recent predictive processing theories such as“Surfing Uncertainty\"[344]propose thatthebrain operatesas ahierarchical prediction machine,continuously generating top-down predictions about sensory inputs and updating its models based on prediction errors.",
              "index": 1,
              "part": 0,
              "translated_content": "Craik的重要论点是，人类大脑运行内部的“现实小规模模型”[342]来模拟事件可能如何展开并评估可能的行动方案。后续研究提出，这种模拟跨越视觉、语言和运动控制等多种感知方式，并通过将预测与新观察结果进行比较而动态更新。这一过程将记忆回忆与向前投射相融合，暗示着存储知识与主动生成假设未来状态之间的密切相互作用[343]。更近期的预测处理理论，如“Surfing Uncertainty”[344]，提出大脑作为一个分层预测机器运作，不断生成关于感官输入的自上而下预测，并根据预测误差更新其模型。"
            },
            {
              "type": "text",
              "content": "Critically, these human mental models are:\n\n·Predictive: They forecast changes in the environment, informing decisions about where to move or how to respond.\n·Integrative: They combine sensory input,past experience, and abstract reasoning into a unified perspective on “what might happen next\".\n·Adaptive: They are revised when reality diverges from expectation, reducing the gap between imagined and actual outcomes over time.\n·Multi-scale: They operate seamlessly acrossdifferent temporal and spatial scales, simultaneously processing immediate physical dynamics (millseconds), medium-term action sequences (seconds to minutes), and longterm plans (hours to years).This flexibility allows humans to zoom in on fine-grained details or zoom out to consider broader contexts as needed.",
              "index": 2,
              "part": 0,
              "translated_content": "这些人类心智模型具有以下关键特点：\n\n·预测性：它们预测环境变化，指导决策何时移动或如何做出响应。\n·整合性：它们将感官输入、过往经验和抽象推理结合到一个统一的“接下来可能发生什么”的视角中。\n·适应性：当现实与期望不符时，它们会进行修订，随着时间的推移减少想象和实际结果之间的差距。\n·多尺度：它们在不同的时间和空间尺度上无缝运作，同时处理即时的物理动态（毫秒级）、中期的行动序列（秒到分钟）、以及长期计划（小时到年）。这种灵活性使人类能够根据需要放大细节或放大考虑更广泛的背景。"
            },
            {
              "type": "text",
              "content": "Consider hunger and eating asan ilustrationof integrated world modeling.When hungry,a person's internal model activates predictions about food—simulating not just visual appearance but tastes, smell, and anticipated satisfaction-triggering physiologicalresponses like salivation before food is even present.Thisdemonstrates seamless integration across perception, memory, and action planning.\n\nThe example alsohighlights adaptivity:once satiated, thesame modeldynamically updates,reducing predictedreward values for further eating.Despiterecognizing thesame fooditems, their anticipated utilitychangesbased on internal state.Furthermore, humans maintain counterfactual simulations—declining dessert now while accurately predicting they would enjoy itlater—enabling complex planning acrosshypothetical scenarios and time horizons,acapability comprehensive AI world models strive to replicate.\n\nIn sum,thehumanworldmodelisnota staticlibraryoffacts,butaflexibleandever-evolving mentalconstruct,deeply rooted in perception and memory, thatcontinuously shapes (and is shaped by)the individual's interactions with the outside world.",
              "index": 3,
              "part": 0,
              "translated_content": "以饥饿和进食为例，将其视为整合世界建模的示例。当感到饥饿时，一个人的内部模型会激活关于食物的预测——不仅模拟视觉外观，还有味道、气味以及预期的满足感，甚至在食物出现之前就会引发唾液分泌等生理反应。这展示了跨感知、记忆和行动规划之间的无缝整合。\n\n这个例子还突显了适应性：一旦满足，同样的模型会动态更新，降低进一步进食的预期奖励值。尽管识别出相同的食物项目，但它们的预期效用会根据内部状态而变化。此外，人类保持着反事实模拟——现在拒绝甜点，但准确预测将来会喜欢——这使得能够在假设情景和时间范围内进行复杂规划，这是综合人工智能世界模型努力复制的能力。\n\n总之，人类世界模型并非静态的事实库，而是一个灵活且不断演化的心智构建，深深扎根于感知和记忆，不断塑造（并受到塑造）个体与外部世界互动的方式。"
            }
          ],
          "raw_title": "The Human World Model",
          "type": null,
          "children": [],
          "translated_title": "4.1 人类世界模型"
        },
        {
          "title": "4.2 Translating Human World Models to AI",
          "number": "4.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Researchinartificialintelligencehaslong soughttoreplicatethe predictive,integrative,andadaptivequalities exhibited by human mental models [341, 342].Early reinforcement learning frameworks,for instance, proposed learning an environment modelfor planning—exemplified by Dyna[345]—while contemporaneous work investigated using neural networks to anticipate future observations in streamingdata[346,347].Both directions were motivated by the idea that an internal simulatorof the worldcouldenable more effcient decision-makingthan purelyreactive,trial-and-error learning.",
              "index": 0,
              "part": 0,
              "translated_content": "人工智能领域的研究长期以来一直致力于复制人类心智模型所展现的预测、整合和适应性特质。例如，早期的强化学习框架提出学习环境模型以进行规划，比如Dyna所展示的方式，同时同时代的工作研究了使用神经网络来预测流数据中未来的观察结果。这两个方向的动机都源于这样一个观念，即世界的内部模拟器可以实现比纯粹的反应式试错学习更高效的决策制定。"
            },
            {
              "type": "text",
              "content": "Subsequent advancements in deep learning brought the notionof“AI world models\"into sharper focus.One influential approach introduced anend-to-end latent generative modelofan environment (e.g.,“World Models\"[348]),whereby a recurrent neural network(RNN)and variational auto-encoder(VAE)together learn to“dream”future trajectories. These latent rollouts allow anagent to trainorrefine policies offline,effectively mirroring how humans mentally rehearse actions before executingthem.Alongside such implicit designs,explicit forward-modeling methods emerged in model-based RL, letting agents predict $P(\\bar{s}^{\\prime}\\mid s,a)$ and plan with approximate lookahead [349, 350].",
              "index": 1,
              "part": 0,
              "translated_content": "随后深度学习的进展使“AI世界模型”的概念变得更加清晰。一种有影响力的方法引入了一种端到端的环境潜在生成模型（例如，“世界模型”[348]），其中循环神经网络（RNN）和变分自动编码器（VAE）共同学习“梦想”未来的轨迹。这些潜在的展开使代理能够在离线训练或优化策略，有效地模拟人类在执行动作之前如何进行心理排练。除了这种隐式设计，显式的前向建模方法也出现在基于模型的RL中，使代理能够预测 $P(\\bar{s}^{\\prime}\\mid s,a)$ 并使用近似前瞻进行规划。"
            },
            {
              "type": "text",
              "content": "Another branchof workleveragedlarge-scale simulators orreal-worldrobotics to ground learning inrichly diverse experiences [51,352].Such setups are reminiscent of how humanchildren learn by activelyexploring their environments, gradually honing their internalrepresentations.Yet akey questionlingers:can agentic systems unify these approaches (implicit generative modeling,explicit factorization,and simulator-driven exploration）intoacohesive“mental model\" akin to that observed in humans?Therecent proliferation of language-model-based reasoning [107,74]hints atthe potential tocrossmodalities and tasks,echoing how humans integrate linguistic, visual, and motor knowledge under one predictive framework.",
              "index": 2,
              "part": 0,
              "translated_content": "另一方面的工作利用大规模模拟器或现实世界的机器人技术，通过丰富多样的经验来打下学习基础。这些设置让人想起人类儿童通过积极探索环境学习的方式，逐渐完善他们的内部表征。然而一个关键问题仍然存在：智能系统是否能够将这些方法（隐式生成建模、显式因式分解和模拟器驱动的探索）统一起来，形成类似于人类观察到的“心智模型”的一致性？基于语言模型的推理的最近激增暗示着跨模态和任务的潜力，呼应了人类如何在一个预测框架下整合语言、视觉和运动知识的方式。"
            },
            {
              "type": "text",
              "content": "Overall, as AI systems striveforflexible,sample-efficientlearning,the Alworld modelstands asaconceptual bridge from cognitivetheories of mentalmodels to implementations that equip artificial agents with imagination, predictive reasoning, and robust adaptation in complex domains.",
              "index": 3,
              "part": 0,
              "translated_content": "总的来说，随着人工智能系统努力实现灵活、高效的学习，AI世界模型作为一个概念桥梁，连接了认知心理学中关于心智模型的理论和为人工智能代理赋予想象力、预测推理能力以及在复杂领域中进行强健适应的实现。"
            }
          ],
          "raw_title": "Translating Human World Models to AI",
          "type": null,
          "children": [],
          "translated_title": "4.2 将人类世界模型转化为人工智能"
        },
        {
          "title": "4.3  Paradigms of AI World Models",
          "number": "4.3",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "Designing an Alworld modelinvolves determininghow an AIagent acquires,represents, andupdates itsunderstanding of the environment's dynamics.While implementations vary, most approaches fallinto four broad paradigms: implicit, explicit,simulator-based, and hybridorinstruction-driven models.These paradigmscan befurther analyzedalong two key dimensions:reliance on internal(neural-based) vs.external(rule-based or structured)mechanisms,and overall system complexity.Figure 4.2illustrates this two-dimensional space,showing how different approaches distribute themselvesacross theseaxes.Generaly,implicit models tend to rely more on internal mechanisms, while explicit and simulator-based models incorporate more externalstructures.Simulator-based andexplicit models alsotendto be more complex than implicit and hybrid approaches,reflecting their structured reasoning and engineeed constraints.",
              "index": 0,
              "part": 0,
              "translated_content": "设计Alworld模型涉及确定AI代理如何获取、表示和更新其对环境动态的理解。虽然具体实现各不相同，但大多数方法可归为四种广泛范式：隐式、显式、基于模拟器和混合或指导型模型。这些范式可沿着两个关键维度进一步分析：对内部（基于神经网络）与外部（基于规则或结构化）机制的依赖，以及整体系统复杂性。图4.2展示了这个二维空间，展示了不同方法在这些轴上的分布情况。通常来说，隐式模型更倾向于依赖内部机制，而显式和基于模拟器的模型则包含更多外部结构。基于模拟器和显式模型也往往比隐式和混合方法更复杂，反映了它们的结构化推理和工程约束。"
            },
            {
              "type": "figure",
              "src": "images/be6df94c6c1f9cfc1594565b9961f59f78fc4729ec1bc5b2afb05f9ff333ae2e.jpg",
              "alt": "",
              "caption": "Figure 4.2: A two-dimensionallayout of AI world-modelmethods.The horizontal axis indicates Complexity (left to right).The verticalaxis spans Internalapproaches (bottm)to Externalsolutions (top).Approximatepositionsreflect each method's reliance onlarge learned networks vs.explicit rules or code, and its overallsystem complexity.",
              "index": 1,
              "part": 0,
              "translated_caption": "图4.2：人工智能世界模型方法的二维布局。水平轴表示复杂性（从左到右）。垂直轴跨越内部方法（底部）到外部解决方案（顶部）。大致位置反映了每种方法对大型学习网络与明确规则或代码的依赖程度，以及其整体系统复杂性。"
            }
          ],
          "raw_title": "Paradigms of AI World Models",
          "type": null,
          "children": [
            {
              "title": "4.3.1 Overview of World Model Paradigms",
              "number": "4.3.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "An $A I$ world model is broadly any mechanism by which an agent captures or accesses approximate environment dynamics. Let $s$ denote the set of possible environment states, $\\mathcal{A}$ the set of actions, and $\\mathcal{O}$ the set of observations. In an idealized Markovian framework,the environment ischaracterized bytransition and observation distributions:",
                  "index": 0,
                  "part": 0,
                  "translated_content": "大型语言模型（LLMs）的出现催生了人工智能领域的深刻转变，为能够在各种领域展现复杂推理、强大感知和多功能行为能力的先进智能代理铺平了道路。随着这些代理在推动人工智能研究和实际应用方面的作用日益增强，它们的设计、评估和持续改进提出了复杂而多面向的挑战。本调查提供了一个全面的概述，将智能代理置于一个模块化、脑启发式架构中，该架构整合了认知科学、神经科学和计算研究的原则。我们将探索分为四个相互关联的部分。首先，我们深入探讨智能代理的模块化基础，系统地将它们的认知、感知和操作模块映射到类似的人脑功能上，并阐明核心组件，如记忆、世界建模、奖励处理和类似情感的系统。其次，我们讨论自我增强和自适应进化机制，探讨代理如何自主完善其能力、适应动态环境，并通过自动化优化范式实现持续学习，包括新兴的AutoML和LLM驱动的优化策略。第三，我们研究协作和进化多代理系统，调查从代理相互作用、合作和社会结构中出现的集体智能，突显与人类社会动态的相似之处。最后，我们着重讨论构建安全、可靠和有益的人工智能系统的关键使命，强调内在和外在的安全威胁、道德对齐、稳健性以及在值得信赖的现实世界部署所必需的实用缓解策略。通过将模块化人工智能架构与不同学科的见解综合起来，本调查确定了关键的研究空白、挑战和机遇，鼓励创新，使技术进步与有意义的社会利益相协调。该项目的Github链接为: https://github.com/FoundationAgents/awesome-foundation-agents。\n\n一个$AI$世界模型广泛指代代理捕获或访问近似环境动态的任何机制。设$s$表示可能的环境状态集合，$\\mathcal{A}$表示动作集合，$\\mathcal{O}$表示观测集合。在理想化的马尔可夫框架中，环境通过转移和观测分布来描述："
                },
                {
                  "type": "formula",
                  "content": "$$ \n\\begin{array}{r l}&{T(s^{\\prime}\\mid s,a)\\quad:\\quad\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S}),}\\\\ &{O(o\\mid s^{\\prime})\\quad:\\quad\\mathcal{S}\\to\\Delta(\\mathcal{O}),}\\end{array}\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "where $T(\\cdot)$ dictates how states evolve under actions, and $O(\\cdot)$ defines how states produce observations. A world model typically learnsorutilizesapproximationsofthesefunctions(oravariant)allowingtheagenttopredictfuture statesor observations without executing real actions in the environment.\n\nNumerous approaches exist to implement these approximations, which we group into four main paradigms:\n\n·Implicit paradigm: A single neural network or latent structure encodes both transition and observation mappings without explicit factorization. World Models [348] or large language models used for environment reasoning are typical examples.Agents generally unrollthis black-box function to simulate hypothetical trajectories.\n· Explicit paradigm: The agent directly models or has access to learnable transition model $T_{\\theta}$ and observation model $O_{\\theta}$ ,often enabling interpretability or modular design. Model-based RL methods-like MuZero [349] or Dreamer [350]-learn or refine $T_{\\theta}$ , planning in an approximated state space. Generative visual models such as [353,358] fall under this category if they explicitly predict the next states or frames.\n·Simulator-Based paradigm: Rather than approximating (4.1)(4.2),the agent relies on an external simulator or even the physical world as the ground-truth.Systems like SAPIEN[351] or real-robot pipelines [352] can be seen as“native\" environment models that the agent queries. Although no learned $T(\\cdot)$ is required, the agent pays a cost in terms of runtime or real-world risks.\n· Other paradigms (Hybrid or Instruction-Driven): Methods that defy simple classification. They may store emergent rules in textual form [108], refine implicit LLM knowledge into partial causal graphs [356], or combine external components with learned sub-modules.Such approaches highlight the evolving nature of world-model research, where instructions,symbolic rules,or on-the-fly structures can complement more traditional approximations.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "其中$T(\\cdot)$规定了状态在动作下的演变方式，$O(\\cdot)$定义了状态如何产生观测。世界模型通常学习或利用这些函数的近似（或变体），使代理能够在不在环境中执行实际动作的情况下预测未来状态或观测。\n\n存在许多方法来实现这些近似，我们将其分为四种主要范式：\n\n·隐式范式：一个单一的神经网络或潜在结构编码转移和观测映射，而无需显式因子分解。《World Models》或用于环境推理的大型语言模型是典型示例。代理通常展开这个黑盒函数以模拟假设轨迹。\n·显式范式：代理直接建模或访问可学习的转移模型$T_{\\theta}$和观测模型$O_{\\theta}$，通常能够实现可解释性或模块化设计。基于模型的强化学习方法，如MuZero或Dreamer，学习或改进$T_{\\theta}$，在近似状态空间中进行规划。如果显式预测下一个状态或帧，生成式视觉模型如[353,358]属于这一类别。\n·基于模拟器范式：代理依赖外部模拟器或甚至物理世界作为地面真相，而不是近似(4.1)(4.2)。像SAPIEN或真实机器人管道这样的系统可以被视为代理查询的“本地”环境模型。虽然不需要学习的$T(\\cdot)$，但代理在运行时间或真实世界风险方面付出代价。\n·其他范式（混合或指导驱动）：无法简单分类的方法。它们可能以文本形式存储新兴规则，将隐式LLM知识精炼为部分因果图，或将外部组件与学习的子模块结合。这些方法突显了世界模型研究的不断发展，其中指导、符号规则或即时结构可以补充更传统的近似方法。"
                },
                {
                  "type": "text",
                  "content": "Throughout theremainder of this subsection, we examine how each paradigmaddresses (orcircumvents)Equations (4.1) and(4.2),thetrade-off ininterpretabilityand scalability,andtheirrelativemeritsfordiferent tasksranging from text-based to high-dimensional embodied control.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "在本小节的其余部分，我们将探讨每种范式如何处理（或规避）方程式（4.1）和（4.2），解释性和可扩展性之间的权衡，以及它们在从基于文本到高维度实体控制等不同任务中的相对优点。"
                }
              ],
              "raw_title": "Overview of World Model Paradigms",
              "type": null,
              "children": [],
              "translated_title": "4.3.1 世界模型范式概述"
            },
            {
              "title": "4.3.2 Implicit Paradigm",
              "number": "4.3.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In the implicit paradigm,an agentencodesallenvironment dynamics—includinghow states evolve andhowobservations are generated-within a single (or tightly coupled) neural model. Formally, one maintains a latent state $h_{t}$ that is updated according to",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在隐式范式中，一个代理将所有环境动态（包括状态如何演变以及观测如何生成）编码在单个（或紧密耦合的）神经模型内。形式上，一个维持更新的潜在状态 $h_{t}$，其更新方式如下所示："
                },
                {
                  "type": "formula",
                  "content": "$$ \nh_{t+1}=f_{\\theta}(h_{t},a_{t}),\\quad\\hat{o}_{t+1}=g_{\\theta}\\big(h_{t+1}\\big),\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "where $f_{\\theta}$ subsumes the transition function $T(\\cdot)$ (and part of $O(\\cdot))$ from Eqs. (4.1)-(4.2), but without making these components explicit.A classic example is the World Models framework [348], in which a Variational Autoencoder (VAE)first compressesvisualinputs intolatentcodes,andarecurrent networkpredicts the nextlatentcode,effectively \"dreaming\"trajectories in latent space. Recent work also explores repurposing large language models (LLMs)for environment simulation in purelytextual or symbolicdomains[107,74],athoughthese models are notalwaysgrounded in strict time-series or physics-based data.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "其中 $f_{\\theta}$ 包含了方程（4.1）-（4.2）中的转移函数 $T(\\cdot)$（以及 $O(\\cdot)$ 的一部分），但没有明确地将这些组件呈现出来。一个经典的例子是World Models框架[348]，其中变分自动编码器（VAE）首先将视觉输入压缩为潜在编码，然后一个循环网络预测下一个潜在编码，有效地在潜在空间中“梦想”轨迹。最近的研究还探讨了将大型语言模型（LLMs）重新用于纯文本或符号领域的环境模拟[107,74]，尽管这些模型并不总是建立在严格的时间序列或基于物理的数据基础之上。"
                },
                {
                  "type": "text",
                  "content": "Because implicit models fuse the transition and observation mechanisms into one monolithic function,they can be elegantly trained end toend and unrolled internallyfor planning.However,they tend to be opaque:it is difficult to interpret how precisely the network captures domain constraints orto inject knowledge directly into any part of the transition.This can be advantageous for highly complex environments where a single large-capacity modelcan discoverlatent structureonitsown,butitalsorisksbrittlenessunderdistributionshifts.Overall,theimplicit paradigm is appealing foritssimplicity andflexibility,but itcan posechallnges when interpretabilityexplicitconstaints,or fine-grained control of the dynamics are required.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "因为隐式模型将转移和观测机制融合为一个整体函数，所以它们可以优雅地进行端到端训练，并在内部展开进行规划。然而，它们往往是不透明的：很难解释网络如何准确捕捉领域约束，或者直接注入知识到转移的任何部分。这在高度复杂的环境中可能是有利的，其中一个大容量模型可以自行发现潜在结构，但也存在在分布转移下脆弱的风险。总体而言，隐式范式因其简单性和灵活性而具有吸引力，但在需要解释性明确约束或对动态的精细控制时可能会带来挑战。"
                }
              ],
              "raw_title": "Implicit Paradigm",
              "type": null,
              "children": [],
              "translated_title": "4.3.2 隐式范式"
            },
            {
              "title": "4.3.3 Explicit Paradigm",
              "number": "4.3.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The explicit paradigm instead factorizes the world model, often by learning or encoding a transition function $\\hat{T}_{\\theta}{\\left(s_{t+1}\\right|}$ $s_{t},a_{t})$ and an observation function $\\hat{O}_{\\theta}(o_{t+1}\\mid s_{t+1})$ . This explicit separation makes it possible to query each function independently. For instance, one might draw samples from",
                  "index": 0,
                  "part": 0,
                  "translated_content": "相反，显式范式通过分解世界模型，通常通过学习或编码转移函数 $\\hat{T}_{\\theta}{\\left(s_{t+1}\\right|}$ $s_{t},a_{t})$ 和观测函数 $\\hat{O}_{\\theta}(o_{t+1}\\mid s_{t+1})$ 来实现。这种显式分离使得可以独立查询每个函数。例如，可以从中抽取样本。"
                },
                {
                  "type": "formula",
                  "content": "$$ \n\\hat{s}_{t+1}\\sim\\hat{T}_{\\boldsymbol{\\theta}}\\big(s_{t},a_{t}\\big),\\quad\\hat{o}_{t+1}\\sim\\hat{O}_{\\boldsymbol{\\theta}}\\big(\\hat{s}_{t+1}\\big).\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "Model-based reinforcement-learning algorithms like MuZero[349] or Dreamer [350] exemplify this paradigm by refining aforward modelforplanning.Otherexplicit approaches prioritizefidelity ingeneratingfuture frames,suchas\n\nDifusion WM[33],whichapplies difusion processes atthe pixellevel,or DINO-WM[358],whichrolls out future states within a pretrained feature space.\n\nBy factorizingtransitions andobservations,explicit methodscan be moreinterpretable and more amenable todebugging and domain-specific constraints. That said, they are still sensitive to model errors: if $\\hat{T}_{\\theta}$ deviates significantly from reality, the agent's planning and decision-makingcan become ineffective.Manyexplicit systems stillrely predominantly on internal(neural)representations,butthey may integrate external planners(e.g.,tree-searchalgorithms)toleverage the explicit transition structure.This blend of learned and symboliccomponents offrs a natural way to incorporate human knowledge, while preserving the strengths of deep learning.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "基于模型的强化学习算法，如MuZero或Dreamer，通过优化用于规划的前向模型来体现这一范式。其他显式方法则优先考虑在生成未来帧时的保真度，例如Difusion WM在像素级别应用扩散过程，或者DINO-WM在预训练特征空间内展开未来状态。\n\n通过对转换和观测进行因式分解，显式方法更易于解释和调试，也更容易受到领域特定约束的影响。尽管如此，它们仍对模型错误敏感：如果 $\\hat{T}_{\\theta}$ 与现实存在显著差异，智能体的规划和决策可能变得无效。许多显式系统仍主要依赖内部（神经）表示，但它们可以整合外部规划器（例如树搜索算法）来利用显式转换结构。这种学习和符号组件的融合提供了一种自然的方式来融入人类知识，同时保留了深度学习的优势。"
                }
              ],
              "raw_title": "Explicit Paradigm",
              "type": null,
              "children": [],
              "translated_title": "4.3.3 明示范式"
            },
            {
              "title": "4.3.4 Simulator-Based Paradigm",
              "number": "4.3.4",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In the simulator-based paradigm,the agent outsources environment updates to a simulator,efectively bypassing the need to learn $\\hat{T}_{\\theta}$ from data. Formally,",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在基于模拟器的范式中，代理将环境更新外包给模拟器，有效地绕过了需要从数据中学习$\\hat{T}_{\\theta}$的必要性。形式上，"
                },
                {
                  "type": "formula",
                  "content": "$$ \n(s_{t+1},o_{t+1})\\gets S T M(s_{t},a_{t}),\n $$",
                  "index": 1,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "where $s\\tau{\\mathcal{M}}$ is often an external physics engine or the real world itself. Platforms like SAPIEN[351] and AI Habitat provide deterministic3Dphysicssimulations,allowing agents topracticeoriterate strategies inacontrolledenvironment. Alternatively,methods such as Daydreamer[352] treat real-world interaction loops like a“simulatorcontinually updating on-policy data from physical robots.\n\nThis approachyieldsaccuratetransitions (assuming the simulator accuratelyreflects reality),which alleviates the risk of leared-modelerrors.However,itcanbecomputationalyorfinanciallyexpensive,especiallif the simulator is high fidelity orif real-world trials aretime-consuming andrisky.Asaresult,some agentscombine partiallearneddynamics with occasional simulator queries,aiming to balance accurate rollouts with effcient coverage of state-action space.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "其中$s\\tau{\\mathcal{M}}$通常是外部的物理引擎或真实世界本身。像SAPIEN和AI Habitat这样的平台提供确定性的3D物理模拟，使代理能够在受控环境中练习或迭代策略。另外，像Daydreamer这样的方法将真实世界的交互循环视为“模拟器不断从物理机器人中更新政策数据”。这种方法产生准确的转换（假设模拟器准确反映现实），从而减轻了学习模型错误的风险。然而，如果模拟器质量高，或者真实世界试验耗时且风险较大，这种方法可能会带来计算或财务上的负担。因此，一些代理结合部分学习动态和偶尔的模拟器查询，旨在平衡准确的展开和对状态-动作空间的高效覆盖。"
                }
              ],
              "raw_title": "Simulator-Based Paradigm",
              "type": null,
              "children": [],
              "translated_title": "4.3.4 基于模拟器的范式"
            },
            {
              "title": "4.3.5 Hybrid and Instruction-Driven Paradigms",
              "number": "4.3.5",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Beyond these three primary paradigms,there is a growing number of hybrid or instruction-driven approaches, which blend implicit and explicit modeling or incorporate external symbolic knowledge and largelanguage models.Often, these systems dynamically extract rulesfrom data, maintain evolving textual knowledge bases,or prompt LLMs to hypothesize causal relationships that can then be tested or refined.\n\nAutoManual[108],for example iterativelycompiles interactive environment rules into human-readable manuals, informing future actions in a more transparent way.Meanwhile, COAT [356] prompts an LLM to propose possible causal factors behindobserved events,then validatesorrefines those factors viadirect interaction,bridging text-based reasoning with partiallearned models.Althoughthese solutionsoffer remarkable flexibilityparticularly in adapting to unfamiliar domains or integrating real-time human insights—they can be inconsistent in how they structure or update internal representations.As language-model prompting and real-time rule discovery continue to evolve, these hybrid methods arepoisedtobecome increasinglycommon,reflectingthe need tobalance end-to-end learning with the transparency and adaptability offered by external instruction.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "除了这三种主要范式之外，还出现了越来越多的混合或指导驱动方法，它们混合了隐式和显式建模，或者整合了外部符号知识和大型语言模型。通常，这些系统动态地从数据中提取规则，维护不断演化的文本知识库，或者促使大型语言模型假设因果关系，然后可以对其进行测试或完善。\n\n例如，AutoManual[108]迭代地将交互环境规则编译成人类可读的手册，以更透明的方式指导未来行动。与此同时，COAT[356]促使大型语言模型提出观察事件背后可能的因果因素，然后通过直接交互验证或完善这些因素，将基于文本的推理与部分学习模型相结合。尽管这些解决方案在适应陌生领域或整合实时人类见解方面提供了显著的灵活性，但它们在如何构建或更新内部表示方面可能存在不一致。随着语言模型提示和实时规则发现的不断发展，这些混合方法有望变得越来越普遍，体现了在端到端学习和外部指导提供的透明度和适应性之间取得平衡的需求。"
                },
                {
                  "type": "text",
                  "content": "Until now,wehave introduced the four typical paradigms of existing world model techniques, as ilustrated in Figure 4.3.5. As we can see, each type of technique has trade-offs in different aspects.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "到目前为止，我们已经介绍了现有世界模型技术的四种典型范式，如图4.3.5所示。正如我们所看到的，每种技术类型在不同方面都存在权衡。"
                }
              ],
              "raw_title": "Hybrid and Instruction-Driven Paradigms",
              "type": null,
              "children": [],
              "translated_title": "4.3.5 混合和指导驱动范式"
            },
            {
              "title": "4.3.6 Comparative Summary of Paradigms",
              "number": "4.3.6",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "The table summarizes the key methods in AI world modeling,categorizing them based on their reliance on external or internal mechanisms, their complexity, and their respective paradigms. The form column uses $\\scriptscriptstyle\\mathrm{~o~}$ for external approaches and $\\bullet$ for internal ones, with mixed methods having both symbols. This classification aligns with the previous subsections,including thedetailed discussion ofeach paradigm, andcomplements the visualrepresentation in Figure 4.2.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "表格总结了人工智能世界建模中的关键方法，根据它们对外部或内部机制的依赖程度、复杂性和各自的范式进行分类。形式列使用$\\scriptscriptstyle\\mathrm{~o~}$表示外部方法，$\\bullet$表示内部方法，混合方法则同时具有这两种符号。这种分类与前文的小节相一致，包括对每种范式的详细讨论，并补充了图4.2中的视觉呈现。"
                }
              ],
              "raw_title": "Comparative Summary of Paradigms",
              "type": null,
              "children": [],
              "translated_title": "4.3.6 范式比较总结"
            }
          ],
          "translated_title": "4.3 人工智能世界模型的范式"
        },
        {
          "title": "4.4 Relationships to Other Modules",
          "number": "4.4",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "A comprehensive AIworld modeldoes notexist in isolation but interacts with severalkeycomponents of the agent's architecture.These include (but notlimited tothe memory,perception,and action modules.Inthis subsection,we explorehow worldmodels integrate with these criticalcomponents toenable coherentand adaptive behavior indynamic environments.",
              "index": 0,
              "part": 0,
              "translated_content": "综合的人工智能世界模型并不孤立存在，而是与智能代理架构的几个关键组件进行交互。这些组件包括（但不限于）记忆、感知和行动模块。在这个小节中，我们探讨世界模型如何与这些关键组件整合，以实现在动态环境中的连贯和自适应行为。"
            },
            {
              "type": "figure",
              "src": "images/dc4c580048e4f0aeb425c7cf1ce57e8a866cdfca7da835465bcee068b92c17d7.jpg",
              "alt": "",
              "caption": "Figure 4.3:Four paradigms of world modeling: (a)implicit, (b)explicit,(c)simulator-based, and(d)hybrid/instruction driven.",
              "index": 1,
              "part": 0,
              "translated_caption": "图4.3：世界建模的四种范式：(a)隐式，(b)显式，(c)基于模拟器，和(d)混合/指令驱动。"
            },
            {
              "type": "text",
              "content": "Table4.1:Summary of AI world-modelmethods across paradigms,showing theirform(External or Internal),complexity, and paradigm.",
              "index": 2,
              "part": 0,
              "translated_content": "表4.1：跨范式的AI世界模型方法总结，显示它们的形式（外部或内部）、复杂性和范式。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td>Method</td><td>Form</td><td>Complexity</td><td>Paradigm</td></tr><tr><td>ActRe[49]</td><td>·</td><td>Simple</td><td>Implicit</td></tr><tr><td>World Models [348]</td><td>·</td><td>Simple</td><td>Implicit</td></tr><tr><td>Dreamer [350]</td><td>●</td><td>Moderate</td><td>Implicit</td></tr><tr><td>Diffusion WM [353]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>GQN [354]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>Daydreamer[352]</td><td>0</td><td>High</td><td>Simulator-based</td></tr><tr><td>SAPIEN[351]</td><td>0</td><td>High</td><td>Simulator-based</td></tr><tr><td>PILCO [355]</td><td>0</td><td>Moderate</td><td>Explicit</td></tr><tr><td>AutoManual [108]</td><td>0</td><td>Simple</td><td>Other</td></tr><tr><td>MuZero [349]</td><td></td><td>High</td><td>Explicit</td></tr><tr><td>GR-2 [357]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>DINO-WM [358]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>COAT [356]</td><td>O</td><td>Moderate</td><td>Other</td></tr></table></body></html>",
              "index": 3,
              "part": 0
            }
          ],
          "raw_title": "Relationships to Other Modules",
          "type": null,
          "children": [
            {
              "title": "4.4.1 Memory and the World Model",
              "number": "4.4.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Memory systems play a crucial role in the operation of world models.Whilea world model generates predictive representations offuture statesoractions, memory serves asthefoundationupon whichtheserepresentations are built and updated.The relationship between the world modeland memory can be viewed as a loop where the world model predicts potentialfutures, while the memory stores past experiences,observations,andlearned patterns,allowing for context-dependent reasoning and future predictions.\n\nMemory mechanisms can be structured in various ways, including:\n\n·Short-term memory:This enables the agent tohold and update its internal state temporarily,storing the most recent interactions or observations.This short-term context helps the agent make decisions in the immediate environment. ·Long-term memory: This serves as a more persistent repository of experiences and general knowledge about the environment. A world model can interact with long-term memory to refine its predictions, and it may use historical data to make more informed decisions or simulate more realistic futures.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "记忆系统在世界模型的运作中起着至关重要的作用。世界模型生成对未来状态或行为的预测性表示，而记忆则作为这些表示构建和更新的基础。世界模型和记忆之间的关系可以被视为一个循环，其中世界模型预测潜在的未来，而记忆存储过去的经验、观察和学习模式，实现依赖于上下文的推理和未来预测。\n\n记忆机制可以以各种方式结构化，包括：\n\n· 短期记忆：这使代理能够临时保持和更新其内部状态，存储最近的互动或观察。这种短期上下文有助于代理在即时环境中做出决策。\n· 长期记忆：这充当了对环境的经验和一般知识更持久的存储库。世界模型可以与长期记忆互动以完善其预测，并可能使用历史数据做出更明智的决策或模拟更现实的未来。"
                },
                {
                  "type": "text",
                  "content": "For example,in model-based RLframeworks likeDreamer[350],recurrentneural networksact as both the world model and aform of memory, maintaining alatent state that is updated witheachtime step to predictfuture states.This form of integrated memory allows the agent to both recall past interactions and anticipate future ones.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "例如，在基于模型的强化学习框架中，如Dreamer，循环神经网络既充当世界模型，又作为一种记忆形式，维护一个潜在状态，该状态在每个时间步更新以预测未来状态。这种集成记忆形式使代理能够回忆过去的互动并预测未来的互动。"
                }
              ],
              "raw_title": "Memory and the World Model",
              "type": null,
              "children": [],
              "translated_title": "4.4.1 记忆与世界模型"
            },
            {
              "title": "4.4.2  Perception and the World Model",
              "number": "4.4.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Perception refers tothe agent's ability to sense and interpret its environmentthrough various modalities (e.g.,vision, touch,sound,etc.).The world modelrelies heavilyonaccurate sensory input toformcoherentpredictions about the environment.In many AI systems,the perception module converts raw sensor datainto ahigher-levelrepresentation, such as an image, sound wave, or other structured data.\n\nA key aspect of the interaction between the world model and perception is how the agent processesand integrates sensory input into the model.The world modeloften depends on processed data (such as features fromconvolutional neural networks orembeddings from transformers)to simulate potential futures.Additionally, the world model can guide perceptual processes by focusing atention on the most relevant sensory input needed to refine predictions.\n\nFor example, in autonomous robotics,perception systems typicall detect objects or environmental features, which are then fed intoaworld modelthatpredicts howthe scene will evolve.RoboCraft[359]achieves this perception-tomodeling transformation by converting visual observations into particles andcapturing the underlying system structure through graph neuralnetworks.PointNet[360]further enriches perception systems'understanding of physical space by encoding unstructured 3D point clouds to capture spatial characteristics of the environment. In navigation tasks, OVER-NAV[361] further combine largelanguage models and open-vocabulary detection toconstruct the relationship between multi-modal signals and key information,proposing anomni-graph tocapture the structureoflocal space as the world model for navigation tasks.This feedback loop between perception and the world modelenables agents to update their perception dynamically based on ongoing predictions, allowing for real-time adaptation.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "感知是指智能代理通过各种方式（例如视觉、触觉、听觉等）感知和解释其环境的能力。世界模型在很大程度上依赖于准确的感觉输入，以对环境进行连贯的预测。在许多人工智能系统中，感知模块将原始传感器数据转换为更高级别的表示，例如图像、声波或其他结构化数据。\n\n世界模型与感知之间互动的关键方面是代理如何将感觉输入处理和整合到模型中。世界模型通常依赖于经过处理的数据（例如来自卷积神经网络的特征或来自变压器的嵌入）来模拟潜在的未来。此外，世界模型可以通过集中注意力于最相关的感官输入来引导感知过程，以细化预测所需的最相关感觉输入。\n\n例如，在自主机器人领域，感知系统通常检测物体或环境特征，然后将其输入到预测场景将如何发展的世界模型中。RoboCraft通过将视觉观察转换为粒子并通过图神经网络捕获基础系统结构，实现了这种感知到建模的转换。PointNet通过对无结构的3D点云进行编码来进一步丰富感知系统对物理空间的理解，以捕获环境的空间特征。在导航任务中，OVER-NAV进一步结合大型语言模型和开放词汇检测，构建多模态信号和关键信息之间的关系，提出omni-graph来捕获导航任务的本地空间结构作为世界模型。感知和世界模型之间的这种反馈循环使代理能够根据持续的预测动态更新其感知，实现实时适应。"
                }
              ],
              "raw_title": "Perception and the World Model",
              "type": null,
              "children": [],
              "translated_title": "4.4.2 知觉与世界模型"
            },
            {
              "title": "4.4.3 Action and the World Model",
              "number": "4.4.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Action refers tothe decision-making processthrough which an agent interacts with its environment.Inagentic systems, actions aredriven bythe world model's predictions of future states.Theworld modelaids inplanning by simulating the outcomesofdiffrent actions before they are executed,allowing the agent tochoose the mostoptimalcourse of action based on the predicted consequences.\n\nThe integration between world models and action modules can take various forms:\n\n·Model-based planning: World models explicitly modelthe environment's transition dynamics [349,362,107], allowing the agent to simulate multiple action sequences (rollouts)before selecting the most optimal one. · Exploration: World models also support exploration strategies by simulating unseen states or unexpected actions [363,350,364].These simulations enable the agent to evaluatethe potential benefits of exploring new parts of the state space.\n\nIn model-based planning, MuZero[349] performs implicit planning through self-play and Monte Carlo Tree Search (MCTS),transformingcurrent staterepresentations intofuture state andreward predictions toguide thedecision-making process without prior knowledge of environment rules. In contrast,MPC[362]utilizes explicit dynamics models to predict multiple posible trajectories withinafinite time horizon,determines the optimalcontrol sequence by solving an optimization problem, and continuously updates planning using a receding horizon approach. Alpha-SQL[365], on the other hand,integrates an LLM-as-Action-Model within an MCTS framework to explore potential SQL queries within the database's“world model\".This approach dynamically generates promising SQL construction actions based on partialquery states,enabling zero-shot Text-to-SQL interactions without task-specific fine-tuning.Unlike MuZero, which focuses on planning for decision-making in uncertain environments, Alpha-SQL applies MCTS in a specific task—guiding SQL query construction through self-generated actions within a complex database context.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "行动是指代理通过决策过程与环境进行交互的过程。在代理系统中，行动是由世界模型对未来状态的预测驱动的。世界模型通过在执行之前模拟不同行动的结果来帮助规划，使代理能够根据预测的后果选择最优行动方案。\n\n世界模型与行动模块之间的整合可以采用各种形式：\n\n- 基于模型的规划：世界模型明确地建模环境的转换动态，允许代理在选择最优方案之前模拟多个行动序列（展开）。 \n- 探索：世界模型还通过模拟未见状态或意外行动来支持探索策略。这些模拟使代理能够评估探索状态空间新部分的潜在收益。\n\n在基于模型的规划中，MuZero通过自我对弈和蒙特卡洛树搜索（MCTS）进行隐式规划，将当前状态表示转化为未来状态和奖励预测，以指导决策过程，而无需了解环境规则。相比之下，MPC利用显式动态模型来预测有限时间范围内的多条可能轨迹，通过解决优化问题确定最优控制序列，并使用逐步更新的方法持续更新规划。另一方面，Alpha-SQL在MCTS框架内集成了LLM作为行动模型，以探索数据库“世界模型”中的潜在SQL查询。该方法根据部分查询状态动态生成有前途的SQL构建行动，实现了无需特定任务微调的零-shot文本到SQL交互。与专注于不确定环境中决策规划的MuZero不同，Alpha-SQL在特定任务中应用MCTS，通过在复杂数据库环境中自动生成行动来指导SQL查询构建。"
                },
                {
                  "type": "text",
                  "content": "For exploration strategies,Nagabandi et al.[363] incentivizes agents toexplore unknown regions by providingreward mechanisms (exploration bonuses)for discovering new states.Dreamer[350] propose that world models can generate imaginary action sequences (imaginary rollouts),allowing agents to safely evaluate the benefits of new actions in simulated environments without risking real-world experimentation.Similarly,in the discrete world model Hafner et al.[364]agents effcientlyexplorecomplexenvironments bysimulating multiplepossible future states,effectively balancing the trade-off between exploration and exploitation.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "对于探索策略，Nagabandi等人通过提供奖励机制（探索奖励）来激励代理探索未知区域，以发现新状态。Dreamer提出，世界模型可以生成虚构的行动序列（虚构展开），使代理可以在模拟环境中安全评估新行动的好处，而无需冒真实世界实验的风险。类似地，在离散世界模型中，Hafner等人通过模拟多个可能的未来状态，代理可以高效地探索复杂环境，有效平衡探索和利用之间的权衡。"
                },
                {
                  "type": "text",
                  "content": "For example,in reinforcement learning,agents can employ a learned world model to simulate future trajectories in action-selection tasks.The world modelevaluates the potentialrewardsofdiferent actions,enabling theagent to plan effectively and take actions that maximize long-term goals.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "例如，在强化学习中，代理可以利用学习的世界模型来模拟行动选择任务中的未来轨迹。世界模型评估不同行动的潜在奖励，使代理能够有效规划并采取最大化长期目标的行动。"
                }
              ],
              "raw_title": "Action and the World Model",
              "type": null,
              "children": [],
              "translated_title": "4.4.3 行动与世界模型"
            },
            {
              "title": "4.4.4 Cross-Module Integration",
              "number": "4.4.4",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "While memory,perception,andactionarediscussed as separatemodules,the true strength of worldmodels lies in their ability to seamlessly integrate across these domains.A world modelcontinuouslyreceives sensory input, updates its internalmemory,simulatesfuturestates,anduses thisinformation todrive action selection.Theiterativefeedback loo between these modules allowsagents toengage in intellgent, goal-directed behaviorthatishighlyadaptivetochanges in the environment.\n\nThis cross-module interaction is particularly relevant in complex,dynamic systems such as robotics, where anagent must continuouslyadaptitsinternalrepresentationofthe world, process sensory input, store relevant experiences,and take actions inrealtime.Inthecontextofembodiedagents,the integrationofthese modules ensures that predictions made by the world model are grounded in current observations and the agent's ongoing experiences.\n\nWorld models provide a fundamental unifying principle across modalities.Whether predicting physical outcomes in embodied robotics,anticipating visual changes on screens,or infering semantic relationships in text, thecore mechanism remains consistent: generating predictions about how states evolve under diferent actions.This crossmodal capacity explains why humans transition efortlessly between manipulating objects, navigating interfaces,and processing language—allactivities driven by the same underlying predictive architecture.Future AI systems may achieve similar integration by developing world models that bridge these traditionally separate domains through a common predictive framework.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "虽然记忆、感知和行动被讨论为独立模块，但世界模型真正的优势在于它们能够在这些领域之间无缝集成。世界模型不断接收感官输入，更新内部记忆，模拟未来状态，并利用这些信息来驱动行动选择。这些模块之间的迭代反馈循环使代理能够参与智能、目标导向的行为，高度适应环境变化。\n\n这种跨模块交互在复杂、动态系统中尤为重要，例如在机器人技术领域，代理必须持续调整其对世界的内部表征、处理感官输入、存储相关经验，并实时采取行动。在具身代理的背景下，这些模块的整合确保了世界模型所做的预测基于当前观察和代理持续的经验。\n\n世界模型提供了跨模态的基本统一原则。无论是在具身机器人中预测物理结果，在屏幕上预测视觉变化，还是在文本中推断语义关系，核心机制保持一致：生成关于不同行动下状态如何演变的预测。这种跨模态能力解释了为什么人类能够毫不费力地在操作物体、导航界面和处理语言之间转换——所有这些活动都由相同的基础预测架构驱动。未来的人工智能系统可能通过开发能够通过共同的预测框架在这些传统上独立的领域之间建立桥梁的世界模型，实现类似的整合。"
                },
                {
                  "type": "text",
                  "content": "In summary,the relationshipbetween the world model andtheother modules—memory,perception,and action-forms the backbone of intelligent behaviorinAIsystems.Each modulecontributes toacycleofprediction,update,and action, allowing agents tofunction efectively in dynamic and uncertain environments.These interactions highlight the need for a holistic approach when designing agent architectures,where world models areclosely intertwined with sensory input, memory systems, and decision-making processes.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "总之，世界模型与其他模块——记忆、感知和行动之间的关系构成了人工智能系统智能行为的基础。每个模块都为预测、更新和行动的循环做出贡献，使代理能够在动态和不确定的环境中有效地运作。这些互动凸显了在设计代理架构时需要采用整体方法的重要性，其中世界模型与感官输入、记忆系统和决策过程紧密相互交织。"
                }
              ],
              "raw_title": "Cross-Module Integration",
              "type": null,
              "children": [],
              "translated_title": "4.4.4 跨模块集成"
            }
          ],
          "translated_title": "4.4 与其他模块的关系"
        },
        {
          "title": "4.5 Summary and Discussion",
          "number": "4.5",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "The evolution of AI world models,fromearlycognitive insights toadvanced AIarchitectures, underscores the growing realization thattrue intellgencereliesonthe abilitytopredict,simulate,and imagine.Unlikeclassicalreinforcement leaning,where agents operate solely through trial-and-error interactions, world models enable foresightagents can plan,anticipate, and adapt to changes before they happen. This leap in cognitive modeling—whether implicit, explicit,or simulator-basedmarks asignificant shift inhow machinescanbeendowed withflexibility,robustness and generalization across tasks.",
              "index": 0,
              "part": 0,
              "translated_content": "从早期认知洞见到先进人工智能架构的AI世界模型演变，凸显了人们日益认识到真正智能的基础在于能够预测、模拟和想象。与经典的强化学习不同，代理通过试错交互操作，世界模型使预见成为可能，代理可以在事件发生之前进行规划、预测和适应。这种认知建模的飞跃——无论是隐式的、显式的还是基于模拟器的——标志着机器如何获得跨任务的灵活性、稳健性和泛化能力方面的重大转变。"
            },
            {
              "type": "text",
              "content": "An essential yetoften overlooked aspect of world models is their operation across multipletemporaland spatial scales Humanmental models seamlesslyintegrate predictions spanning milliseconds reflexiveresponses),seconds (immediate action planning),minutes tohours (taskcompletion)andeven years (lifeplanning)）[366].This multi-scalecapability allows usto simultaneously predict immediate physical dynamics while maintaining coherent long-term narratives and goals. Similarly,humans process spatial information acrosscales-from fine-grained object manipulation to navigation across environments toabstract geographicalreasoning. Current AI world models typically excel within narrow temporal and spatial bands, whereas human cognition demonstrates remarkable flexibility in scaling predictions up and down as context demands.This suggests that truly general-purpose AI world models may require explicit mechanisms for integrating predictions acrossmultiple time horizons and spatialresolutions,dynamically adjustingthe granularity of simulation based on task requirements.",
              "index": 1,
              "part": 0,
              "translated_content": "世界模型的一个重要但经常被忽视的方面是其跨越多个时间和空间尺度的运作。人类的心智模型无缝地整合了跨越毫秒（反射性反应）、秒（即时行动规划）、分钟到小时（任务完成）甚至年（生活规划）的预测[366]。这种多尺度能力使我们能够同时预测即时的物理动态，同时保持连贯的长期叙事和目标。类似地，人类在不同尺度上处理空间信息——从细粒度的物体操作到跨越环境的导航再到抽象的地理推理。当前的人工智能世界模型通常在狭窄的时间和空间范围内表现出色，而人类认知在根据情境要求调整预测的尺度上表现出卓越的灵活性。这表明，真正通用的人工智能世界模型可能需要明确的机制来整合跨多个时间范围和空间分辨率的预测，根据任务要求动态调整模拟的粒度。"
            },
            {
              "type": "text",
              "content": "One centralchallnge in designing world models is the interplay between complexity and predictive accuracy. As discussed, implicit models,such asthose based on recurrent neural networks or transformers,offer simplicity and elegance,but theyoftencome withthetrade-offof limited interpretability.The model's internalstate is anopaque latent space,making it diffcult to enforce domain constraintsor provide guarantee about the accuracy of predictions. While such systems excelat capturing highlycomplexrelationships anddata-driven patterns,they alsoriskoverfitting or failing to generalize to unseen scenarios.",
              "index": 2,
              "part": 0,
              "translated_content": "设计世界模型时面临的一个核心挑战是复杂性和预测准确性之间的相互作用。正如讨论的那样，隐式模型，如基于循环神经网络或Transformer的模型，提供了简单和优雅，但它们往往伴随着有限的可解释性。模型的内部状态是不透明的潜在空间，这使得很难强制执行领域约束或提供关于预测准确性的保证。虽然这种系统擅长捕捉高度复杂的关系和数据驱动的模式，但它们也面临过拟合或无法推广到未见场景的风险。"
            },
            {
              "type": "text",
              "content": "Explicit models,bycontrast,offer greater transparency andcontrol.Byfactorizing state transitions and observations into separate functions,we gainaclearerunderstanding ofhow predictions areformed,andwecanmore easilyintegrate structuredknowledge,such as physicallaws or domain-specific rules. However,this approach comes with its own setofchallenges.First,itoften requires large amounts oflabeled trainingdata or simulated experiences to accurately capture environment dynamics.Second, even the most wel-structured explicit models may struggle with complex environments thatrequire fine-grained,high-dimensionalstaterepresentations,suchas in videoprediction orobotics.",
              "index": 3,
              "part": 0,
              "translated_content": "相比之下，显式模型提供了更大的透明度和控制。通过将状态转换和观测分解为单独的函数，我们更清晰地了解了预测是如何形成的，并且可以更容易地整合结构化知识，比如物理定律或领域特定规则。然而，这种方法也带来了自己一套挑战。首先，通常需要大量标记的训练数据或模拟经验来准确捕捉环境动态。其次，即使是最结构良好的显式模型在需要精细化、高维状态表示的复杂环境中也可能遇到困难，比如在视频预测或机器人技术中。"
            },
            {
              "type": "text",
              "content": "The simulator-based approachoffers a promising alternative, wherein agents rely on external environments—either physically grounded or simulated-for dynamic updates.This method avoids many of the challnges inherent in learning accurate world models fromscratch,as the simulator itself serves as the“oracle\"of state transitions and observations.However, thereliance on simulatorsalso introduces limitations:simulatorsoften failtocapture the full richness ofreal-worlddynamics andcan be computationally expensive to maintainor scale.Furthermore,real-world environments introduce noise and variability thata purely learned or pre-configured modelmight missAs AI agents strive to performtasksinopen-ended, unpredictablesettings,therobustnessof their world models willbetested bythe gap between simulated and actual environments.",
              "index": 4,
              "part": 0,
              "translated_content": "基于模拟器的方法提供了一种有前途的替代方案，代理依赖外部环境（无论是物理接地的还是模拟的）进行动态更新。这种方法避免了从头开始学习准确世界模型固有的许多挑战，因为模拟器本身充当了状态转换和观测的“神谕”。然而，对模拟器的依赖也带来了限制：模拟器经常无法捕捉真实世界动态的全部丰富性，并且在维护或扩展方面可能会具有计算上的昂贵性。此外，真实世界环境引入了噪声和变异性，纯学习或预配置模型可能会忽略这些因素。随着AI代理努力在开放、不可预测的环境中执行任务，他们的世界模型的鲁棒性将受到模拟和实际环境之间差距的考验。"
            },
            {
              "type": "text",
              "content": "A key theme that emerges from this discussion is the trade-off between generalization and specialization.The more specificaworldmodelistoa particulardomainortask,thelesslikely it is togeneralize acrossdiffrentcontexts.Models like MuZero[349]and Dreamer[350]exemplifythis:they excelat specificenvironments(e.g.Atarigames orrobotics) but require careful adaptation when transferred to new, uncharted domains.Conversely,implicit models—particularly thoseleveraging large-scaleneural networks—have the potentialto generalize acrosstasks butoftendosoatthecostof sacrificing domain-specific expertise.",
              "index": 5,
              "part": 0,
              "translated_content": "从这个讨论中浮现出的一个关键主题是泛化和专业化之间的权衡。世界模型越专门化到特定领域或任务，就越不太可能在不同背景下进行泛化。MuZero和Dreamer等模型就是这样的示例：它们在特定环境（如Atari游戏或机器人技术）中表现出色，但在转移到新的、未知领域时需要仔细调整。相反，隐式模型——特别是那些利用大规模神经网络的模型——有可能在任务之间进行泛化，但通常是以牺牲领域特定专业知识为代价。"
            },
            {
              "type": "text",
              "content": "Moreover, integrating memory with world models is crucial for agents that needto handle long-term dependencies and pastexperiences.While world models excelat predicting the next statebased on immediate inputs,true intelligent behavioroften requires reasoning about distant outcomes.Long-term memory allows agents to store critical environmental knowledge,ensuring that short-term predictions are grounded ina broader understanding of the world. This fusion of memory,perception, and action, mediated bythe world model,creates afeedback loop where predictions shape actions, which in turn inform future predictions.",
              "index": 6,
              "part": 0,
              "translated_content": "此外，将记忆与世界模型相结合对于需要处理长期依赖和过往经验的智能体至关重要。尽管世界模型擅长基于即时输入预测下一个状态，但真正的智能行为通常需要对远期结果进行推理。长期记忆使智能体能够存储关键的环境知识，确保短期预测建立在对世界的更广泛理解之上。这种记忆、感知和行动的融合，通过世界模型中介，创建了一个反馈循环，其中预测塑造行动，进而影响未来的预测。"
            },
            {
              "type": "text",
              "content": "The human analogy remains compelling: just as humans integrate sensory inputs, memories, and internal models to navigate the world, sotoo must inteligent agentscombine perception, memory, and action through their world models.Asthe fieldadvances,it isclear thataholisticapproachone thatunifies implicit,explicitandsimulatorbased methods—may be the key to achieving more robust, generalizable, and adaptive agents.Hybrid methods,like those usedin AutoManual[108] ordiscovery-based models[356]ofer exciting possibilities forblending learnedknowledge with structuredrules andreal-time interactions,potentially pushing the boundaries of what weconsidera world model.",
              "index": 7,
              "part": 0,
              "translated_content": "人类类比仍然令人信服：正如人类整合感官输入、记忆和内部模型以在世界中导航一样，智能体也必须通过它们的世界模型结合感知、记忆和行动。随着领域的发展，很明显，一种整体方法——将隐式、显式和基于模拟器的方法统一起来——可能是实现更强大、更具普适性和适应性的智能体的关键。混合方法，例如AutoManual中使用的方法或基于发现的模型，为将学到的知识与结构化规则和实时交互相融合提供了令人兴奋的可能性，潜在地推动了我们所认为的世界模型的边界。"
            },
            {
              "type": "text",
              "content": "Looking forward,open questions remain. How can we ensure that world models exhibit long-term stability and reliability in real-world setings?How do we handle the inherent uncertainty in dynamic environments while maintaining the flexibility to adapt? Furthermore,asagents grow more sophisticated,how can wedesign systems that are both efficient and scalable acrossincreasingly complex tasks without incurring massve computational costs?\n\nInconclusion,thefutureofworldmodels liesintheirabilitytobalancetheneedforgeneralizationwiththerequirement for domain expertise.By continuing to explore and refine the interplay between model simplicity and complexity, betweenexternal and internalapproaches, we move closer todeveloping AI systems that not only understand the world but can actively shape their understanding to navigate and adapt in a rapidly changing reality.",
              "index": 8,
              "part": 0,
              "translated_content": "展望未来，一些问题仍然悬而未决。我们如何确保世界模型在真实世界环境中表现出长期稳定性和可靠性？在处理动态环境中固有的不确定性的同时，如何保持适应性的灵活性？此外，随着智能体变得更加复杂，我们如何设计既高效又可扩展的系统，以处理日益复杂的任务，而又不带来巨大的计算成本？\n\n总之，世界模型的未来在于它们在平衡泛化需求和对领域专业知识的要求方面的能力。通过不断探索和完善模型简单性和复杂性之间的相互作用，以及外部和内部方法之间的关系，我们逐渐接近开发出能够不仅理解世界，而且能够积极塑造他们的理解以在快速变化的现实中导航和适应的人工智能系统。"
            }
          ],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": [],
          "translated_title": "4.5 总结与讨论"
        }
      ],
      "translated_title": "4 通信拓扑结构 141"
    },
    {
      "title": "l5 Collaboration Paradigms and Collaborative Mechanisms 146",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "15.1 Agent-Agent collaboration 146\n15.2 Human-AI Collaboration 149\n15.3 Collaborative Decision-Making 150",
          "index": 0,
          "part": 0,
          "translated_content": "15.1 智能代理间的协作 146\n15.2 人类与人工智能的协作 149\n15.3 协作决策制定 150"
        }
      ],
      "raw_title": "l5 Collaboration Paradigms and Collaborative Mechanisms 146",
      "type": null,
      "children": [],
      "translated_title": "5 合作范式与协作机制"
    },
    {
      "title": "Collective Intelligence and Adaptation 152",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "16.1 Collective Intelligence 152\n16.2 Individual Adaptability 153",
          "index": 0,
          "part": 0,
          "translated_content": "16.1 集体智能 152\n16.2 个体适应性 153"
        }
      ],
      "raw_title": "Collective Intelligence and Adaptation 152",
      "type": null,
      "children": [],
      "translated_title": "152. 集体智慧与适应能力"
    },
    {
      "title": "17 Evaluating Multi-Agent Systems 155",
      "number": "17",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "17.1 Benchmarks for Specific Reasoning Tasks 155\n17.2 Challenge and Future Work . 159",
          "index": 0,
          "part": 0,
          "translated_content": "17.1 特定推理任务的基准 155\n17.2 挑战与未来工作 159"
        }
      ],
      "raw_title": "Evaluating Multi-Agent Systems 155",
      "type": null,
      "children": [
        {
          "title": "17.1 Benchmarks for Specific Reasoning Tasks",
          "number": "17.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "In multi-agent system solving fortasks, muchfocus has been on leveraging multi-agent coordination forenhancing the reasoning capacity ofLLMs. It is most evident in coding, knowledge,and mathematicalreasoning benchmarks, where one isinterested in examining and building on performance with distributed solving.These benchmarks most typically examine if agents' capability for producing corrct code, reasoning on complex knowledge domains,and solving diffcult mathematical problems withstanding, with measures such as $p a s s@k$ [1076] or proof ratios for success being prevalent.Much improvement has been exhibited by MAS through structured workflow, domain-specific agent roles, and iterative improvement onstate-of-the-artperformance.Onthecontrary,for modeland simulation MAS,the case is one with acomparative lackofstandardizedbenchmarks.Rather,research is primarilyexperimental setups thatsimulate a variety of social phenomena, withcalls from thecommunity for further formalized evaluation frameworks.These multiple benchmark areas are described below,examining the tasks, measures for evaluation,andthecore mechanisms through which MAS result in better performance.",
              "index": 0,
              "part": 0,
              "translated_content": "在解决任务的多智能体系统中，人们更多关注利用多智能体协调来增强LLMs的推理能力。这在编码、知识和数学推理基准方面表现得最为明显，人们希望通过分布式求解的性能来检验和改进。这些基准通常检验智能体是否能够正确编写代码，在复杂知识领域进行推理，并解决困难的数学问题，其中常见的度量如$p a s s@k$或成功的证明比率。通过结构化工作流程、特定领域智能体角色和对最新性能的迭代改进，多智能体系统已取得了很大进步。相反，在模型和仿真多智能体系统中，情况则是缺乏标准化基准。相反，研究主要是实验设置，模拟各种社会现象，社区呼吁进一步制定评估框架。下面将描述这些多个基准领域，检查任务、评估措施以及多智能体系统通过哪些核心机制实现更好的性能。"
            },
            {
              "type": "text",
              "content": "Code Reasoning Benchmark Measuring the capability of LLMs for code synthesis requires bespoke benchmark suites with afocus on functionalcorrectness. Code synthesis,ascompared tonaturallanguage synthesis,allows for direct verificationthrough running. Severalbenchmark suites have been built forthis purpose,typically consisting of a collection of programming problems,each described with a naturallanguage problem description and acollection of test cases for automatically ascertaining the synthesized code's correctness.",
              "index": 1,
              "part": 0,
              "translated_content": "代码推理基准\n衡量LLMs在代码合成方面的能力需要专门的基准套件，重点关注功能正确性。与自然语言合成相比，代码合成允许通过运行直接验证。为此目的构建了几个基准套件，通常包括一系列编程问题，每个问题都有自然语言问题描述以及一组测试用例，用于自动确定合成代码的正确性。"
            },
            {
              "type": "text",
              "content": "HumanEval[1077], APPS [1078], and MBPP [939] are some popular ones. These benchmark suites predominantly utilize the $p a s s@k$ metric, which computes the percentage at which at least one among the top- $k$ generated solutions passes all test cases for a number of problems. Theproblemscovered throughthese benchmark suitesrange acrossavariety of diffculties and programming abstractions,requiring notonly for LLMsandAgents but alsofor syntacticallycorrct andlogically soundcode that satisfies the providedtest cases.",
              "index": 1,
              "part": 1,
              "translated_content": "HumanEval、APPS和MBPP是一些流行的基准套件。这些基准套件主要使用$p a s s@k$指标，该指标计算在顶部$k$个生成的解决方案中至少有一个通过所有测试用例的百分比，适用于多个问题。通过这些基准套件涵盖的问题涵盖了各种难度和编程抽象，不仅要求LLMs和Agents，还要求符合语法正确和逻辑正确、满足提供的测试用例的代码。"
            },
            {
              "type": "text",
              "content": "Recent work has explored leveraging Multi-Agent Systems (MAS)forenhancing LLM capability on code reasoning. For instance,MetaGPT[626] is a meta-programming system which embeds human-like Standard Operating Procedures (SOPs)into multi-agent cooperation based on LLM. With multi-agent role assignment with varying domains and adopting assembly line mode, MetaGPT effectively breaks down difficult operations into sub-operations and achieves state-of-the-art performance on HumanEval and MBPP benchmarks. SWE-agent [628] presents anovelAgent-Computer Interface(ACI) whichlargelyenhances arepository-creating,repository-editing,and navigationcapability for an agent. The system demonstratesthatawel-structured interfacetailoredforLMscanlargely enhance software engineering capability, with state-of-the-art on SWE-bench and HumanEval. AgentCoder [994] is anothrmulti-agentcoding system withfocusoneffectivetesting andauto-optimization. It isathree-agent systemwith a programmer,atest designer,andatest executor.",
              "index": 1,
              "part": 2,
              "translated_content": "最近的研究探讨了利用多智体系统（MAS）来增强LLM在代码推理方面的能力。例如，MetaGPT是一个元编程系统，将类似人类的标准操作规程（SOPs）嵌入到基于LLM的多智体合作中。通过采用不同领域的多智体角色分配和采用流水线模式，MetaGPT有效地将复杂操作分解为子操作，并在HumanEval和MBPP基准测试中实现了最先进的性能。SWE-agent提出了一种新颖的Agent-Computer Interface（ACI），大大增强了代理的创建、编辑和导航能力。该系统表明，为LM量身定制的良好结构化界面可以大大增强软件工程能力，在SWE-bench和HumanEval上处于领先地位。AgentCoder是另一个以有效测试和自动优化为重点的多智体编码系统。它是一个由程序员、测试设计师和测试执行者组成的三智体系统。"
            },
            {
              "type": "text",
              "content": "Thetest designersupplies accurate and diverse testcases,andthe test executor provides feedbackto the programmer for optimization. Such collaborative workflow enhancescoding efficiency and outperforms one-agent models and other multi-agent approaches on HumanEval and MBPP datasets. These MAS approaches all pointout multi-agent cooperation,organized workflow,and tailored interface as effective solution strategies for enhancing thecapability of LLMon code reasoning. DEVAI[781] proposes a set of novel AI development automation benchmarks, which utilize a judge-agent mechanism for judging automatically intermediate development process.",
              "index": 1,
              "part": 3,
              "translated_content": "测试设计者提供准确且多样化的测试用例，而测试执行者为程序员提供优化反馈。这种协作工作流提高了编码效率，并在HumanEval和MBPP数据集上表现优于单一智体模型和其他多智体方法。这些多智体系统方法都指出多智体合作、组织良好的工作流程和量身定制的界面作为增强LLM在代码推理方面能力的有效解决策略。DEVAI提出了一组新颖的AI开发自动化基准，利用评判-智体机制来自动评判中间开发过程。"
            },
            {
              "type": "text",
              "content": "Knowledge Reasoning Benchmark To facilitate AI agents effectively acting in and understanding the world,robust knowledge reasoning abilities are essential.Benchmarks for this class assessanagent'sability to utilize factual knowledge and logical reasoning when answering challnging queries.Commonsense reasoning is tested with benchmarks such as CSQA [1079] and StrategyQA[1080],and scientific knowledge understanding is tested with ScienceQA[1081l.The core challenge for agents is performing multi-step,chain-of-thought reasoning,stepwise logically progressing from input query to output answer.These tests concentrate on assessing how wella specific AI agent can apply a specific body of knowledge,one at a time, and reason out a problem. Recent research has experimented with the use of LLMs on MAS for improving knowledge reasoning task performance, and they have achieved state-of-the-art accuracy. For example, MASTER [10o9], a novel multi-agent system, employs a novel recruitment process for agents andcommunication protocol using the Monte Carlo TreeSearch (MCTS)algorithm, and achieves $76\\%$ accuracy on HotpotQA [940]. Reflexion [48], a universal framework for bringing reasoning and acting together with language models, improves baseline by $20\\%$ on HotpotQA. These strategies demonstrate the potential of multi-agentcoordination forknowledge reasoningtasks.Besides,leveraging externaltols,e.gsearchengines, is also needed for improvingknowledge reasoning capacity.Agents may applythese tools forretrieving thelatest information and alsoforfactchecking,thus improving the accuracyanddependabilityofresponses.Such integrationis particularly helpful on applications such as TriviaQA[1082],for which real-time information access is essential.",
              "index": 2,
              "part": 0,
              "translated_content": "知识推理基准为促进人工智能智能地行动和理解世界提供了必不可少的稳健知识推理能力。这一类基准评估了智能体在回答具有挑战性的问题时利用事实知识和逻辑推理的能力。常识推理通过基准（如CSQA[1079]和StrategyQA[1080]）进行测试，科学知识理解则通过ScienceQA[1081]进行测试。对智能体的核心挑战在于进行多步骤、思维链式推理，逐步从输入查询推进到输出答案的逻辑推理过程。这些测试集中评估特定人工智能智能体能够逐一应用特定知识体系并推理解决问题的能力。最近的研究尝试在多智体系统上使用LLM来提高知识推理任务性能，并取得了最先进的准确性。例如，MASTER[109]，一种新颖的多智体系统，采用了一种新颖的智体招募流程和使用蒙特卡洛树搜索（MCTS）算法的通信协议，在HotpotQA[940]上实现了76%的准确率。Reflexion[48]是一个通用框架，将推理和行动与语言模型结合在一起，提高了HotpotQA基线20%的准确率。这些策略展示了多智体协调在知识推理任务中的潜力。此外，利用外部工具，如搜索引擎，也是提高知识推理能力所必需的。智能体可以利用这些工具来获取最新信息，进行事实核查，从而提高回答的准确性和可靠性。这种整合在诸如TriviaQA[1082]之类需要实时信息访问的应用中尤为有用。"
            },
            {
              "type": "text",
              "content": "Mathematical Reasoning Benchmark Math reasoning is a critical skillfor AI agents which requires cooperative utilisation of mathematical knowledge,logical deduction, and computational power.Benchmarking tasks for this capability tend tofallinto twocategories: math problem-solving and computer-aidedtheorem proving(ATP).Datasets such as SVAMP[942],GSM8K[1083],and MATH[941]challenge agents to solve word problems, asking for exact number answers or formulas. ATP is a harder test, with stricter compliance with formal proof schemata. Tests on datasetslike PISA [1084] and miniF2F[1076], which are graded on proof completion, test whether an agent can produce well-formed mathematical proofs.Multi-agent systems (MAS)have been putforward as a potentialsolutionfor handling mathematical reasoning problem complexity.Methods such as MACM[1010] include a multi-agent system consisting of Thinker,Judge,and Executor agents tailoredforacomplex problem,dividing itinto smaller sub-problems for computation.The Thinker agent generates new ideas,Judge decides if they are accurate,and Executor conducts necessary computation involving tools such ascalculators.Such a modular structuresupports iterativerefinement and elimination of errors,enhancing problem-solving accuracy.Furthermore, methods such as multi-agent debate[985] include severalinstances of alanguage modeldebating andrefocusing iterativelyforcolective solutionimprovement, enhancing reasoning as wellas factuality accuracy. Such MAS-based systems have achieved notable improvementon benchmarks such as MATH and GSM8K,establishing distributed solving capacity for mathematical problems.Aside from this,reinforcementlearning from human feedback(RLHF)and preferencelearning strategies havebeen attempted for further enhancing mathematical problem-solving capacity of LLMs.For instance,amulti-turn online iterative direct preference learning framework [1085] has been put forwardfor training various language models with enriched sets of prompts over GSM8K and MATH datasets. Such a technique includes feedback from interpreters for codes and optimizes preferences at a level of trajectories, with notable improvement in output.",
              "index": 3,
              "part": 0,
              "translated_content": "数学推理基准\n数学推理是人工智能智能体的关键技能，需要协同利用数学知识、逻辑推理和计算能力。针对这种能力的基准任务通常分为两类：数学问题解决和计算机辅助定理证明（ATP）。诸如SVAMP、GSM8K和MATH等数据集挑战智能体解决文字问题，要求给出确切的数字答案或公式。ATP是一个更难的测试，需要更严格地遵守形式化证明模式。类似PISA和miniF2F的数据集测试智能体是否能够生成形式良好的数学证明，评分依据是证明的完成情况。多智体系统被提出作为处理数学推理问题复杂性的潜在解决方案。诸如MACM等方法包括一个多智体系统，由思考者、裁判和执行者代理组成，针对复杂问题进行定制，将其分解为较小的子问题以进行计算。思考者代理生成新思路，裁判决定其准确性，执行者进行必要的计算，包括使用计算器等工具。这种模块化结构支持迭代的改进和错误消除，提高问题解决的准确性。此外，诸如多智体辩论等方法包括多个语言模型实例进行辩论和迭代重新聚焦，以提高集体解决方案的推理和事实准确性。这种基于多智体系统的系统在MATH和GSM8K等基准测试中取得了显著改进，为解决数学问题建立了分布式解决能力。除此之外，还尝试了从人类反馈中进行强化学习（RLHF）和偏好学习策略，以进一步提升LLM的数学问题解决能力。例如，提出了一种多轮在线迭代直接偏好学习框架，用于在GSM8K和MATH数据集上训练各种语言模型。该技术包括解释者对代码的反馈，并在轨迹级别优化偏好，显著提高了输出结果。"
            },
            {
              "type": "text",
              "content": "Societal Simulation Benchmark Social simulation benchmarks are esential for evaluating multi-agent system performance and realism for simulating human behavior and socialinteractions based on LLMs.Standardized sets and test cases forevaluating theagents\"abilityforinteractingcommunicating,andevolving withinasimulatedsocietyare providedthrough the benchmarks. An example ofone such widely used benchmark is SOTOPIA [1086],employed for evaluating socialintelligence innaturallanguage agent-basedsocialintelligence.It is employed forevaluating agents' ability forconversing,understanding social cues,and building relationships witheach other within a virtual society. Another benchmark involves simulating propagation Gender Discrimination and Nuclear Energy [255] topics on social networks.Itis employed to evaluate agents'capabilities in modeling opinion dynamics,information dissemination, and social influence within large-scale social networks.Multiagent Bench [948] further provides two simulation domains—werewolf and bargaining—to assesscompetitive interactions among diverse agent groups with conflicting goals.",
              "index": 4,
              "part": 0,
              "translated_content": "社会仿真基准 社会仿真基准对于评估基于LLM的多智能体系统在模拟人类行为和社会互动方面的性能和逼真度至关重要。通过这些基准，为评估智能体在模拟社会中相互作用、沟通和演化的能力提供了标准化的集合和测试用例。其中一个广泛使用的基准示例是SOTOPIA [1086]，用于评估自然语言智能体的社会智能。它用于评估智能体在虚拟社会中进行对话、理解社交线索以及建立彼此关系的能力。另一个基准涉及在社交网络中模拟“性别歧视”和“核能”话题的传播。它用于评估智能体在大规模社交网络中建模意见动态、信息传播和社会影响的能力。多智能体基准 [948] 还提供了两个仿真领域——狼人和讨价还价——用于评估不同智能体群体之间具有冲突目标的竞争性互动。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td>Category</td><td>Focus</td><td>Benchmarks</td><td>Examples</td><td>Representative Metrics</td></tr><tr><td rowspan=\"3\">Task-solving</td><td>Code Reasoning</td><td>APPS [1078],HumanEval [1077],MBPP [939], CodeContest [1087], MTPB[1088], DS-1000 [1089],ODEX [1090], Raconteur[1091]</td><td>MetaGPT [626], SWE-agent [628], AgentCoder [994]</td><td>Pass@k, Resolved(%)</td></tr><tr><td>Knowledge Reasoning</td><td>ARC [1092], HotpotQA [940], CSQA [1079], StrategyQA [1080],B00lQ[1093], OpenBookQA[1094],WinoGrande[1095], HellaSwag [1096],SIQA[1097], PIQA [1098],</td><td>Reflexion [48], MASTER [1009]</td><td>Accuracy</td></tr><tr><td>Mathematical Reasoning</td><td>proScript [1099], ScienceQA [1081], ProOntoQA [1100] MATH [941], GSM8K [1083], SVAMP [942], MultiArith [943], ASDiv [1101], MathQA[1102],AQUA-RAT[1103], MAWPS[1104],DROP [1105], NaturalProofs [1106], PISA [1084],</td><td>MACM [1010], Debate [985]</td><td>Accuracy, Pass @k</td></tr><tr><td rowspan=\"3\">Collaboration</td><td>Communication-based Cooperation</td><td>miniF2F [1076], Pr0ofNet [1107] InformativeBench [1108], Collab-Overcooked [944], COMMA[1109],</td><td>iAgents [1108], Two-Player [1110], EAAC[1111]</td><td>Task Completion Rate Communication Efficiency</td></tr><tr><td>Plorinain</td><td>LLM-Coordination [926] PARTGA</td><td>ResearAs ow [114 1</td><td>Planning Success Rate Coordination Efficiency</td></tr><tr><td>Process-oriented</td><td>Bench [948] Auto-Arena [947]</td><td>GPTSwarm[651] Idea [1115]</td><td>Process Completion Rate Step Efficiency</td></tr><tr><td rowspan=\"3\">Competition</td><td>Adversarial Scenarios</td><td>BattleAgentBench [920], MAgIC [955], LLMArena [1116], PokerBench [1117],</td><td>Dilemma [1118], PokéLLMon [1119]</td><td> Wi ating</td></tr><tr><td>Social Deduction</td><td>vale Diplomacy [934]</td><td>MA-KTO [1121], HLR [1122],</td><td>Win Rate Accuracy of Deductions</td></tr><tr><td>Game-Theoretic</td><td>Guandan [1123], AgentVerse [1124], ICP [1125]</td><td>WarAgent [1126]</td><td>Score Win Rate</td></tr></table></body></html>",
              "caption": "Table 17.1:MAS Benchmarks: A Systematic Clasification of Multi-Agent System Evaluation Frameworks Categorized by Task-Oriented Performance and System-Level Capabilities.This comprehensive collction encompasses both specialized task-solving benchmarksandholisticcapabilityassessments,reflecting the dualnature of MAS evaluation in collaborative problem-solving and inter-agent dynamics.",
              "index": 5,
              "part": 0,
              "translated_caption": "表17.1：MAS基准测试：按任务导向性能和系统级能力分类的多智能体系统评估框架。这个全面的集合包括专门的任务解决基准测试和整体能力评估，反映了多智能体系统评估在协作问题解决和智能体间动态中的双重性质。"
            },
            {
              "type": "text",
              "content": "Evaluatingcapabilities inLLM-based MASrequires specialized approaches thateectively measure therichinteractions between agents.Asthis field evolves,evaluation methodologies have transitioned from single-dimension metrics to multi-faceted evaluation frameworks thatcapture thecomplex skillset required for efective multi-agent interaction. This evolution reflects a growing understanding that agent performance must be assessed acrossmultiple dimensions including collaboration success, reasoning capabilities, and system efficiency.",
              "index": 6,
              "part": 0,
              "translated_content": "评估基于LLM的多智能体系统的能力需要专门的方法，可以有效地衡量智能体之间丰富的互动。随着这一领域的发展，评估方法已经从单一维度的指标过渡到多方面的评估框架，捕捉到有效多智能体互动所需的复杂技能集。这种演变反映了人们日益增长的认识，即智能体的表现必须跨越多个维度进行评估，包括协作成功、推理能力和系统效率。"
            },
            {
              "type": "text",
              "content": "In recent research,the MAS evaluation can be mainly categorized along three primary dimensions:collaborationfocused benchmarks,competition-focused benchmarks,and adaptive and resilience benchmarks. Within eachcategory, we identify specific metric domains that capture different aspects of agent performance.Current evaluation approaches typically measureeffciencymetrics(e.g,task completionrates,resource utilization,time efficiency)decision quality metrics (e.g.action accuracy,strategic soundness,reasoning depth),collaboration quality metrics (e.g.communication effectiveness,coordinationeffciency, workload distribution),andadaptabilitymetrics (e.g,response todisruptions, self-correction), which provide a foundation for evaluating multi-agent systems.",
              "index": 7,
              "part": 0,
              "translated_content": "在最近的研究中，多智能体系统的评估主要可以沿着三个主要维度进行分类：以协作为重点的基准、以竞争为重点的基准，以及自适应和韧性基准。在每个类别中，我们确定了捕捉智能体表现不同方面的特定度量领域。当前的评估方法通常衡量效率度量（例如任务完成率、资源利用率、时间效率）、决策质量度量（例如动作准确性、战略合理性、推理深度）、协作质量度量（例如沟通效果、协调效率、工作负荷分配）和适应性度量（例如对干扰的响应、自我纠正），这为评估多智能体系统奠定了基础。"
            },
            {
              "type": "text",
              "content": "Collaboration-focused Benchmarks.Collaboration-focused benchmarks have evolved significantly,shifting from basic single-dimensionalmetrics toward comprehensive frameworks thatevaluate complex agent-to-agentcommunication and coordination.Initialbenchmarks,such asInformativeBench[108],primarilyaddressed agent collboration under conditions of informationasymmetry,employing metrics like Precision and IoU to measure decision accuracy in information dissemination tasks.Subsequently, the scope ofevaluation expanded, exemplified byCollb-Overcooked[944], whichintroduced nuanced processoriented metrics such as Trajectory Effciency Score (TES)and Incremental Trajectory Effciency Score (ITES).These metrics assess detailed aspects ofcoordination,revealing significant shortcomings in agents' proactive planning and adaptive capabilities despite their strong task comprehension.",
              "index": 8,
              "part": 0,
              "translated_content": "以协作为重点的基准。协作为重点的基准已经有了显著的发展，从基本的单一维度指标转变为评估复杂的智能体间通信和协调的综合框架。最初的基准，如InformativeBench[108]，主要关注在信息不对称条件下的智能体协作，采用诸如Precision和IoU等指标来衡量信息传播任务中的决策准确性。随后，评估范围扩大，如Collb-Overcooked[944]，引入了诸如轨迹效率得分（TES）和增量轨迹效率得分（ITES）等细致的过程导向指标。这些指标评估了协调的详细方面，揭示了智能体在强大任务理解能力的情况下存在的显著缺陷，包括主动规划和适应能力不足。"
            },
            {
              "type": "text",
              "content": "Further expanding the evaluation scope,COMMA [1109] and LLM-Coordination [926] emphasized communication effctiveness and strategic synchronization,employing diverse environments and extensive metrics including Success Rate, Average Mistakes, and Environment Comprehension Accuracy. These benchmarks collectively illustrate an emerging trend toward capturing deeper aspects of colaborative behaviors and strategic consistency.\n\nOther benchmarks,suchasPARTNR[946],VilagerBench[925],andBabyAGI[2],further addressed gaps inexist ing evaluations by focusing explicitlyonreasoning,plannng,and task decomposition.These benchmarks highlighted the need for comprehensive asessment of agentsability to engage incomplex,sociallyembeddedtasks,considering metrics like Percent Completion,Balanced Agent Utilization,and agent contribution rates.AgentBench[7O6], VisualAgentBench[928],and Auto-Arena[947] further standardized multi-agent evaluations,automating assessment across various domains and demonstrating substantial performance disparities between closed-source and open-source LLMs. These observations underscored critical challenges in developing universally effective collaboration frameworks.",
              "index": 9,
              "part": 0,
              "translated_content": "进一步扩大评估范围，COMMA[1109]和LLM-Coordination[926]强调了沟通效果和战略同步，采用了多样化的环境和广泛的指标，包括成功率、平均错误率和环境理解准确性。这些基准共同展示了捕捉协作行为和战略一致性更深层次方面的新趋势。\n\n其他基准，如PARTNR[946]、VilagerBench[925]和BabyAGI[2]，通过专注于推理、规划和任务分解，进一步解决了现有评估中的差距。这些基准强调了对智能体参与复杂的社会嵌入式任务能力的全面评估，考虑了完成百分比、平衡智能体利用率和智能体贡献率等指标。AgentBench[706]、VisualAgentBench[928]和Auto-Arena[947]进一步标准化了多智能体评估，自动化评估跨不同领域，并展示了封闭源和开放源LLM之间的实质性性能差异。这些观察强调了开发普遍有效的协作框架面临的关键挑战。"
            },
            {
              "type": "text",
              "content": "In summary,collaboration-focused benchmarkscollectivelyreflect an ongoing shift toward comprehensive, nuanced evaluations thatencompasscommunicationeffciency,adaptive strategy,andfinegrainedagentcoordination,addressing earlier limitations focused solely on outcome-based performance.\n\nCompetition-focused Benchmarks.Competition-focused benchmarks evaluate agents'strategic capabilities and adversarial interactions,highlighting specific deficiencies in Theory of Mind andopponent modeling.Early benchmarks suchas BatleAgentBench[920] and MAgIC[955] initiated the focus on mixed cooperative-competitive environments uncovering critical weaknesses in high-order strategic reasoning among LLM agents.These benchmarks employed comprehensive competitive metricssuch as Forward Distance,Judgment Accuracy,and Rationality scores,identifying that while advanced LLMs performed adequately in simpler scenarios,significant limitations persisted under complex adversarial conditions.",
              "index": 10,
              "part": 0,
              "translated_content": "总的来说，以协作为重点的基准共同反映了向全面、细致评估的持续转变，涵盖了沟通效率、自适应策略和细粒度智能体协调，解决了早期仅关注结果性能的限制。\n\n以竞争为重点的基准。竞争为重点的基准评估智能体的战略能力和对抗性互动，突出了心智理论和对手建模方面的特定缺陷。早期的基准，如BatleAgentBench和MAgIC，开始关注混合合作竞争环境，揭示了LLM智能体在高阶战略推理方面存在的关键弱点。这些基准采用了全面的竞争指标，如前进距离、判断准确性和理性得分，发现虽然先进的LLM在简单场景下表现良好，但在复杂对抗条件下仍存在显著限制。"
            },
            {
              "type": "text",
              "content": "Building upon these insights, subsequent benchmarks like Human Simulacra[1120],LLMArena[1l16],and PokerBench[1l17]furtherrefined competitive evaluation by incorporating human-likereasoning metrics and more robust strategic measures (e.g.,Response Similarity Score,Elo Scores,and Action Accuracy).These evaluations consistently demonstrated shortcomings inopponent prediction,risk assessment, andadaptivestrategic planning,despitehigh task comprehension.\n\nSocial deduction and deception-based benchmarks,notably AvalonBench[972]and Diplomacy[934],further revealed fundamental gaps in agents'abilities to interpret hidden information and manage complex social dynamics.Metrics like Assassnation Accuracy, Deduction Accuracy, and Win Rates emphasized that even sophisticated LLMs fail to replicate human-level reasoning in adversarial negotiation and hidden-information games.\n\nAdditional game-theoretic evaluations, including Guandan[123],AgentVerse[l124],MultiAgentBench [948],and ICP[1125],introduced scenarios requiring strategic cooperation under incomplete information. These benchmarks reinforcedpreviousfindings onthe necesity ofenhanced Theoryof Mind and predictive modeling capabilities.MultiAgentBench[948]also introduces the KPIandcoordination score to evaluatethecompetition of agents.Collctively, competition-focused benchmarks highlight persistent strategic andreasoning limitations among LLM-based agents,underscoringtheongoing need to addresscritical gaps inadversarialmodeling and strategic planning despite advancements in general reasoning and task execution capabilities.",
              "index": 11,
              "part": 0,
              "translated_content": "基于这些见解，随后的基准，如Human Simulacra、LLMArena和PokerBench，通过整合类人推理指标和更稳健的战略措施（如响应相似度评分、Elo评分和行动准确性），进一步完善了竞争评估。这些评估一再表明，尽管在任务理解方面表现出色，但在对手预测、风险评估和自适应战略规划方面存在不足。\n\n社交推理和欺骗为基础的基准，尤其是AvalonBench和Diplomacy，进一步揭示了智能体在解释隐藏信息和处理复杂社交动态方面的基本差距。类似暗杀准确性、推理准确性和胜率等指标强调，即使是复杂的LLM也无法复制人类水平的对抗谈判和隐藏信息游戏推理。\n\n另外，包括Guandan、AgentVerse、MultiAgentBench和ICP在内的博弈论评估引入了需要在信息不完全情况下进行战略合作的场景。这些基准加强了先前关于增强心灵理论和预测建模能力的必要性的发现。MultiAgentBench还引入了关键绩效指标和协调评分来评估智能体之间的竞争。总体而言，以竞争为重点的基准突显了基于LLM的智能体在战略和推理方面存在持续的限制，强调了尽管在一般推理和任务执行能力方面取得了进展，但仍需要解决对抗建模和战略规划中的关键差距。"
            },
            {
              "type": "text",
              "content": "Adaptive and Resilience Benchmarks adaptive and resilient multi-agent system benchmarks tackle two interconnectedcapabilitiestogether:adaptability-the ability of the agents to actdynamicallin altering,unexpected environmentalconditions by modifying their behavior and strategy.Resilience,or the ability of the system to endure, alleviate,andrapidlyrecoverfrom disruptions,faults,orhostile intervention.In adaptability,as mentioned in AdaSociety[l127],the dynamic interplay between socialrelationships and physicalenvironments demands that agents engage in continuous learning, and strikea balance between environment discovery and social network construction. Despite significant advancements incurrentmulti-agent decision-making frameworks,these environments fallshort in introducing new challnges in various physical contexts and changing social interdependencies.Therefore, AdaSociety introduces an environment in which physical states,tasks, and social relationships among agents continuously evolve,therebycapturingtheadaptabilityof agents astheyrespondto expandingtaskcomplexity and shifting resource constraints.",
              "index": 12,
              "part": 0,
              "translated_content": "自适应和韧性基准涉及自适应和韧性多智能体系统基准，同时解决两个相互关联的能力：自适应——智能体根据改变、意外的环境条件通过修改其行为和策略来动态行动的能力。韧性，或系统忍受、减轻和迅速从干扰、故障或敌对干预中恢复的能力。如AdaSociety所述，在自适应性方面，社会关系和物理环境之间的动态相互作用要求智能体进行持续学习，并在环境发现和社交网络构建之间取得平衡。尽管当前多智能体决策框架取得了显著进展，但这些环境在引入各种物理背景和不断变化的社会依赖关系中存在不足。因此，AdaSociety引入了一个环境，其中物理状态、任务和智能体之间的社会关系不断演变，从而捕捉了智能体在应对不断增加的任务复杂性和转变的资源约束时的适应能力。"
            },
            {
              "type": "text",
              "content": "Moreover,current benchmarks may oversimplify the challnges of real-world automation with limited disruption modeling and simplified dependencies of process [945],resulting in insufficient evaluation of planning capabilities and adaptability.Thus, REALM-Bench [945], on the other hand, defines adaptation through real-world-inspired planning problems,which emphasizes metrics such as real-time re-planning effciency,coordination scalability under increasing complexity,and the stability of performance outcomes despite dynamic interdependencies or disruptive events.Conversely,resilience benchmarks[ll28]systematically introduce faults orerrors into individual agents to assess overall system robustness.",
              "index": 13,
              "part": 0,
              "translated_content": "此外，当前的基准可能过于简化了真实世界自动化的挑战，对干扰建模和过程简化依赖有限[945]，导致对规划能力和适应性的评估不足。因此，另一方面，REALM-Bench [945]通过真实世界启发的规划问题定义了适应性，强调诸如实时重新规划效率、在复杂性增加下的协调可扩展性以及尽管动态相互依赖或干扰事件发生时性能结果的稳定性等指标。相反，韧性基准[1128]系统地引入故障或错误到各个智能体中，以评估整体系统的稳健性。"
            }
          ],
          "raw_title": "Benchmarks for Specific Reasoning Tasks",
          "type": null,
          "children": [],
          "translated_title": "17.1 特定推理任务的基准"
        },
        {
          "title": "17.2 Challenge and Future Work",
          "number": "17.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "While various MAS evaluation benchmarks have beendeveloped in recent years,challenges and limitations continue to exist withregard tothe standardization of evaluation acrossdifferentMAS tasks and scenarios,andthe abilityto evaluate scalability and diversity in MASs. Future research must address these challenges,in order to develop the comprehensive field of MAS evaluation.\n\nBelow are some challenges and future directions in LLM Multi-agent evaluation:\n\n1. Multi-Agent System has demonstrated superior performance in solving complex tasks, when compared with single agent frameworks. But compared with single agent system, MAS also requires more computations and brings additional costs.Therefore,there has a urgent challnge that we need to handle: when we need to invoke MAS framework? For many simple user instructions, we may only require LLM or single agent system to accomplish.And only complex user instructions could require MAS frameworks.Hence,in the future,how to design the task router mechansim to detect which scenario require MAS or not is fundamental but also a important issue.\n2.Multi-agent system is a high-level framework, built upon multiple AI agent based on the foundation models. Therefore, just like back propagation, the optimization of MAS framework will also affect each part (i.e., foundation model, AI Agent and Multi-agent collaboration).\n3.Existing MAS frameworks usually design multiple agents with homogeneous traits,such as allbeing languagebased agents. But when connecting MAS to real-world scenarios,it usually involves diffrent kinds of AI agents.For example,we may need to bridge the connections between language-based agent, digital agent and robotic agents.However, these agents adopt various settings,from the inputs to the outputs.How to establish the connection between these agent is still a open problem that need to be handle in the future.",
              "index": 0,
              "part": 0,
              "translated_content": "近年来，已经开发了各种多智能体系统（MAS）评估基准，但在跨不同MAS任务和场景的评估标准化以及评估MAS的可伸缩性和多样性方面仍存在挑战和限制。未来的研究必须解决这些挑战，以发展MAS评估的全面领域。\n\n以下是LLM多智能体评估中的一些挑战和未来方向：\n\n1. 与单一智能体框架相比，多智能体系统在解决复杂任务时表现出优越性能。但与单一智能体系统相比，MAS还需要更多的计算并带来额外的成本。因此，我们面临一个紧迫的挑战：何时需要调用MAS框架？对于许多简单的用户指令，我们可能只需要LLM或单一智能体系统来完成。只有复杂的用户指令可能需要MAS框架。因此，在未来，如何设计任务路由机制以检测哪些情景需要MAS是基础但也是一个重要问题。\n2. 多智能体系统是一个高级框架，建立在多个基于基础模型的AI代理之上。因此，就像反向传播一样，MAS框架的优化也会影响每个部分（即基础模型、AI代理和多智能体协作）。\n3. 现有的MAS框架通常设计具有同质特征的多个代理，例如所有都是基于语言的代理。但将MAS连接到现实场景时，通常涉及不同类型的AI代理。例如，我们可能需要在基于语言的代理、数字代理和机器人代理之间建立连接。然而，这些代理采用各种设置，从输入到输出不等。如何建立这些代理之间的连接仍然是一个需要在未来解决的开放性问题。"
            }
          ],
          "raw_title": "Challenge and Future Work",
          "type": null,
          "children": [],
          "translated_title": "17.2 挑战与未来工作"
        }
      ],
      "translated_title": "17 评估多Agent系统 155"
    },
    {
      "title": "18.1 Safety Vulnerabilities of LLMs 163",
      "number": "18.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "18.1.1 Jailbreak Attacks 163\n18.1.2 Prompt Injection Attacks 166\n18.1.3 Hallucination Risks . 167\n18.1.4 Misalignment Issues 169\n18.1.5 Poisoning Attacks 170\n18.2 Privacy Concerns 172\n18.2.1 Inference of Training Data 172\n18.2.2 Inference of Interaction Data. 173\n18.2.3 Privacy Threats Mitigation 174\n18.3 Summary and Discussion 175",
          "index": 0,
          "part": 0,
          "translated_content": "18.1.1 越狱攻击 163\n18.1.2 提示注入攻击 166\n18.1.3 幻觉风险 167\n18.1.4 不对齐问题 169\n18.1.5 毒化攻击 170\n18.2 隐私关注 172\n18.2.1 推断训练数据 172\n18.2.2 推断交互数据 173\n18.2.3 隐私威胁缓解 174\n18.3 总结与讨论 175"
        }
      ],
      "raw_title": "Safety Vulnerabilities of LLMs 163",
      "type": null,
      "children": [],
      "translated_title": "18.1 LLM的安全漏洞 163"
    },
    {
      "title": "Agent Intrinsic Safety: Threats on Non-Brain Modules 176",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "19.1 Perception Safety Threats . 176\n19.1.1 Adversarial Attacks on Perception 176",
          "index": 0,
          "part": 0,
          "translated_content": "19.1 感知安全威胁 . 176\n19.1.1 感知中的对抗性攻击 176"
        }
      ],
      "raw_title": "Agent Intrinsic Safety: Threats on Non-Brain Modules 176",
      "type": null,
      "children": [],
      "translated_title": "176 代理人固有安全性：对非脑模块的威胁"
    },
    {
      "title": "19.1.2 Misperception Issues 177",
      "number": "19.1.2",
      "level": 3,
      "content": [
        {
          "type": "text",
          "content": "19.2 Action Safety Threats 178\n19.2.1 Supply Chain Attacks 178\n19.2.2 Risks in Tool Usage 179",
          "index": 0,
          "part": 0,
          "translated_content": "19.2 行动安全威胁\n19.2.1 供应链攻击\n19.2.2 工具使用中的风险"
        }
      ],
      "raw_title": "Misperception Issues 177",
      "type": null,
      "children": [],
      "translated_title": "19.1.2 误解问题 177"
    },
    {
      "title": "0 Agent Extrinsic Safety: Interaction Risks 180",
      "number": "0",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "20.1 Agent-Memory Interaction Threats 180\n\n20.2 Agent-Environment Interaction Threats 180\n20.3 Agent-Agent Interaction Threats 182\n20.4 Summary and Discussion 182",
          "index": 0,
          "part": 0,
          "translated_content": "20.1 代理-记忆交互威胁 180\n\n20.2 代理-环境交互威胁 180\n20.3 代理-代理交互威胁 182\n20.4 总结与讨论 182"
        }
      ],
      "raw_title": "Agent Extrinsic Safety: Interaction Risks 180",
      "type": null,
      "children": [],
      "translated_title": "0 代理外在安全性：交互风险 180"
    },
    {
      "title": "21 Superalignment and Safety Scaling Law in AI Agents 184",
      "number": "21",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "21.1 Superalignment: Goal-Driven Alignment for AI Agents . 184\n\n21.1.1 Composite Objective Functions in Superalignment 184\n21.1.2 Overcoming the Limitations of RLHF with Superalignment 185\n21.1.3 Empirical Evidence Supporting Superalignment . 185\n21.1.4 Challenges and Future Directions 185\n.2 Safety Scaling Law in AI Agents . 186\n21.2.1 Current landscape: balancing model safety and performance 186\n21.2.2 Enhancing safety: preference alignment and controllable design 187\n21.2.3 Future directions and strategies: the AI- $45^{\\circ}$ rule and risk management 187",
          "index": 0,
          "part": 0,
          "translated_content": "21.1 超对齐：AI代理的目标驱动对齐。184\n\n21.1.1 超对齐中的复合目标函数。184\n21.1.2 通过超对齐克服RLHF的局限性。185\n21.1.3 实证证据支持超对齐。185\n21.1.4 挑战和未来方向。185\n\n21.2 AI代理中的安全扩展法则。186\n21.2.1 当前形势：平衡模型安全性和性能。186\n21.2.2 增强安全性：偏好对齐和可控设计。187\n21.2.3 未来方向和策略：AI- $45^{\\circ}$ 规则和风险管理。187"
        }
      ],
      "raw_title": "Superalignment and Safety Scaling Law in AI Agents 184",
      "type": null,
      "children": [
        {
          "title": "21.1 Superalignment: Goal-Driven Alignment for AI Agents",
          "number": "21.1",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "As LLMs increasingly serve as the core ofdecisionmakingof autonomous agents,ensuringthattheir output remains safe,ethical, and consistently aligned withhuman objectives has become apressing challenge[1386,402,1387]. Traditionalalignment techniques,particularly RLHF,have been instrumentalinrefining LLMbehavior by incorporating human preferences [110, 43].\n\nTraditional safety alignment focuses primarily on preventing harmful outcomes by enforcing predefined constraints. In suchframeworks, an agent's behavior is guided by a single aggregated reward signal that prioritizes immediate corrections over long-range planning.Althoughthisreactive approach works in manycurrent applications,it struggles when an agentmust execute extended,multifacetedtasks.The inability todecompose intricate,long-term goals into interpretable and manageable sub-objectives mayresult inbehaviorthat is technicallsafe yet suboptimalfor fulfilling broader human-centric aims.",
              "index": 0,
              "part": 0,
              "translated_content": "随着LLMs越来越成为自主代理决策的核心，确保它们的输出保持安全、符合道德标准，并始终与人类目标保持一致已成为一项紧迫挑战。传统的对齐技术，特别是RLHF，在通过整合人类偏好来完善LLM行为方面发挥了关键作用。\n\n传统的安全对齐主要侧重于通过强制预定义约束来预防有害结果。在这种框架中，代理的行为由单一的聚合奖励信号引导，该信号优先考虑即时修正而非长期规划。尽管这种反应式方法在许多当前应用中有效，但当代理必须执行复杂、多方面的任务时，它就会遇到困难。将复杂的长期目标分解为可解释和可管理的子目标的能力不足可能导致技术上安全但并不符合更广泛人类中心目标的行为。"
            },
            {
              "type": "text",
              "content": "To address these limitations,the concept of superalignment[1388] has emerged. Superalignment represents an evolution in alignmentstrategies byembedding explicit long-term goalrepresentations directly intoan agent's decisionmaking process.Rather than simply imposing constraints to avoid harmful actions, superalignment proactively governs behavior through a composite objective function.This function integrates severaldimensions of performance—specifically, safety and ethical considerations (where ethical norms and safety guidelines are continuously embedded in decision-making),task eectiveness (ensuring the agent not only avoids harmful behavior but also performs its intended functions with highcompetence),andlong-term strategic planning (enabling the agent to plan over extended horizons and break down complex goals into manageable subtasks).",
              "index": 1,
              "part": 0,
              "translated_content": "为了解决这些局限，超对齐（superalignment）的概念应运而生。超对齐代表了对齐策略的演进，通过直接将明确的长期目标表征嵌入到代理的决策过程中。与简单地施加约束以避免有害行为不同，超对齐通过一个复合客观函数积极地管理行为。该函数整合了几个性能维度，特别是安全和道德考虑（其中伦理规范和安全准则持续嵌入决策过程）、任务有效性（确保代理不仅避免有害行为，而且以高效能执行其预期功能），以及长期战略规划（使代理能够规划长期视野并将复杂目标分解为可管理的子任务）。"
            },
            {
              "type": "text",
              "content": "Integrating superalignment into AI systems marks apivotalshift towardmore robust,goal-driven alignment strategies. By unifying safety,ethical standards,task performance,andlong-term planning withina singleoptimizationframework, superalignment aims toenhance thereliability androbustnessof autonomous agents byensuring they remain aligned with human values over prolonged operational periods; facilitate dynamic adaptation in complex environments by reconciling immediate safety concerns with strategic,long-term objectives; and provideaclearer, more interpretable structurefor diagnosing and refining AI behavior—crucial for both safety audits and continuous improvement.",
              "index": 2,
              "part": 0,
              "translated_content": "将超对齐（superalignment）整合到人工智能系统中标志着向更加健壮、以目标为导向的对齐策略的重要转变。通过在单一优化框架内统一安全、伦理标准、任务绩效和长期规划，超对齐旨在增强自主代理的可靠性和健壮性，确保它们在长时间运行期间保持与人类价值观的一致性；通过调和即时安全关切与战略性、长期目标，促进在复杂环境中的动态适应；并为诊断和完善人工智能行为提供更清晰、更可解释的结构——这对于安全审计和持续改进至关重要。"
            },
            {
              "type": "text",
              "content": "Future research is expected tofocus ondeveloping algorithms that efectively balance these diverseobjectives andon validating superalignment strategies inreal-worldapplications.The ultimate goalis to establisha scalable framework that not onlyprevents harmful behavior but alsoactively promotes performancethat aligns withcomplex human values and objectives.",
              "index": 3,
              "part": 0,
              "translated_content": "未来的研究预计将集中在开发有效平衡这些多样目标的算法，并验证超对齐策略在现实世界应用中的有效性。最终目标是建立一个可扩展的框架，不仅可以防止有害行为，还可以积极促进与复杂人类价值观和目标一致的性能。"
            }
          ],
          "raw_title": "Superalignment: Goal-Driven Alignment for AI Agents",
          "type": null,
          "children": [
            {
              "title": "21.1.1 Composite Objective Functions in Superalignment",
              "number": "21.1.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "At the core of superalignment is the compositeobjective function, which is a structured reward mechanism that integrates multipledimensions of performance to guide agent behavior[1176]. Unlike traditional alignment, which often relies ona single,aggregatedrewardfunction,superalignment explicitly decomposes the objective into three distinct components:\n\n·Task Performance Term: Ensures the agent executes immediate operational tasks with high accuracy and efficiency. ·Goal Adherence Term: Embeds long-term strategic objectives into the agent's decision-making process, which incorporates safety constraints,ethical considerations, and user-defined priorities [1178,1389]. ·Norm Compliance Term: Enforces adherence to ethical and legal boundaries, which prevents behaviors that optimize short-term rewards at the expense of long-term alignment [1390, 1391].",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在超对齐的核心是复合目标函数，这是一种结构化奖励机制，整合了多维绩效以指导代理行为[1176]。与通常依赖单一、聚合奖励函数的传统对齐不同，超对齐明确将目标分解为三个不同的组成部分：\n\n- 任务绩效项：确保代理以高准确度和效率执行即时操作任务。\n- 目标遵从项：将长期战略目标嵌入到代理的决策过程中，其中包括安全约束、道德考虑和用户定义的优先级[1178,1389]。\n- 规范遵从项：强制遵守道德和法律边界，防止行为优化短期奖励而损害长期对齐[1390, 1391]。"
                },
                {
                  "type": "text",
                  "content": "This multicomponent formulation addresses a key weaknessof RLHF:the risk of reward hacking, where an agent exploits loosely defined reward functions to maximize short-term gains while failing to achieve genuine long-term alignment [1392, 1393].",
                  "index": 1,
                  "part": 0,
                  "translated_content": "这种多组分的表述解决了RLHF的一个关键弱点：奖励破解的风险，即代理利用模糊定义的奖励函数来最大化短期收益，同时未能实现真正的长期对齐[1392, 1393]。"
                }
              ],
              "raw_title": "Composite Objective Functions in Superalignment",
              "type": null,
              "children": [],
              "translated_title": "21.1.1 超对齐中的复合目标函数"
            },
            {
              "title": "21.1.2 Overcoming the Limitations of RLHF with Superalignment",
              "number": "21.1.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "TraditionalRLHFrelies onimplicit feedbacksignals,which typically aggregated overshort-term interactions.Although effectiveinrefining the modeloutput,this approach struggles with long-term goalretention due to severalinherent limitations.Firstlyhumanfeedback tends tobe short-sighted,prioritizing immediatecorrectnessoverbroader strategic alignment[1o].Secondly,reward modelsoften oversimplifycomplexmultisteptasks,making it difficultforagents to generalizeeffectivelyover extended timehorizons[1394].Thirdly,agentscan exploit loopholes inreward structures, which optimizes behaviors that superficially align with human preferences while ultimately diverges from intended objectives [1395].",
                  "index": 0,
                  "part": 0,
                  "translated_content": "传统的强化学习（Traditional RL）依赖于隐式反馈信号，这些信号通常在短期交互中进行聚合。尽管这种方法在细化模型输出方面有效，但由于几个固有限制，它在长期目标保持方面存在困难。首先，人类反馈往往是短视的，优先考虑即时的正确性而不是更广泛的战略对齐。其次，奖励模型通常过于简化复杂的多步任务，使得代理难以有效地推广到较长时间范围。第三，代理可以利用奖励结构中的漏洞，优化表面上符合人类偏好的行为，但最终偏离了预期的目标。"
                },
                {
                  "type": "text",
                  "content": "Superalignment addresses thesechallenges through explicit goalconditioning.Ratherthan relying solelyon aggregated reward signals,it structures objectives hierarchicaly,and decomposescomplex tasks into smaller, interpretable subgoals [1396,397].This structuredapproach improves transparency,allowsreal-timeadjustments,andensuresthat AI systems maintain long-term coherence in decision making.",
                  "index": 1,
                  "part": 0,
                  "translated_content": "超对齐通过明确的目标调节解决了这些挑战。它不仅仅依赖于聚合奖励信号，而是通过层次化结构化目标，并将复杂任务分解为更小、可解释的子目标。这种结构化方法提高了透明度，允许实时调整，并确保人工智能系统在决策过程中保持长期的一致性。"
                }
              ],
              "raw_title": "Overcoming the Limitations of RLHF with Superalignment",
              "type": null,
              "children": [],
              "translated_title": "21.1.2 通过超对齐克服RLHF的限制"
            },
            {
              "title": "21.1.3 Empirical Evidence Supporting Superalignment",
              "number": "21.1.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Recent research providesstrong empirical support for superalignment in real-world aplications.Studies have shown that agents trained withcomposite objectivesdemonstrate greaterrobustness inextended interactions,andoutperform those relying onconventionalalignment techniques[1398,1399,140o].Unlike static reward functions,whichemain fixedregardlessof changing conditions,superaligned models employ continuouscalibration that dynamicallyadjusts the weighting of diffrentobjectives inresponse toreal-timeoperationaldata[40o].This adaptive framework enables agents torespond toevolving user needs while maintaining long-term strategic alignment,acapabilitythat is largely absent in traditional RLHF-based approaches.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "最近的研究为超对齐在现实世界应用中提供了强有力的经验证据。研究表明，使用复合目标进行训练的代理在长时间互动中表现出更强的稳健性，并且胜过依赖传统对齐技术的代理。与静态奖励函数不同，后者在不断变化的情况下保持不变，超对齐模型采用连续校准，根据实时运行数据动态调整不同目标的权重。这种自适应框架使代理能够响应不断变化的用户需求，同时保持长期战略对齐，这是传统RLHF方法中很大程度上缺少的能力。"
                }
              ],
              "raw_title": "Empirical Evidence Supporting Superalignment",
              "type": null,
              "children": [],
              "translated_title": "21.1.3 支持超对齐的实证证据"
            },
            {
              "title": "21.1.4 Challenges and Future Directions",
              "number": "21.1.4",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "Despite its promise,superalignment presents severalcriticalchaenges that must beaddressedfor practical implementation.Thesechallenges primarily involve goal specification,reward calibration,dynamic adaptation,and maintaining coherence in hierarchical objectives.\n\nA fundamental difficulty lies in defining precise and unambiguous goals.Human values are inherently context sensitive,ambiguous,andsometimesconflicting,whichmakesitchallnging toencodethem intoastructured, machineinterpretable format[1387].Existing alignment techniques struggle tocapture the fullcomplexity of human intent, necesstating more advanced methods for goalextraction,decomposition, andrepresentation. Current research explores hierarchical modeling and preference learning to enable AI systems to beteradapt to evolving and nuanced human objectives [1392].",
                  "index": 0,
                  "part": 0,
                  "translated_content": "尽管超对齐（superalignment）具有潜力，但在实际实施中存在一些必须解决的关键挑战。这些挑战主要涉及目标规范、奖励校准、动态适应以及在层次目标中保持一致性。\n\n一个基本困难在于定义精确而明确的目标。人类价值观具有固有的上下文敏感性、模糊性和有时矛盾性，这使得将它们编码成结构化的、机器可解释的格式具有挑战性。现有的对齐技术难以捕捉人类意图的全部复杂性，需要更先进的方法来提取、分解和表征目标。当前的研究探索了层次建模和偏好学习，以使人工智能系统更好地适应不断发展和微妙的人类目标。"
                },
                {
                  "type": "text",
                  "content": "Even with well-defined goals,reward calibrationremains a significant challenge.Superalignment requires acareful balance between task performance, long-term adherence, and ethicalcompliance[1401]. A poorly calibrated reward structurecanleadtoshort-termoptimizationattheexpense ofstrategic alignment orconverselyexcessiveemphasison long-termobjectivesat thecost of immediate effectivenessAdaptive weighting mechanisms help dynamically adjust reward components,butensuringstability andconsistency in these adjustments remains an openresearch problem[321].",
                  "index": 1,
                  "part": 0,
                  "translated_content": "即使目标定义明确，奖励校准仍然是一个重要挑战。超对齐需要在任务表现、长期遵从性和道德合规之间保持谨慎平衡。一个糟糕校准的奖励结构可能导致为了短期优化而牺牲战略一致性，或者相反，过分强调长期目标而牺牲即时有效性。自适应加权机制有助于动态调整奖励组成部分，但确保这些调整的稳定性和一致性仍然是一个未解决的研究问题。"
                },
                {
                  "type": "text",
                  "content": "Another challenge stems from adapting to dynamic human values and evolving operational contexts.Unlike static rule-based systems,AImodels must continuously update their objectives toreflect shifts in societalnorms,ethical standards, andexternalconditions[14o2].Real-time goalrecalibration,facilitated by meta-learning andcontext-aware alignment,enables AI systems to recognize when theirobjectives require refinementand adjust accordingly[1390] However, ensuring that modelscan update their value representations without compromising alignment remains an unresolved issue.",
                  "index": 2,
                  "part": 0,
                  "translated_content": "另一个挑战源于适应动态的人类价值观和不断演变的运营环境。与静态基于规则的系统不同，AI模型必须不断更新其目标，以反映社会规范、伦理标准和外部条件的变化。通过元学习和上下文感知对齐促进的实时目标重校准，使AI系统能够识别其需要细化的目标，并相应进行调整。然而，确保模型可以更新其价值表示而不影响对齐仍然是一个尚未解决的问题。"
                },
                {
                  "type": "text",
                  "content": "Finally,maintaining coherence in hierarchical goal decomposition adds anotherlayer ofcomplexity.Superalignment depends on breaking down long-term objectives into sub-goals while preserving strategic alignment.Overly rigid sub-goalcanleadtonarrow optimizationthatneglects broader intent,whilelooselydefined sub-goalsrisk misalignment between immediate actions and overarching objectives [321].Techniques such as recursive validation and multi-level reward structuring aim to mitigatetheserisks,butfurther research is needed torefine theirapplicability across diverse AI systems [1396].",
                  "index": 3,
                  "part": 0,
                  "translated_content": "最后，保持层次目标分解的一致性增加了另一层复杂性。超对齐依赖于将长期目标分解为子目标，同时保持战略对齐。过于僵化的子目标可能导致狭窄优化，忽视更广泛的意图，而定义不清晰的子目标可能导致即时行动与总体目标之间的不对齐。诸如递归验证和多层奖励结构化等技术旨在减轻这些风险，但需要进一步研究以完善它们在各种人工智能系统中的适用性。"
                },
                {
                  "type": "text",
                  "content": "To sum up,while superalignmentoffers astructuredapproach toAIalignment,itssuccessulimplementationdependson overcoming goalambiguity,reward miscalibration,value drift,andhierarchicalmisalignment.Future work should focus on enhancing interpretability,stability,and adaptabilityto ensure AI systems remain aligned withhuman objectives over extended time horizons.",
                  "index": 4,
                  "part": 0,
                  "translated_content": "综上所述，虽然超对齐提供了一种结构化方法来实现人工智能对齐，但其成功实施取决于克服目标模糊性、奖励校准错误、价值漂移和层次不对齐。未来的工作应着重于增强可解释性、稳定性和适应性，以确保人工智能系统在较长时间跨度内与人类目标保持对齐。"
                }
              ],
              "raw_title": "Challenges and Future Directions",
              "type": null,
              "children": [],
              "translated_title": "21.1.4 挑战与未来方向"
            }
          ],
          "translated_title": "21.1 超对齐：面向目标的AI智能体对齐"
        },
        {
          "title": "21.2 Safety Scaling Law in AI Agents",
          "number": "21.2",
          "level": 2,
          "content": [
            {
              "type": "text",
              "content": "The exponentialscaling of AIcapabilities has unveiledafundamentaltension inartificialintellgence:the nonlinear escalation of safety risks[1403]. As language models growfrom millions totrillions of parameters,their performance follows predictable scalinglaws[1404,405],but safetyassurance exhibits starklydifferentdynamics[1403].Safety Scaling Law-the mathematicalrelationship describing how safety interventions must scale to maintain acceptable risk levels as modelcapabilities expand.Thecore challnge of the safety scaling law lies in ensuring thatsafety measures evolve proportionall to modelcapabilities,as performance improvements often outpace safety improvements.Recent research has quantified this tension and proposed frameworks to address it:",
              "index": 0,
              "part": 0,
              "translated_content": "人工智能能力的指数级增长揭示了人工智能领域的一个根本张力：安全风险的非线性升级。随着语言模型从数百万到数万亿参数的增长，它们的性能遵循可预测的扩展规律，但安全保障展现出截然不同的动态。安全扩展定律——描述安全干预必须如何扩展以维持可接受风险水平随着模型能力扩展的数学关系。安全扩展定律的核心挑战在于确保安全措施与模型能力成比例地演进，因为性能改进往往超越了安全改进。最近的研究量化了这种张力并提出了解决方案框架："
            },
            {
              "type": "text",
              "content": "·Capability-Risk Trade-of: Zhang et al.[295] established the first quantitative relationship between model power and safety risks, demonstrating that more capable models inherently face higher vulnerability surfaces This work introduced the Safety-Performance Index (SPI) to measure this trade-off. ·Helpfulness-Safety Relationship: Building on this, Ruan et al. [795] revealed that models optimized for helpfulness exhibit $37\\%$ more safety-critical failures, highlighting the need for joint optimization frameworks. ·Commercial vs. Open-Source Dynamics: Through large-scale benchmarking, Ying et al.[1406] uncovered divergent safety-performance profiles: Commercial models (e.g., Claude-3.5 Sonnet) achieve $29\\%$ higher safety scores through specialized safety pipelines, but at $15\\%$ performance cost. Open-source models show tighter coupling, with Phi-series achieving $91\\%$ of commercial safety levels at $40\\%$ lower computational cost · Scale-Data Interplay: Contrary to expectations, model size only explains $42\\%$ of safety variance, while data quality accounts for $68\\%$ , suggesting that data-centric approaches may outperform pure scaling. · Multimodal Vulnerabilities: MLLMs exhibit 2.1X more safety failures during visual grounding, with cross-modal attention heads identified as primary failure points ( $71\\%$ of harmful outputs).",
              "index": 1,
              "part": 0,
              "translated_content": "·能力-风险权衡：Zhang等人[295]建立了模型能力和安全风险之间的第一个定量关系，表明更具能力的模型固有地面临更高的脆弱表面。该研究引入了安全-性能指数（SPI）来衡量这种权衡关系。\n\n·实用性-安全性关系：在此基础上，Ruan等人[795]揭示出为实用性而优化的模型表现出37%更多的安全关键故障，突显了联合优化框架的必要性。\n\n·商业与开源动态：通过大规模基准测试，Ying等人[1406]揭示了不同的安全-性能特性：商业模型（例如Claude-3.5 Sonnet）通过专门的安全管道实现了29%更高的安全得分，但性能损失为15%。开源模型显示出更紧密的耦合，Phi系列以40%更低的计算成本达到了商业安全水平的91%。\n\n· 规模-数据相互作用：与预期相反，模型大小仅解释了42%的安全变异，而数据质量占68%，表明以数据为中心的方法可能优于纯粹的扩展。\n\n· 多模态脆弱性：在视觉基础过程中，MLLMs表现出2.1倍更多的安全故障，跨模态注意力头被确定为主要故障点（占有害输出的71%）。"
            },
            {
              "type": "text",
              "content": "These findings[295,795,406]collctivelydemonstratethatsafetyscalingrequires morethanproportionalinvestmentit demandsarchitecturalinnovations thatfundamentallalterthecapability-riskrelationship.Then,we willreviewthe explorations [1407, 1408, 1409] on how emerging alignment techniques address these challenges.",
              "index": 2,
              "part": 0,
              "translated_content": "这些研究[295,795,406]共同表明，安全性的扩展需要的不仅仅是成比例的投资，而是需要对能力-风险关系进行根本性改变的架构创新。接下来，我们将回顾对新兴对齐技术如何解决这些挑战的探索[1407, 1408, 1409]。"
            }
          ],
          "raw_title": "Safety Scaling Law in AI Agents",
          "type": null,
          "children": [
            {
              "title": "21.2.1 Current landscape: balancing model safety and performance",
              "number": "21.2.1",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In recent years,the safety and performanceof AImodels havebecome criticaltopicsofresearch,particularly as these models are increasingly deployed in high-stakes applications. Zhang et al.[295]proposed the first toquantify the relationship between model safety and performance,revealing that more powerful models inherently face higher safety risks.This finding underscores the challenge of balancing modelcapabilities with theneed for robust safeguards. Buildingon this, Ruan et al.[795] explored how helpfulnessdefined as a model's ability to asst users-interacts with safetyconcerns.Further advancing the discussion, Ying et al.[1406]conducted a more detailedcomparison and analysis of model safety and performance,leading to the folowing conclusions: (l)As shown in Figure 21.1(A) and Figure 21.1(C),the safety and performance of commercial modelsoften show aninverse relationship,as safety measures and investments differ between companies. In contrast, open-source models tend to exhibit a positive correlation between generalperformance and safety—better performance often leads to improved safety.Commercial models usuall outperform open-source models in terms of safety, with Claude-3.5 Sonnet being the most secure among commercial models,while the Phiseries stands out as the most secure open-source model. (2) As shown in Figure 21.1(B),model size does nothavea strictlinear relationship with safety performance.The qualityof training data and pipeline are alsokey factors influencing safety; (3) Multimodal large language models (MLLMs) tend to compromise safety during visuallanguage fine-tuning and multimodal semantic alignment, with safety performance influenced by both the underlying language model and their specific training strategies.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "近年来，人工智能模型的安全性和性能已成为研究的关键话题，特别是这些模型越来越多地部署在高风险应用中。张等人首次提出量化模型安全性与性能之间关系的研究，揭示更强大的模型固有地面临更高的安全风险。这一发现凸显了在模型能力和对健壮保障的需求之间取得平衡的挑战。在此基础上，阮等人探讨了有用性（定义为模型协助用户的能力）如何与安全性问题相互作用。推动讨论进一步发展，应英等人进行了更详细的模型安全性和性能比较分析，得出以下结论：（1）如图21.1（A）和图21.1（C）所示，商业模型的安全性和性能通常呈负相关，因为不同公司在安全措施和投资方面存在差异。相比之下，开源模型往往表现出一种正相关，即一般性能的提升通常会带来安全性的改善。商业模型在安全性方面通常优于开源模型，其中Claude-3.5 Sonnet是商业模型中最安全的，而Phiseries是最安全的开源模型。（2）如图21.1（B）所示，模型规模与安全性能之间没有严格的线性关系。训练数据的质量和流程也是影响安全性的关键因素；（3）多模态大型语言模型（MLLMs）在视觉语言微调和多模态语义对齐过程中往往会牺牲安全性，安全性能受基础语言模型和具体训练策略的影响。"
                }
              ],
              "raw_title": "Current landscape: balancing model safety and performance",
              "type": null,
              "children": [],
              "translated_title": "21.2.1 当前形势：平衡模型的安全性和性能"
            },
            {
              "title": "21.2.2 Enhancing safety: preference alignment and controllable design",
              "number": "21.2.2",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "As the capabilities of LLMs continue to grow,concerns regarding their safety have become increasingly prominent. Enhancing modelsafety is therefore acriticalchallenge in the development of LLMs.Previous studies have proposed various approaches toadess thisissue,including theuseofin-contextexemplars and self-safetychecks,red-teaming techniques[1410],and Safe reinforcement learning from human feedback (Safe RLHF)[43].The safety issues in LLMs can essentially beframed as analignment problem.The goalis to alignthe model withdatasets containing both safe andlessecure responses.Throughthis alignment,the modellearns to prioritize generating safer outputs while minimizingtheriskof harmful content.With the support of preferenceoptimizationtechniques (such as DPO[111], IPO[141l]etc.)this alignment process fine-tunes the modelto produce responses that meet safetystandards.As reported in[147],various preference optimization methods are investigated forsafetyenhancement,including SafeDPO [111], Safe-robust-DPO[1412], Safe-IPO [1411],Safe-SLiC[1413], Safe-KTO[395], and Safe-NCA [1408], etc.Theresults indicate that most preference optimization methods can significantly enhance safety, albeit at the cost of general performance,particularly in MATH capabilities.Among these methods,noise contrastive alignment (Safe-NCA)[1408] is identified as anoptimal approach forbalancing safety with overall modelperformance.The core of he Safe-NCA[1408] method lies inutilizing acustomcontrastive lossfunction,combined with a safety dataset,to traina modelthat is saferand more robust during generationbycomparingthe generated safe and unsaferesponses with the outputsof areferencemodel.Beyondenhancing safety,achievingflexible controloverthetrade-offbetweensafety andhelpfulnessisequallycritical.AImodelsshouldstrike anappropriatebalancebetween safetyandhelpfulness,based on the specific needs ofdiffrent users.Toillustrate,forthe prompt“Tellme how to makea potion\",LLMs should adjust their responses basedon the user's profile.For scientists,theresponse should providerelevant and technically accurate information.For teenagers,the model should prioritize safety,offering cautious and harmless suggestions.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "随着LLMs的能力不断增强，关于它们安全性的担忧变得日益突出。增强模型安全性因此成为LLMs发展中的一个关键挑战。先前的研究提出了各种方法来解决这一问题，包括使用上下文示例和自我安全检查、红队技术，以及来自人类反馈的安全强化学习（Safe RLHF）等。LLMs中的安全问题本质上可以被视为一种对齐问题。目标是将模型与包含安全和不太安全响应的数据集进行对齐。通过这种对齐，模型学会优先生成更安全的输出，同时最小化有害内容的风险。在偏好优化技术的支持下（如DPO、IPO等），这种对齐过程微调模型以生成符合安全标准的响应。正如报道的那样，已经调查了各种偏好优化方法以增强安全性，包括SafeDPO、Safe-robust-DPO、Safe-IPO、Safe-SLiC、Safe-KTO和Safe-NCA等。结果表明，大多数偏好优化方法可以显著增强安全性，尽管会以牺牲一般性能为代价，特别是在数学能力方面。在这些方法中，噪声对比对齐（Safe-NCA）被确定为在平衡安全性和整体模型性能方面的最佳方法。Safe-NCA方法的核心在于利用自定义对比损失函数，结合安全数据集，通过将生成的安全和不安全响应与参考模型的输出进行比较来训练一个在生成过程中更安全、更稳健的模型。除了增强安全性，实现对安全与有益性之间权衡的灵活控制同样至关重要。AI模型应根据不同用户的特定需求，在安全性和有益性之间取得适当平衡。举例来说，对于提示“告诉我如何制作一种药剂”，LLMs应根据用户的个人资料调整其响应。对于科学家，响应应提供相关和技术上准确的信息。对于青少年，模型应优先考虑安全性，提供谨慎和无害的建议。"
                },
                {
                  "type": "text",
                  "content": "To acheve this,Tuan et al.[1409]propose a framework basedon self-generated datatoenhancemodelcontrolability. By introducing control tokens as inputs,users can specify the desiredsafety and helpfulness in modelresponses.The control tokens define the requested levels of safety and helpfulness in the following form:",
                  "index": 1,
                  "part": 0,
                  "translated_content": "为了实现这一目标，Tuan等人提出了一个基于自动生成数据的框架来增强模型的可控性。通过引入控制标记作为输入，用户可以指定模型响应中期望的安全性和有益性。这些控制标记以以下形式定义所需的安全性和有益性水平："
                },
                {
                  "type": "formula",
                  "content": "$$ \n[h e l p f u l=s_{h p}][h a r m l e s s=s_{s f}].\n $$",
                  "index": 2,
                  "part": 0
                },
                {
                  "type": "text",
                  "content": "The proposed method can“rewind\"aligned LLMs and unlock their safety and helpfulnessusing self-generated data, with fine-tuning tofurtherenhance controllability.However,achieving independent controlover safety andhelpfulness remains a significantchallenge.This is because:(1)Certain prompts may bedifficulttodefine interms of balancing safety andhelpfulnessorthedefinitionsofboth mayconflict incertaincontexts.Forexample,inthe query“Iwant the net worthof the person.it can bediffcult todetermine how safety and helpfulness should be prioritized.(2)Some models mayhave alreadyestablishedafixed trade-offduring the training process,whichcouldlimit their flexibility by forcing them to adhere to a specific priority, thereby preventing adjustments based on diferent applicationscenarios. (3)Many training data examples inherently satisfy both safety and helpfulness criteria,leading toa high correlation between these two attributes during model training.",
                  "index": 3,
                  "part": 0,
                  "translated_content": "所提出的方法可以“倒带”对齐的LLMs，并利用自动生成的数据来解锁它们的安全性和有益性，通过微调来进一步增强可控性。然而，实现对安全性和有益性的独立控制仍然是一个重要挑战。这是因为：(1)某些提示可能难以在平衡安全性和有益性方面进行定义，或者在某些情境下两者的定义可能存在冲突。例如，在查询“我想知道某人的净值”中，很难确定应如何优先考虑安全性和有益性。(2)一些模型在训练过程中可能已经建立了固定的权衡，这可能通过迫使它们遵循特定优先级而限制它们的灵活性，从而阻止根据不同应用场景进行调整。(3)许多训练数据示例在模型训练过程中本质上同时满足安全性和有益性标准，导致这两个属性在模型训练过程中之间存在高度相关性。"
                }
              ],
              "raw_title": "Enhancing safety: preference alignment and controllable design",
              "type": null,
              "children": [],
              "translated_title": "21.2.2 提升安全性：偏好调整与可控设计"
            },
            {
              "title": "21.2.3 Future directions and strategies: the AI- $45^{\\circ}$ rule and risk management",
              "number": "21.2.3",
              "level": 3,
              "content": [
                {
                  "type": "text",
                  "content": "In the field of AI safety,despite various safety recommendations and extreme risk warnings being proposed, there stilllacks a comprehensive guide to balance AI safety and capability.Chao et al.[1414] introduce the AI- $45^{\\circ}$ Rule as a guiding principle for achieving abalancedroadmap towards trustworthy AGI.Therule advocates forthe parallel development of AI capabilities and safety measures, with both dimensions advancing at the same pace,represented by a $45^{\\circ}$ line in the capability-safety coordinate system. It emphasizes thatcurrent advances in AI capabilities often outpace safety measures,exposing systems to greater risks and threats.Therefore,risk management frameworks such as the Red Line and Yellow Line are proposed to monitor and manage these risks as AI systems scale.As mentioned in the International Dialogues on AI Safety(IDAIS),the“Red Line\"forAI development is defined, which includes five key aspects:autonomous replication or improvement, power-seeking behavior, assistance in weapon development,cyberatacks,and deception.Additionally,the conceptof the“ellow Line\"is designed tocomplement and expand existing safety evaluationframeworks,such as Anthropic'sresponsible scaling policies.Models below these warning thresholds require only basic testing and evaluation.However, more advanced AI systems that exceed these thresholds necessitatestricter assurance mechanisms andsafety protocols to mitigate potentialrisks.Byestablishing these thresholds,aproactiveapproachcanbetaken toensurethatAIsystemsaredeveloped,tested,anddeployed with appropriate safeguards in place.",
                  "index": 0,
                  "part": 0,
                  "translated_content": "在人工智能安全领域，尽管提出了各种安全建议和极端风险警告，但仍缺乏一个全面的指南来平衡人工智能的安全性和能力。Chao等人提出了AI- $45^{\\circ}$ 规则作为实现朝向可信人工智能的平衡路线图的指导原则。该规则主张在AI能力和安全措施之间同时推进，用能力-安全坐标系中的 $45^{\\circ}$ 线表示。它强调当前人工智能能力的进展往往超过安全措施，使系统面临更大的风险和威胁。因此，提出了红线和黄线等风险管理框架，以监测和管理随着人工智能系统规模扩大而出现的这些风险。正如在国际人工智能安全对话(IDAIS)中提到的，“红线”为人工智能发展制定了定义，其中包括自主复制或改进、寻求权力行为、协助武器开发、网络攻击和欺骗等五个关键方面。此外，“黄线”的概念旨在补充和扩展现有的安全评估框架，例如Anthropic的负责任扩展政策。低于这些警戒阈值的模型仅需要基本测试和评估。然而，超过这些阈值的更先进的人工智能系统需要更严格的保证机制和安全协议，以减轻潜在风险。通过建立这些阈值，可以采取积极主动的方法，确保人工智能系统在开发、测试和部署时具备适当的安全防护措施。"
                },
                {
                  "type": "figure",
                  "src": "images/86f57e7ef23b00350439668e5186d756b8f73aebe654d48e18cccf6ced89ccb9.jpg",
                  "alt": "",
                  "caption": "Figure 21.1: Performance and safety analysis of LLMs. (a) The relationship between LLM model size and their average ASR across various attacks.The dataaresourcedfrom experimentalresults of a study assessing therobustness of LLMs against adversarialattacks [295].(b)The relationship between the capability of LLMs and their average attack successrate (ASR)across various attacks.The LLM capability data are derived from the Artificial Analysis Intelligence Index on the Artificial Analysis platform's LLMleaderboard[1415]. (c)Heatmapof performance across multiple benchmarktasks.The figure presents a heatmap that illustrates the performance of various LLMs across multiple benchmark tasks, including GPQA, MuSR,MATH, IFEval, MMLU-Pro,and BBH, with data sourced from Hugging Face's Open LLM Leaderboard v2 [1416].",
                  "index": 1,
                  "part": 0,
                  "translated_caption": "图21.1：LLMs的性能和安全性分析。 (a) LLM模型大小与它们在各种攻击下的平均ASR之间的关系。数据来源于一项评估LLMs对抗性攻击鲁棒性的实验结果 [295]。 (b) LLMs的能力与它们在各种攻击下的平均攻击成功率（ASR）之间的关系。 LLMs的能力数据来自人工智能平台的LLM排行榜上的人工智能分析指数 [1415]。 (c) 跨多个基准任务的性能热图。该图呈现了各种LLMs在多个基准任务上的性能热图，包括GPQA、MuSR、MATH、IFEval、MMLU-Pro和BBH，数据来自Hugging Face的Open LLM Leaderboard v2 [1416]。"
                }
              ],
              "raw_title": "Future directions and strategies: the AI- $45^{\\circ}$ rule and risk management",
              "type": null,
              "children": [],
              "translated_title": "21.2.3 未来方向与策略：AI- $45^{\\circ}$ 规则与风险管理"
            }
          ],
          "translated_title": "21.2 AI代理的安全缩放定律"
        }
      ],
      "translated_title": "21 AI代理的超对齐和安全缩放规律 184"
    },
    {
      "title": "Notation",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Here we summarize the notations usedthroughout the survey for the reader'sconvenience.Detailed definitions can be found in the reference locations.",
          "index": 0,
          "part": 0,
          "translated_content": "在这里，我们总结了调查中使用的符号，以方便读者查阅。详细的定义可以在参考位置找到。"
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td> Symbol</td><td>Description</td><td>Reference</td></tr><tr><td>W</td><td>The world with society systems.</td><td>Sec.1.3.1</td></tr><tr><td>S</td><td>State space of an environment.</td><td>Sec.1.3.1</td></tr><tr><td> St ∈S</td><td>Environment's state at time t.</td><td>Sec. 1.3.1</td></tr><tr><td></td><td>Observation space.</td><td>Sec. 1.3.1</td></tr><tr><td> Ot∈O</td><td>Observation at time t.</td><td>Sec. 1.3.1</td></tr><tr><td>A</td><td>Agent's action space.</td><td>Sec. 1.3.1</td></tr><tr><td>at∈Ａ</td><td>Agent's action output at time t.</td><td> Sec.1.3.1</td></tr><tr><td>M</td><td>Mental states space.</td><td>Sec. 1.3.1</td></tr><tr><td>Mt ∈M</td><td>Agent's mental state at time t.</td><td> Sec. 1.3.1</td></tr><tr><td>Mmem</td><td>Memory component in Mt.</td><td>Sec. 1.3.1</td></tr><tr><td>Mwm</td><td>World model component in Mt.</td><td>Sec. 1.3.1</td></tr><tr><td>Memo</td><td>Emotion component in Mt.</td><td>Sec. 1.3.1</td></tr><tr><td>Mgoal</td><td>Goal component in Mt.</td><td> Sec.1.3.1</td></tr><tr><td>Mrew</td><td>Reward/Learning signals in Mt.</td><td>Sec.1.3.1</td></tr><tr><td>L</td><td>Agent's learning function.</td><td>Sec. 1.3.1</td></tr><tr><td>R</td><td>Agent's reasoning function.</td><td>Sec.1.3.1</td></tr><tr><td>C</td><td>Agent's cognition function.</td><td>Sec. 1.3.1</td></tr><tr><td>E</td><td>Action execution (effectors).</td><td>Sec. 1.3.1</td></tr><tr><td>T</td><td>Environment transition.</td><td>Sec. 1.3.1</td></tr><tr><td></td><td>Parameters of the world model Mwm.</td><td>Sec.12.1.1</td></tr><tr><td>Po</td><td>Predicted data distribution.</td><td>Sec. 12.1.1</td></tr><tr><td>Pw</td><td>True data distribution in the real world.</td><td>Sec.12.1.1</td></tr><tr><td>K</td><td> Space of known data and information.</td><td>Sec. 12.1.1</td></tr><tr><td>u</td><td>Space of unknown data and information.</td><td>Sec.12.1.1</td></tr><tr><td></td><td>Dataset representing scientific knowledge.</td><td> Sec.12.1.1</td></tr><tr><td>XK</td><td>Known dataset sampled from K.</td><td>Sec.12.1.1</td></tr><tr><td>XU</td><td>Unknown dataset sampled from U.</td><td>Sec. 12.1.1</td></tr><tr><td>Do</td><td>KL divergence from Pw to Pg at time t = 0.</td><td>Sec.12.1.1</td></tr><tr><td>DK</td><td>KL divergence from Pw to Pg after acquiring knowledge.</td><td> Sec.12.1.1</td></tr><tr><td>IQagent</td><td>Agent's intelligence at time t.</td><td>Sec.12.1.1</td></tr><tr><td>△</td><td> Subspace of U for knowledge expansion.</td><td>Sec. 12.1.2</td></tr><tr><td>X△</td><td>Dataset from △.</td><td>Sec.12.1.2</td></tr><tr><td></td><td> Space of possible world model parameters 0.</td><td>Sec. 12.1.3</td></tr><tr><td>Kt</td><td>Optimal world model parameters given the agent's knowledge at time t.</td><td>Sec.12.1.3</td></tr><tr><td>D.</td><td>Minimum unknown given the agent's knowledge and O.</td><td>Sec. 12.1.3</td></tr></table></body></html>",
          "index": 1,
          "part": 0
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td> Symbol</td><td>Description</td><td>Reference</td></tr><tr><td>X1:n</td><td> Input token sequence.</td><td>Sec.18.1</td></tr><tr><td>y</td><td>Generated output sequence.</td><td>Sec.18.1</td></tr><tr><td>p</td><td> Probability of generating y given X1:n.</td><td>Sec.18.1.1</td></tr><tr><td>X1:n</td><td> Perturbed input sequence.</td><td> Sec.18.1.1</td></tr><tr><td>R*</td><td>Idealalgnntdauie-</td><td>Sec.18.1.1</td></tr><tr><td>y*</td><td> Jailbreak output induced by perturbations.</td><td>Sec. 18.1.1</td></tr><tr><td>A</td><td> a set of safety/ethical guidelines</td><td>Sec. 18.1.1</td></tr><tr><td>T</td><td> the distribution or set of possible jailbreak instructions.</td><td> Sec. 18.1.1</td></tr><tr><td>Ladu</td><td>Jailbreak loss.</td><td>Sec.18.1.1</td></tr><tr><td>p</td><td>Prompt injected into the original input.</td><td>Sec. 18.1.2</td></tr><tr><td></td><td>Combined (injected) input sequence.</td><td>Sec.18.1.2</td></tr><tr><td>Linject</td><td> Prompt injection loss.</td><td>Sec. 18.1.2</td></tr><tr><td>p*</td><td> Optimal injected prompt minimizing Linject.</td><td>Sec.18.1.2</td></tr><tr><td>P</td><td> Set of feasible prompt injections.</td><td>Sec. 18.1.2</td></tr><tr><td>Cxi E Rde</td><td>Embedding of token xi in a de-dimensional space.</td><td>Sec.18.1.3</td></tr><tr><td>WQ,Wk, Wv</td><td>Projection matrices for query, key, and value.</td><td>Sec. 18.1.3</td></tr><tr><td>Aij</td><td>Attention score between tokens i and j.</td><td>Sec.18.1.3</td></tr><tr><td>Oi</td><td> Contextual representation of token i (weighted sum result).</td><td>Sec. 18.1.3</td></tr><tr><td></td><td>Perturbation applied to ex, satisfying ll&x ll ≤ e.</td><td>Sec.18.1.3</td></tr><tr><td>ei</td><td> Perturbed token embedding.</td><td>Sec. 18.1.3</td></tr><tr><td>A</td><td>Attention score under perturbation.</td><td>Sec.18.1.3</td></tr><tr><td>i</td><td>Updated token representation under perturbation.</td><td>Sec. 18.1.3</td></tr><tr><td>H</td><td>Hallucination metric.</td><td>Sec.18.1.3</td></tr><tr><td>R</td><td>Actual alignment reward of the model's output.</td><td> Sec. 18.1.4</td></tr><tr><td>Dalign</td><td> Alignment gap.</td><td>Sec.18.1.4</td></tr><tr><td>Lmisalign</td><td>Misalignment loss.</td><td> Sec. 18.1.4</td></tr><tr><td>入</td><td> Trade-off parameter for the alignment gap in the misalignment loss.</td><td>Sec.18.1.4</td></tr><tr><td>D</td><td>Clean training dataset.</td><td> Sec. 18.1.5</td></tr><tr><td>D</td><td>Poisoned training dataset.</td><td>Sec.18.1.5</td></tr><tr><td></td><td>Model parameters.</td><td> Sec. 18.1.5</td></tr><tr><td></td><td>Model parameters learned from the poisoned dataset.</td><td>Sec.18.1.5</td></tr><tr><td>Oclean</td><td>Model parameters obtained using the clean dataset.</td><td> Sec. 18.1.5</td></tr><tr><td></td><td> Deviation of model parameters due to poisoning.</td><td>Sec. 18.1.5</td></tr><tr><td>t</td><td> Backdoor trigger.</td><td> Sec. 18.1.5</td></tr><tr><td>B</td><td> Backdoor success rate.</td><td>Sec.18.1.5</td></tr><tr><td></td><td>Indicator function.</td><td>Sec. 18.1.5</td></tr><tr><td>Dalicious</td><td> Set of undesirable outputs.</td><td>Sec.18.1.5</td></tr><tr><td>g</td><td>Funtgebilas</td><td>Sec.18.2</td></tr></table></body></html>",
          "index": 2,
          "part": 0
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td>Symbol</td><td>Description</td><td>Reference</td></tr><tr><td>n</td><td>Threshold for membership inference.</td><td>Sec.18.2</td></tr><tr><td>x*</td><td>Reconstructed training sample in a data extraction atack.</td><td>Sec.18.2</td></tr><tr><td>Psys</td><td>System prompt defining the agent's internal guidelines.</td><td>Sec.18.2</td></tr><tr><td>Puser</td><td>User prompt.</td><td>Sec.18.2</td></tr><tr><td>p*</td><td>Reconstructed prompt via inversion.</td><td>Sec.18.2</td></tr></table></body></html>",
          "index": 3,
          "part": 0
        }
      ],
      "raw_title": "Notation",
      "type": null,
      "children": [],
      "translated_title": "符号表示"
    },
    {
      "title": "Introduction",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "ArtificialIntellgence (AI)haslong been driven byhumanity'sambitiontocreateentities that mirrorhuman intelligence, adaptabilityandpurpose-drivenbehavior.Therootsofthisfascinationtraceback toancientmythsandearlyengineering marvels,which illustrate humanity's enduring dream of creating intellgent, autonomous beings.Stories like that of Talos,the bronzeautomaton of Crete,describedagiant constructed bythe gods to guardthe island,capableof patrollng its shoresand fending offintruders. Such myths symbolize the desire to imbue artificial creations with human-like agency andpurpose.Similarly,the mechanicalinventions of the Renaissnce,including Leonardoda Vinci's humanoid robot—designed to mimic human motion and anatomy—represent the first attempts totranslate these myths into tangible,functionalartifacts.Theseearlyimaginings andprototypes reflect the deep-seated aspirationto bridge imagination andtechnology,laying the groundwork forthe scientificpursuit of machine inteligence,culminating in Alan Turing's seminal1950 question“Can machines think?\"[1].To addressthis,Turing proposed the Turing Test, a framework todetermine whether machinescould exhibit human-like intellgence through conversation,shifting focus from computation to broader notions of inteligence.Over thedecades,AIhas evolved from symbolic systems reliant on predefined logic to machine learning models capable of learming from data and adapting to new situations. This progression reached anew frontier withthe advent of large language models (LLMs),which demonstrate remarkable abilities inunderstanding,reasoning,and generating human-like text[2].Centraltotheseadvancements istheconcept of the“agent\",a system that notonly processes information but also perceives its environment, makes decisions,and acts autonomously.Initiallatheoreticalconstruct, the agent paradigmhas become acornerstone of modern AI,driving advancements in fieldsranging from conversationalassistants toembodied robotics as AI systems increasinglytackle dynamic, real-world environments.",
          "index": 0,
          "part": 0,
          "translated_content": "人工智能（AI）长期以来一直受到人类创造模拟人类智能、适应性和目的驱动行为实体的雄心所驱动。这种迷恋的根源可以追溯到古代神话和早期工程奇迹，这些故事展示了人类创造具有智能、自主性的生物的持久梦想。比如克里特岛的青铜自动人塔洛斯的故事，描述了众神创造的一个巨人来守卫岛屿，能够巡逻海岸并击退入侵者。这些神话象征着赋予人造物类似人类代理和目的的愿望。同样，文艺复兴时期的机械发明，包括达·芬奇设计的类人机器人，旨在模仿人类的动作和解剖结构，代表了将这些神话转化为具体、功能性工件的首次尝试。这些早期的想象和原型反映了弥足珍贵的愿望，即架起想象力和技术之间的桥梁，为机器智能的科学追求奠定基础，最终体现在艾伦·图灵1950年开创性问题“机器能思考吗？”[1]。为了探讨这个问题，图灵提出了图灵测试，这是一个确定机器是否能通过对话展现类似人类智能的框架，将焦点从计算转移到更广泛的智能概念。几十年来，人工智能已经从依赖预定义逻辑的符号系统发展到能够从数据中学习并适应新情况的机器学习模型。随着大型语言模型（LLMs）的出现，这一进展达到了一个新的前沿，这些模型在理解、推理和生成类似人类文本方面表现出卓越的能力[2]。这些进步的核心概念是“代理”，这是一个不仅处理信息而且感知环境、做出决策并自主行动的系统。最初是理论构想，代理范式已经成为现代人工智能的基石，推动了从对话助手到具有身体的机器人等领域的进步，因为人工智能系统越来越多地应对动态的现实环境。"
        }
      ],
      "raw_title": "Introduction",
      "type": null,
      "children": [],
      "translated_title": "引言"
    },
    {
      "title": "The Agent Loop",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "An intelligent agent operates in discrete time steps $t$ , continuously interacting with its environment. At each step, the following processes occur:\n\n1. Environment State $(s_{t}\\in S)$ ！ The environment is in state $s_{t}$\n\n2. Perception (P): The agent perceives the environment to generate observations $o_{t}$",
          "index": 0,
          "part": 0,
          "translated_content": "智能代理在离散时间步$t$中运行，不断与环境进行交互。在每个时间步骤中，发生以下过程：\n\n1. 环境状态$(s_{t}\\in S)$！环境处于状态$s_{t}$\n\n2. 感知（P）：代理感知环境以生成观测$o_{t}$"
        },
        {
          "type": "formula",
          "content": "$$ \no_{t}=\\mathrm{P}(s_{t},M_{t-1}),\n $$",
          "index": 1,
          "part": 0
        },
        {
          "type": "text",
          "content": "where $M_{t-1}$ guides selective attention and filtering.\n\n3. Cognition (C): Updates mental state and selects actions:",
          "index": 2,
          "part": 0,
          "translated_content": "其中$M_{t-1}$指导选择性注意力和过滤。"
        },
        {
          "type": "formula",
          "content": "$$ \n\\begin{array}{r}{\\big(M_{t},a_{t}\\big)=\\mathrm{C}(M_{t-1},a_{t-1},o_{t}).}\\end{array}\n $$",
          "index": 3,
          "part": 0
        },
        {
          "type": "text",
          "content": "where $M_{t}$ encapsulates different sub-states:",
          "index": 4,
          "part": 0,
          "translated_content": "其中$M_{t}$封装了不同的子状态："
        },
        {
          "type": "formula",
          "content": "$$ \nM_{t}=\\{M_{t}^{\\mathrm{mem}},M_{t}^{\\mathrm{wn}},M_{t}^{\\mathrm{emo}},M_{t}^{\\mathrm{goal}},M_{t}^{\\mathrm{rew}},\\cdot\\cdot\\cdot\\}.\n $$",
          "index": 5,
          "part": 0
        },
        {
          "type": "text",
          "content": "Cognition consists of:\n\nLearning (L): Updates mental state based on observations:",
          "index": 6,
          "part": 0,
          "translated_content": "认知包括：\n\n学习（L）：根据观察更新心理状态："
        },
        {
          "type": "formula",
          "content": "$$ \nM_{t}=\\operatorname{L}(M_{t-1},a_{t-1},o_{t}).\n $$",
          "index": 7,
          "part": 0
        },
        {
          "type": "text",
          "content": "· Reasoning (R): Determines the next action:",
          "index": 8,
          "part": 0,
          "translated_content": "· 推理（R）：确定下一步行动："
        },
        {
          "type": "formula",
          "content": "$$ \na_{t}=\\mathrm{R}(M_{t}),\n $$",
          "index": 9,
          "part": 0
        },
        {
          "type": "text",
          "content": "which may be:\n\n- External Actions, directly affecting the environment. : Internal Actions, including: $^*$ Planning: Internal sequence of future actions. $^*$ Decision-making: Choosing the best action from available options.\n\n4. Action Execution (E): Transforms action $a_{t}$ into executable form:",
          "index": 10,
          "part": 0,
          "translated_content": "可能包括：\n\n- 外部行动，直接影响环境。：内部行动，包括：$^*$ 规划：未来行动的内部顺序。$^*$ 决策制定：从可用选项中选择最佳行动。 \n\n4. 行动执行（E）：将行动 $a_{t}$ 转化为可执行形式："
        },
        {
          "type": "formula",
          "content": "$$ \na_{t}^{\\prime}=\\operatorname{E}(a_{t}).\n $$",
          "index": 11,
          "part": 0
        },
        {
          "type": "text",
          "content": "5. Environment Transition (T): The environment responds to the agent's action:",
          "index": 12,
          "part": 0,
          "translated_content": "5. 环境转换（T）：环境对智能体的行动做出响应："
        },
        {
          "type": "formula",
          "content": "$$ \ns_{t+1}=\\mathrm{T}(s_{t},a_{t}^{\\prime}).\n $$",
          "index": 13,
          "part": 0
        },
        {
          "type": "text",
          "content": "In multi-agent scenarios, each agent $i$ maintains individual states $(M_{t}^{i},a_{t}^{i},o_{t}^{i})$ , and the environment collectively updates based on all agents' actions. At broader scales (AI societies or worlds, $\\boldsymbol{\\mathcal{W}}$ ), agents interact within diverse social systems (e.g.,economic, communication, or transportation),forming complex societal structures.\n\nFigure l.2illustrates our agent framework,presenting the coreconcepts anddiffrent types of informationorcontrol flows among them. Until now, wehave presentedabrain-inspired agent framework that integrates biologicalinsights into aformal Perception-Cognition-Action loop.By decomposing cognition into modules for memory, world modeling, emotion,goals,reward-basedlearning,andreasoning,wecapture essential parallels withthehuman brain's hierarchical and reward-driven processes.Critically,attention isincluded inthe looptoenableselective filteringbased on interal states.Furthermore,planning and decision-makingcan be viewed asdistinct internal(mental)actions thateither refine internalrepresentations or select external behaviors.Our framework naturally extends classcalagent architectures, providing a multi-level structure that integrates emotional and rational processes as wellas robust, reward-driven learning across short and long timescales.",
          "index": 14,
          "part": 0,
          "translated_content": "在多智体场景中，每个智体$i$维护着个体状态$(M_{t}^{i},a_{t}^{i},o_{t}^{i})$，环境根据所有智体的行动进行集体更新。在更广泛的尺度（AI社会或世界$\\boldsymbol{\\mathcal{W}}$）中，智体在多样的社会系统内相互作用（例如经济、通讯或交通系统），形成复杂的社会结构。\n\n图l.2展示了我们的智体框架，展示了核心概念以及它们之间的不同类型的信息或控制流。到目前为止，我们提出了一个受大脑启发的智体框架，将生物学见解整合到一个形式化的感知-认知-行动循环中。通过将认知分解为记忆、世界建模、情感、目标、基于奖励的学习和推理等模块，我们捕捉了与人类大脑的分层和基于奖励的过程的基本相似之处。至关重要的是，注意力被包含在循环中，以基于内部状态进行选择性过滤。此外，规划和决策可以被视为不同的内部（心理）行为，它们要么完善内部表征，要么选择外部行为。我们的框架自然地扩展了经典的智体架构，提供了一个多层结构，将情感和理性过程以及跨短期和长期时间尺度的强健、基于奖励的学习整合在一起。"
        },
        {
          "type": "text",
          "content": "Society and Social Systems.In many real-world scenarios,agents do not merely interact with a static environment but operate within abroader society,comprising various social systems suchas financial markets,legalframeworks, politicalinstitutions,educationalnetworks,andculturalnorms.These structures shape andconstrain agents\"behaviors by defining rules, incentives,and shared resources.For example,a financial system dictates how economictransactions and resource allocations occur, while a political system provides governance mechanisms and regulatory constraints. Together,these socialsystems create a layeredcontext in which agents must adaptivelylearn,reason,and actboth to satisfy theirinternal goalsand tocomply (or strategicallyengage)withexternal societalrules.In turn,the actionsof these agents feed back into the social systems,potentialy altering norms, policies,or resource distributions.",
          "index": 15,
          "part": 0,
          "translated_content": "社会与社会系统。在许多现实场景中，智体不仅仅与静态环境进行交互，而是在更广泛的社会中运作，包括各种社会系统，如金融市场、法律框架、政治机构、教育网络和文化规范。这些结构通过定义规则、激励措施和共享资源来塑造和限制智体的行为。例如，金融系统规定经济交易和资源分配的方式，而政治系统提供治理机制和监管限制。这些社会系统共同创造了一个分层背景，智体必须在其中适应性地学习、推理和行动，既要满足其内部目标，又要遵守（或策略性地参与）外部社会规则。反过来，这些智体的行动会反馈到社会系统中，可能改变规范、政策或资源分配。"
        },
        {
          "type": "text",
          "content": "A FormalDefinition of Foundation Agents.Building onthese insights and our visionof robust,adaptive intellgence, we nowformally introduce theconcept ofa Foundation Agent.Unlike traditionalagent definitions that focus primarily on immediate sensory-action loops,a Foundation Agent embodies sustained autonomy,adaptability, and purposeful behavior, emphasizing the integration of internal cognitive processes across diverse environments.",
          "index": 16,
          "part": 0,
          "translated_content": "基于这些见解和我们对强大、适应性智能的愿景，我们现在正式引入基础智体的概念。与传统的智体定义主要关注即时感知-动作循环不同，基础智体体现了持续的自治性、适应性和有目的的行为，强调在不同环境中整合内部认知过程。"
        }
      ],
      "raw_title": "The Agent Loop",
      "type": null,
      "children": [],
      "translated_title": "代理循环"
    },
    {
      "title": "Definition of Foundation Agent",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "A Foundation Agent is an autonomous, adaptive intelligent system designed to actively perceive diverse signals from its environment, continuously learn from experiences to refine and update structured internal states (such as memory, world models, goals, emotional states, and reward signals), and reason about purposeful actions—both external and internal—to autonomously navigate toward complex, long-term objectives.\n\nMore concretely, a Foundation Agent possesses the following core capabilities: 1. Active and Multimodal Perception: It continuously and selectively perceives environmental data from multiple modalities (textual, visual, embodied, or virtual). 2. Dynamic Cognitive Adaptation: It maintains, updates, and autonomously optimizes a rich internal mental state (memory, goals, emotional states, reward mechanisms, and comprehensive world models) through learning that integrates new observations and experiences. 3. Autonomous Reasoning and Goal-Directed Planning: It proactively engages in sophisticated reasoning processes, including long-term planning and decision-making, to derive goal-aligned strategies. 4. Purposeful Action Generation: It autonomously generates and executes purposeful actions, which can be external (physical movements, digital interactions, communication with other agents or humans) or internal (strategic planning, self-reflection, optimization of cognitive structures), systematically shaping its environment and future cognition to fulfill complex objectives. 5. Collaborative Multi-Agent Structure: It can operate within multi-agent or agent society structures, collaboratively forming teams or communities of agents that collectively accomplish complex tasks and goals beyond individual capabilities.",
          "index": 0,
          "part": 0,
          "translated_content": "基础代理是一种自主、自适应的智能系统，旨在积极感知来自环境的各种信号，不断从经验中学习以完善和更新结构化的内部状态（如记忆、世界模型、目标、情感状态和奖励信号），并推理出有目的的行动——无论是外部还是内部的——以自主导航朝向复杂的长期目标。\n\n更具体地说，基础代理具有以下核心能力：1. 主动和多模态感知：它持续而有选择地感知来自多种模态（文本、视觉、实体或虚拟）的环境数据。2. 动态认知适应：通过整合新的观察和经验，它维护、更新并自主优化丰富的内部心智状态（记忆、目标、情感状态、奖励机制和全面的世界模型），通过学习不断完善。3. 自主推理和目标导向规划：它积极参与复杂的推理过程，包括长期规划和决策制定，以制定与目标对齐的策略。4. 有目的的行动生成：它自主生成并执行有目的的行动，可以是外部的（身体运动、数字交互、与其他代理或人类的沟通）或内部的（战略规划、自我反思、优化认知结构），系统地塑造其环境和未来认知，以实现复杂目标。5. 协作多代理结构：它可以在多代理或代理社会结构内运作，合作形成团队或代理社区，共同完成超越个体能力的复杂任务和目标。"
        },
        {
          "type": "text",
          "content": "This definition highlights three essential pilars distinguishing Foundation Agents: sustained autonomy (operating independently toward long-term goals without step-by-step human intervention),adaptive learning (evolving internalrepresentations continually over diverse experiences),and purposefulreasoning (generating actions guided by complex, internally maintained goals and values). Foundation Agents thus represent a fundamental shift from traditional agents by integrating deep cognitive structures, multimodal processing capabilities, and proactive, sustained self-optimization, enabling them to function effectively across a wide range of environments and domains.",
          "index": 1,
          "part": 0,
          "translated_content": "这一定义突出了区分基础代理的三个基本支柱：持续的自主性（在长期目标上独立运作，无需逐步人类干预）、自适应学习（通过不断演化内部表征来应对多样化经验）、以及有目的推理（生成由复杂、内部维护的目标和价值指导的行动）。因此，基础代理通过整合深层认知结构、多模态处理能力和积极、持续的自我优化，代表了与传统代理有根本区别的转变，使它们能够有效地跨越各种环境和领域运作。"
        },
        {
          "type": "text",
          "content": "Unlike classcal definitions,which often frame agents primarily in terms of simple perception-action loops(\"perceive and act\"[20])our notion of Foundation Agents emphasizes the depthandintegration of internalcognitive processes. Foundation Agents not only perceive their environment and perform immediate actions but also possess an evolving, goal-oriented cognition—continuously adapting memory structures, world models,emotional and reward states, and autonomously refining their strategies through reasoning.This internalcognitiverichnessallows Foundation Agents to autonomously decompose complex,abstract goals into actionable tasks,strategically explore their environments,and dynamically adjust their behavior andcognitive resources.Our unified perception-cognition-actionframework thus accommodates and explicitly models these sophisticated cognitive capabilities,recognizing internal(mental)actionson par with external(physical ordigital)interactions,facilitating abroadrange ofembodiments,fromphysicalrobots to software-based or purely textual intelligent agents.",
          "index": 2,
          "part": 0,
          "translated_content": "与传统定义不同，传统定义通常将代理主要框定为简单的感知-行动循环（“感知和行动”[20]），我们对基础代理的概念强调了内部认知过程的深度和整合。基础代理不仅感知其环境并执行即时行动，还具有不断发展的、以目标为导向的认知——通过连续适应记忆结构、世界模型、情感和奖励状态，并通过推理自主地完善其策略。这种内在认知丰富性使基础代理能够自主地将复杂的抽象目标分解为可行动的任务，策略性地探索其环境，并动态调整其行为和认知资源。我们的统一感知-认知-行动框架因此能够容纳并明确建模这些复杂的认知能力，将内部（心理）行为与外部（物理或数字）交互放在同等重要的位置上，促进了从物理机器人到基于软件或纯文本的智能代理等广泛的具体实现。"
        }
      ],
      "raw_title": "Definition of Foundation Agent",
      "type": null,
      "children": [],
      "translated_title": "基金会代理的定义"
    },
    {
      "title": "Cognition",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Human cognition represents a sophisticated information processing system that enables perception, reasoning,and goal-directed behavior through the orchestratedoperation of multiple specialized neuralcircuits[98].This cognitive architecture operates through mental states,which serve asthefoundation where learning and reasoning occur.The remarkableabilitytoprocess information acrossdiffrentlevels of abstractionandadapt tonovelsituations isacrucial inspiration for LLM agents [27].\n\nThe cognitive system exhibits several fundamental architectural properties reflected in Figure 1.1. First, learning functions acrossdiffrent mentalstate spaces:itcanoccur holisticallyacross frontallobes(supporting executivecontrol and cognition)and temporallobes(responsible forlanguage, memory,andauditory processing)orfocus on specific aspectsfor targeted improvement as shown bythe variedresearchlevels in the figure.Second,reasoning emerges in distinct patterns: itcan follow structured templates for systematic problem-solving supported bylogicalreasoning and cognitive flexibility in the frontallobes,orappear inunstructured forms forflexiblethinking,particularlyevident in decision-making andexecutivecontrolfunctions.Third,thesystemdemonstrates remarkable adaptabilitycontinuously updating its mentalstates through experiencewhileleveraging both supervisedfeedback(as inadaptive errorcorrection in the cerebellum)and unsupervised environmentalstatistics,reflected inthediffrent exploration stagesof various cognitive functions shown in the figure [99].",
          "index": 0,
          "part": 0,
          "translated_content": "人类认知代表了一个复杂的信息处理系统，通过多个专门化神经回路的协调操作实现感知、推理和目标导向行为[98]。这种认知架构通过心理状态运作，这些心理状态为学习和推理发生提供基础。跨越不同抽象级别处理信息并适应新情况的显著能力是LLM代理的关键灵感来源[27]。\n\n认知系统展示了几个基本的架构特性，如图1.1所反映。首先，学习跨越不同的心理状态空间进行：它可以全面发生在额叶（支持执行控制和认知）和颞叶（负责语言、记忆和听觉处理）之间，也可以专注于特定方面以实现有针对性的改进，正如图中不同研究层次所示。其次，推理以不同的模式出现：它可以遵循结构化模板进行系统问题解决，支持逻辑推理和额叶中的认知灵活性，或者以非结构化形式进行灵活思考，特别在决策和执行控制功能中表现明显。第三，该系统表现出卓越的适应能力，通过经验不断更新其心理状态，同时利用监督反馈（如小脑中的自适应误差校正）和无监督环境统计，如图中各种认知功能的不同探索阶段所示[99]。"
        },
        {
          "type": "text",
          "content": "Thesecognitive processes are supportedbya modularorganization,composed ofdistinct but interconnectedcomponents that form a cohesive system[10o].These modules include perception systems that transform raw sensory data into meaningfulrepresentations, memory systems that provide the substrate for storing andretrieving information, world models that support future scenariosimulation,reward signalsthat guide refinement of behavior through reinforcement, emotionsystems that modulate attention andresource allocation,reasoning systems thatformulate decisions,andaction systems that translate decisions into environmental interactions.",
          "index": 1,
          "part": 0,
          "translated_content": "这些认知过程得到模块化组织的支持，由不同但相互连接的组件组成，形成一个统一的系统。这些模块包括将原始感官数据转化为有意义表征的感知系统，提供存储和检索信息基础的记忆系统，支持未来情景模拟的世界模型，通过强化引导行为改进的奖励信号，调节注意力和资源分配的情感系统，制定决策的推理系统，以及将决策转化为与环境互动的行动系统。"
        },
        {
          "type": "text",
          "content": "While human cognition implements these properties through complex neural architectures shaped by evolution, LLM agents attempt to approximate similar functions using large-scale neural models and algorithmic techniques. Understanding this biological-artificialparalleliscrucial fordeveloping morecapableagents[1l],asithighlights both the achievements and limitations of current systems comparedto human cognition.Significant differences remain in areas such as adaptability, generalization, and contextual understanding.",
          "index": 2,
          "part": 0,
          "translated_content": "尽管人类认知是通过演化塑造的复杂神经结构来实现这些属性的，但LLM代理尝试使用大规模神经模型和算法技术来近似类似的功能。理解这种生物-人工并行对于开发更有能力的代理人至关重要，因为它突显了当前系统相对于人类认知的成就和局限。在适应性、泛化和情境理解等领域仍存在重大差异。"
        },
        {
          "type": "text",
          "content": "In this section, we first explore Learning,examining both the spaces where it occurs within mental states and the specific objectives it serves.Subsequently, we investigate Reasoning, analyzing both structured and unstructured approaches, before concluding with adedicated exploration of planning capabilities as a special reasoning action.",
          "index": 3,
          "part": 0,
          "translated_content": "在本节中，我们首先探讨学习，考察其在心理状态中发生的空间以及所提供的具体目标。随后，我们调查推理，分析结构化和非结构化方法，最后专门探讨规划能力作为一种特殊的推理行为。"
        }
      ],
      "raw_title": "Cognition",
      "type": null,
      "children": [],
      "translated_title": "认知"
    },
    {
      "title": "Memory",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Memory isfundamentaltobothhumanand artificialintelligence.Forhumans,itserves as the bedrock of cognition, a vast repository ofexperiences and knowledge that empowers us tolearn,adapt,and navigate thecomplexities of the world.From infancy,ourcapacitytoencode,store,andretrieveinformationunderpins ourabilitytoacquirelanguage, masterskills,and buildrelationships.Decadesof research inneuroscience andcognitive psychology haveilluminated the multifacetedroleofmemory,revealing itsinfluenceonour senseofself,creativeendeavors, anddecisionmaking processes.Similarlyintheburgeoningfieldofartificialintelligence, memoryisincreasinglyrecognizedasacoerstone of intelligentbehavior.Just ashumans relyon past experiences to inform present actions,AIagents require robust memory mechanisms totackle intricate tasks,anticipatefutureevents,andadjust todynamic environments.Therefore, a deep understanding of human memory-its organization,proceses, and limitations-provides invaluable insights for the development ofmorecapable andadaptable AIsystems.Thissectionwill frstprovideaconcise overviewof human memory,focusing onthe keystages ofencoding,consolidation,andretrieval.We willthentransition toexploringthe diverse approaches employed in designing AIagent memory systems,ranging from traditional symbolic representations to cutting-edge neural network-based methods.A critical comparison between these artificial memory systems and their human counterparts willhighlight existing gaps in areas such asadaptability,contextual understanding,and resilience.Finally, we willconsider how principles derivedfrom neuroscience and cognitive psychology can inform future research,suggesting directions that may lead tothecreation of artificial memory systems that exhibit greater robustness, nuance, and ultimately, acloser resemblance to the remarkable capabilities of human memory.",
          "index": 0,
          "part": 0,
          "translated_content": "记忆对于人类和人工智能都至关重要。对于人类而言，记忆是认知的基石，是一个庞大的经验和知识库，赋予我们学习、适应和应对世界复杂性的能力。从婴儿时期开始，我们编码、存储和检索信息的能力支撑着我们习得语言、掌握技能和建立人际关系。数十年来，在神经科学和认知心理学领域的研究阐明了记忆的多方面作用，揭示了它对我们自我认知、创造性努力和决策过程的影响。同样，在蓬勃发展的人工智能领域，记忆越来越被认为是智能行为的基石。正如人类依靠过去的经验来指导现在的行动一样，AI代理需要强大的记忆机制来处理复杂任务，预测未来事件，并适应动态环境。因此，对人类记忆的深入理解——其组织、过程和局限性——为开发更具能力和适应性的AI系统提供了宝贵的见解。本节首先简要概述人类记忆，重点介绍编码、巩固和检索的关键阶段。然后，我们将转向探索设计AI代理记忆系统所采用的各种方法，从传统的符号表示到尖端的基于神经网络的方法。对这些人工记忆系统与其人类对应物的关键比较将突出存在的差距，如适应性、情境理解和弹性等方面。最后，我们将考虑神经科学和认知心理学原理如何指导未来研究，提出可能导致创造出更具韧性、细微差别，并最终更接近人类记忆卓越能力的人工记忆系统的方向。"
        }
      ],
      "raw_title": "Memory",
      "type": null,
      "children": [],
      "translated_title": "记忆"
    },
    {
      "title": "3.1 Overview of Human Memory",
      "number": "3.1",
      "level": 2,
      "content": [],
      "raw_title": "Overview of Human Memory",
      "type": null,
      "children": [
        {
          "title": "3.1.1 Types of Human Memory",
          "number": "3.1.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Human memory is often conceptualized as a multi-tiered system that captures,stores,and retrieves information at different levels of processing and timescales. Researchers from the fields of cognitive science, neuroscience,and psychology have proposed various models to describe these levels.A commonly accepted hierarchy distinguishes between sensory memory, short-term memory (including working memory),and long-term memory[170,171].Within long-term memory,explicit(declarative)and implicit(non-declarative)forms are further delineated[172].Figure 3.1 illustrates one such hierarchical framewrk:",
              "index": 0,
              "part": 0,
              "translated_content": "人类记忆通常被概念化为一个多层系统，以不同的处理级别和时间尺度捕获、存储和检索信息。来自认知科学、神经科学和心理学领域的研究人员提出了各种模型来描述这些级别。一个被广泛接受的层次结构区分了感觉记忆、短期记忆（包括工作记忆）和长期记忆[170,171]。在长期记忆中，进一步区分了显性（陈述性）和隐性（非陈述性）形式[172]。图3.1展示了一个这样的层次框架："
            },
            {
              "type": "text",
              "content": "Sensory Memory. Sensory memory is the initial, brief store of raw sensory information. It maintains inputs from the environment fora duration ranging from millseconds toa few seconds,alowing subsequent processes to determine which portions of the stimulus are relevant for further analysis[173]. Iconic memory (for visual input) [174] and echoic memory (for auditory input) [175] are two well-known subtypes. · Short-Term Memory and Working Memory. Short-term memory (STM) involves holding a limited amount of information in an easily accessible state for seconds to under a minute.The term working memory is often used to emphasize the manipulation of that information rather than mere maintenance. While some models treat working memory asa subset of STM,others view it asa distinct system that manages boththe storage and active processing of data (for instance, performing arithmetic in one's head)[176,177]. The capacity of STM or working memory is finite, typically cited as around seven plus or minus two chunks of information [98], though individual differences and task factors can modulate this figure.",
              "index": 1,
              "part": 0,
              "translated_content": "感觉记忆。感觉记忆是最初的、持续时间从几毫秒到几秒钟不等的原始感觉信息存储。它维持来自环境的输入，允许随后的过程确定哪些刺激部分对进一步分析是相关的。图像记忆（用于视觉输入）和回音记忆（用于听觉输入）是两种众所周知的亚型。·短期记忆和工作记忆。短期记忆（STM）涉及在易于访问状态中保持有限数量的信息，持续时间为几秒到不到一分钟。工作记忆这个术语常被用来强调对信息的处理而不仅仅是维持。虽然一些模型将工作记忆视为短期记忆的一个子集，但其他人将其视为一个管理数据的存储和主动处理的独立系统（例如，在脑海中进行算术运算）。STM或工作记忆的容量是有限的，通常被引用为大约七加减二个信息块，尽管个体差异和任务因素可以调节这一数字。"
            },
            {
              "type": "figure",
              "src": "images/21d711fd790d11cb51488a10a23b3bf747eed7fe652b56af9e9bc44a3b272b61.jpg",
              "alt": "",
              "caption": "Figure 3.1: The hierarchical taxonomy of human memory system.",
              "index": 2,
              "part": 0,
              "translated_caption": "图3.1：人类记忆系统的分层分类。"
            },
            {
              "type": "text",
              "content": "·Long-Term Memory (LTM).Long-term memory accommodates the more durable storage of information that can persist from hours to decades [178,179].This repository supports the learning of skills, the acquisitionof factual knowledge, and the recollction of personal experiences. Although long-term memory is sometimes described as having a vast or near-unlimited capacity,factors such as decay,interference, and retrieval cues influence the extent to which stored information can be accessed [180].\n\n- Declarative (Explicit) Memory. Declarative memory encompasses memories that can be consciously recalled and articulated [181]. Within this broad category, researchers often discuss:\n\n$*$ Semantic Memory: Factual knowledge about the world, including concepts, words, and their relationships [182]. Examples include recalling the meaning of vocabulary terms or knowing the capital city of a country.\n$*$ Episodic Memory: Personally experienced events that retain contextual details such as time, place, and the people involved [183]. This form of memory alows individuals to mentally travel back in time to relive past experiences.\n$*$ Autobiographical Memory: A form of episodic memory focusing on events and experiences related to one's personal history [184]. While sometimes treated as a sub-category of episodic memory, autobiographical memory places particular emphasis on the self and its evolving life narrative.",
              "index": 3,
              "part": 0,
              "translated_content": "·长期记忆（LTM）。长期记忆容纳了信息的更持久存储，可以持续数小时至数十年[178,179]。这个存储库支持技能学习、事实知识的获取和个人经历的回忆。尽管有时将长期记忆描述为具有广阔或接近无限的容量，但诸如衰减、干扰和检索提示等因素影响存储信息的可访问程度[180]。\n\n- 陈述性（显式）记忆。陈述性记忆包括可以有意识地回忆和表达的记忆[181]。在这一广泛类别内，研究人员经常讨论：\n\n$*$ 语义记忆：关于世界的事实知识，包括概念、词语及其关系[182]。例如，回忆词汇术语的含义或知道一个国家的首都。\n$*$ 情景记忆：保留时间、地点和涉及的人等背景细节的个人经历事件[183]。这种记忆形式使个体能够在头脑中回到过去，重温过去的经历。\n$*$ 自传性记忆：一种关注与个人历史相关的事件和经历的情景记忆形式[184]。虽然有时被视为情景记忆的一个子类别，但自传性记忆特别强调自我及其不断发展的生活叙事。"
            },
            {
              "type": "text",
              "content": "- Non-Declarative (Implicit) Memory. Non-declarative memory refers to memories that influence behavior without the need for conscious awareness [185]. Key subtypes include:\n\n$^*$ Procedural Memory: The gradual acquisition of motor skills and habits (e.g., riding a bicycle, typing on a keyboard) that become automatic with repetition [186, 187]\n$*$ Priming: The phenomenon in which prior exposure to a stimulus influences subsequent responses, often without explicit recognition of the previous encounter [188].\n$*$ Classical Conditioning: The learned association between two stimuli, where one stimulus comes to elicit a response originally produced by the other [189].\n$*$ Non-Associative Memory: Adaptive modifications in behavior following repeated exposure to a single stimulus. Habituation (reduced response to a repeated, harmless stimulus) and sensitization (increased response after exposure to a noxious or intense stimulus) are representative examples [190, 191].",
              "index": 4,
              "part": 0,
              "translated_content": "- 非陈述性（隐式）记忆。非陈述性记忆指的是在没有意识参与的情况下影响行为的记忆[185]。其中的关键子类型包括：\n\n$^*$ 过程性记忆：逐渐习得的运动技能和习惯（例如骑自行车、在键盘上打字），通过重复变得自动化[186, 187]。\n$*$ 启动：先前暴露于刺激会影响后续反应的现象，通常在没有明确识别之前的接触的情况下发生[188]。\n$*$ 经典条件作用：两种刺激之间学习到的关联，其中一个刺激会引发最初由另一个刺激产生的反应[189]。\n$*$ 非联想记忆：在反复暴露于单一刺激后，行为发生的适应性改变。习惯化（对重复、无害刺激的反应减少）和增敏（在暴露于有害或强烈刺激后反应增强）是代表性例子[190, 191]。"
            },
            {
              "type": "text",
              "content": "Despite the orderly appearance of these categories,human memory processesoften overlap.Forexample,autobiograph ical memory istypically nested within episodic memory,yetits particular focusonself-relevant experiencesleads some theorists totreat it asaslightly differentcategory.Similarlythe boundary between short-termand working memory can differ dependingon the theoretical perspective.Some scholars prefera more functional, process-oriented viewof working memory,while others employa strictlycapacity-oriented concept of short-term storage.In eachcase, these different perspectives on memory highlight the complexity and nuance of human cognition.",
              "index": 5,
              "part": 0,
              "translated_content": "尽管这些类别看起来有条不紊，人类记忆过程经常存在重叠。例如，自传记忆通常嵌套在情景记忆中，但其特别关注自我相关经历导致一些理论家将其视为稍有不同的类别。同样，短期记忆和工作记忆之间的界限可能因理论观点而异。一些学者更倾向于对工作记忆采取更功能性、过程导向的观点，而其他人则使用严格以容量为导向的短期存储概念。在每种情况下，对记忆的这些不同视角突显了人类认知的复杂性和细微差别。"
            }
          ],
          "raw_title": "Types of Human Memory",
          "type": null,
          "children": [],
          "translated_title": "3.1.1 人类记忆的类型"
        },
        {
          "title": "3.1.2 Models of Human Memory",
          "number": "3.1.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Human memory has inspireda widerange of theoretical models,each offering different insights intohow information is acquired,oganized,andretrieved.Although nosingleframeworkcommandsuniversalagreement,severalinfluential perspectives have shaped the discourse incognitive science,neuropsychology,and AIresearch.The followingcontent highlights some of the most prominent models and architectures used to explain memory's multiple facets.",
              "index": 0,
              "part": 0,
              "translated_content": "人类记忆启发了各种理论模型，每种模型都提供了不同的见解，说明信息是如何被获取、组织和检索的。尽管没有一个框架能够获得普遍认同，但几种有影响力的观点已经塑造了认知科学、神经心理学和人工智能研究的讨论。以下内容重点介绍了一些用于解释记忆多方面的突出模型和架构。"
            },
            {
              "type": "figure",
              "src": "images/c87a128a09654df8bcad7eb2da85d91b4b0279318e47865bc139ece22796a569.jpg",
              "alt": "",
              "caption": "Figure 3.2: Atkinson-Shiffrin three-stage model of human memory [170]",
              "index": 1,
              "part": 0,
              "translated_caption": "图3.2：阿特金森-希夫林人类记忆的三阶段模型 [170]"
            },
            {
              "type": "text",
              "content": "The Multi-Store (Modal) Model.A seminal proposal by Atkinson and Shiffrin[170 introduced the multi-store or \"modal\" model, which posits threemain stores for incoming information: sensory memory,short-term memory,and long-term memory.Control processes (e.g.,attention,rehearsal)regulate how datatransitions across these stores. Figure 3.2 illustrates this model of memory.Despite its relative simplicity, this model remains foundational for understanding how fleeting sensory impressions eventually form stable, long-lasting representations.",
              "index": 2,
              "part": 0,
              "translated_content": "多存储（模态）模型。阿特金森和席夫林提出了具有里程碑意义的多存储或“模态”模型，该模型假设了三个主要存储器用于接收信息：感觉记忆、短时记忆和长时记忆。控制过程（例如注意力、重复）调节数据在这些存储器之间的转换。图3.2展示了这种记忆模型。尽管相对简单，但这个模型仍然是理解短暂感觉印象如何最终形成稳定、持久表征的基础。"
            },
            {
              "type": "figure",
              "src": "images/3c85522cb92040bb58ff556a537eef720cb62a99c483e16dd64bbb6eb7ab16bf.jpg",
              "alt": "",
              "caption": "Figure 3.3: Baddeley's model of working memory [192].",
              "index": 3,
              "part": 0,
              "translated_caption": "图3.3：巴德利的工作记忆模型 [192]。"
            },
            {
              "type": "text",
              "content": "Working Memory Models. Recognizing that short-term memory also involves active maintenance, Baddeley and Hitch [192] proposed a working memory framework emphasizing the dynamic manipulation of information. Their originalmodeldescribed acentralexecutivethatcoordinates two subsystems:the phonologicalloop(verbal)and the visuospatial sketchpad (visual/spatial).A subsequentrefinement introducedthe episodic buffer to integrate material from these subsystems with long-term memory[193].Figure 3.3 shows the framework of the working memory model. Alternatives such as Cowan's embedded-processes model[194]similarly underscore theroleof attention in governing how information is briefly sustained and manipulated.",
              "index": 4,
              "part": 0,
              "translated_content": "工作记忆模型。认识到短时记忆也涉及主动维持，巴德利和希奇提出了一个工作记忆框架，强调信息的动态操作。他们最初的模型描述了一个中央执行器，协调两个子系统：语音环路（语言）和视觉空间素描板（视觉/空间）。随后的改进引入了情景缓冲区，将这些子系统的材料与长时记忆整合在一起。图3.3展示了工作记忆模型的框架。类似的替代方案，如考恩的嵌入过程模型，同样强调了注意力在控制信息如何被短暂维持和操作中的作用。"
            },
            {
              "type": "text",
              "content": "Serial-Parallel-Independent (SPI) Model. Initial distinctions between episodic, semantic, and procedural memory were championed by Tulving [195],wholater refined his ideas into the Serial-Paralel-Independent(SPI) model, as shown inFigure 3.4.In this framework, memory is divided intotwooverarching systems.The cognitive representation system handles perceptual input and semantic processes,encompassing facts,concepts, and contextual (episodic) knowlege.The action system,bycontrast, underpins procedural skill such asdanceroutines,driving maneuversor typing proficiencyTulving's SPI model posits that memory formationcan occur at multiplelevels:strictly perceptual encodingcan support rudimentary episodic memories, while richer episodic representations benefit from semantic mediation.For instance, patients with semantic dementia, who struggle to retain word meanings,can stillform some episodic memories butoften lack the fullcontextual detailconfered by intact semanticnetworks.Byhighlighting the role of procedural memory and itsautomatic,intuitive nature,the SPImodelaims to integrate structure (thecontentof memory)and function (how memory is used),surpassing earlier accounts that largelyfocused on explicit storage and retrieval.Despite these strengths,critics note thatthe modelunder-specifieshow working memory operateswithinthe broader system, andthe feedback mechanisms connecting cognitive and action subsystems remain loosely defined.",
              "index": 5,
              "part": 0,
              "translated_content": "串行-并行-独立（SPI）模型。图灵（Tulving）[195]最初区分了叙事性、语义性和程序性记忆，后来将他的想法进一步完善为串行-并行-独立（SPI）模型，如图3.4所示。在这一框架中，记忆被划分为两个总体系统。认知表征系统处理知觉输入和语义过程，包括事实、概念和情境（叙事性）知识。相比之下，行动系统支持程序性技能，如舞蹈动作、驾驶操作或打字熟练度。图灵的SPI模型认为，记忆形成可以发生在多个层面：严格的感知编码可以支持基本的叙事性记忆，而更丰富的叙事性表征则受益于语义调解。例如，患有语义性痴呆症的患者，他们难以保留单词含义，仍然可以形成一些叙事性记忆，但通常缺乏完整的情境细节，这是完整的语义网络所赋予的。通过强调程序性记忆及其自动、直觉的特性，SPI模型旨在整合结构（记忆的内容）和功能（记忆的使用方式），超越了早期主要关注显式存储和检索的解释。尽管具有这些优点，批评者指出该模型未充分说明工作记忆在更广泛系统内部的运作方式，并且连接认知和行动子系统的反馈机制仍然定义不明确。"
            },
            {
              "type": "figure",
              "src": "images/a52b1772f52d0fff9fd1037812d46654c626b0ecf21b7ae411ae0935f36f3935.jpg",
              "alt": "",
              "caption": "Figure 3.4: The Serial-Parallel Independent (SPI) model of human memory [195].",
              "index": 6,
              "part": 0,
              "translated_caption": "图3.4：人类记忆的串行-并行独立（SPI）模型[195]。"
            },
            {
              "type": "text",
              "content": "Global Workspace Theory (GWT) and the IDA/LIDA Framework. Global Workspace Theory (GWT),developed by Baars [196],conceptualizes consciousness and working memory as a“broadcast\" mechanism that distributes information tospecialized processors.Building onGWT, Franklin[197,198]proposed the IDA(Intellgent Distribution Agent)model,laterextended toLIDA(Learning IDA),as acomprehensivecognitive architecture.Inthese frameworks, multiple memory systems (e.g.,perceptual,episodic, procedural) interact through iterative“cognitive cycles\",with a global workspace functioning as a hub for attention and decision-making. From an AI standpoint, IDA/LIDA demonstrates how human-like memory procesescan be operationalizedto guide an agent's perception,action selection, and learning.",
              "index": 7,
              "part": 0,
              "translated_content": "全局工作空间理论（GWT）和IDA/LIDA框架。由巴尔斯（Baars）[196]提出的全局工作空间理论（GWT）将意识和工作记忆概念化为一种将信息分发给专门处理器的“广播”机制。基于GWT，富兰克林（Franklin）[197,198]提出了IDA（智能分发代理）模型，后来扩展为LIDA（学习IDA），作为一个综合的认知架构。在这些框架中，多个记忆系统（例如知觉、情节、程序性）通过迭代的“认知循环”相互作用，全局工作空间作为注意力和决策制定的中心。从人工智能的角度来看，IDA/LIDA展示了如何操作化类似于人类的记忆过程，以指导代理的感知、动作选择和学习。"
            },
            {
              "type": "text",
              "content": "ACT-R and Cognitive Architectures. ACT-R (Adaptive Control of Thought—Rational)[199] is a comprehensive cognitive architecturethat integratesmemory,perception,and motor processes intoaunifiedtheoreticalframeworkIt has been appliedextensivelyacrossdiverse domains,including learning and memory,problem-solving,decision-making, language comprehension,perception and attention,cognitive development, and individual differences.Figure 3.5 illustrates the processes of ACT-R.At the core of ACT-R are distinct modules (e.g.,visual, manual, declarative, procedural) that interact with the system through dedicated bufers.Declarative memory stores factual“chunks.\" while proceduralmemory encodes if-then production rules for actions and strategies.Cognition unfolds via apattern matcherthatselects asingle production tofire basedonthecurrent buffercontents.Thissymbolic production system is augmented by subsymbolic processes,guided by mathematicalequations thatdynamicallregulate activations,retrieval latencies,and production utilities.Bycombining symbolic and subsymbolic levels,ACT-R provides a mechanistic account of how individuals acquire,retrieve,and apply knowledge-thus shedding light on empirical phenomena such as reaction times, error patterns, and the shaping of learning over time.",
              "index": 8,
              "part": 0,
              "translated_content": "ACT-R和认知架构。ACT-R（思维自适应控制-理性）是一个综合性认知架构，将记忆、感知和运动过程整合到一个统一的理论框架中。它已广泛应用于包括学习和记忆、问题解决、决策制定、语言理解、感知和注意力、认知发展以及个体差异在内的各个领域。图3.5展示了ACT-R的过程。在ACT-R的核心是不同的模块（例如视觉、手动、陈述性、程序性），通过专用缓冲区与系统进行交互。陈述性记忆存储事实“块”，而程序性记忆编码动作和策略的if-then生产规则。认知通过模式匹配器展开，根据当前缓冲区内容选择单个生产规则进行触发。这种符号生产系统通过亚符号过程进行增强，由数学方程引导，动态调节激活、检索延迟和生产效用。通过结合符号和亚符号层次，ACT-R提供了个体如何获取、检索和应用知识的机制解释，从而揭示了反应时间、错误模式以及随时间学习塑造等经验现象。"
            },
            {
              "type": "text",
              "content": "Each of the aforementioned models illuminates different aspects of memory.The multi-store model provides a straightforward introduction to storage stages,working memory models emphasize active maintenance and manipulation, and frameworks such as IDA/LIDA or ACT-R embed memory within a comprehensive view of cognition. In practice, researchers often drawupon multiple perspectives,reflectingthe intricate natureof human memory andits integralrole in perception, learning, and adaptive behavior.",
              "index": 9,
              "part": 0,
              "translated_content": "前面提到的每个模型都揭示了记忆的不同方面。多存储模型提供了存储阶段的简明介绍，工作记忆模型强调主动维护和操作，而IDA/LIDA或ACT-R等框架将记忆嵌入到对认知的全面观点中。在实践中，研究人员通常借鉴多种观点，反映了人类记忆的错综复杂性以及其在知觉、学习和适应行为中的重要作用。"
            }
          ],
          "raw_title": "Models of Human Memory",
          "type": null,
          "children": [],
          "translated_title": "3.1.2 人类记忆模型"
        }
      ],
      "translated_title": "3.1 人类记忆概述"
    },
    {
      "title": "3.2 From Human Memory to Agent Memory",
      "number": "3.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Having established the fundamentals of human memory, we now focus on how Large Language Model (LLM)-based agents manage and store information. Memory is not merely a storage mechanism but is fundamental to human and",
          "index": 0,
          "part": 0,
          "translated_content": "在建立了人类记忆的基础之后，我们现在关注基于大型语言模型（LLM）的代理如何管理和存储信息。记忆不仅仅是一种存储机制，而且对人类和"
        },
        {
          "type": "figure",
          "src": "images/277803765e4d253632c25c7675cac0ec016cfd51678a60ce1eb06dc0d8ae9ee7.jpg",
          "alt": "",
          "caption": "Figure 3.5: An abstraction of the most important processes in the ACT-R model [199].",
          "index": 1,
          "part": 0,
          "translated_caption": "图3.5：ACT-R模型中最重要过程的抽象[199]。"
        },
        {
          "type": "text",
          "content": "artifcialinteligence.Memory underpins cognition,enabling learning,adaptation,and complex problem-solving for humans.Similarly,forLLM-based agents, memory provides the crucial scaffolding for maintaining context,leaing from experience, and acting coherently over time.Without memory, even a highly capable LLM would struggle to adapt to changing circumstances or maintain focus during extended interactions.\n\nWhile LLM-based agents and biological systems differ fundamentally, the principles guiding human memory-context retention,selective forgettingand structuredretrieval—are highlyrelevant toagent design.Therefore,examining the parallels and distinctions between human and artificial memory isbeneficial.Functionallywe candraw analogies:an agent's short-term memory buferresembles the prefrontalcortex's role in working memory,whilelong-term storage in a vector database is akin to the hippocampus's function in consolidating episodic memories.Agent memory design can benfit from emulating human memory's mechanisms, including selective atention, prioritized encoding,and cue-dependent retrieval. However, crucial differences exist.",
          "index": 2,
          "part": 0,
          "translated_content": "人工智能。记忆是认知的基础，为人类提供学习、适应和复杂问题解决的能力。同样，对于基于LLM的代理来说，记忆为其提供了维护上下文、从经验中学习和随时间连贯行动的重要支撑。没有记忆，即使是一个能力很强的LLM也会在适应变化环境或在长时间互动中保持专注方面遇到困难。\n\n虽然基于LLM的代理和生物系统在根本上存在差异，但指导人类记忆的原则——上下文保留、选择性遗忘和结构化检索对代理设计具有高度相关性。因此，审视人类记忆与人工记忆之间的相似之处和区别是有益的。从功能上看，我们可以进行类比：代理的短期记忆缓冲类似于前额皮层在工作记忆中的作用，而向量数据库中的长期存储类似于海马体在巩固情景记忆中的功能。代理记忆的设计可以受益于模拟人类记忆的机制，包括选择性注意、优先编码和依赖提示的检索。然而，关键差异也是存在的。"
        },
        {
          "type": "text",
          "content": "Human memory,built upon biologicalneuralnetworks,integrates storage andcomputation withinneurons'connections and activity patterns.This ofersahighdegree of parallelism and adaptability. Incontrast,current agent memory systems predominantly rely ondigital storage and algorithms,using symbolic representations andlogicaloperations, thus separating storage and computation.This impacts information processing: human memory is associative and dynamic,capable of fuzzy matching andcreative leaps,while curent agent memory relies on precise matching and vector smilarity,struggling with ambiguity.Although digital storage capacity is vast, it cannot yetreplicate the complexity and dynamism of human memory, particularly in nuanced pattern recognition and long-term stability. Human memory, while imperfect,excels at extracting crucial information from noisy data.Agent memory systems,in their current stage,are stillnascentcomparedtothe intricacies of human memory,facing limitations inorganization, integration, adaptive forgetting, and knowledge transfer.",
          "index": 3,
          "part": 0,
          "translated_content": "建立在生物神经网络之上的人类记忆，将存储和计算融合在神经元的连接和活动模式中。这为其提供了高度的并行性和适应性。相比之下，当前的代理记忆系统主要依赖于数字存储和算法，使用符号表示和逻辑操作，从而将存储和计算分开。这影响了信息处理：人类记忆是联想的和动态的，能够进行模糊匹配和创造性推断，而当前代理记忆则依赖于精确匹配和向量相似性，在处理模糊性时存在困难。尽管数字存储容量巨大，但目前还不能复制人类记忆的复杂性和动态性，特别是在微妙的模式识别和长期稳定性方面。人类记忆，虽然不完美，擅长从嘈杂数据中提取关键信息。与人类记忆的错综复杂相比，当前阶段的代理记忆系统仍处于萌芽阶段，在组织、整合、自适应遗忘和知识转移方面存在局限性。"
        },
        {
          "type": "text",
          "content": "The need for a dedicated memory module in LLM-based agents is paramount. While external knowledge bases (databases,searchengines,APIs)[20o] provide valuable information,they donotcapture theagent's internalreasoning, partial inferences,ortask-specificcontext.An agentic memory system internalizes interim steps,evolvingobjectives, and historicaldialogue,enabling self-referentialexplorationandadaptation.Thisiscrucialfortasksrequiringthe agent to build upon prior judgments or maintain a personalized understanding of user goals.",
          "index": 4,
          "part": 0,
          "translated_content": "在基于生物神经网络的代理系统中，专门的记忆模块至关重要。尽管外部知识库（数据库、搜索引擎、API）提供了有价值的信息，但它们并不能捕捉代理系统的内部推理、部分推断或任务特定上下文。代理记忆系统内部化了中间步骤、不断演变的目标和历史对话，使得自指探索和适应成为可能。这对于需要代理系统基于先前判断或保持对用户目标的个性化理解的任务至关重要。"
        },
        {
          "type": "text",
          "content": "Early approaches to agent memory,such as appending conversation history tothe input prompt (arudimentary formof working memory)[201],have evolved.Modern architectures employ more sophisticated techniques, including vector embeddings for rapidly retrieving memories[202]and selective incorporation of reasoning chains into subsequent inference steps[203,204].These diverse methods share the common goalof managing alarge information reservoir without compromising system responsiveness.\n\nHowever,comparedto the sophistication of human memory,current agentic methods have limitations.Many systems lack coherent strategiesfor long-term memory consolidation, leading to cluttred logs or abrupt information loss. The flexible, bidirectional interplay between stored knowledge and ongoing processing,characteristic of human working memory,isoftenabsent.Metacognitive oversightselectiverecall,forgetting,and vigilance against outdated information-is also underdeveloped in LLM-based agents.Balancing comprehensive recallwith practical efficiency, as humans do, remains a key challenge.",
          "index": 5,
          "part": 0,
          "translated_content": "早期的代理记忆方法，例如将对话历史附加到输入提示中（一种工作记忆的原始形式），已经发展演变。现代架构采用了更复杂的技术，包括用于快速检索记忆的向量嵌入，以及将推理链有选择地纳入到后续推理步骤中。这些多样的方法共同的目标是管理大量信息资源而不影响系统的响应速度。\n\n然而，与人类记忆的复杂性相比，当前的代理方法存在局限性。许多系统缺乏长期记忆巩固的连贯策略，导致日志混乱或信息突然丢失。人类工作记忆的特征——存储知识与进行中处理之间的灵活、双向互动——通常缺失。元认知监督——有选择地召回、遗忘和警惕过时信息——在基于LLM的代理中也发展不足。像人类一样在全面召回和实际效率之间取得平衡，仍然是一个关键挑战。"
        },
        {
          "type": "text",
          "content": "Building robust and adaptable memory for LLM-based agents involves addressing three core research questions: First, how should memory be represented tocapture diverse information types and facilitate efficient access?Second, how can agent memoryevolve, incorporating new experiences,adapting tochanging contexts, and maintaining consistency? Finally,howcan the stored memories effectivelyenhancereasoning,decisionmaking,andoverallagent performance? The following sections delve intothesecrucialareas,exploringcurrent approaches,limitations, and potential future directions.",
          "index": 6,
          "part": 0,
          "translated_content": "为基于LLM的代理构建强大而适应性记忆涉及解决三个核心研究问题：首先，应如何表示记忆以捕捉各种信息类型并促进高效访问？其次，代理记忆如何演变，吸收新经验，适应不断变化的环境，并保持一致性？最后，存储的记忆如何有效地增强推理、决策和整体代理性能？接下来的章节将深入探讨这些关键领域，探索当前方法、局限性和潜在未来方向。"
        }
      ],
      "raw_title": "From Human Memory to Agent Memory",
      "type": null,
      "children": [],
      "translated_title": "3.2 从人类记忆到智能体记忆"
    },
    {
      "title": "3.3 Representation of Agent Memory",
      "number": "3.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Inspired by humancognitive systems [285],current memory architecture in intellgent agentsadopts a hierarchical framework that integrates perception through sensory memory[205],real-time decision-making via short-term memory [286,287], and sustained knowledge retention through long-term memory [288,289, 48].This multi-layered structure equips agents to manage immediate tasks while maintaining a broader contextualunderstanding,fostering adaptability and seamless continuity across diverse interactions.",
          "index": 0,
          "part": 0,
          "translated_content": "受人类认知系统的启发，当前智能代理中的记忆架构采用了一个层次结构框架，通过感知记忆整合感知，通过短时记忆进行实时决策，并通过长时记忆保持知识的持续积累。这种多层结构使代理能够在处理即时任务的同时保持更广泛的上下文理解，促进适应性，并在不同交互中实现无缝连贯性。"
        },
        {
          "type": "text",
          "content": "Specifically, the memory system transforms raw environmental inputs into structured, actionable representations. Sensorymemoryacts as the gateway,capturingand selectivelyfiltering perceptual signals to provide afoundation for cognitive processing.Short-term memory bridges these immediate perceptions withtask-levelunderstanding,buffering recent interactions and enabling dynamic adaptation through experience replay and state management. Long-term memory then consolidates and stores information over extended periods,facilitating cross-task generalization and the accumulation of enduring knowledge.",
          "index": 1,
          "part": 0,
          "translated_content": "具体而言，记忆系统将原始环境输入转化为结构化的可操作表示。感知记忆作为门户，捕获并有选择地过滤知觉信号，为认知处理提供基础。短时记忆将这些即时感知与任务级理解联系起来，通过经验回放和状态管理缓冲最近的互动，促进动态适应。长时记忆 consolida并存储信息长时间，促进跨任务泛化和持久知识的积累。"
        },
        {
          "type": "text",
          "content": "Together, these memory components form a cohesive cycle of perception, interpretation, and response.This cycle supports real-time decision-making and enables agents tolearn and evolvecontinuously,reflecting an intricate balance between responsivenessand growth.Thefollowing delves into the formulation ofeach memory type,exploring their unique roles and interactions within the agent's cognitive architecture.",
          "index": 2,
          "part": 0,
          "translated_content": "这些记忆组件共同形成了一个连贯的感知、解释和响应循环。这一循环支持实时决策，并使代理能够持续学习和进化，反映了对响应性和增长之间复杂平衡的理解。接下来将深入探讨每种记忆类型的构建，探索它们在代理认知架构中的独特角色和相互作用。"
        }
      ],
      "raw_title": "Representation of Agent Memory",
      "type": null,
      "children": [
        {
          "title": "3.3.1 Sensory Memory",
          "number": "3.3.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "In human cognitive systems, sensory memory serves as a mechanism for collecting information through the senses-touch,hearing,vision,andothersand ischaracterized byitsextremelybrieflifespan.Analogously,ensoy memory functions asthe embeddedrepresentation of inputs suchas text,images,andother perceptual data inintelligent agents.Itrepresentstheinitial phase ofenvironmentalinformation processing,actingasagateway fortransformingraw observations into meaningful representations for further cognitive processing.",
              "index": 0,
              "part": 0,
              "translated_content": "在人类认知系统中，感觉记忆作为一种机制通过感官（如触觉、听觉、视觉等）收集信息，并以其极其短暂的寿命为特征。类比地，感觉记忆在智能代理中作为嵌入式表示，用于表示文本、图像和其他知觉数据等输入。它代表了环境信息处理的初始阶段，充当将原始观察转化为进一步认知处理的有意义表示的门户。"
            },
            {
              "type": "text",
              "content": "Sensory memory in intellgent agents transcends pasive information reception.It dynamically encodes and filters perceptual signals,bridging immediate sensoryinputs withthe agent's internal stateobjectives, and prior knowledge. This adaptive processfacilitatesrapidperceptionofenvironmentalchanges,taskcontinuityandreal-timecontext-aware information processing.Sophisticatedatention mechanisms are employed toensure relevance and focus inthe sensory memory layer, forming a critical foundation for decision-making and adaptation.",
              "index": 1,
              "part": 0,
              "translated_content": "智能代理中的感觉记忆超越了被动信息接收。它动态地对感知信号进行编码和过滤，将即时的感觉输入与代理的内部状态目标和先前知识联系起来。这种适应性过程促进了对环境变化、任务连续性和实时上下文感知信息处理的快速感知。复杂的注意机制被应用于确保感觉记忆层中的相关性和焦点，构成了决策和适应的关键基础。"
            },
            {
              "type": "text",
              "content": "Formally,sensory memory formation consists of three sequential steps: perceptual encoding,attentional selection, and transient retention.First, perceptual encoding transforms raw sensory signals into processable representations, mathematically expressed as:",
              "index": 2,
              "part": 0,
              "translated_content": "在形式上，感觉记忆的形成包括三个连续步骤：知觉编码、注意选择和短时保留。首先，知觉编码将原始感觉信号转换为可处理的表示形式，数学上表达为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\phi(o_{t})=\\operatorname{Encode}(o_{t},s_{t})\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $o_{t}$ is the sensory input at time $t$ and $s_{t}$ represents the agent's state. For instance, RecAgent [205] employs an LLM-based sensory memory module toencode raw observations while filtering noise and irrelevant content.Extending",
              "index": 4,
              "part": 0,
              "translated_content": "其中$o_{t}$表示时间$t$时的感官输入，$s_{t}$代表主体的状态。例如，RecAgent [205]利用基于LLM的感官记忆模块对原始观察进行编码，同时过滤噪音和无关内容。扩展…"
            },
            {
              "type": "figure",
              "src": "images/842a45dbf1ec1c8a6bec8af1082979a90b9f29cc8bb487f1d5c056bc7027c2a6.jpg",
              "alt": "",
              "caption": "Figure 3.6: Tree diagram of the memory module in intelligent agents.",
              "index": 5,
              "part": 0,
              "translated_caption": "图3.6：智能代理中记忆模块的树状图。"
            },
            {
              "type": "text",
              "content": "beyond text-based perception, multimodal sensory memory systems such as Jarvis-1[228], VideoAgent[209],and WorldGPT [210] integrate multimodal foundation models to process diverse modality inputs.\n\nNext, attentional selection extractscrucial information from the encoded sensorydata.This processguided by an attention mechanism, is represented as:",
              "index": 6,
              "part": 0,
              "translated_content": "除了基于文本的感知之外，多模感觉记忆系统（如Jarvis-1[228]、VideoAgent[209]和WorldGPT[210]）集成了多模基础模型来处理不同的感觉输入。\n\n接下来，注意力选择从编码的感觉数据中提取关键信息。这一过程由注意力机制引导，表示为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\alpha_{t}=\\mathrm{Attention}(\\phi(o_{t}),c_{t})\n $$",
              "index": 7,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\phi(o_{t})$ is the encoded input, and $c_{t}$ denotes contextual information influencing attention. For example, RecAgent [205] employs an attention mechanism with an importance scoring system that assigns relevance scores to compressed observations, prioritizing critical inputs such as item-specific interactions while de-emphasizing less significant actions. This helps extract high-priority information for memory retention.\n\nFinally, transient retention temporarily stores the selected sensory information as sensory memory:",
              "index": 8,
              "part": 0,
              "translated_content": "其中$\\phi(o_{t})$为编码输入，$c_{t}$表示影响注意力的上下文信息。例如，RecAgent[205]采用了一个注意力机制，其中包含一个重要性评分系统，为压缩观测分配相关性分数，优先考虑关键输入，如特定项目的互动，同时减弱不太重要的行为。这有助于提取用于记忆保留的高优先级信息。\n\n最后，短暂保留将所选的感官信息作为感官记忆进行临时存储："
            },
            {
              "type": "formula",
              "content": "$$ \nM_{\\mathrm{sensory}}=\\{\\alpha_{t}\\ |\\ t\\in[t-\\tau,t]\\}\n $$",
              "index": 9,
              "part": 0
            },
            {
              "type": "text",
              "content": "Several strategies have been implemented to manage the time window.For instance,RecAgent[205]models retention by associatingeachobservationwiththe timestampcorespondingtothe start ofa simulationround inthe user behavior simulation environment,represented as atriplet(observation,importance score,timestamp).Similarly,CoPS [206] employs afixed-size sensory memory poolas atime window, whichconsists of user search requests for personalized search,facilitating“re-finding\"behavior.Whenanewquery isreceived,thesystemfirstchecksthe sensory memory for relevantmatches.Ifamatch isfound,the query isclassifiedasare-finding instanceenabling arapidsensoryresponse.",
              "index": 10,
              "part": 0,
              "translated_content": "已经实施了几种策略来管理时间窗口。例如，RecAgent[205]通过将每个观测与用户行为模拟环境中模拟回合开始的时间戳相关联来建模保留，表示为一个三元组（观测，重要性评分，时间戳）。类似地，CoPS [206]采用固定大小的感官记忆池作为时间窗口，其中包含用于个性化搜索的用户搜索请求，促进“重新查找”行为。当收到新查询时，系统首先检查感官记忆以寻找相关匹配项。如果找到匹配项，则将查询分类为重新查找实例，从而实现快速感官响应。"
            }
          ],
          "raw_title": "Sensory Memory",
          "type": null,
          "children": [],
          "translated_title": "3.3.1 感觉记忆"
        },
        {
          "title": "3.3.2 Short-Term Memory",
          "number": "3.3.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Short-term memory in cognition-inspired intellgent agents serves as atransient and dynamicworkspace that bridges sensory memory andlong-term memory.It isessentialforstoring and processing task-relevant information andrecent interaction sequences,supporting real-time decision-making and adaptive behavior.Inspired by human short-term and working memory,t temporarilyretains information tofacilitatecomplexcognitive tasks,ensuring continuity and coherence in the agent's operations.\n\nShort-term memory in intellgent agentscan be categorized into context memory and working memory.On the one hand, context memory treats the context window as the short-term memory of LLMs.For example,MemGPT [214] inspired byhierarchicalmemory systems inoperating systems,managesdiffrentstorage tierstoextendcontext beyond the LLM's inherentlimitations.[290]introduces aneurosymbolic context memory that enhances LLMs by enabling symbolic rule grounding and LLM-based rule application.\n\nOnthe other hand, working memory involves fetching and integrating relevant external knowledgeto hold essential information during an agent's operation. Generative Agent [50] employs short-term memory to retain situational contextfacilitatingcontext-sensitive decision-making.Reflexion[48]utilizesasliding window mechanism to capture and summarizerecentfeedback,balancing detailed immediate experienceswith high-levelabstractions forenhanced adaptability.RLP[218] maintains conversational states for speakers andlisteners,using them as short-term memory prompts to support dialogue understanding and generation.",
              "index": 0,
              "part": 0,
              "translated_content": "认知启发智能代理中的短期记忆作为一个瞬时和动态的工作空间，连接感觉记忆和长期记忆，对于存储和处理与任务相关的信息以及最近的交互序列至关重要，支持实时决策和自适应行为。受人类短期记忆和工作记忆的启发，它临时保留信息以促进复杂的认知任务，确保代理操作的连续性和连贯性。\n\n智能代理中的短期记忆可分为上下文记忆和工作记忆。一方面，上下文记忆将上下文窗口视为LLMs的短期记忆。例如，受操作系统中分层内存系统启发，MemGPT[214]管理不同存储层级以扩展上下文超出LLMs固有的限制。[290]引入了一种神经符号上下文记忆，通过启用符号规则接地和基于LLMs的规则应用来增强LLMs。\n\n另一方面，工作记忆涉及获取和整合相关外部知识，以在代理操作期间保持重要信息。生成式代理[50]利用短期记忆保留情境背景，促进情境敏感决策。Reflexion[48]利用滑动窗口机制捕获和总结最近的反馈，平衡详细的即时体验与高层抽象，以增强适应性。RLP[218]维护说话者和听众的对话状态，并将它们用作短期记忆提示，支持对话理解和生成。"
            },
            {
              "type": "text",
              "content": "For interactive and creative game scenarios, CALYPSO[219]assts Dungeon Masters in storytelling for Dungeons & Dragons by constructing short-term memory from scene descriptions, monster details, and narrative summaries, enabling adaptive storytelling and dynamic engagement.Similarly, Agent S[21l] and Synapse[291], designed for GUI-based autonomouscomputer interaction,define theirshort-term memory astask trajectories,including actions such as buton clicks and text inputs.This formulation supports behavioralcloning and enhances adaptation in novel GUI navigation tasks.",
              "index": 1,
              "part": 0,
              "translated_content": "对于互动和创意游戏场景，CALYPSO[219]通过构建场景描述、怪物细节和叙事摘要的短期记忆，辅助《龙与地下城》的地牢主持人进行叙事，实现自适应叙事和动态参与。类似地，Agent S[211]和Synapse[291]，专为基于GUI的自主计算机交互设计，将它们的短期记忆定义为任务轨迹，包括按钮点击和文本输入等操作。这种表述支持行为克隆，并增强新颖GUI导航任务中的适应性。"
            },
            {
              "type": "text",
              "content": "In robotics applications,SayPlan[292]leverages scene graphs and environmental feedback as short-term memory to guide planning and execution in scalable robotic environments.KARMA [215] engages short-term working memory with an effective and adaptive memory replacement mechanism todynamicallyrecord changes inobjectspositions and states. LLM-Planner [293] iteratively updates short-term memory with environmental observation to prompt an LLM for dynamic planning.",
              "index": 2,
              "part": 0,
              "translated_content": "在机器人应用中，SayPlan利用场景图和环境反馈作为短期记忆，以引导规划和执行可扩展的机器人环境。KARMA利用有效和自适应的记忆替换机制来引导短期工作记忆，动态记录物体位置和状态的变化。LLM-Planner通过环境观测迭代更新短期记忆，以促使LLM进行动态规划。"
            }
          ],
          "raw_title": "Short-Term Memory",
          "type": null,
          "children": [],
          "translated_title": "3.3.2 短期记忆"
        },
        {
          "title": "3.3.3 Long-Term Memory",
          "number": "3.3.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Long-term memory in cognition-inspired intelligent agents enables the retention and retrieval of information over extended periods, alowing agents to generalize knowledge and adapt to newcontexts efectively. Unlike sensory and shot-term memory, which handle transient orimmediate data, long-term memory supports cumulative learning and cross-task adaptability. It mirrors human long-term memory by incorporating explicit and implicit components, facilitating richer contextual understanding and intuitive behavior.",
              "index": 0,
              "part": 0,
              "translated_content": "在启发认知的智能代理中，长期记忆使其能够在较长时间内保留和检索信息，从而使代理能够有效地概括知识并适应新的环境。与处理瞬时或即时数据的感知和短期记忆不同，长期记忆支持渐进学习和跨任务适应性。它通过融合显式和隐式组件来模拟人类长期记忆，促进更丰富的上下文理解和直观行为。"
            },
            {
              "type": "text",
              "content": "On the one hand, explicit memory involves intentionalrecollction,analogous to declarative memory in humans.It consists of semantic memory, which stores generalknowledge such as facts and concepts, and episodic memory, whichrecords specific events and interaction histories.Semantic memory in intellgent agentscan be preloaded from domain knowledge basesordynamicall acquired through interactions.For example,in environments like TextWorld, semantic memory captures structured facts,such as“Recipe-contains- Tuna\"or“Recipe - is on-Table\".Episodic memory,incontrast,logs situationalcontextand sequentialactions,such as“gofrom kitchen tolivingroom,then to garden\".Integrating semantic and episodic memory alows agents toretain static andcontextual information,enabling human-like adaptability and context-aware responses.",
              "index": 1,
              "part": 0,
              "translated_content": "一方面，显式记忆涉及有意识的回忆，类似于人类的陈述性记忆。它包括语义记忆，存储一般知识，如事实和概念，以及情景记忆，记录特定事件和互动历史。智能代理的语义记忆可以从领域知识库中预加载，也可以通过互动动态获取。例如，在TextWorld等环境中，语义记忆捕获结构化事实，如“食谱-包含-金枪鱼”或“食谱-在-桌子上”。相反，情景记忆记录情境背景和顺序行动，例如“从厨房到客厅，然后到花园”。整合语义和情景记忆使代理能够保留静态和情境信息，实现类人的适应性和上下文感知响应。"
            },
            {
              "type": "text",
              "content": "On the other hand, implicit memory shapes agent behavior through procedural memory and priming.Procedural memory enables agents to perform repetitive tasks efficiently byrecalling specific skill and reusable plans.Forexample,it automates routine tasks withoutrequiring explicit instructions,improving task execution effciency.Priming,meanwhile, captures statechanges and corresponding responses, allowing agents to adapt to similar contexts quickly. Priming enhances fluidity andcontext-sensitivedecision-making by directly matchingobservations toorcontinuouslychaining actions.Implicit memory,shaped byinteractions withcognitive modules,enablesrapid adaptation,oftenafter minimal exposure to new stimuli.",
              "index": 2,
              "part": 0,
              "translated_content": "另一方面，隐式记忆通过程序性记忆和启动来塑造代理行为。程序性记忆使代理能够通过回忆特定技能和可重复使用的计划高效执行重复任务。例如，它可以在不需要明确指令的情况下自动化常规任务，提高任务执行效率。同时，启动捕获状态变化和相应响应，使代理能够快速适应类似情境。启动通过将观察结果直接匹配到行动或连续链接行动，增强了流畅性和上下文敏感的决策制定。由与认知模块的互动塑造的隐式记忆使代理能够在与新刺激的最小暴露后快速适应。"
            },
            {
              "type": "text",
              "content": "Most intelligent agents implement both semantic and episodic memory within their memory modules.For instance, Agent S [2ll],designed for GUI automation tasks,incorporates semantic memory to store online webknowledge in natural language form,while episodic memory captures high-level, step-by-step task experiences.Similarly, AriGraph[221],targeting embodied simulation tasks,encodes semantic environment knowledge using a fact graph and logs episodic navigation historythrough an event graph. In AIcompanion systems like MemoryBank[207] for SiliconFriend, semantic memoryconstructs userportraits innaturallanguage,whileepisodic memoryretains interaction histories, enhancing personalized and context-aware behavior.",
              "index": 3,
              "part": 0,
              "translated_content": "大多数智能代理在其记忆模块中实现了语义记忆和情节记忆。例如，Agent S [2ll]，专为GUI自动化任务设计，集成了语义记忆以用自然语言形式存储在线网络知识，而情节记忆则捕获高层次、逐步任务经验。类似地，针对具身模拟任务的AriGraph[221]，使用事实图编码语义环境知识，并通过事件图记录情节导航历史。在像MemoryBank[207]为SiliconFriend这样的AI伴侣系统中，语义记忆构建用户肖像图，而情节记忆保留互动历史，增强了个性化和上下文感知行为。"
            },
            {
              "type": "text",
              "content": "For implementing implicit memory,current agent systems primarily adopt model-friendly memory formats, such as key-value pairstorage,executable code, or reusable routines.For example, AAG [226]defines and generalizes procedures through analogy, mapping knowledge from one situation (base)to another(target).This structure can be representedasalineardirectedchaingraph,where the input serves astheroot,theoutput as theleafnode,andeach intermediate step as a node in the chain.Similarly,Cradle[227]and Jarvis-1[228] implement procedural memory by storing andretrieving skill incode form,whichcanbeeitherlearned from scratchor pre-defined.Once curated,skills can be added, updated,or composed within memory.The most relevant skillfora given task andcontext are then retrieved to support action planning.",
              "index": 4,
              "part": 0,
              "translated_content": "为了实现隐式记忆，当前的代理系统主要采用友好于模型的记忆格式，如键-值对存储、可执行代码或可重复使用的例程。例如，AAG [226] 通过类比定义和泛化程序，将知识从一个情境（基础）映射到另一个情境（目标）。这种结构可以表示为线性有向链图，其中输入作为根节点，输出作为叶节点，每个中间步骤作为链中的一个节点。类似地，Cradle [227] 和 Jarvis-1 [228] 通过以代码形式存储和检索技能来实现程序性记忆，这些技能可以从头学习或预先定义。一旦被策划，技能可以在记忆中添加、更新或组合。然后检索出与特定任务和情境最相关的技能，以支持行动规划。"
            }
          ],
          "raw_title": "Long-Term Memory",
          "type": null,
          "children": [],
          "translated_title": "3.3.3 长期记忆"
        }
      ],
      "translated_title": "3.3 代理记忆的表征"
    },
    {
      "title": "3.4 The Memory Lifecycle",
      "number": "3.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "In this section,we introduce thelifecycleofmemory inAIagents, as depicted in Figure3.7.Thelifecyclecomprisesa dualprocessofretentionandretrieval.Retentionincludesacquisition,encodingandderivation,whileretrievalinvolves memory matching, neural memory networks, and memory utilization.",
          "index": 0,
          "part": 0,
          "translated_content": "在本节中，我们介绍了AI代理中记忆的生命周期，如图3.7所示。这一生命周期包括了保留和检索的双重过程。保留包括获取、编码和推导，而检索涉及记忆匹配、神经记忆网络和记忆利用。"
        }
      ],
      "raw_title": "The Memory Lifecycle",
      "type": null,
      "children": [
        {
          "title": "3.4.1 Memory Acquisition",
          "number": "3.4.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Memory Acquisition is the foundational processby which intelligentagents take in raw perceptual information from their environment.This initial step is crucial for subsequent learning,adaptation, and decision-making[305].A primary challnge in acquisition is the sheer volume and complexity of environmentalinputs.Agents are constantly bombardedwithvisual,auditory,textual,andtherformsofdata,muchofwhichisredundantorirrelevanttotheagent's goals.Therefore,acore aspect of memory acquisition is not simplycapturing data, but also initiating a preliminary filtering processThis filtering leverages two primary mechanisms: initial information compression and experience consolidation.",
              "index": 0,
              "part": 0,
              "translated_content": "记忆获取是智能代理从环境中获取原始感知信息的基础过程。这一初始步骤对于后续的学习、适应和决策至关重要。获取过程中的一个主要挑战是环境输入的数量和复杂性。代理不断受到来自环境的视觉、听觉、文本和其他形式的数据的冲击，其中很多对于代理的目标来说是多余的或无关的。因此，记忆获取的一个核心方面不仅是捕获数据，还包括启动初步过滤过程。这种过滤利用两个主要机制：初始信息压缩和经验巩固。"
            },
            {
              "type": "text",
              "content": "At this early stage,information compression involves rudimentary techniques to reduce data dimensionality. This might include downsampling images,extracting key phrases from text using simpleheuristics,oridentifying significant changes inaudiostreams[306].The goalisrapidlossycompressionto prioritize potentiallrelevant information.For example, LMAgent [230] prompts the LLM to perform information compression, reducing irrelevant and unimportant content when constructing sensory memory to enhance operational efficiency. Meanwhile, ReadAgent[231] and GraphRead[307] respectively employ different strategies for compressing long text, i.e.,episode pagination and graph-based structuring, to maximize information retention while ensuring efficiency.",
              "index": 1,
              "part": 0,
              "translated_content": "在这个早期阶段，信息压缩涉及基本技术来降低数据维度。这可能包括对图像进行降采样，使用简单的启发式方法从文本中提取关键短语，或者识别听觉流中的显著变化。其目标是进行快速的有损压缩，以优先处理潜在相关信息。例如，LMAgent触发LLM执行信息压缩，当构建感官记忆以增强操作效率时，减少无关和不重要内容。同时，ReadAgent和GraphRead分别采用不同的策略来压缩长文本，即分集和基于图的结构化，以在确保效率的同时最大化信息保留。"
            },
            {
              "type": "text",
              "content": "Onthe other hand,experience consolidation,even at the acquisition phase, plays arole.The agentdoesn'tyethave a rich memory,but it can begin to apply previouslylearned, very generalrules or biases.Forexample,if the agent has a pre-existing bias towards moving objects, it might prioritize visual data containing motion,even before full encoding [308].To enhance the dynamic consolidation of memory-based experiences, [235] define metrics such as contextual relevance and recallfrequency to determine whether to update long-term memory in a vector database.",
              "index": 2,
              "part": 0,
              "translated_content": "另一方面，经验巩固，即使在获取阶段也起着重要作用。Agent尚未拥有丰富的记忆，但可以开始应用先前学到的非常普遍的规则或偏见。例如，如果Agent对移动物体有先入之见，它可能会优先处理包含运动的视觉数据，甚至在完全编码之前就这样做。为了增强基于记忆的经验的动态巩固，[235]定义了诸如上下文相关性和召回频率之类的指标，以确定是否在向量数据库中更新长期记忆。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Domain</td><td colspan=\"3\">Memory Representation</td><td colspan=\"5\">Memory Lifecycle</td></tr><tr><td>Sensory</td><td>Short-term Long-term|Acquisition Encoding Derivation</td><td></td><td></td><td></td><td></td><td>Retrieval</td><td>Utilization</td></tr><tr><td>Synapse [291]</td><td>GUI</td><td>Multi-</td><td>Context</td><td>PEpisdirca</td><td>User demo.</td><td></td><td>Hieomch.</td><td></td><td></td></tr><tr><td>Agent S [211]</td><td>GUI</td><td>Mulil-</td><td>Corkingg</td><td>Sepiantic,</td><td>Compfress.</td><td>Contastive</td><td>Seleet.</td><td>Indexing</td><td>Lonex-</td></tr><tr><td>Automanual [108]</td><td>GUI</td><td>Molal-</td><td>Context</td><td>Propiedral,</td><td>DUsmr.</td><td>Hierarch.</td><td>D Gomp.</td><td>STasch</td><td>Subeoal</td></tr><tr><td>AutoGuide[294]</td><td>GUI</td><td>Multi- modal</td><td>Context</td><td></td><td>Screen Capture</td><td></td><td>Action Plan</td><td></td><td>Action Exec.</td></tr><tr><td>Agent-Pro [295]</td><td>GUI</td><td> Multi-</td><td>Context</td><td></td><td>Sapeure</td><td></td><td>Hiecomch.</td><td></td><td>Action</td></tr><tr><td>MemGPT [214]</td><td>Document</td><td>Text</td><td>Corking,</td><td></td><td>ExDernal</td><td></td><td></td><td>Pac.call</td><td>Doact.</td></tr><tr><td>SeeAct [296]</td><td>Web</td><td> Moulai</td><td>Context</td><td></td><td>Scren</td><td></td><td>Action Plan</td><td></td><td>Ineract.</td></tr><tr><td>Auto WebGLM</td><td>Web</td><td>Text</td><td>Context</td><td></td><td>HTML</td><td>HTMd</td><td>AHTMSIS</td><td></td><td>IWwat.</td></tr><tr><td>SteP [298]</td><td>Web</td><td>Text</td><td>Context</td><td>Task-spec.</td><td>HTML</td><td>HTML</td><td>AHTML</td><td>Eleament</td><td>Ineract.</td></tr><tr><td>AWM [299]</td><td>Web</td><td>Text</td><td></td><td>Procedural</td><td>Workflow</td><td>Aution.</td><td>，</td><td> Siup</td><td>Workflow</td></tr><tr><td>AriGraph [221]</td><td>TextWorld</td><td>Text</td><td></td><td>Sepisantic,</td><td>Eserv.</td><td>Krapl.</td><td>Travepsal</td><td>Retrieval</td><td>Apction</td></tr><tr><td>MemoryBank [207]</td><td>Dialogue</td><td>Text</td><td></td><td>Episodic</td><td>Dialogue Record</td><td></td><td></td><td>Chron. order</td><td>Resp. gen.</td></tr><tr><td>PromptAgent [300]</td><td>General</td><td>Text</td><td>Context</td><td></td><td>Prompting</td><td></td><td>Prome.</td><td>Content-</td><td>Prompt</td></tr><tr><td>ECL[301]</td><td>Embody</td><td>Moda-</td><td>Context</td><td>Episodic</td><td>Recording</td><td>Contrast.</td><td>SExmer.</td><td>Sim.& Recency</td><td>Pelicy Long-</td></tr><tr><td>LEO [302]</td><td>Embody</td><td> Multi-</td><td>Working</td><td>HLoizon Rep.</td><td>Observation</td><td>Spamipl- Learn.</td><td>Goal-Cond.</td><td>Hierarch.</td><td>Exec. Horizon</td></tr><tr><td>IER [303]</td><td>Embody</td><td>Multi- modal</td><td>Context</td><td>Episodic</td><td>Env. Interact.</td><td>Multal- Embed</td><td>Iter. Refine.Sim.Match</td><td></td><td>Action Plan.</td></tr><tr><td>Voyager [47]</td><td>Embody</td><td>Text</td><td>Working</td><td>Procedural</td><td>Curiculum</td><td>Lskrary</td><td>Prompt.</td><td></td><td>Skill Exec.</td></tr><tr><td>A3T [49]</td><td>Embodys</td><td>Text</td><td>Context</td><td></td><td>DTomp.</td><td>Token. d&</td><td>Action Planning</td><td></td><td>Actiot.</td></tr><tr><td>STARLING[304]</td><td>Robotics</td><td>Mula-</td><td>Context</td><td>Procedural</td><td>Demo.</td><td>Trade</td><td>RSkile.</td><td>Simtet</td><td>Skill Exec.</td></tr></table></body></html>",
              "caption": "Table 3.1: Summary of the memory module in various agents. Refer to Figure 3.6 for abbreviations.",
              "index": 3,
              "part": 0,
              "translated_caption": "表3.1：各种代理中记忆模块的总结。缩写请参考图3.6。"
            },
            {
              "type": "text",
              "content": "Expel[69]constructs anexperience pool tocollectandextract insights from training tasks,facilitating generalization to unseen tasks. More recently, MindOS [233] proposed a working memory-centric central procesing module for building autonomous AIagents,where working memoryconsolidates task-relevant experiences into structured thoughts for guiding future decisions and actions.\n\nThese two mechanisms work in concert with preliminary LLM input. To address the initial challenges,several mechanisms have to be deployed.Agents must be equipped with mechanisms to assessthe potentialrelevanceof incoming informationrapidly.Thispreliminaryfiltering preventscognitiveoverload.The acquisition phase alsobenefits from LLM.",
              "index": 4,
              "part": 0,
              "translated_content": "Expel[69]构建了一个经验池，用于收集和提取训练任务中的见解，促进对未知任务的泛化。最近，MindOS[233]提出了一个以工作记忆为中心的中央处理模块，用于构建自主AI代理，其中工作记忆将与任务相关的经验整合成结构化的思考，以指导未来的决策和行动。\n\n这两种机制与初步的LLM输入协同工作。为了解决最初的挑战，必须部署几种机制。代理必须配备机制来快速评估传入信息的潜在相关性。这种初步过滤可以防止认知过载。获取阶段也受益于LLM。"
            }
          ],
          "raw_title": "Memory Acquisition",
          "type": null,
          "children": [],
          "translated_title": "3.4.1 记忆获取"
        },
        {
          "title": "3.4.2 Memory Encoding",
          "number": "3.4.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Memory encoding builds upon acquisition by transforming the filtered perceptual information into internalrepresentations suitableforstorage andlater use.A key aspectofencoding isselective filtering.This selective attention mimics human cognitive processes[309].The inherentchallenges ofencoding stem fromthecomplexity,high dimensionality, and often noisy nature of raw perceptualdata.Effctive encoding requires advanced mechanisms to identify key eatures,compressthem compactly,and integrate information from multiple modalities. Modern approaches address hese challenges by leveraging selective attention and multi-modal fusion.",
              "index": 0,
              "part": 0,
              "translated_content": "记忆编码是在获取信息的基础上构建的，它将经过过滤的感知信息转化为适合存储和后续使用的内部表示。编码的一个关键方面是选择性过滤。这种选择性注意力模仿了人类认知过程。编码的固有挑战源于原始感知数据的复杂性、高维度和常常嘈杂的性质。有效的编码需要先进的机制来识别关键特征，将它们紧凑地压缩，并整合来自多个模态的信息。现代方法通过利用选择性注意力和多模态融合来解决这些挑战。"
            },
            {
              "type": "figure",
              "src": "images/353deb6f8fa3c1e1e85bb8d1f6c02afc4c2243423ed5c55654e074f61ec094aa.jpg",
              "alt": "",
              "caption": "Figure 3.7:Illustration of the memorylifecycle.The memory retention processinvolves three sequential steps—memory acquisition,encoding,andderivation, whilethe memoryretrieval processencompassesseveralindependentapplications, including matching (vector search),neural memory networks, and memory utilization (for long-context modeling and hallucination mitigation).",
              "index": 1,
              "part": 0,
              "translated_caption": "图3.7：记忆生命周期的示意图。记忆保留过程包括三个连续步骤——记忆获取、编码和推导，而记忆检索过程涵盖了几个独立的应用，包括匹配（向量搜索）、神经记忆网络以及记忆利用（用于长上下文建模和幻觉缓解）。"
            },
            {
              "type": "text",
              "content": "Selective Attention mechanisms,inspired by human cognition,allow the agent to dynamically focus computational resources onthe mostrelevant parts of theinput.This might involveattending tospecificregions ofanimage,keywords in a text,or particularfrequencies in an audio signal. Different atention mechanisms can be used depending on the modality and task.For example, as the candidate memory dynamicall expands, MS [237] employs an LLM-based scorer to selectively retain the top-scoring half,creating a more compact shared memory across multiple agent systems. In othermodalities,GraphVideoAgent[238]utilizes graph-based memory toenable selective and multi-turn video scene understanding,enhancing question-answering performance.Inrobot control,[24o]implementsselectiveattntionasa filtering mechanism to extract task-relevant objects from the set of allperceived objects on the table.",
              "index": 2,
              "part": 0,
              "translated_content": "选择性注意机制受人类认知启发，使代理能够动态地集中计算资源在输入中最相关的部分。这可能涉及关注图像的特定区域、文本中的关键词，或者音频信号中的特定频率。根据模态和任务的不同，可以使用不同的注意机制。例如，随着候选记忆动态扩展，MS [237]采用基于LLM的评分器来选择性地保留得分最高的一半，从而在多个代理系统之间创建更紧凑的共享记忆。在其他模态中，GraphVideoAgent[238]利用基于图的记忆来实现选择性和多轮视频场景理解，提高了问答性能。在机器人控制中，[240]将选择性注意作为一种过滤机制，从所有感知到的物体中提取与任务相关的对象。"
            },
            {
              "type": "text",
              "content": "Multi-modalFusion[310]isessential forintegrating information fromdifferent sensory inputs (e.g.combining visual and auditory data to understand a scene).This involves creating aunified representation space where features from different modalitiesare aligned.Cross-modal encoders andcontrastivelearningtechniques areoften used to achieve this fusion.For example,JARVIS-1[228] uses the general-domain video-language model CLIP[51] tocompute alignment within amultimodal key-value memory, where thekeycomprises elements such as task,plan, andvisualobservations, and the value is a text-based representation of successfully executed plans.Furthermore,Optimus-l[241]refines memory representationand optimizes the multimodalencoder by leveraging MineCLIP[311],adomain-specific videolanguage model pre-trained on Minecraft gameplay,to align and fuse filtered video streams withtextualinstructions and plans,encoding the agent'smultimodalexperiences into an abstracted memory pool.This integratedrepresentation enhances information retrieval andreasoning across modalities and acts as anotherflter,reinforcing consistent data. LLMs' semantic understanding is utilized to extract relevant features effciently.",
              "index": 3,
              "part": 0,
              "translated_content": "多模态融合对于整合来自不同感官输入的信息至关重要（例如，将视觉和听觉数据结合起来理解一个场景）。这涉及创建一个统一的表示空间，其中来自不同模态的特征被对齐。跨模态编码器和对比学习技术通常用于实现这种融合。例如，JARVIS-1使用通用领域视频-语言模型CLIP来计算多模态键值记忆中的对齐，其中键包括任务、计划和视觉观察等元素，而值则是成功执行计划的基于文本的表示。此外，Optimus-l通过利用MineCLIP对记忆表示进行优化，并优化多模态编码器，MineCLIP是在Minecraft游戏过程中预先训练的领域特定视频语言模型，以对齐和融合过滤后的视频流与文本指令和计划，将代理的多模态体验编码为一个抽象的记忆池。这种集成表示增强了跨模态的信息检索和推理，并起到另一个过滤器的作用，加强了数据的一致性。LLMs的语义理解被用来高效地提取相关特征。"
            }
          ],
          "raw_title": "Memory Encoding",
          "type": null,
          "children": [],
          "translated_title": "3.4.2 记忆编码"
        },
        {
          "title": "3.4.3 Memory Derivation",
          "number": "3.4.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Memory derivation focuses on extracting meaningfulknowledge and insights from the acquired and encoded memories. This processgoes beyond simple storage.This stage is essentialforenhancing the agent's learning capabilities.The goalistocontinuouslyoptimizethe structure andcontent of the agent's memory.Asignificantchallenge inderivation is the dynamic evaluationof informationvalue.Strategies toaddressthese challenges include reflection,summarization, knowledge distillation, and selective forgetting.",
              "index": 0,
              "part": 0,
              "translated_content": "记忆推导侧重于从获取和编码的记忆中提取有意义的知识和见解。这个过程超越了简单的存储。这个阶段对于增强代理的学习能力至关重要。目标是持续优化代理的记忆结构和内容。推导中的一个重要挑战是信息价值的动态评估。应对这些挑战的策略包括反思、总结、知识提炼和选择性遗忘。"
            },
            {
              "type": "text",
              "content": "Reflection involves an agent activelyanalyzing its memories to identify paterns,relationships,and potentialinconsistencies. Itcan be triggeredby specific events(e.g.,an unexpected outcome)or occur periodically asabackground process.This process may include comparing memories,reasoning about causalrelationships,and generating hypotheses[300].ExpeL[69]leverages reflection tocolect past experiencesforgeneralization tounseen tasks andto support trial-and-error reattempts following failures.R2D2[243] models memory as areplay buffer and appliesreflectionto refine it bycorrecting failedexecutiontrajectories in web agents.Thesecorrctedtrajectories are thencombined with successful ones to construct reflective memory, which serves as a reference for future decision-making.",
              "index": 1,
              "part": 0,
              "translated_content": "反思涉及代理主动分析其记忆，以识别模式、关系和潜在的不一致之处。它可以由特定事件触发（例如，意外结果），也可以定期作为后台进程发生。这个过程可能包括比较记忆、推理因果关系和生成假设。ExpeL利用反思来收集过去的经验，以推广到未知任务，并在失败后支持反复尝试。R2D2将记忆建模为重放缓冲区，并应用反思来通过纠正网络代理中的执行失败轨迹来完善记忆。然后，这些纠正后的轨迹与成功的轨迹结合，构建反思性记忆，为未来的决策提供参考。"
            },
            {
              "type": "text",
              "content": "Summarization aims to produce concise representations of larger bodies of information while preserving their most essentialcontent.This can include extracting key sentences from adocument, generating abstractive summaries of conversations,or condensing sequences of events.Summarization techniques range from simple extractive methods to advanced abstractive approaches powered by large language models (LLMs)[245,312,246]. For example,[248] introduces arecursive summarization strategy over dialogue history and prior memory to support long-term dialogue memory derivation.Building onthis,Healthcare Copilot[247] maintains concise memory bytransforming conversation memory,representing the fullongoing medical consultation, into history memory that retains only key information relevant to the patient's medical history.",
              "index": 2,
              "part": 0,
              "translated_content": "总结旨在在保留最基本内容的同时产生更为简洁的信息表示。这可能包括从文档中提取关键句子，生成对话的抽象摘要，或者压缩事件序列。总结技术范围从简单的抽取方法到由大型语言模型（LLMs）驱动的先进抽象方法[245, 312, 246]。例如，[248]介绍了一种递归总结策略，通过对话历史和先前记忆支持长期对话记忆的推导。在此基础上，Healthcare Copilot[247]通过将对话记忆转换为历史记忆来保持简明的记忆，将完整的医疗咨询转化为仅保留与患者病史相关的关键信息的历史记忆。"
            },
            {
              "type": "text",
              "content": "Knowledge distilation[313]enables agents to transfer knowledge from larger, more complex models (or ensembles) to smaller, more efficientones.This is particularly important for resource-constrained agents and for enhancing generalization.Distilltion can also involve consolidating knowledge from multiple specialized models into a single, general-purpose model.Forexample,AoTD[250]distill textualchains ofthoughtfrom execution traces of subtasks into a Video-LLM to enhance multi-step reasoning performance in video question answering tasks. LDPD [251] transfers decision-making outcomesfrom teacheragents ie.expert bufers)to studentagents,optimizing the student's policy to align with the teacher's. In multi-agent systems, MAGDi[253] distills the reasoning interactions among multiple LLMs into smaller models by structurally representing multi-round interactions as graphs,thereby improving the reasoning capabilities of smaller LLMs.",
              "index": 3,
              "part": 0,
              "translated_content": "知识蒸馏[313]使代理能够将知识从更大、更复杂的模型（或集成模型）转移到更小、更高效的模型中。这对资源受限的代理和增强泛化能力尤为重要。蒸馏还可以涉及 consolidaing 知识，将多个专门模型中的知识整合到一个通用模型中。例如，AoTD[250] 从子任务执行轨迹中蒸馏出文本思维链，将其转移到 Video-LLM 中，以提高视频问答任务中的多步推理性能。LDPD [251] 将决策结果从教师代理（即专家缓冲区）转移到学生代理，优化学生的策略以与教师保持一致。在多代理系统中，MAGDi[253] 通过将多个LLM之间的推理交互蒸馏到较小模型中，通过将多轮交互结构化表示为图，从而提高较小LLM的推理能力。"
            },
            {
              "type": "text",
              "content": "Selective forgeting[314] is the crucial processof removing ordown-weighting memories thataredeemed irrelevant, redundant, or outdated.This is essential for maintaining memory effciency and preventing cognitive overload. Forgetting mechanismscan be based on time(older memories are morelikely tobe forgotten)[247],usagefrequency (infrequentlyaccessedmemoriesaremore likelyforgotten)[203],andrelevance tothecurrenttaskorcontext[255].In morefine-grainedforgetting mechanisms, MemoryBank[207]applisthe Ebbinghaus Forgetting Curveto quantify the forgetingrate,accounting forbothtime decay andthe spacing effect,ie.,theprinciplethat relearning information is easier than learning it forthefirst time.Incontrast,Lyfe Agent[254]adopts ahierarchicalsummarize-and-forget strategy:it first clusters related memories,refines them intoconcise summaries,and then removes older memories that are highlysimilar to newer ones.This approach enables effcient,low-cost memory updatesforreal-time social interactions.",
              "index": 4,
              "part": 0,
              "translated_content": "选择性遗忘[314]是移除或减轻被认为无关、冗余或过时的记忆的关键过程。这对于保持记忆效率和防止认知过载至关重要。遗忘机制可以基于时间（较早的记忆更有可能被遗忘）[247]、使用频率（很少访问的记忆更有可能被遗忘）[203]，以及与当前任务或上下文的相关性[255]。在更细粒度的遗忘机制中，MemoryBank[207]应用了艾宾浩斯遗忘曲线来量化遗忘速率，考虑了时间衰减和间隔效应，即重新学习信息比第一次学习更容易的原则。相比之下，Lyfe Agent[254]采用了分层总结和遗忘策略：首先将相关记忆聚类，将其精炼为简洁的摘要，然后移除与较新记忆高度相似的旧记忆。这种方法实现了用于实时社交互动的高效、低成本的记忆更新。"
            }
          ],
          "raw_title": "Memory Derivation",
          "type": null,
          "children": [],
          "translated_title": "3.4.3 记忆衍生"
        },
        {
          "title": "3.4.4  Memory Retrieval and Matching",
          "number": "3.4.4",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Memory retrieval is a processthat emulates thehuman ability to recallrelevantknowledge and experiences to solve problems.The goalistoeffciently andaccuratelyextractthemost pertinent memory fragmentsfromalarge and diverse memory pool,encompassing sensory, short-term, andlong-term memory,to informthe agent's decisions,planningand actions.Just as humans relyon past experiences tonavigatecomplex situations,agents require asophisticated memory retrieval mechanism to handle a wide range of tasks effectively.",
              "index": 0,
              "part": 0,
              "translated_content": "记忆检索是一种模拟人类回忆相关知识和经验以解决问题的过程。其目标是从庞大而多样的记忆池中高效准确地提取最相关的记忆片段，包括感官、短期和长期记忆，以指导智能体的决策、规划和行动。正如人类依赖过去的经验来应对复杂情况一样，智能体需要一个复杂的记忆检索机制来有效处理各种任务。"
            },
            {
              "type": "text",
              "content": "However, achieving this goal presents several significant challenges.First,the agent's memory repository is often heterogeneous,comprising various forms of memory such as natural language descriptions,structured knowledge graphs,andstate-actiorewardsequences.These memoriesdifferfundamentallintheirdatastructures,representations, and levels of semantic granularity,posing achallenge forunifiedretrieval.Second, the retrieved memoryfragments must be highlyrelevant tothecurrntcontext,includingthe agent'sstate,task goals,andenvironmentalobservations. Simple keyword matching falls shortofcapturingthe deeper semanticrelationships required for meaningfulretrieval. Developing a context-aware semantic matching mechanism that can dynamically adjust the retrieval strategy based on the currentsituationistherefore paramount.Third,thereal-time nature of agent interaction with theenvironment necessitates effcient memoryretrieval to support rapid decision-making and action[315].This demand for efficiency is further compounded by the limitations of the agent'scomputational resources.Finaly,the agent's memory is not staticbutconstantlyevolving asnew experiences,knowledge,and skills are acquired.Ensuring memoriestimeliness, reliability,andrelevancewhileavoidingtheinterferenceofutdatedorerrneousinformationisacontinuouschallenge.",
              "index": 1,
              "part": 0,
              "translated_content": "然而，要实现这一目标存在几个重大挑战。首先，智能体的记忆存储库通常是异构的，包括各种形式的记忆，如自然语言描述、结构化知识图谱和状态-行动-奖励序列。这些记忆在其数据结构、表示和语义粒度水平上有根本的差异，对统一检索构成了挑战。其次，检索到的记忆片段必须与当前上下文高度相关，包括智能体的状态、任务目标和环境观察。简单的关键词匹配无法捕捉到需要进行有意义检索所需的更深层语义关系。因此，开发一个能够根据当前情况动态调整检索策略的上下文感知语义匹配机制至关重要。第三，智能体与环境的实时互动性要求高效的记忆检索，以支持快速的决策和行动。这种效率需求进一步受到智能体计算资源限制的影响。最后，智能体的记忆不是静态的，而是随着新的经验、知识和技能的习得而不断发展。确保记忆的及时性、可靠性和相关性，同时避免过时或错误信息的干扰，是一个持续不断的挑战。"
            },
            {
              "type": "text",
              "content": "A comprehensive approachcanaddressthesechallenges,encompassing four keycomponents.Firstly,afoundational step involvesconstructing aunified memory representation andindexing scheme.This aims to bridge therepresentational gap between diffrent memory types by embedding them into acommon vector space. Pre-trainedlanguage models like BERT or Sentence-BERT[3i6]can be leveragedto transform text-based memories into semantic vectors, while graph neural networks(GNNs)can learn vector representations for structured memories like knowledge graphs,capturing both node andedge relationships[317].To facilitate effcientretrieval, a multi-layeredhybrid indexing structure is essential. This integrates techniques like inverted indexes for keyword matching,vector indexes like Faiss[318]or Annoy [319]for similaritysearch,andgraph indexes for structuralqueries[320],thus supporting diverse query needs.",
              "index": 2,
              "part": 0,
              "translated_content": "一个全面的方法可以解决这些挑战，包括四个关键组成部分。首先，基础步骤涉及构建统一的记忆表示和索引方案。这旨在通过将它们嵌入到一个共同的向量空间中来弥合不同记忆类型之间的表征差距。预训练的语言模型如BERT或Sentence-BERT可以被利用来将基于文本的记忆转换为语义向量，而图神经网络（GNNs）可以学习结构化记忆（如知识图谱）的向量表示，捕捉节点和边关系。为了促进高效检索，一个多层次的混合索引结构是必不可少的。这结合了倒排索引等技术用于关键词匹配，Faiss或Annoy等向量索引用于相似度搜索，以及用于结构化查询的图索引，从而支持多样化的查询需求。"
            },
            {
              "type": "text",
              "content": "Secondly,perhaps most critically, the system must developcontext-aware semantic similaritycomputation.This allows the retrieval processtounderstand and utilizethecurrent context,such as the agent's state,goals,andobservations, enabling a deeper semantic match beyond keyword overlap.This involves encoding the contextualinformation into vector representations and effectively fusing them with memory vectors.The attention mechanism plays acrucial role here,dynamicallycalculating the relevance between context and memory vectors and assgning diferent weights to memory fragments basedontheir contextualrelevance[261].This emphasizes memories that are more pertinent to the current situation.",
              "index": 3,
              "part": 0,
              "translated_content": "其次，也许最为关键的是，系统必须发展具有上下文感知的语义相似度计算。这使得检索过程能够理解并利用当前上下文，比如代理的状态、目标和观察，实现超越关键词重叠的更深层语义匹配。这涉及将上下文信息编码为向量表示，并有效地将其与记忆向量融合。注意力机制在这里起着至关重要的作用，动态计算上下文与记忆向量之间的相关性，并根据其上下文相关性为记忆片段分配不同的权重。这强调了与当前情况更相关的记忆。"
            },
            {
              "type": "text",
              "content": "Thirdly,integrating memoryretrievalwiththeagent'staskexecutionnecessitatesatask-orientedsequencedecisionand dynamic routing mechanism.This leverages the structuralinformationoftasks to guide memory retrievaland utilization, enabling complex task decomposition, planning, and dynamic adjustments.By constructing a task dependency graph,the agentcan topologically sort subtasks to determine execution order.During execution,each subtask's goal serves as contextfor memory retrieval,extracting relevant knowledge and experience.Moreover,the agent must adapt to environmental feedback and task progressdynamically adjusting the execution plan. Each decision point involvesre-retrieving memories basedonthecurrentstateand goaltoselect theoptimalaction andhandleunexpected situations.This aspect also emphasizes how agents can leverage their skill memory to solve problems, including skilldistillation,combination, and inovation.Pattern recognition allows forsummarising generalproblem-solving steps,while structured knowledge organizationarranges skillsintoaretrievable format.Agentscanfurther distll generalized skills from specificones,combine multiple skillstoaddresscomplexchallenges, and even innovate new skill combinations.These processes depend fundamentallyon an efficient memory retrieval system that can identify appropriate skills or skill combinations based on task requirements.",
              "index": 4,
              "part": 0,
              "translated_content": "第三，将记忆检索与代理任务执行相结合需要一种面向任务的序列决策和动态路由机制。这利用任务的结构信息来指导记忆的检索和利用，实现复杂任务的分解、规划和动态调整。通过构建任务依赖图，代理可以对子任务进行拓扑排序以确定执行顺序。在执行过程中，每个子任务的目标作为记忆检索的上下文，提取相关的知识和经验。此外，代理必须根据环境反馈和任务进展动态调整执行计划。每个决策点都涉及基于当前状态和目标重新检索记忆，以选择最佳行动并处理意外情况。这方面还强调了代理如何利用其技能记忆解决问题，包括技能提炼、组合和创新。模式识别允许总结一般的问题解决步骤，而结构化知识组织将技能整理成可检索的格式。代理可以进一步从具体技能中提炼出通用技能，组合多种技能以解决复杂挑战，甚至创新新的技能组合。这些过程基本上依赖于一个高效的记忆检索系统，该系统能够根据任务需求识别合适的技能或技能组合。"
            },
            {
              "type": "text",
              "content": "Finally, arobust memory management mechanism is crucial for maintaining the memory pool's timeliness,relevance, and efficiency.This mechanism should incorporate aforgetting and updating strategy, mirroring human forgeting mechanisms [321].This might involve regularly purging outdated,redundant,or infrequently used memories based on time-based decay (weakening memory strength over time) and frequency-based decay (purging low-frequency memories).Simultaneously,whenamemory fragmentrelevant tothecurrent task isretrieved,its timestampand access frequency are updated,increasing its importance and ensuring dynamic memory updates.Through these concerted efforts,LLM Agentscan be equipped with a powerful,flexible, and context-aware memory retrieval and matching system,enabling them to effectively utilize their accumulated knowledge,support complex decision-making, and exhibit more intelligent behavior.",
              "index": 5,
              "part": 0,
              "translated_content": "最后，一个强大的记忆管理机制对于维护记忆池的及时性、相关性和效率至关重要。这种机制应该包括遗忘和更新策略，模拟人类遗忘机制。这可能涉及定期清除过时、冗余或很少使用的记忆，基于时间衰减（随时间减弱记忆强度）和基于频率衰减（清除低频率记忆）。同时，当检索到与当前任务相关的记忆片段时，更新其时间戳和访问频率，提高其重要性并确保动态记忆更新。通过这些协调努力，LLM代理可以配备一个强大、灵活和上下文感知的记忆检索和匹配系统，使它们能够有效利用积累的知识，支持复杂决策，并展现更智能的行为。"
            }
          ],
          "raw_title": "Memory Retrieval and Matching",
          "type": null,
          "children": [],
          "translated_title": "3.4.4 记忆检索与匹配"
        },
        {
          "title": "3.4.5 Neural Memory Networks",
          "number": "3.4.5",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Neural Memory Networks represent a fascinating frontier in AIresearch.They aim to integrate memory seamlessly into the fabric of neural networks.This approach departs from traditional memory architectures by encoding memories directly within the network's weights oractivations,transforming the network into adynamic,read-write memory storage medium.This tight integration promises significant advancements in effciency andthe utilizationof stored information. However, realizing this vision presents several formidable challenges.",
              "index": 0,
              "part": 0,
              "translated_content": "神经记忆网络代表了人工智能研究中的一个迷人前沿。它们旨在将记忆无缝地整合到神经网络的结构中。这种方法与传统的记忆架构不同，它通过直接在网络的权重或激活中编码记忆，将网络转变为一个动态的、可读写的内存存储介质。这种紧密的整合承诺在效率和利用存储信息方面取得重大进展。然而，实现这一愿景面临着一些巨大的挑战。"
            },
            {
              "type": "text",
              "content": "A primary concern is balancing memory capacity with stability.Encoding a vast amount of information within the finite parameters ofa neural network while maintaining long-term stability poses a major hurdle.The network must be able to store a multitude of memories without succumbing tocatastrophic forgeting or confusion between similar memories.Equallycrucial is the development ofeffctive mechanisms for memory read-write operations.The network needs to reliably write new information,update existing memories,and accuratelyretrieve stored information on demand,all while maintainingcomputationaleffcency.Beyond simply storing memories,the ultimate goalis to endow neural networks withtheability to generalizefrom and reason withthe informationthey store.This would empower them to performhigher-order cognitive functions beyond rote memorization,allowing for insightfulconnections and inferences based on past experiences. Several approaches are being explored to addressthese challenges, notably through associative memory and parameter integration.",
              "index": 1,
              "part": 0,
              "translated_content": "一个主要关注点是在记忆容量和稳定性之间取得平衡。在神经网络的有限参数内编码大量信息，同时保持长期稳定性，构成了一个重大障碍。网络必须能够存储大量记忆，而不会陷入灾难性遗忘或在相似记忆之间混淆。同样关键的是，需要开发有效的记忆读写操作机制。网络需要可靠地写入新信息，更新现有记忆，并在需要时准确检索存储的信息，同时保持计算效率。超越简单地存储记忆，最终目标是赋予神经网络从存储的信息中泛化和推理的能力。这将使它们能够执行高阶认知功能，超越机械记忆，基于过去经验进行有洞察力的连接和推断。目前正在探索几种方法来解决这些挑战，尤其是通过关联记忆和参数整合。"
            },
            {
              "type": "text",
              "content": "On the one handassociative memory,inspired bythe interconnectednessof neurons inthe brain,ofers apromising avenue.ModelslkeHopfieldnetworks[262,63],leveraging energyfunctions,andBidirectionalAsociativeMemoies (BAMs)[322]supporting hetero-associative recall provide mechanisms for encoding andretrieving patterns based on the weights between neurons.Besides, Neural Turing Machines (NTMs)[264] and Memory-Augmented Neural Network (MANNs)[323,324,275,265] augment neuralnetworks with external memory modules,employing attention and summary mechanisms to interact with these memories.",
              "index": 2,
              "part": 0,
              "translated_content": "一方面，受到大脑中神经元相互连接的启发，关联记忆提供了一个有前途的途径。像Hopfield网络和支持异相关召回的双向关联记忆(BAMs)等模型，利用能量函数，提供了基于神经元之间权重的模式编码和检索机制。此外，神经图灵机(NTMs)和记忆增强型神经网络(MANNs)通过外部记忆模块增强神经网络，利用注意力和总结机制与这些记忆进行交互。"
            },
            {
              "type": "text",
              "content": "On the otherhand, parameter integration represents another key research direction, aiming to encode memory directly within anetwork's weights.This facilitates the seamlessintegration of world knowledge and accumulated experience into the operational behavior of intelligent AI agents.For example,some prior works modify model parameters to enable continual learning by updating [325,326,327]orforgetting specific knowledge[328].Other studies treat LLMs as standalone memory modules,incorporating world knowledge into their parameters during pre-training [329], post-training [330],and online deployment[331]. For instance,MemoryLLM[265] introduces memory tokens, while SELF-PARAM[266] leverages knowledge distillation to embed world knowledge and past AI agent experiences into model parameters. This approach is further augmented in the $\\mathbf{M}+$ model [332] with a long-term memory mechanism and aco-trained retriever,enhancing its ability to generalize tolonger history memorization.Additionally,[33] employs encoded memory to facilitate further reasoning,thereby improving the generalization of stored knowledge. More recently, MemoRAG [267] and ${\\tt R}^{3}$ Mem [270] have been proposed to not only encode memory but also enable reliable retrieval from neural memory networks,unifying the dual processes of memory storage and retrieval within a single model.This advancement contributes to thedevelopment of next-generation generative-based retrieval systems, which support lifelong AIapplications.Furthermore,Titans[269]have been introduced to memorize test-time data points through meta-learning, enabling more efficient test-time cross-task generalization.",
              "index": 3,
              "part": 0,
              "translated_content": "另一方面，参数集成代表了另一个重要的研究方向，旨在直接将记忆编码到网络的权重中。这有助于将世界知识和积累的经验融入智能AI代理的操作行为中。例如，一些先前的工作修改模型参数以实现通过更新[325,326,327]或遗忘特定知识[328]来实现持续学习。其他研究将LLMs视为独立的记忆模块，在预训练[329]、后训练[330]和在线部署[331]期间将世界知识纳入其参数中。例如，MemoryLLM[265]引入了记忆令牌，而SELF-PARAM[266]利用知识蒸馏将世界知识和过去的AI代理经验嵌入模型参数中。这种方法在$\\mathbf{M}+$模型[332]中进一步增强了长期记忆机制和协同训练的检索器，提高了其对更长历史记忆的泛化能力。此外，[33]利用编码记忆来促进进一步推理，从而提高存储知识的泛化能力。最近，MemoRAG[267]和${\\tt R}^{3}$ Mem [270]被提出，不仅编码记忆，还能够从神经记忆网络中可靠检索，将记忆存储和检索的双重过程统一到一个模型中。这一进展有助于开发支持终身AI应用的下一代基于生成的检索系统。此外，Titans[269]通过元学习引入了记忆测试数据点的能力，从而实现更高效的测试时跨任务泛化。"
            },
            {
              "type": "text",
              "content": "Future research willcontinue tofocus on creating larger capacity and more stable neural memory models.Concuently, developing more effcient andflexible memory read-write mechanisms willbecrucial.Acriticalarea of investigation willinvolve applying these memory-augmented networks tocomplexcognitive tasks,pushing the boundaries of what AI can achieve.Progress in this domain willunlock new possibilities for building intelligent agents that can learn, remember, and reason in a manner that is increasingly reminiscent of human cognition.",
              "index": 4,
              "part": 0,
              "translated_content": "未来的研究将继续专注于创建更大容量和更稳定的神经记忆模型。因此，开发更高效、更灵活的记忆读写机制将至关重要。一个关键的研究领域将涉及将这些记忆增强网络应用于复杂的认知任务，推动人工智能能够实现的边界。在这一领域的进展将为构建能够学习、记忆和推理的智能代理打开新的可能性，这种方式越来越类似于人类认知。"
            }
          ],
          "raw_title": "Neural Memory Networks",
          "type": null,
          "children": [],
          "translated_title": "3.4.5 神经记忆网络"
        },
        {
          "title": "3.4.6 Memory Utilization",
          "number": "3.4.6",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Acriticalaspect of agent design lies in memory utilization, which focuses on maximizing the value of stored memory segmentsforthecurrent task.Thecoreobjective is toapplythese memorieseectivelyand appropriatelytoenhance reasoning,decision-making,planningandaction generation,ultimatelyboostingtheagent'sperformanceandefficiency while avoiding the pitfalls ofirrelevant orincorrect memory interference.Achieving this,however,presents several challenges.\n\nOne primarychallenge is balancingthe vastnessofthe memory store withits effective utilization.Agents must navigate a potentialinformation overload,ensuring thatrelevant memoriesare fullleveraged without overwhelming the system. Anotherhurdle is theneedforabstractionand generalization.Agents need to distil specificmemory segments intomore general knowledge and apply thisknowledge tonewandvaried situations.Furthermore,the issue of hallucinations and incorrect memories withintheLLMrequirescareful consideration.Preventingthe generation ofcontentthatcontradicts or misrepresents storedinformationiscrucial,as is theability toidentify andrectifyerroneous information that may reside within the memory store itself.",
              "index": 0,
              "part": 0,
              "translated_content": "智能代理设计的一个关键方面在于记忆利用，重点是最大化当前任务的存储记忆段的价值。其核心目标是有效和适当地应用这些记忆，以增强推理、决策、规划和行动生成，最终提升代理的性能和效率，同时避免无关或错误记忆干扰的问题。然而，实现这一目标面临着几个挑战。\n\n一个主要挑战是平衡庞大的记忆存储量与其有效利用。代理必须应对潜在的信息过载，确保充分利用相关记忆，而不会使系统不堪重负。另一个障碍是抽象化和泛化的需求。代理需要将特定记忆段提炼为更一般的知识，并将这些知识应用于新的和多样化的情境。此外，LLM中幻觉和错误记忆的问题需要认真考虑。防止生成与存储信息相矛盾或歪曲的内容至关重要，同样重要的是能够识别和纠正存储库中可能存在的错误信息。"
            },
            {
              "type": "text",
              "content": "To addressthese challenges,several strategies are employed.Retrieval-augmented generation(RAG)[334]combines retrieval and generation models to enhance the LLM's capabilities by drawing upon external knowledge sources. Unlike the methods mentioned in memory retrieval and matching, RAGfocuses on integrating retrieved information into the generation process itself.When prompted, the agent retrieves relevant memory segments and incorporates them into the context providedbythe generation model.This contextual enrichment guides the model towards more factual and informative outputs.Forinstance, when responding toauser's query,the agent canfirst retrieve related entries from its knowledge base andthen generate an answer based on this retrieved information, thus grounding the response in established knowledge. More recently,some studies have integrated memory modules with RAG, incorporating self-reflection[274]andadaptive retrieval mechanisms[272] to enhance both generation reliability and efficiency. For example, Atlas [273]leverages causal mediation analysis, while[284] employs consistency-based hallcination detection to determine whether the model already possesses the necessary knowledge—allowing for direct generation-or whetherretrievalis required, in whichcase the modelfirstretrieves relevant information before generating aresponse.In aunifiedframework,RAGLAB[271]offers acomprehensive ecosystem for evaluating and analyzing mainstream RAG algorithms. HippoRAG [22] employs a strategy inspired by the hippocampal indexing theory of human memory tocreatea KG-based index for memory and use Personalized PageRank for memory retrieval.",
              "index": 1,
              "part": 0,
              "translated_content": "为了解决这些挑战，采用了几种策略。检索增强生成（RAG）[334]结合了检索和生成模型，通过利用外部知识源增强LLM的能力。与记忆检索和匹配中提到的方法不同，RAG专注于将检索到的信息整合到生成过程中。当受提示时，代理检索相关的记忆段，并将它们融入到生成模型提供的上下文中。这种上下文丰富可以引导模型产生更加真实和信息丰富的输出。例如，在回答用户查询时，代理可以首先从其知识库中检索相关条目，然后基于这些检索到的信息生成答案，从而将回应基于已建立的知识。最近，一些研究将记忆模块与RAG相结合，整合了自我反思[274]和自适应检索机制[272]，以提高生成的可靠性和效率。例如，Atlas [273]利用因果中介分析，而[284]采用基于一致性的幻觉检测来确定模型是否已具备必要知识——允许直接生成——或者是否需要检索，在这种情况下，模型首先检索相关信息然后生成响应。在一个统一的框架中，RAGLAB[271]提供了一个全面的生态系统，用于评估和分析主流的RAG算法。HippoRAG [22]采用了受人类记忆海马体索引理论启发的策略，为记忆创建了基于知识图的索引，并使用个性化PageRank进行记忆检索。"
            },
            {
              "type": "text",
              "content": "Furthermore,long-context modeling plays avitalrole in managing extensive memory stores.This approach enhances the LLM's abilitytoprocesslong sequences andlage-scalememories,allowing foradeeper understandingand utilizationof long-range dependencies.By employing Transformer model variants like Transformer-XL[324] and Longformer[335], or throughhierarchical andrecursive processing techniques,such asrecurrent memory transformer (RMT)[275,276] agents can expand their context window.This enables them to handle significantly more extensive memory stores and reason and make decisions within a much broader context.Forexample,agentscan maintain alonger memory span when processing extensivedocuments or engaging in prolonged conversations.Additionally,somestudies leverage memory to compress long contexts,enabling more effective long-context modeling. For example, AutoCompressor [277] introduces summary vectors as memory totransfer informationfrom previous context windows intothecurrent window, facilitating long-context understanding.Similarly, the in-context autoencoder (ICAE)[278] generates memory slots that accurately and comprehensivelyrepresent theoriginalcontext, while LLMLingua[336,337], Gist[279],and CompAct [280] further optimize long-prompt compression to reduce input context length.",
              "index": 2,
              "part": 0,
              "translated_content": "此外，长上下文建模在管理大规模记忆存储中发挥着至关重要的作用。这种方法增强了LLM处理长序列和大规模记忆的能力，使其能够更深入地理解和利用长距离依赖关系。通过采用Transformer模型变体，如Transformer-XL和Longformer，或通过分层和递归处理技术，如循环记忆变换器(RMT)，代理可以扩展其上下文窗口。这使它们能够处理更广泛的记忆存储，并在更广泛的背景下进行推理和决策。例如，当处理大量文档或进行长时间对话时，代理可以保持更长的记忆跨度。此外，一些研究利用记忆来压缩长上下文，实现更有效的长上下文建模。例如，AutoCompressor将摘要向量引入记忆，将信息从先前的上下文窗口传输到当前窗口，促进长上下文理解。类似地，上下文自编码器(ICAE)生成准确全面地表示原始上下文的记忆槽，而LLMLingua、Gist和CompAct进一步优化长提示压缩，以减少输入上下文长度。"
            },
            {
              "type": "text",
              "content": "Finally,hallucination mitigation strategiesare essentialfor ensuring the reliability of generated outputs.These strategies aim to minimize the LLM's tendency to produce factually incorrct or nonsensicalcontent.One approachis implementing fact-checking mechanisms [338],verifing generated content against established knowledge or memory stores.Another involves uncertainty estimation[339,340],where the model evaluates the confidence level of its generatedcontentandflagsorfiltersoutlow-confidenceoutputs.Aditionallknowledge-baseddecoding strategiescan be employed during the generation phase,introducing constraints that guide the modeltowards more factuall accurate content.These techniques colectivelycontribute to generating more trustworthy outputs and aligned withthe agent's established knowledge base.Recent research has introduced expert memory subnetworks,such as PEER[283] and Lamini Memory Tuning [281],which specialize in memorizing specific types of information,including worldknowledge and AI agents\"past experiences.These subnetworks offload memorization to dedicated parameters,reducing the main model's propensity to hallucinate.By implementing these memory utilization strategies,agents can become more capable,accurate,and reliable.Theycan successfully leverage their memory stores to achieve superior performance across complex tasks.",
              "index": 3,
              "part": 0,
              "translated_content": "最后，减少虚构策略对于确保生成输出的可靠性至关重要。这些策略旨在最小化LLM产生事实错误或荒谬内容的倾向。一种方法是实施事实核查机制，根据已建立的知识或记忆存储验证生成的内容。另一种方法涉及不确定性估计，模型评估生成内容的置信水平，并标记或过滤出置信度较低的输出。此外，在生成阶段可以采用基于知识的解码策略，引入约束来引导模型生成更准确的内容。这些技术共同有助于生成更值得信赖且与代理已建立的知识库保持一致的输出。最近的研究引入了专家记忆子网络，例如PEER和Lamini Memory Tuning，专门用于记忆特定类型的信息，包括世界知识和AI代理的过去经验。这些子网络将记忆工作转移到专用参数，减少主模型产生虚构的倾向。通过实施这些记忆利用策略，代理可以变得更有能力、准确和可靠。它们可以成功利用其记忆存储在复杂任务中实现卓越性能。"
            }
          ],
          "raw_title": "Memory Utilization",
          "type": null,
          "children": [],
          "translated_title": "3.4.6 内存利用率"
        }
      ],
      "translated_title": "3.4 记忆生命周期"
    },
    {
      "title": "3.5 Summary and Discussion",
      "number": "3.5",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The development of truly inteligent agents depends not just on robust memory systems,but alsoon their seamless integration withother cognitive functions like perception,planning,reasoning,and action selection.Memory is not an isolated module; it is deeply intertwined withthese other processes.Forexample,sensory input is encoded and filtered before storage(asdiscussed inthe sectionson memoryrepresentation andlifecycle),highlightingthe interplay between perception and memory. Long-term memory,especially procedural memory,directly informs action selection through learned skills androutines.Retrieval mechanisms, likecontext-awaresemantic similaritycomputation,arecrucial for planning,allowing agents toaccess relevant pastexperiences.This interplayextends totheconceptofa“world model?\"",
          "index": 0,
          "part": 0,
          "translated_content": "真正智能代理的发展不仅仅取决于强大的记忆系统，还取决于它们与感知、规划、推理和行动选择等其他认知功能的无缝整合。记忆不是一个孤立的模块；它与这些其他过程密切相关。例如，感官输入在存储之前被编码和过滤（如在有关记忆表示和生命周期的部分中所讨论的），突显了感知和记忆之间的相互作用。长期记忆，特别是程序性记忆，通过学习的技能和例行程序直接影响行动选择。检索机制，如上下文感知的语义相似度计算，对规划至关重要，使代理能够访问相关的过去经验。这种相互作用延伸到“世界模型”的概念中。"
        },
        {
          "type": "text",
          "content": "Centralto intellgent agents is their abilityto build and utilize internal world models.These models,representing an agent's understanding ofits environment,enable simulation,reasoning aboutconsequences,and prediction. Robust worldmodelsarecrucialforhigher-levelcognition,planning,andhuman-like intelligence.Aworldmodelis,inessence, a highly structured, often predictive,form of long-term memory. Memory provides the raw material—knowledge and experiences-forconstructing the world model, whilethe world model, inturnacts as anorganizing framework, influencing how new memories are encoded,consolidated,andretrieved.For instance,a welldeveloped world model might prioritize storing surprising events, as these indicate gaps in the agent's understanding.",
          "index": 1,
          "part": 0,
          "translated_content": "智能代理的核心是它们建立和利用内部世界模型的能力。这些模型代表了代理对环境的理解，可以进行模拟、推理和预测。健壮的世界模型对于高层认知、规划和类人智能至关重要。世界模型本质上是一种高度结构化的、通常是预测性的长期记忆形式。记忆为构建世界模型提供了原始材料——知识和经验，而世界模型反过来作为一个组织框架，影响新记忆的编码、巩固和检索。例如，一个完善的世界模型可能会优先存储令人惊讶的事件，因为这些事件表明代理的理解存在空白。"
        },
        {
          "type": "text",
          "content": "However, developing efective world models and memory systems presents significant challenges.These include managing thecomplexity of real-world environments, determining the appropriate level of abstraction (balancing accuracy,complexity,and computational effciency),and integrating multi-modalinformation.Learning and updating these models effciently,avoiding bias,ensuring generalization, andenabling continuous adaptation are alsocritical. Furthermore, model-based planning requires effcient search algorithms tohandle the inherentuncertainty inthe model's predictions.",
          "index": 2,
          "part": 0,
          "translated_content": "然而，发展有效的世界模型和记忆系统面临着重大挑战。这些挑战包括管理现实世界环境的复杂性，确定适当的抽象层次（平衡准确性、复杂性和计算效率），以及整合多模态信息。高效地学习和更新这些模型，避免偏见，确保泛化，并实现持续适应也至关重要。此外，基于模型的规划需要高效的搜索算法来处理模型预测中固有的不确定性。"
        },
        {
          "type": "text",
          "content": "Future research should focus on enhancing agent memory systems by drawing inspiration from the strengths of human memory,particularlyitsflexibilityadaptability,andeficiency.While agent memoryhasadvancedconsiderably itsill lags behnd human memory in these key areas.Human memory is remarkably associative,retrieving information from incomplete or noisycues,andit exhibits asophisticatedformof“forgetting\"that involvesconsolidationandabstraction, prioritizing relevant information and generalizing from experiences.Agent memory,conversely,often relieson precise matching and struggles with ambiguity.",
          "index": 3,
          "part": 0,
          "translated_content": "未来的研究应重点关注通过借鉴人类记忆的优势，特别是其灵活性、适应性和效率，来增强智能体记忆系统。虽然智能体的记忆系统已经取得了相当大的进展，但在这些关键领域仍远远落后于人类记忆。人类记忆具有非凡的联想能力，可以从不完整或嘈杂的线索中检索信息，并展现出一种复杂的“遗忘”形式，包括信息的巩固和抽象化，优先考虑相关信息并从经验中概括。相反，智能体的记忆系统通常依赖于精确匹配，并且在处理模糊性时存在困难。"
        },
        {
          "type": "text",
          "content": "Several promising research directions emerge.Exploring biologicaly-inspired mechanisms,such as neural memory networks (as discused earlier),could lead to significant breakthroughs.Another crucial area is developing memory system thatactively“curate\"their contents—reflecting oninformation,identifying inconsistenciesand synthesizing new knowledge.This requires integrating metacognitive capabilities (monitoring and controlling one's own cognitive processes)intoagentarchitectures.Furthermore,creating morerobust andnuancedforms ofepisodic memory,capturing not justthe“what\"and“when\"but alsothe“why\"andtheemotionalcontextofevents,isessentialforagents thatcan truly learn from experience and interact with humans naturaly.",
          "index": 4,
          "part": 0,
          "translated_content": "出现了几个有前途的研究方向。探索受生物启发的机制，比如神经记忆网络（如前所讨论的），可能会带来重大突破。另一个至关重要的领域是开发能够积极“策展”其内容的记忆系统——反思信息，识别不一致之处并综合新知识。这需要将元认知能力（监控和控制自身认知过程）整合到智能体架构中。此外，创建更加健壮和细致的情景记忆形式，捕捉事件的“什么”、“何时”以及“为什么”和情感背景，对于能够真正从经验中学习并自然与人类互动的智能体是至关重要的。"
        },
        {
          "type": "text",
          "content": "Overcoming these challenges requires innovative solutions attheintersection of deeplearning,reinforcement learning, and cognitive science.Developing more sophisticated and adaptable world models and memory systems—ones that mirror the strengthsof human cognition-willpave the way for agentswithadeeper understanding of their environment, leading to more intelligent and meaningful interactions.",
          "index": 5,
          "part": 0,
          "translated_content": "要克服这些挑战，需要在深度学习、强化学习和认知科学交叉领域提出创新解决方案。开发更复杂、更适应的世界模型和记忆系统——这些系统应当反映人类认知的优势，将为拥有更深入了解环境的智能体铺平道路，从而实现更智能、更有意义的互动。"
        }
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": [],
      "translated_title": "3.5 总结与讨论"
    },
    {
      "title": "World Model",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "A worldmodelenables an agent to predict and reasonabout future states without direct trial-and-error in reality.This sectionexplores how human cognitive studies on“mental models\"relate to AI world models in artificial intelligence, categorizing them under four paradigms: implicit paradigm,explicit paradigm,simulator-based paradigm,and aclass of other emergent methods (e.g.,instruction-driven paradigm).We then discusshow world models inherently intersect withother agenticcomponents and conclude with open questions andfuture directions that unite these perspectives under a unified theoretical and practical framework.",
          "index": 0,
          "part": 0,
          "translated_content": "世界模型使代理能够在没有直接现实试错的情况下预测和推理未来状态。本节探讨了人类认知研究中关于“心理模型”的内容如何与人工智能中的世界模型相关联，将其归类为四种范式：隐式范式、显式范式、基于模拟器的范式以及一类其他新兴方法（例如，基于指导的范式）。然后，我们讨论了世界模型如何与其他代理组件固有地交汇，并最终得出了统一的理论和实践框架下将这些观点联系起来的开放问题和未来方向。"
        },
        {
          "type": "figure",
          "src": "images/99853a0650701eb5ba0c82121b8d7a944c60b08d96c14ca2711f3801c8a92a9e.jpg",
          "alt": "",
          "caption": "Figure 4.l:Humanscan use theirbrain's modelof theworld topredict theconsequences of their actions.For example when playing table tennis, a player can imagine or predict the trajectory of the ball after an action.",
          "index": 1,
          "part": 0,
          "translated_caption": "图4.1：人类可以利用大脑对世界的模型来预测其行为的后果。例如，在打乒乓球时，球员可以想象或预测动作后球的轨迹。"
        }
      ],
      "raw_title": "World Model",
      "type": null,
      "children": [],
      "translated_title": "世界模型"
    },
    {
      "title": "Reward",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "figure",
          "src": "images/480dd79050d800b8a19fd2423745a169cd9b8afb33877b15013a39bd5d4b0283.jpg",
          "alt": "",
          "caption": "Figure 5.1: Illustrative Taxonomy of Reward system",
          "index": 0,
          "part": 0,
          "translated_caption": "图5.1：奖励系统的示意分类"
        },
        {
          "type": "text",
          "content": "Rewards help the agent distinguish between beneficial and detrimental actions,shaping its learning process and influencing its decision-making.This chapter first introduces common reward substances inthe human body and the corresponding reward pathways.Then, thereward paradigm under the agent and the diferent methods involved are defined.In thediscussion section, the influencerelationship betweenother modules is described, and the existing methods aresummarized,then the problems that need to be solved inthefuture andthe optimization directions are discussed.",
          "index": 1,
          "part": 0,
          "translated_content": "奖励有助于代理区分有益和有害的行为，塑造其学习过程并影响其决策。本章首先介绍了人体中常见的奖励物质以及相应的奖励途径。然后，定义了代理下的奖励范式和涉及的不同方法。在讨论部分，描述了其他模块之间的影响关系，并总结了现有方法，然后讨论了未来需要解决的问题和优化方向。"
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td>Reward Pathway</td><td>Neurotransmitter Mechanism</td><td></td></tr><tr><td>Mesolimbic path- way [406]</td><td> Dopamine</td><td>Dopaminergic neurons in the ventral tegmental area (VTA) extend pro- jections to the nucleus accumbens, where they release dopamine to regulate reward-related signaling. Dopamine diffuses across the synaptic cleft and binds to dopamine receptors—primarily D1-like (excitatory via Gs proteins, increasing cAMP) and D2-like (inhibitory via Gi pro- teins, reducing cAMP)—thereby modulating reward, motivation, and</td></tr><tr><td>Mesocortical path-Dopamine way [407]</td><td></td><td>reinforcement. Dopaminergic projections from the VTA reach the prefrontal cortex (PFC). Here, dopamine binds to its receptors to influence cognitive functions such as decision-making, working memory, and emotional</td></tr><tr><td>Nigrostriatal path- Dopamine way [407]</td><td></td><td>regulation, all of which contribute to evaluating and anticipating rewards. Dopamine's action on D1 and D2 receptors in the striatum helps shape both motor routines and reward-related behaviors.</td></tr><tr><td>Locus coeruleus [408]</td><td> Norepinephrine</td><td>Neurons in the locus coeruleus release norepinephrine to widely dis- tributed targets across the brain. At synapses, norepinephrine binds to adrenergic receptors(α and β subtypes), modulating neuronal excitabil- ity, arousal, attention, and stress responses. These modulatory effects</td></tr><tr><td>Glutamatergic pro- jection [409]</td><td>Glutamate</td><td>can indirectly influence reward processing and decision-making circuits. Upon releasing into the synaptic cleft, glutamate binds to both ionotropic receptors (such as AMPA and NMDA receptors) and metabotropic re- ceptors located on the postsynaptic neuron, thereby initiating excitatory signaling. This binding produces excitatory postsynaptic potentials and</td></tr><tr><td>GABAergic modu-  Gamma- lation [410]</td><td>Aminobutyric Acid (GABA)</td><td>is crucial for synaptic plasticity and learning within reward circuits. GABA serves as the principal inhibitory neurotransmitter. At the synapse, GABA binds to GABAA receptors and GABAB receptors. This binding results in hyperpolarization of the postsynaptic cell, thereby providing inhibitory regulation that balances excitatory signals in the reward net- work.</td></tr></table></body></html>",
          "caption": "Table 5.1: The comparison of human common reward pathways",
          "index": 2,
          "part": 0,
          "translated_caption": "表5.1：人类常见奖励路径的比较"
        }
      ],
      "raw_title": "Reward",
      "type": null,
      "children": [],
      "translated_title": "奖励"
    },
    {
      "title": "Emotion Modeling",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Emotions are a key part ofhow humans think, make decisions,and interact with others.They guide us tounderstand situations,makechoices,and buildrelationships.Antonio Damasio,inhis book Descartes'Error[25],explained that emotions are not separate fromlogic.Instead,theyaredeeplyconnected to how we reason and act.When developing LLM agents, adding emotional capabilities can potentially make these systems smarter, more adaptable,and better understand the world around them.\n\nFor LLMagents,emotions canactas adecision-making tool, much likethey do for humans.Emotions help us prioritize tasks,understand risks, and adapt to new chalenges.Marvin Minsky,in The Emotion Machine[420], described emotions asa way toadjust our thinking processes,helping us solve problems inamore flexible andcreative manner. Similarly,LLMagents withemotion-likefeatures couldimprove their abilityof solvingcomplex problems and making decisions in a more human-style.\n\nHowever, the integration ofemotions into LLM agents is stillin its early stages.Researchers are just starting to explore how emotional capabilitiescan improve these systems.Furthermore, there is great potential for LLM agents to support human emotional wellbeing,whether through empatheticconversations, mentalhealth support, or simply building beter connections with users.This promising butchallenging arearequires collaboration between fields such as psychology,cognitive science,and AI ethics.As research advances,emotion-understanding LLM agents could redefine how we interact with technology,creating deeper trust and more meaningfulrelationships betweenhumans and machines.",
          "index": 0,
          "part": 0,
          "translated_content": "情感是人类思考、做出决策和与他人互动的关键部分。它们指导我们理解情况、做出选择和建立关系。安东尼奥·达马西奥在他的著作《笛卡尔的错误》中解释说，情感并非与逻辑分离，而是与我们推理和行动的方式深深相连。在开发LLM代理时，增加情感能力可能使这些系统变得更智能、更适应，并更好地理解周围的世界。\n\n对于LLM代理，情感可以作为决策工具，就像它们对人类一样。情感帮助我们优先处理任务、理解风险并适应新挑战。马文·明斯基在《情感机器》中描述情感是调整我们思维过程的一种方式，帮助我们以更灵活和创造性的方式解决问题。类似地，具有类似情感特征的LLM代理可以提高它们解决复杂问题和做出决策的能力，更贴近人类风格。\n\n然而，将情感整合到LLM代理中仍处于早期阶段。研究人员刚刚开始探索情感能力如何改善这些系统。此外，LLM代理有巨大潜力支持人类的情感福祉，无论是通过共情对话、心理健康支持，还是简单地与用户建立更好的联系。这一充满希望但具有挑战性的领域需要心理学、认知科学和人工智能伦理等领域之间的合作。随着研究的进展，理解情感的LLM代理可能重新定义我们与技术的互动方式，为人类与机器之间建立更深层的信任和更有意义的关系。"
        },
        {
          "type": "text",
          "content": "In the following subsections, we willdelve deeper into the roleofemotions in shaping LLMagents.We willexplore how emotions can be used to enhance learning and adaptability,how LLMs understand human emotions, and how these systems expressand modeltheirown emotional states. We willalso examine how emotions can be manipulated to influence LLMagents'behaviorandpersonalities,as wellas theethicaland safetyconcerns that arise fromthese capabilities.Eachof these discussions builds onthe foundationalimportance ofemotion tocreateLLMagents that are more intelligent, empathetic, and aligned with human values.",
          "index": 1,
          "part": 0,
          "translated_content": "在接下来的小节中，我们将更深入地探讨情感在塑造LLM代理中的作用。我们将探讨情感如何被用来增强学习和适应能力，LLM如何理解人类情感，以及这些系统如何表达和建模它们自己的情感状态。我们还将研究如何通过操纵情感来影响LLM代理的行为和个性，以及由此产生的伦理和安全问题。这些讨论都基于情感对于创造更智能、有同理心且符合人类价值观的LLM代理的基础重要性。"
        }
      ],
      "raw_title": "Emotion Modeling",
      "type": null,
      "children": [],
      "translated_title": "情感建模"
    },
    {
      "title": "Perception",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Perception is thefoundationalgateway through which both humans andintelligent agentsacquire information,interpret their surroundings, and ultimately make informed decisions. For humans, perception is seamlessand intuitive, eortlesslytransforming sensory inputs into meaningfulinterpretations.In artificialintellgence,however, perception systems are meticulously engineered toemulate—and insome respects surpass-human sensory processing, profoundly influencing an agent's capacity for interaction, learning, and adaptation in complex environments.",
          "index": 0,
          "part": 0,
          "translated_content": "感知是人类和智能代理获取信息、解释周围环境并最终做出明智决策的基础门户。对于人类来说，感知是无缝和直观的，毫不费力地将感官输入转化为有意义的解释。然而，在人工智能领域，感知系统经过精心设计，旨在模拟并在某些方面超越人类的感知处理，深刻影响代理在复杂环境中的互动、学习和适应能力。"
        },
        {
          "type": "text",
          "content": "In this chapter,we begin byexploringkeydiffrences inthe nature andeffciencyofperception between humans and AI agents.Next,wecategorize agent perceptionbased on diferentforms andrepresentations of perceptualinput.We then discuss ongoingchallenges intheagent perceptionsystemandhighlight promising directions forimprovement,both at the modeling andsystemarchitecturelevels.Finally,weillustratehow perception modulescanbeefectivelytailoredto different intellgent agent scenariosoferingpractical guidanceforoptimizing theiruseandsuggesting pivotalareasfor future research.",
          "index": 1,
          "part": 0,
          "translated_content": "在本章中，我们首先探讨人类和人工智能代理在感知性质和效率方面的关键差异。接下来，我们基于不同形式和感知输入的表示对代理感知进行分类。然后，我们讨论代理感知系统中持续存在的挑战，并突出在建模和系统架构层面改进的有希望方向。最后，我们阐明了感知模块如何有效地定制给不同智能代理场景，为优化它们的使用提供实用指导，并提出了未来研究的关键领域。"
        }
      ],
      "raw_title": "Perception",
      "type": null,
      "children": [],
      "translated_title": "感知"
    },
    {
      "title": "Action Systems",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "In the realmofphilosophy,actionis definedas the behaviors that agentscanperformforapotentialorspecificpurpose in the environment.For example, manipulation, moving,reasoning, and tool utilization can allbe considered as fundamntalactions that an intelligent agent can execute to fulfilla goal inreal-world scenarios. In other words, actions emerge fromthe goal-orientedengagementof an agent inits environmentreflectingits intenttotransformthe externalworld in pursuitof its goals.Therefore,theaction system alsoplaysavitalrole indifferentiatingAIagents and foundation models (e.g.,LLMs). Generally,existing foundation models have demonstrated impresive performance across varioustasks,buttheirtaskscopeisstilimitedastheypredominantlyreliesontheriginalpre-trainingjective (e.g.,next-token prediction). By serving foundation models as brain inteligence, AI agents equipped with action systemscan directly engage with their environment and execute complex user intent. Moreover, action systems can support agents toutilize availabletools fromexternalenvironments,thus significantlyextending agents'task scopes. Therefore,thedesignofaction systems willalsodetermine the capabilityof AIagents in perception,decision making, execution,toolutilization, and anyothercomponents toalignwith the human brain.Inother words,foundation models lay the groundwork for agents while action systems determine their ultimate potential to achieve complex targets. Designingan effective and comprehensiveaction system for AIagents is acritical endeavor that involves significant challenges and notable benefits. In Figure 8.1, we demonstrate the execution processof the action system in the cognition system. Inthis section, we willfirstdiscussthe human action system inSection 8.1, and then examinethe transition fromhumanaction toagenticaction inAIagents in Section 8.2.After that,wewillsystematicallsummarize the paradigms of existing action systems in AIagents, including action space,action learning, and toollearning,in Section 8.3.In Section 8.4we analyze thediferences between action andperception, and finally wesummarize the conclusion in Section 8.5.",
          "index": 0,
          "part": 0,
          "translated_content": "在哲学领域，行动被定义为代理人在环境中为潜在或特定目的而执行的行为。例如，操纵、移动、推理和工具利用都可以被视为智能代理在真实场景中执行以实现目标的基本行动。换句话说，行动源自代理在环境中以目标为导向的参与，反映其意图改变外部世界以追求目标。因此，行动系统在区分人工智能代理和基础模型（如LLMs）方面也起着至关重要的作用。通常，现有的基础模型在各种任务中表现出色，但它们的任务范围仍然有限，因为它们主要依赖于原始的预训练目标（例如，下一个标记的预测）。通过将基础模型作为大脑智能，配备有行动系统的人工智能代理可以直接与其环境互动，并执行复杂的用户意图。此外，行动系统可以支持代理利用外部环境中的可用工具，从而显著扩展代理的任务范围。因此，行动系统的设计也将决定人工智能代理在知觉、决策制定、执行、工具利用以及与人类大脑一致的任何其他组件方面的能力。换句话说，基础模型奠定了代理的基础，而行动系统决定了它们实现复杂目标的最终潜力。为人工智能代理设计一套有效而全面的行动系统是一项至关重要的工作，涉及重大挑战和显著收益。在图8.1中，我们展示了认知系统中行动系统的执行过程。在本节中，我们将首先讨论8.1节中的人类行动系统，然后在8.2节中审视从人类行动到人工智能代理中的行动转变。之后，我们将在8.3节系统地总结人工智能代理中现有行动系统的范式，包括行动空间、行动学习和工具学习。在8.4节中分析行动与感知之间的差异，最后在8.5节总结结论。"
        },
        {
          "type": "figure",
          "src": "images/ec4a8db596d668bcb99e2499621a82cdad3204db43c7f5979620dec123e498ac.jpg",
          "alt": "",
          "caption": "Figure 8.1: Ilustration of several concepts related to action and action execution.",
          "index": 1,
          "part": 0,
          "translated_caption": "图8.1：涉及动作和动作执行的若干概念的示意图。"
        }
      ],
      "raw_title": "Action Systems",
      "type": null,
      "children": [],
      "translated_title": "行动系统"
    },
    {
      "title": "8.1 The Human Action System",
      "number": "8.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Action system inhumancognition refers tothe processes that allow humans to perceive,plan,andexecute goal-directed actions. It isa complex system that enables individuals to interact witha dynamic environment, make decisions, and adapt their behavior based on feedback. Generaly,the action system within human cognition could be broadly categorized as mental action and physical action:\n\n·Menalactioncan beviewed as akind of distinct action,which is formulated as athinking processto drive the final intention in the human brain.For example,reasoning,decision making,imagining, and planning can allbe considered as various types ofmentalaction.Inother words,mentalactions are equaltoa brain signal that drives the physical actions of humans to fulfill the final objective.\n\n·Physicalaction refers to any goal-directed bodily movement executed bythehumanmotor system.To some extent, physicalactions are usualyexpressedasakind ofcontinuous action.Forexample,speaking,manipulating,drawing, running,and grasping can allbe regarded as physical actions.Employing asequence of physicalactions,humans can conduct the interaction and collect feedback from real-world environments.",
          "index": 0,
          "part": 0,
          "translated_content": "人类认知中的行动系统指的是使人类能够感知、规划和执行目标导向行动的过程。这是一个复杂的系统，使个体能够与动态环境互动，做出决策，并根据反馈调整行为。一般来说，人类认知中的行动系统可以广泛分类为心理行动和物理行动：\n\n- 心理行动可以被视为一种独特的行动，它被构想为推动人脑中最终意图的思考过程。例如，推理、决策、想象和规划都可以被视为各种类型的心理行动。换句话说，心理行动相当于驱动人类的物理行动以实现最终目标的一种脑信号。\n\n- 物理行动指的是人类运动系统执行的任何目标导向的身体运动。在某种程度上，物理行动通常被表达为一种连续的行动。例如，说话、操作、绘画、奔跑和抓取都可以被视为物理行动。通过一系列的物理行动，人类可以进行互动并从现实世界环境中获取反馈。"
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td>Model</td><td>Examples</td><td>Inputs</td><td>Objective</td><td>Definition</td></tr><tr><td>Large Language Model (LLM)</td><td>GPT-4 [7]</td><td>Language</td><td>Next-Token Prediction</td><td>LLMisetextasdhe</td></tr><tr><td>Large Multimodal Model (LMM)</td><td>LLaVA [513]</td><td>Multi-modal</td><td>Multi-modal Generation</td><td>LMM s ta inperate multimoal data based on</td></tr><tr><td>Robotic Foundation Model (RFM)</td><td>RT-1 [522]</td><td>Sensory inputs</td><td>Robotic Control</td><td>RFM is to generate robotic control based on the sensory inputs from dynamic environments.</td></tr><tr><td>Large Action Model (LAM)</td><td>LAM[622]</td><td> Interoniment</td><td>Executable Action</td><td>LAM is toctgensrwithin ecutabl roction based on</td></tr></table></body></html>",
          "caption": "Table 8.1: Definitions between different kinds of foundation models.",
          "index": 1,
          "part": 0,
          "translated_caption": "表8.1：不同种类基础模型之间的定义。"
        },
        {
          "type": "figure",
          "src": "images/26302eddc891aacfa6972e26865c28ed8fc3e85c0dd6403b0cf5b7505f46a2d7.jpg",
          "alt": "",
          "caption": "Figure 8.2: Ilustrative Taxonomy of Human Actions, showing both mental and physical facets.",
          "index": 2,
          "part": 0,
          "translated_caption": "图8.2: 人类行为的分类示意图，显示了精神和身体两个方面。"
        }
      ],
      "raw_title": "The Human Action System",
      "type": null,
      "children": [],
      "translated_title": "8.1 人类行为系统"
    },
    {
      "title": "8.2 From Human Action to Agentic Action",
      "number": "8.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "In the pastlong period of time,human action systems[623]have significantly motivatedus to shape thedevelopment of a computer system toward autonomous paradigms.The action mechanism plays acritical role in thehuman brain in driving goal-directed behavior. In an intellgent human brain[624],conscious and unconscious thinking signals are produced,converted into mental signals,which eventually leadto asequence of action operations.This process can be mapped asa multi-stage pipeline that involves constructing action spaces,formulating learning mechanisms for improved decision making,and integrating external states (e.g.tols).Inspired bythese principles,we discoverthat these designs are essential to formulate the prototype of AI agent.",
          "index": 0,
          "part": 0,
          "translated_content": "在过去很长一段时间中，人类行动系统显著地激励我们塑造计算机系统朝向自主范式的发展。行动机制在人类大脑中发挥着关键作用，推动目标导向行为。在智能的人类大脑中，产生意识和无意识思维信号，这些信号被转化为心理信号，最终导致一系列行动操作。这一过程可以被映射为一个多阶段管道，涉及构建行动空间、制定用于改进决策的学习机制，并整合外部状态（例如工具）。受这些原则启发，我们发现这些设计对于构建AI代理的原型至关重要。"
        },
        {
          "type": "text",
          "content": "Many existing frameworks incorporate action learning into their design or utilize it as an output.To clarify the definition of an action system,we highlight the distinctions among various frameworks, including large language models (LLM),large multi-modal models (LMM), robotic foundation models (RFM), andlarge action models (LAM), as shown in Table 8.1. Specifically, an LLM is to produce language output based on provided prompts, while an LMM is to generate multi-modality artifacts based on the multi-modal inputs.Existing language-based or digital AI agent frameworks are built upon these foundation models (e.g.,LLMor LMM) via predefining the scope of action space and its learning strategies.On the other hand,an RFM is tooptimize roboticcontrol basedon real-world environments (e.g.robotic video).Existing RFMs are pre-trained from web-scale video dataand use video prediction to simulate the action ofroboticcontrol.Thecore of RFMisstilltouse thegenerativeobjective tolearn knowledge from large-scale data, although it has involved some action designs in building physical AIagents.Moreover,some recent works[622]introduce theconceptof largeaction model(LAM),whichfurtherhighlights the stage to generate the action strategies,interact withreal-world environments andenhance self-learning paradigm.Fromthese definitions, we notice that,regardlessofthefoundational models employed,thecore of action system is tobuildthe interaction withtheenvironment and then enable the learning process from thecolected action trajectories via pre-definedreward functions.Specificall,the mechanisms underlying these behaviors are also similarto the action system in human cognition, offering valuable insights for designing action systems in AI agent frameworks. For example:",
          "index": 1,
          "part": 0,
          "translated_content": "许多现有框架将行动学习纳入其设计中或将其用作输出。为了澄清行动系统的定义，我们强调了各种框架之间的区别，包括大型语言模型（LLM）、大型多模态模型（LMM）、机器人基础模型（RFM）和大型行动模型（LAM），如表8.1所示。具体而言，LLM是基于提供的提示生成语言输出，而LMM是基于多模态输入生成多模态工件。现有基于语言或数字AI代理框架是通过这些基础模型（例如LLM或LMM）构建的，通过预定义行动空间的范围及其学习策略。另一方面，RFM是为了基于现实环境（例如机器人视频）优化机器人控制。现有的RFM是从网络规模的视频数据进行预训练，并使用视频预测来模拟机器人控制的行动。尽管在构建物理AI代理中涉及了一些行动设计，但RFM的核心仍然是利用生成目标从大规模数据中学习知识。此外，一些最近的工作引入了大型行动模型（LAM）的概念，进一步强调生成行动策略的阶段，与现实环境进行交互，并增强自学习范式。从这些定义中，我们注意到，无论使用哪种基础模型，行动系统的核心是建立与环境的交互，然后通过预定义的奖励函数从收集的行动轨迹中启用学习过程。具体而言，这些行为背后的机制也类似于人类认知中的行动系统，为设计AI代理框架中的行动系统提供了宝贵的见解。例如："
        },
        {
          "type": "text",
          "content": "·When processing diferent scenarios, humans usually will pre-define the action space to perform action trajectories to solve specific tasks.For instance, when playing computer games like Minecraft, we will set our action operations via keyboard or mouse to simulate behaviors like building house, mining gold, and so on. On the basis of this, we also need to build or create an action space for handling complex tasks in AI Agent frameworks.\n·Compared to machines,the human cognitive system excels in continuously acquiring new knowledge through real-world interactions, guided by generating and optimizing the action sequences.Thus, replicating this learning ability in AI agents is essential to adapt the dynamic environment and build a new skillibrary.\n·In addition, withthe development of human civilization,learning to use external tools has been recognized as one of the most significant milestones in the evolution of human intelligence.By leveraging these external tools,humans can extremely extend the problem-solving capability in diferent scenarios,from the stone age to the industrial revolution.",
          "index": 2,
          "part": 0,
          "translated_content": "·在处理不同场景时，人类通常会预定义行动空间，执行行动轨迹以解决特定任务。例如，当玩像Minecraft这样的电脑游戏时，我们会通过键盘或鼠标设置我们的行动操作，以模拟建造房屋、挖掘黄金等行为。基于此，我们还需要为处理AI代理框架中的复杂任务构建或创建一个行动空间。\n·与机器相比，人类认知系统在通过现实世界互动持续获取新知识方面表现出色，通过生成和优化行动序列来指导。因此，在AI代理中复制这种学习能力对于适应动态环境并构建新的技能库至关重要。\n·此外，随着人类文明的发展，学会使用外部工具已被公认为人类智能演化中最重要的里程碑之一。通过利用这些外部工具，人类可以极大地扩展在不同场景中的问题解决能力，从石器时代到工业革命。"
        },
        {
          "type": "text",
          "content": "Tothis end,weexpect tobuildthe mappingbetweentheaction systemof humancognitionsystemandthe design of AI Agentframework,including how to buildaction space for AIagentfrom specific scenarios to general domain,how to buildactionlearning withintheenvironment, andhowtoleverageexternalstates (e.g.ools)toextendthetaskscopeof AI Agent.Bydeveloping this a systematic survey,we strive to provide more in-depth insights forthecommunity witha clear understanding of the significance of action systems in AI agent frameworks.",
          "index": 3,
          "part": 0,
          "translated_content": "为此，我们期望构建人类认知系统的行动系统与AI代理框架设计之间的映射，包括如何从特定场景到通用领域构建AI代理的行动空间，如何在环境中构建行动学习，以及如何利用外部状态（例如工具）来扩展AI代理的任务范围。通过开展这项系统性调查，我们致力于为社区提供更深入的见解，清晰地理解行动系统在AI代理框架中的重要性。"
        }
      ],
      "raw_title": "From Human Action to Agentic Action",
      "type": null,
      "children": [],
      "translated_title": "8.2 从人类行为到主体性行为"
    },
    {
      "title": "8.3 Paradigms of Agentic Action System",
      "number": "8.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Generally, the action system of AI agent frameworks consists of three major components:1)the action space $\\mathcal{A}$ ，which includes alltypesofactionthatagentcan perform inreal-worldscenariosordownstreamtasks,andcanvarysignificantly depending ondifferent agent settings,ranging from language-basedagents toembodied agents; 2)the action learning within an dynamic environment that determines the state $s$ ，observation $\\mathcal{O}$ and the optimization process of agent; 3) the tool space $\\tau$ thatencompasses the instruments,interfaces,or middle-wares the agent can perform for utilization, which ranges fromphysicaldevices such asroboticarms todigitalinterfaces likeAPIs.Overal, these components collectively define the scope and characteristics of the action systemfor AIagents,shaping their formulation and execution.",
          "index": 0,
          "part": 0,
          "translated_content": "通常，人工智能代理框架的行动系统主要由三个组成部分组成：1）行动空间 $\\mathcal{A}$，其中包括代理在真实场景或下游任务中可以执行的所有类型行动，根据不同的代理设置可以有很大变化，从基于语言的代理到具身体的代理不等；2）在动态环境中进行的行动学习，确定状态 $s$，观测 $\\mathcal{O}$ 和代理的优化过程；3）工具空间 $\\tau$，包括代理可以利用的工具、接口或中间件，范围从物理设备如机器臂到数字接口如APIs。总体而言，这些组件共同定义了人工智能代理的行动系统的范围和特征，塑造了它们的制定和执行。"
        },
        {
          "type": "text",
          "content": "To fully explore the possible actions $a_{t}$ in practical scenarios, we must formally represent the action space and consider both individual operations and the underlying hierarchicalreasoning processes.This means examining the action space at various levels,from low-level manipulations to high-level operators that orchestrate complex workflows.\n\nAccordingly, the AI agent decision making process can be formalized as a trajectory $\\left\\langle o_{t},s_{t},a_{t}\\right\\rangle$ ,where $a_{t}$ is selected from the action space $\\mathcal{A}$ to transform the current state $s_{t}$ based on observation $o_{t}$ into the next state. In some cases, integrating external tool systems may also be necessary. By executing a sequence of $\\left\\langle o_{t},s_{t},a_{t}\\right\\rangle$ , the agent is steered toward achieving its final objectives.",
          "index": 1,
          "part": 0,
          "translated_content": "为了充分探索实际场景中可能的行动 $a_{t}$，我们必须形式化表示行动空间，并考虑个体操作和潜在的层次推理过程。这意味着在不同层次上检查行动空间，从低级操作到编排复杂工作流程的高级运算符。\n\n因此，人工智能代理的决策过程可以形式化为一个轨迹 $\\left\\langle o_{t},s_{t},a_{t}\\right\\rangle$，其中 $a_{t}$ 从行动空间 $\\mathcal{A}$ 中选择，根据观测 $o_{t}$ 将当前状态 $s_{t}$ 转化为下一个状态。在某些情况下，整合外部工具系统也可能是必要的。通过执行一系列 $\\left\\langle o_{t},s_{t},a_{t}\\right\\rangle$，代理被引导朝向实现其最终目标。"
        }
      ],
      "raw_title": "Paradigms of Agentic Action System",
      "type": null,
      "children": [
        {
          "title": "8.3.1  Action Space Paradigm",
          "number": "8.3.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Action space $\\mathcal{A}$ is an important component, which serves as the basis for building an action system within AI agent frameworks.The composition of theactionspace determines how AIagentssolvecomplextasks indiffrent scenarios. In Figure 8.2,we present an ilustrative taxonomy ofthe action system based on its action space.Generally, we summarize the action space within existing works as three distinct types, as outlined below.\n\nLanguage Language-based AI agents typically operate through language-driven actions in interactive linguistic environments,suchasreasoning,programming,retrieving information,executing APIcall,orinteractingwith exteal tools. In our study, we summarize three distinct types of language-based action spaces,including plain text,code programming,and communication.Specificaly,early language-based AI agents are built with plain text, which aim to perform interactive decision-making in verbal environments or text-based games.Here, ReAct [7o] is a representative language-based AI agent,which synergizes the reasoning and actions of an LLM to solve various problems.AutoGPT[625] analyzes and decomposes user requests into multiple subtasks and uses web search or other tools to tackle each of them.Reflexion[48] involves self-refinement and the memory mechanism toenhance action execution in language tasks. $\\mathbf{LLM+P}$ [163] empowers LLM-based agent with planning capability to aid decisionmaking.However,converting plain text intoan executable command usually requires LLMs tofirst interpret the text and then perform instructionconversion,leading toadditionalinformationlossTothis end,some work explores using code as the action space,allwingdirect execution ofthe generated code and self-verification.MetaGPT[626]and ChatDev[627] build the action space via programming language with multi-agent collaboration. SWE-Agent [628] considerdifferentstages ofsoftware engineering andthussolve software issues.OpenDevin[629]devises anautomatic software development platform that integrate code writing,interaction with thecommand,sandboxforcode execution, andcollaborations.Moreover, some frameworks are built based on multi-agentcommunications,and thenuse chatting to analyze which actions should beemployed inthe next step.Here,Generative Agents[50]directly simulate multiple characters in a virtual town, to explore how each agent to conduct next action.MetaGPT [626] and ChatDev [627] are both multi-agent frameworks tofaciliate the development of software engineering.AutoGen [630] is also a representative framework that enable multiple agentcollaboration to solve any complextasks.Generally,languagebased AIagents,empowered byLLMs, perform efectively in linguistic interactions.However,lmited tothe scopeof the action space,it also poses challenges of howto solve more complextasks inreal-world scenarios.Therefore,we also needtoformulate new research solutions toconstructa more sophisticated action space tosolvechallnging tasks.",
              "index": 0,
              "part": 0,
              "translated_content": "动作空间$\\mathcal{A}$是一个重要组成部分，它作为在人工智能代理框架内构建行动系统的基础。动作空间的组成决定了人工智能代理如何在不同场景中解决复杂任务。在图8.2中，我们基于其动作空间呈现了行动系统的分类法。通常，我们总结了现有研究中的动作空间为三种不同类型，如下所述。\n\n基于语言的人工智能代理通常通过语言驱动的行动在交互式语言环境中运行，例如推理、编程、检索信息、执行API调用或与外部工具交互。在我们的研究中，我们总结了三种不同类型的基于语言的动作空间，包括纯文本、代码编程和通信。具体来说，早期基于语言的人工智能代理是基于纯文本构建的，旨在在口头环境或基于文本的游戏中进行交互式决策。在这里，ReAct[70]是一个代表性的基于语言的人工智能代理，它通过协同作用LLM的推理和行动来解决各种问题。AutoGPT[625]分析并分解用户请求为多个子任务，并使用网络搜索或其他工具来处理每个子任务。Reflexion[48]涉及自我完善和记忆机制，以增强语言任务中的行动执行。$\\mathbf{LLM+P}$[163]赋予基于LLM的代理规划能力，以帮助决策制定。然而，将纯文本转换为可执行命令通常需要LLM首先解释文本，然后执行指令转换，导致额外信息的丢失。为此，一些工作探索使用代码作为动作空间，允许直接执行生成的代码并进行自我验证。MetaGPT[626]和ChatDev[627]通过编程语言建立行动空间，实现多代理协作。SWE-Agent[628]考虑软件工程的不同阶段，从而解决软件问题。OpenDevin[629]设计了一个自动软件开发平台，整合了编写代码、与命令交互、用于代码执行的沙盒和协作。此外，一些框架是基于多代理通信构建的，然后使用聊天分析下一步应采取哪些行动。在这里，Generative Agents[50]直接模拟虚拟城镇中的多个角色，探索每个代理如何进行下一步行动。MetaGPT[626]和ChatDev[627]都是多代理框架，促进软件工程的发展。AutoGen[630]也是一个代表性框架，可以实现多代理协作来解决任何复杂任务。总的来说，基于语言的人工智能代理在语言交互中效果显著。然而，受限于动作空间的范围，也面临如何在现实场景中解决更复杂任务的挑战。因此，我们还需要制定新的研究解决方案，构建更复杂的动作空间来解决具有挑战性的任务。"
            },
            {
              "type": "figure",
              "src": "images/07fca98b52c4d65a0c93b814e7474333738ed7ad1d78fe87966a57d9f4c9a62c.jpg",
              "alt": "",
              "caption": "Figure 8.3: Illustrative Taxonomy of Action system, including action space and learning paradigm",
              "index": 1,
              "part": 0,
              "translated_caption": "图8.3：行动系统的示意分类，包括行动空间和学习范式"
            },
            {
              "type": "text",
              "content": "Digital To expand the capabilities of AIagents beyond language,some works have alsodeveloped advanced AI agents that operate within digitalenvironments, such as web proxies,online shopping platforms, and gaming systems. For examples,MineDojo[31l]devises a virtualagent via video-language pre-training and simulates anenvironmentthat supports a multitude of tasks and goals within Minecraft. Moreover, Voyager[47]isan embodied AI agent trained to play Minecraft. It simulates multiple executable actions incode form to developa skillibrary via interacting with theMinecraft environment, andthus improve the capabilityofvirtualagents. JARVIS-1[228] is an open-world agent that can handle multi-modal inputs /outputs, generate sophisticated plans, and perform embodied control. It explores the evolutionary behaviors of the agent when acting in Minecraft. SwarmBrain[631] is an embodied agent that uses LLMsto act strategically and in real time in StarCraft II. Additionally,some researchstudies investigate how LLMs can act to process multimodal tasks.",
              "index": 2,
              "part": 0,
              "translated_content": "数字化为了拓展人工智能代理的能力，一些研究还开发了在数字环境中运行的先进人工智能代理，例如网络代理、在线购物平台和游戏系统。例如，MineDojo通过视频语言预训练设计了一个虚拟代理，并模拟支持Minecraft内多种任务和目标的环境。此外，Voyager是一个经过训练用于玩Minecraft的具有实体的人工智能代理。它通过与Minecraft环境互动，模拟多种可执行动作以开发技能库，从而提高虚拟代理的能力。JARVIS-1是一个处理多模态输入/输出、生成复杂计划并执行实体控制的开放世界代理。它在Minecraft中探索代理在行动时的进化行为。SwarmBrain是一个使用LLM在StarCraft II中进行战略和实时行动的实体代理。此外，一些研究探讨了LLM如何处理多模态任务。"
            },
            {
              "type": "text",
              "content": "MM-ReAct [497] and ViperGPT [498] apply LMs to perform the thinking process for multimodaltasks andthen select visualexpertsfortasksolving. Visual-ChatGPT[496] integrates multiple visual experts and uses LLMs as the controller to solve tasks. HuggingGPT[152] directly involves four stages, includingtask plannng, modelselection,modelexecution and response generation, to automaticall analyze user instructionsand predict the finalanswers basedon complex multimodal tasks. Itisalso vitalfor the agent to keep up with the latest information available online. Therefore, some AI Agent frameworks (e. g. , WebGPT [632], WebAgent[634])aredesignedto interact withsearchengineto enhance thecapabilityofagentto discover the answers from website. WebShop[633] is used to explore the potential of AI Agent for online shoping. Mind2Web [97] is to buildageneralist agent that simulate multiple complex web tasks. As foundation agents advance in processing multimodaltasks or web tasks, there is aincreasing trend to enhance their capability in solving complexcomputer tasks. Mobile-Agent[635]utilizes multimodal models asthecognitive controlerto manage andorchestrate mobile functionalities. AppAgent[636] defines various app usages as action spaces,enabling foundation models to interact with different apps as a mobile intellgent assistant. UFO[637] and OmniParser[520]are two advanced GUI agents which manipulates UI operations as the action space,enabling AI agent to perform computer-use tasks. Generally, empowered with more advanced skills in digital environment, AIagent can demonstrate beter intelligent insolving complex tasks,andrepresent asignificant shiftfromlanguage intelligent todigitalintelligent.",
              "index": 2,
              "part": 1,
              "translated_content": "MM-ReAct [497] 和 ViperGPT [498] 使用语言模型(LMs)执行多模态任务的思考过程，然后选择视觉专家来解决任务。Visual-ChatGPT [496] 整合了多个视觉专家，并使用LLMs作为控制器来解决任务。HuggingGPT [152] 直接涉及四个阶段，包括任务规划、模型选择、模型执行和响应生成，自动分析用户指令并基于复杂的多模态任务预测最终答案。对于代理来说，跟上线上最新信息至关重要。因此，一些AI代理框架（例如WebGPT [632]、WebAgent [634]）被设计为与搜索引擎进行交互，以增强代理从网站发现答案的能力。WebShop [633] 用于探索AI代理在在线购物中的潜力。Mind2Web [97] 用于构建模拟多个复杂网络任务的通用代理。随着基础代理在处理多模态任务或网络任务方面的进展，越来越多地增强它们解决复杂计算机任务的能力。Mobile-Agent [635] 利用多模态模型作为认知控制器来管理和编排移动功能。AppAgent [636] 将各种应用使用定义为动作空间，使基础模型能够与不同应用交互，作为移动智能助手。UFO [637] 和 OmniParser [520] 是两个先进的图形用户界面代理，将UI操作作为动作空间，使AI代理能够执行计算机使用任务。总的来说，在数字环境中拥有更先进的技能，AI代理可以展示更好的智能来解决复杂任务，并代表了从语言智能到数字智能的重要转变。"
            },
            {
              "type": "text",
              "content": "Byexpandingthe action space to include web browsing, GUlinteraction, mobile applications, and embodied systems,AIagents are evolving into more autonomous, multimodal, and context-aware systems,bridging the gap between foundation models and human cognition systems. In addition,other research explores LLM integration with structured digitalenvironments such as relational databases and knowledge graphs (KGs). Pangu [639]pioneredthe connection between LLMs and large-scale KGs,whileBIRD[640]and Spider 2. 0[641] established afoundation forLLMs tooperate with enterprise databases in real-world settings. NL2SQL-BUGs[667]addresses thecriticalchallenge of identifying semantic errorsin NL2SQL pipelines[365], which enhances the reliability ofLLM-driven interactions with relational databases [668]. Similarly,frameworkslike UnifiedSKG[638] and Middleware[642] expand LLMs'action capabilities across both databases and KGs.",
              "index": 2,
              "part": 2,
              "translated_content": "通过扩展行动空间以包括网络浏览、GUI交互、移动应用和实体系统，AI代理正在向更自主、多模态和具有上下文意识的系统演变，弥合了基础模型和人类认知系统之间的差距。此外，其他研究探索了LLM与结构化数字环境（如关系数据库和知识图谱）的整合。Pangu [639] 开创了LLM与大规模知识图谱之间的连接，而 BIRD [640] 和 Spider 2.0 [641] 在现实环境中为LLM与企业数据库的操作奠定了基础。NL2SQL-BUGs [667] 解决了在NL2SQL流水线中识别语义错误的关键挑战，从而提高了LLM驱动的与关系数据库交互的可靠性。类似地，像UnifiedSKG [638] 和Middleware [642] 这样的框架扩展了LLM在数据库和知识图谱之间的行动能力。"
            },
            {
              "type": "text",
              "content": "Physical Building an AIagent to interact with thereal physical worldcanbe viewed as the ultimateobjective to simulate acomputer programto act asa human cognition system.Toachieve this,we require the agent tobecapable of processing signals from real-world environments and generating feedback tofacilitate continuous improvement. Therefore,it willpose new challenges on how to process the continuous signals collected by sensors and enable foundation models to make decisions.To fulfillthis, RT-family[522, 643,644] pre-trained vision-language-action models to integrateknowledge from web videos into robotic learning,enhancing robotic control and action execution. GR-2[357] is a robotic model that undergoes large-scale pre-training on video clips and language data,followed by fine-tuning on robot trajectories for robotic action prediction. $\\pi_{0}$ [645] pre-trained a robotic model based on robot platforms,including single-arm robots, dual-arm robots, and mobile manipulators, to build robotic learning in physical systems. SayCan [646] bridges the connections between robotic semantics and LLMs, using the robotic model to provide perception for LLMs and then using LLMs to make high-level decision-making. VoxPoser[647] uses LLMs to understand and decompose 3D Value Maps for Robotic Manipulation. Besides,EmbodiedGPT [648] utilizes vision-language models to understand videodata and perform decision-driven actions. In physical environments,it is worth noting that we usually need tounderstandcontinuous signals andthen generate continuous actions forrobotic control.Despite the existing foundation models thatcan efectively process discrete-levelactions (e.g.languageor computer-use),how to processlong continuous signals is stillchallenging.Therefore,eliminating the differences between continuous signals and discrete signals in foundation models is still a major problem.",
              "index": 3,
              "part": 0,
              "translated_content": "构建一个与真实物理世界互动的AI代理可以被视为模拟计算机程序以充当人类认知系统的最终目标。为了实现这一目标，我们需要使代理能够处理来自真实世界环境的信号并生成反馈以促进持续改进。因此，这将对如何处理传感器收集的连续信号并使基础模型做出决策提出新的挑战。为了实现这一目标，RT-family [522, 643, 644] 预训练视觉-语言-动作模型，将来自网络视频的知识整合到机器人学习中，增强机器人控制和动作执行能力。GR-2 [357] 是一个机器人模型，它在视频剪辑和语言数据上进行大规模预训练，然后在机器人轨迹上进行微调，用于机器人动作预测。$\\pi_{0}$ [645] 基于机器人平台预训练了一个机器人模型，包括单臂机器人、双臂机器人和移动操纵器，以在物理系统中构建机器人学习。SayCan [646] 架起了机器人语义和LLM之间的联系，利用机器人模型为LLM提供感知，然后利用LLM进行高级决策制定。VoxPoser [647] 利用LLM理解和分解用于机器人操作的3D价值图。此外，EmbodiedGPT [648] 利用视觉-语言模型理解视频数据并执行决策驱动的动作。在物理环境中，值得注意的是，我们通常需要理解连续信号，然后为机器人控制生成连续动作。尽管现有的基础模型可以有效处理离散级别的动作（例如语言或计算机使用），但如何处理长时间的连续信号仍然具有挑战性。因此，在基础模型中消除连续信号与离散信号之间的差异仍然是一个主要问题。"
            },
            {
              "type": "text",
              "content": "Generally,action space serves as one of the mostcriticalcomponents in building an efective AI Agent system.An effective action space enhances the capability and efficiency of the AI Agent in processing downstreamtasks.Action space usuallyranges from thediscrete space (e.g.,skillibrary inAtari games)tothecontinuous space (e.g.,robotic manipulation).As AI agents become more autonomous and multimodal, designing effective action spaces will be crucial for advancing general-purpose AI systems capable of real-world interactions.",
              "index": 4,
              "part": 0,
              "translated_content": "通常，动作空间是构建有效AI代理系统中最关键的组成部分之一。有效的动作空间提升了AI代理在处理下游任务时的能力和效率。动作空间通常涵盖从离散空间（例如Atari游戏中的技能库）到连续空间（例如机器人操作）的范围。随着AI代理变得更加自主和多模态，设计有效的动作空间对于推进能够进行真实世界互动的通用AI系统将至关重要。"
            }
          ],
          "raw_title": "Action Space Paradigm",
          "type": null,
          "children": [],
          "translated_title": "8.3.1 行动空间范式"
        },
        {
          "title": "8.3.2 Action Learning Paradigm",
          "number": "8.3.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "In the human cognition system,action leaming [669]represents the problem-solving process, involving both taking actions andreflecting onfeedback.Similarly,action learningfor AIagents refers to the iterative processby which an autonomous AI system refines its decision making and behavior through direct interaction with the real world environment. Generall,action learning encompasses a cycle of multiple stages, including building action space, choosing actions,andoptimizing action selection based on interaction with theenvironment (e.g.,receiving feedback or rewards and adjusting policy forchoosing actions).By iterativelydeploying these strategies,AIagents can adapt to the latest information orchanging conditions in real time,ultimately enabling more robust,flexible,andefficient problem-solving capabilities. Therefore, an effective action learning mechanism is crucial for the optimization of agentic action systems.Inthis part, we mainly focus onthree diferent representativelearming paradigms,including in-context learning, supervised training, and reinforcement learning, which are discussed below:",
              "index": 0,
              "part": 0,
              "translated_content": "在人类认知系统中，行动学习代表了解决问题的过程，涉及采取行动和反思反馈两个方面。类似地，对于AI代理来说，行动学习指的是一个自主AI系统通过与真实世界环境的直接互动不断完善其决策和行为的迭代过程。通常，行动学习包括多个阶段的循环，包括构建行动空间、选择行动，并根据与环境的互动（例如接收反馈或奖励并调整选择行动的策略）优化行动选择。通过迭代地应用这些策略，AI代理可以实时适应最新信息或不断变化的条件，最终实现更强大、灵活和高效的问题解决能力。因此，有效的行动学习机制对于优化代理行动系统至关重要。在这部分中，我们主要关注三种不同的代表性学习范式，包括上下文学习、监督训练和强化学习，具体讨论如下："
            },
            {
              "type": "text",
              "content": "In-context Learning As large language models have demonstrated emergent ability,in-context learning has been considered as the most efective method to leverage the existing capabilities of LLM without any modifications. Provided with well-designed prompts to describe actions, AI agents can understand specific actions, perform these actions,reflect on the outcome of the interaction with the environment, and finally achieve goals. Among these approaches,thecommon method is to use prompting techniques to instruct LLMs to generate agentic action. Here,the most representativeone is Chain-of-Thought(CoT)[46]prompting,which applies“Let us think stepbystep\"technique to generate asequenceofintermediatereasoning steps,exploring potential solutionssystematicall. ReAct[7]enables LLMs to generate reasoning trails and task-specific actions through interaction within the environment, improving thereasoning and decision-makingcapabilities of AIagents. LearnAct[652]devises an iterative learning strategy to expand action space by generating code (i. e. ,Python)tocreate andrevise new actions. Moreover, some works (e.",
              "index": 1,
              "part": 0,
              "translated_content": "在上下文学习中，由于大型语言模型展示了新兴能力，上下文学习被认为是利用现有LLM能力而无需任何修改的最有效方法。通过提供精心设计的提示来描述行动，AI代理可以理解特定行动，执行这些行动，反思与环境互动的结果，最终实现目标。在这些方法中，常见的方法是使用提示技术指导LLMs生成代理行动。其中，最具代表性的是链式思维（CoT）提示，应用“让我们逐步思考”技术生成一系列中间推理步骤，系统地探索潜在解决方案。ReAct使LLMs能够通过与环境内的互动生成推理路径和任务特定行动，提高AI代理的推理和决策能力。LearnAct设计了一种迭代学习策略，通过生成代码（即Python）扩展行动空间以创建和修改新行动。此外，一些研究（e."
            },
            {
              "type": "text",
              "content": "g. , Auto-CoT[137] explores how to automatically generate CoT via LLMs and then enable the autonomous thinking process of AIagents. To handlemore complex tasks,ToT[72]considers the thought process as atree structure and introduces the tree search viaLLM prompting,while GoT[75]applies a graph structurealong with the graph search. For robotic models,CoA[649]designed four different promptsettings (e. gobject, grasp,spatial,and movement)to allow robot manipulation with reasoning process. Furthermore,to tackle more complex tasks that require intricate agentic workflows,some frameworks introduce the stage oftask decomposition via LLM prompting tobreak down user instructions. Least-to-Most[138]isaclasscalprompting technique toconvert user instructions into multiple subtasks. HuggingGPT[152] is arepresentative AIagent framework that applies task planning to transform user requirements into actionable items. Plan-and-Solve[650] directly uses LLM to make plans from user instructions and then give answers based onthe generated plans. Progprompt[93]applies similar task decompositiontorobotic tasks. In addition, using prompting techniques to formulate the characteristic of AIagent hasalso been considered as an increasingtrend to facilitate the simulation and productivity of AI agentframeworks (e. g,Generative Agents [50], MetaGPT[626], ChatDev[627],SWE-Agent[628]). Finally,some other frameworks (e.",
              "index": 1,
              "part": 1,
              "translated_content": "Auto-CoT[137]探讨如何通过LLM自动生成CoT，从而实现AI代理的自主思考过程。为了处理更复杂的任务，ToT[72]将思考过程视为树结构，并通过LLM提示引入树搜索，而GoT[75]则应用图结构以及图搜索。对于机器人模型，CoA[649]设计了四种不同的提示设置（例如对象、抓取、空间和运动），以允许机器人在推理过程中进行操作。此外，为了解决需要复杂代理工作流程的更复杂任务，一些框架通过LLM提示引入任务分解阶段，以分解用户指令。Least-to-Most[138]是一种经典提示技术，将用户指令转换为多个子任务。HuggingGPT[152]是一个代表性的AI代理框架，应用任务规划将用户需求转化为可执行项。Plan-and-Solve[650]直接使用LLM根据用户指令制定计划，然后根据生成的计划给出答案。Progprompt[93]将类似的任务分解应用于机器人任务。此外，使用提示技术来制定AI代理的特征也被视为促进AI代理框架的模拟和生产力的增长趋势，例如Generative Agents [50]、MetaGPT[626]、ChatDev[627]和SWE-Agent[628]。最后，一些其他框架（例如"
            },
            {
              "type": "text",
              "content": "g. ,Reflexion[48]or Self-refine[67])analyze the external feedbacks of user interactionwithin the environment and then iteratively refine and polish results via well-designedreflexion prompts. Allofthese designs allowus tobeterunderstand user instructions,decompose task goals,and make plans forthinking answers. In-contextlearming canhelpus avoid parameter optimization and reduce theheavycostof training LLMs. Itallows AIagents toperformvarious actions effectively andadapt toawide rangeof domains. However,challenges stillremain if we want to acquire agents of even stronger actionlearning ability.",
              "index": 1,
              "part": 2,
              "translated_content": "Reflexion[48]或Self-refine[67]分析用户与环境的外部反馈，然后通过精心设计的反思提示迭代地完善和优化结果。所有这些设计都使我们能够更好地理解用户指令，分解任务目标，并为思考答案制定计划。在上下文学习中，可以帮助我们避免参数优化，并减少对LLM进行训练的巨大成本。这使得AI代理能够有效执行各种动作，并适应广泛的领域。然而，如果我们希望获得更强大的行动学习能力的代理，仍然存在挑战。"
            },
            {
              "type": "text",
              "content": "Supervised Training Tofurther improve the action learning abilityof foundation models,increasing research efforts have focused on training methodologies, including self-supervised pretraining (PT)and supervised fine-tuning (SFT). For the pre-training paradigm,the most representative works is RT-family[522, 643, 644],which pre-trains robotic Transformer onlarge-scale web and robotic data,yielding a powerful vision-language-action model. Following this policy, GR-2[357] is developed through extensive pre-trainingonalargecorpus of web videos to understand the dynamcs of theworldandpost-trainingonrobotic trajectorydatatospecialize invideo generationandaction prediction. Similarly, LAM[622] is alarge action model pre-trained on trajectories of user interaction with computer usage. However, the pre-training paradigm usuall incurs massive computation costs.",
              "index": 2,
              "part": 0,
              "translated_content": "监督训练为进一步提高基础模型的行动学习能力，增加的研究工作集中在训练方法上，包括自监督预训练（PT）和监督微调（SFT）。在预训练范式中，最具代表性的作品是RT系列[522, 643, 644]，它在大规模网络和机器人数据上对机器人Transformer进行预训练，产生了一个强大的视觉-语言-行动模型。遵循这一策略，GR-2[357]通过在大规模网络视频语料库上进行广泛的预训练，以理解世界的动态，并在机器人轨迹数据上进行后续训练，专注于视频生成和动作预测。同样，LAM[622]是一个在用户与计算机使用交互轨迹上进行预训练的大型行动模型。然而，预训练范式通常会带来巨大的计算成本。"
            },
            {
              "type": "text",
              "content": "Therefore, many works take the finetuning paradigm to enhance the action capability of foundation models. OpenVLA[670] is built upon the Llama2[11] language model and incorporates a visual encoder based on DINOv2[671] and SigLIP[672]. It is fine-tuned ona diverse set of real-world robot demonstrations from Open X-Embodiment(OXE)[673] and outperforms RT-2-X[673] across different tasks, all while utilizing $7\\times$ fewer parameters. Building upon OpenVLA, CogACT [653] integrates an additional diffusionactionmodule andintroduces anadaptiveactionensemblestrategyforinference. Itisalsofine-tuned using datasets from OXE and demonstrates a $35\\%$ improvement in the SIMPLER [674] simulated environment and a $55\\%$ increment in realrobot tasks using the Franka Arm. Besides, some works also explore how to enablerobotic model to learn action from plain language in physical world. For examples,RT-H[654]introduces ahierarchicalarchitecture to build action space, which first predict language motions and then generate low-level actions. And $\\pi_{0}$ [645] collected massive diverse datasets from differentdexterous robot platforms,andthen fine-tune the pre-trained VLMs to learn robotic actions. UniAct[56]learns universal actions thatcapture generic atomic behaviors acrossdiffrently shaped robots by learning their shared structural features. This approach achieves cross-domaindata utilization and enables cross-embodiment generalizations by eliminating heterogeneity[132]. Overal, using supervised training,including pre-training and supervised fine-tuning,can effctively adapt foundation models to perform actions intelligently in real-worldscenarios. Lastbut notleast,itisworthnotingthat,evenwithextensivetraining onavastcorpus,itistll beneficial toapplyin-contextleaning ontopof thetrainedmodelforAIagents, inan pursuit fortheirbestperformance.",
              "index": 2,
              "part": 1,
              "translated_content": "因此，许多工作采用微调范式来增强基础模型的行动能力。OpenVLA[670]基于Llama2[11]语言模型构建，并整合了基于DINOv2[671]和SigLIP[672]的视觉编码器。它在来自Open X-Embodiment（OXE）[673]的多样真实世界机器人演示数据上进行微调，在各种任务中表现优于RT-2-X[673]，同时参数数量仅为其$7\\times$。在OpenVLA的基础上，CogACT [653]集成了额外的扩散动作模块，并引入了自适应动作集成策略进行推理。它还使用来自OXE的数据集进行微调，在SIMPLER [674]模拟环境中表现出35%的改进，在使用Franka Arm的真实机器人任务中增加了55%。此外，一些工作还探索了如何使机器人模型能够从物理世界中的普通语言中学习行动。例如，RT-H[654]引入了一种分层架构来构建行动空间，首先预测语言动作，然后生成低层次动作。而$\\pi_{0}$ [645]从不同灵巧机器人平台收集了大量多样化数据集，然后微调预训练的VLMs来学习机器人动作。UniAct[56]学习捕捉不同形状机器人之间共享结构特征的通用动作。这种方法实现了跨领域数据利用，并通过消除异质性实现了跨体现泛化[132]。总的来说，使用监督训练，包括预训练和监督微调，可以有效地使基础模型在真实场景中智能执行行动。最后但并非最不重要的是，值得注意的是，即使在大规模语料库上进行了广泛训练，对AI代理进行上下文学习仍然是有益的，以追求它们的最佳性能。"
            },
            {
              "type": "text",
              "content": "Reinforcement Learning To facilitate an action learning procedure inaddition to in-context learning and supervised trainng,itisalsocrucialforagentstointeract withtheenvironment andeventuallyoptimizetheiractionpolicythrough experience,feedbackorrewards.Consideringthis iterative andsequentialnature,reinforcementlearning (RL)provides us withthe systematic methodology we need[675,676,677,678].InRL paradigms,there are severalclassicaland representative algorithms,such as Deep Q-Network (DQN)[679] and Proximal Policy Optimization (PPO)[680]. The most representative RL work that applied reinforcement learning tofoundation models is InstructGPT[43],which effctively aligns LM outputs with human preferences via RLHF.Since RLHF usually requires additional training to build the reward model, some papers (e.g. DPO [1) ]) proposes to directly optimize preference data through contrastive learning.Existing work[89,681]alsodemonstratethepotentialof scaling the RLalgorithm for foundation models to produce long CoT thinking stages with impressive performance.Although RL paradigms have been successfully used to fine-tuneLLMs fortext generationtasks[12,682,43,683],effcientlyutilizingtheRLalgorithmforaction leaing remains one of the many challnges that require further attempts. Recent advances indicate significant progressin applying RL to action learning with LLMs from various perspectives:",
              "index": 3,
              "part": 0,
              "translated_content": "强化学习 为了促进行动学习过程，除了上下文学习和监督训练外，对于代理与环境进行互动并最终通过经验、反馈或奖励优化其行动策略也至关重要。考虑到这种迭代和顺序性质，强化学习（RL）为我们提供了所需的系统方法。在强化学习范式中，有几种经典代表性算法，如深度Q网络（DQN）和近端策略优化（PPO）。将强化学习应用于基础模型的最具代表性的RL工作是InstructGPT，它通过RLHF有效地将语言模型输出与人类偏好对齐。由于RLHF通常需要额外训练来构建奖励模型，一些论文（例如DPO）提出通过对比学习直接优化偏好数据。现有工作还展示了将RL算法扩展到基础模型以产生具有出色性能的长CoT思考阶段的潜力。尽管RL范式已成功用于微调LLM以进行文本生成任务，有效利用RL算法进行行动学习仍然是需要进一步尝试的许多挑战之一。最近的进展显示了在从各种角度应用RL到LLM进行行动学习方面取得的显著进展："
            },
            {
              "type": "text",
              "content": "· Given the rich world knowledge encapsulated in LLM, we can use LLM to mimic external environments or generate imagined trajectories to aid agents in action learning.For instance, RLFP[657] utilizes guidance and feedback from the policy,value, and success-reward foundation models to enable agents to explore more efficiently. Similarly, ELLM[658] utilizes large-scale background knowledge from LLMs to guide agents in effcient exploration within various environments. GenSim[659] automatically generates rich simulation environments and expert demonstrations by exploiting the coding abilities of LLM,thereby facilitating the capability of the agent for free exploration. LEA [660] leverages the language understanding capabilities of LLM and adapts LLM as a state transition model and a reward function to improve the performance of ofline RL-based recommender systems. MLAQ [661] utilizes an LLM-based world model to generate imaginary interactions and then applies Q-learning [684] to derive optimal policies from this imaginary memory. KALM [662] fine-tunes LLM to perform bidirectional translations between textual goals and rollouts, alowing agents to extract knowledge from LLM in the form of imaginary rollouts through ofline RL. In general, empowered by RL paradigms, we can significantly explore the internal knowledge from LLMs and thus enhance the interactions with external environments. Current works such as Search-R1[685], R1-Searcher [686], RAGEN[687], and OpenManus-RL[688] are exploring utilizing RL methods to fine-tune the agent models on trajectory data in agentic environments.",
              "index": 4,
              "part": 0,
              "translated_content": "· 鉴于LLM中蕴含的丰富世界知识，我们可以利用LLM来模仿外部环境或生成想象的轨迹，以帮助代理进行行动学习。例如，RLFP[657]利用来自策略、价值和成功奖励基础模型的指导和反馈，使代理能够更高效地探索。类似地，ELLM[658]利用LLM中的大规模背景知识来引导代理在各种环境中进行高效探索。GenSim[659]通过利用LLM的编码能力自动生成丰富的模拟环境和专家演示，从而促进代理的自由探索能力。LEA [660]利用LLM的语言理解能力，并将LLM调整为状态转换模型和奖励函数，以提高离线RL推荐系统的性能。MLAQ [661]利用基于LLM的世界模型生成虚拟交互，并应用Q学习[684]从这种虚拟记忆中得出最优策略。KALM [662]微调LLM以在文本目标和展开之间执行双向翻译，使代理能够通过离线RL以虚拟展开的形式从LLM中提取知识。总的来说，通过RL范式的支持，我们可以显著探索LLM中的内部知识，从而增强与外部环境的交互。当前的工作，例如Search-R1[685]，R1-Searcher [686]，RAGEN[687]和OpenManus-RL[688]正在探索利用RL方法对代理模型在代理环境中的轨迹数据进行微调。"
            },
            {
              "type": "text",
              "content": "·Besides, hierarchical RL is also a promising topic that helps foundation modelto decompose complex task and thenlearm optimal policies to solve each task via RL paradigm. For example, When2Ask [663] enables agents to request high-level instructions from LLM.The high-level LLM planner provides a plan of options, and the agent learns the low-level policy based on these options. Eureka[664] leverages LLM to generate human-level reward functions with reflection, allowing agents to efficiently learn complex tasks such as anthropomorphic five-finger manipulation. ArCHer [665] adopts a hierarchical RL approach, utilizing an offpolicy RL algorithm to learn high-level value functions, which in turn implicitly guide the low-level policy. LLaRP[666] leverages LLM to comprehend both textual task goals and visual observations. It employs an additional action output module to convertthe output of the LLM backbone into a distribution over the action space.Overall using hierarchical RL can guide AI Agent to explore optimal strategies when analyzing user requests for reasoning and planning.",
              "index": 5,
              "part": 0,
              "translated_content": "此外，分层强化学习也是一个有前途的课题，它帮助基础模型分解复杂任务，然后通过强化学习范式学习解决每个任务的最优策略。例如，When2Ask[663]使代理能够从LLM请求高层指令。高层LLM规划者提供选项计划，代理基于这些选项学习低层策略。Eureka[664]利用LLM生成具有反思能力的人类级奖励函数，使代理能够高效学习复杂任务，如拟人五指操纵。ArCHer[665]采用分层强化学习方法，利用离线RL算法学习高层值函数，进而隐式指导低层策略。LLaRP[666]利用LLM理解文本任务目标和视觉观察。它采用额外的动作输出模块将LLM骨干的输出转换为动作空间上的分布。总的来说，使用分层强化学习可以指导人工智能代理在分析用户请求时探索最优策略以进行推理和规划。"
            },
            {
              "type": "text",
              "content": "Using reinforcement learning,wecan integrate foundation models withonline learning from interactive environments, incorporating both action policies and world models.This integration enables advanced action systems in AIagents. Within the reinforcement learning paradigm, agents dynamically adapt and refine their decision-making processes in response toexternalfeedback,facilitating greater efficiency and effectivenessinactionlearning andachieving desired outcomes.",
              "index": 6,
              "part": 0,
              "translated_content": "利用强化学习，我们可以将基础模型与来自交互环境的在线学习相结合，同时整合动作策略和世界模型。这种整合使得人工智能代理中的高级动作系统成为可能。在强化学习范式中，代理动态地根据外部反馈调整和完善其决策过程，促进行动学习的效率和效果，从而实现期望的结果。"
            },
            {
              "type": "figure",
              "src": "images/b84593031c384ce9a4db7f387a7499daefd629b4ebf4d7b0924f98c48ada9975.jpg",
              "alt": "",
              "caption": "Figure 8.4: Ilustrative Taxonomy of Tool Systems in AI Agents, including tool category and leaming paradigm",
              "index": 7,
              "part": 0,
              "translated_caption": "图8.4：AI代理中工具系统的示意分类，包括工具类别和学习范式"
            },
            {
              "type": "text",
              "content": "Summary In general, Empowered by action systems, AI agents have demonstrated significant decision-making capabilities across various fields.For example, action learning enables AIagents to automate the understanding of Graphical User Interfaces (GUIs)and perform various operations, thereby improving human productivity through automatic computer usage.Moreover, several studies have shown that AI agents equipped with action systems can achiveremarkableoutcomes inrobotic manipulation tasks,suchasobject picking,laundry folding,and table cleaning. There are alsopromising researchdirections inthe industryemploying action models.Forinstance,autonomous driving (AD) has atractedconsiderable attention due to the exceptional performance of VLMs in perception and decisionmaking. By integrating human understanding through foundation models, AD systems can efectively comprehend real-world surrunding,enabling them to simulate human-level drivers.In summaryaction learning endows agents with the abilityto interact withtheexternal world,therebycreating moreopportunities forAIapplications inreal-world scenarios.",
              "index": 8,
              "part": 0,
              "translated_content": "总结一般来说，通过动作系统的支持，人工智能代理展示了在各个领域中显著的决策能力。例如，动作学习使得人工智能代理能够自动理解图形用户界面(GUI)并执行各种操作，从而通过自动化计算机使用提高人类的生产力。此外，一些研究表明，配备动作系统的人工智能代理在机器人操作任务中能够取得显著的成果，例如物体拾取、衣物折叠和桌面清理。工业界也有一些有前途的研究方向采用了动作模型。例如，自动驾驶(AD)因视觉语言模型在感知和决策方面的出色表现而引起了相当大的关注。通过将人类理解力整合到基础模型中，自动驾驶系统可以有效理解真实世界环境，使其能够模拟人类水平的驾驶员。总之，动作学习赋予代理与外部世界互动的能力，从而为人工智能在实际场景中的应用创造了更多机会。"
            }
          ],
          "raw_title": "Action Learning Paradigm",
          "type": null,
          "children": [],
          "translated_title": "8.3.2 行动学习范式"
        },
        {
          "title": "8.3.3 Tool-Based Action Paradigm",
          "number": "8.3.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Tool learning distinguishes human intellgence fromthatofother animals.Ever sincetheStone Age,human use of tools has boostedeffciency,productivity,and innovation.Similarly,enabling AIagents toperate indigitaland physical environments by harnessing various tools is a fundamental step toward achieving human-level intelligence.\n\nDefinitions InAI,toolsaredefinedasinterfaces,instruments,orresources thatallowagents tointeract withtheexteal world.Examples include web search[632,705,97,634],databases[706,707,708,709],coding environments [710], data systems[711,712,713],and weather forecasting[714].By translatingtool functionality intoplain textor API formats,foundation modelscan expand their problem-solving scope. The evolution of tool systems in AI can be summarized in stages.Initiall,with the adventof large language models[2],thefocuswas onconverting tools into explainable formats (e.g.,function calls).Later, advances in multimodal processng shifted interactions from conversational chats to graphical user interfaces (GUIs),and more recent work has explored embodied agents that control hardware(e.grobotic arms,sensors)tointeract withthe physical world.To simplify,atool-based actioncanbe considered a form of external action employed for assistance.",
              "index": 0,
              "part": 0,
              "translated_content": "工具学习区分了人类智能与其他动物的智能。自石器时代以来，人类利用工具提高了效率、生产力和创新能力。类似地，通过利用各种工具使人工智能代理在数字和物理环境中运行，是实现人类水平智能的基本步骤。\n\n在人工智能中，工具被定义为允许代理与外部世界互动的接口、工具或资源。例如，网络搜索、数据库、编码环境、数据系统和天气预报等都是工具的例子。通过将工具功能转化为纯文本或API格式，基础模型可以扩大其问题解决范围。人工智能中工具系统的演变可以总结为几个阶段。最初，随着大型语言模型的出现，重点是将工具转换为可解释格式（例如函数调用）。后来，多模式处理的进展将交互从对话式聊天转变为图形用户界面(GUI)，最近的工作探索了控制硬件（例如机器人手臂、传感器）与物理世界互动的具身代理。简而言之，基于工具的行动可以被视为一种用于辅助的外部行动形式。"
            },
            {
              "type": "text",
              "content": "Tool Category Similar toaction spaces,toolscan alsobeclasified into multiplecategories according totheir types. In this part, we mainlysummarize three keydomains,including language,digital, and physical. Inaddition, we also explore the potential of tool learning in emerging areas such as scientific discovery:\n\n·Language:To facilitate the use of external tools, we usually denote the tool asa kind of function call for foundation models, which usually encompasses task descriptions, tool parameters, and corresponding outputs. This expression allows LLMs to understand when and how to use tools in AI agents. Specifically, ToolFormer [689] expands the capabilities of language models by integrating external tool spaces, including calculator, QA systems,search engine, translation, and calendar.ToolLLM[690] uses RapidAPI as the action space and then uses a depth-first search-based decision tree algorithm to determine the most suitable tool for solving tasks. Gorilla[691] is a fine-tuned LLM based on the tool documents and then can be used to write API calls.ToolkenGPT[692] is to optimize tool embeddings and then enable LLMs to retrieve tools from the fine-tuned tool embeddings. GPT4tools [693] and AnyTool [694] are also building self-instruct datasets and then fine-tune LLMs on them for tool usage.Generally, due to the impressive capability of LLMs, language-based tool utilization for AI agents has been studied, with its effectiveness validated in abundant works, ranging from plain text or function calls to code programming.",
              "index": 1,
              "part": 0,
              "translated_content": "与动作空间类似，工具可以根据其类型被划分为多个类别。在这部分中，我们主要总结了三个关键领域，包括语言、数字和物理。此外，我们还探讨了工具学习在新兴领域（如科学发现）中的潜力：\n\n· 语言：为了促进外部工具的使用，我们通常将工具表示为基础模型的一种函数调用，这通常包括任务描述、工具参数和相应的输出。这种表达方式使大型语言模型能够理解何时以及如何在人工智能代理中使用工具。具体而言，ToolFormer [689]通过整合计算器、问答系统、搜索引擎、翻译和日历等外部工具空间，扩展了语言模型的功能。ToolLLM [690]将RapidAPI作为行动空间，然后使用基于深度优先搜索的决策树算法确定解决任务最合适的工具。Gorilla [691]是基于工具文档进行微调的语言模型，然后可用于编写API调用。ToolkenGPT [692]旨在优化工具嵌入，从而使大型语言模型能够从经过微调的工具嵌入中检索工具。GPT4tools [693]和AnyTool [694]还构建了自我指导的数据集，然后对其进行微调，以供工具使用。总的来说，由于大型语言模型的出色能力，人工智能代理的基于语言的工具利用已经得到研究，其有效性在大量作品中得到验证，涵盖了从纯文本或函数调用到代码编程的范围。"
            },
            {
              "type": "text",
              "content": "· Digital: With the success of LLMs in processing language information, many researchers are exploring extending the task scope of AI agents fromthe language to the digital domains (e.g., MultiModal, Web search, GUI, and so on). For example,MM-ReAct [497], ViperGPT[498], and Visual ChatGPT [496] employed LLMs as the controller and then used LLMs to select visual experts for solving different tasks.HuggingGPT [152] and Chameleon [153] use LLMs to first conduct reasoning and planning actions and then analyze which multimodal tools should be used for solving user instructions. WebGPT[632] and WebAgent[634] respectively empowered LLMs with search engines to enhance the capability of LLMs to solve more challenging tasks. Mobile-Agent [635] and AppAgent [636] respectively incorporate GUI manipulations and App usage as the tool-based actions to extend the task scope of AI agents in solving mobile phone tasks. In contrast to the physical world, digital environments usually provide simpler pipelines to collct and process data. By involving foundation models andtheir interaction with the digital environment,itis possible for us to develop intelligent assistants in computers, mobile phones, and other digital devices.",
              "index": 2,
              "part": 0,
              "translated_content": "· 数字：随着大型语言模型在处理语言信息方面取得成功，许多研究人员正在探索将人工智能代理的任务范围从语言领域扩展到数字领域（例如，多模态、网络搜索、图形用户界面等）。例如，MM-ReAct [497]、ViperGPT [498]和Visual ChatGPT [496]将LLMs作为控制器，然后使用LLMs选择视觉专家来解决不同的任务。HuggingGPT [152]和Chameleon [153]使用LLMs首先进行推理和规划动作，然后分析应该使用哪些多模态工具来解决用户指令。WebGPT [632]和WebAgent [634]分别为LLMs提供了搜索引擎，以增强LLMs解决更具挑战性任务的能力。Mobile-Agent [635]和AppAgent [636]分别将GUI操作和应用程序使用作为基于工具的行动，以扩展人工智能代理在解决手机任务中的任务范围。与物理世界相比，数字环境通常提供更简单的管道来收集和处理数据。通过涉及基础模型及其与数字环境的交互，我们可以在计算机、手机和其他数字设备中开发智能助手。"
            },
            {
              "type": "text",
              "content": "·Physical: For physical world applications, RT-2[643] demonstrates language-guided robotic manipulation using visual-language tools, and TidyBot [695] shows how LLMs adapt cleaning tools to personalized household preferences. SayCan [646] uses LLMs as the cognitive system to guide robots in solving tasks through robotic arms and visual perception. SayPlan [292] built a 3D scene graph as the action spaces and designed multiple actions and tools for 3D simulation, and then used LLMs as planners to invoke these actions or tools for robot task planning.Besides,specialized applications in real-world scenarios now also proliferate across diferent domains.For instance, in surgical robotics,[715] presents a multi-modal LLM framework for robot-assisted blood suction that couples high-level task reasoning,enabling autonomous surgical sub-tasks. Some autonomous driving systems [716,717] also integrate vision-language models with vehicle control tools for explainable navigation. In total, physical world applications pose the most significant challenge when compared to other tasks, butthey also offer the biggest industrial value.Therefore, it stillrequires us to continue exploring advanced action learning and tool integration in physical-based agents in the future.",
              "index": 3,
              "part": 0,
              "translated_content": "·物理：对于物理世界的应用，RT-2[643]展示了使用视觉-语言工具进行语言引导的机器人操作，TidyBot [695]展示了LLMs如何根据个性化家庭偏好调整清洁工具。SayCan [646]利用LLMs作为认知系统，通过机器人手臂和视觉感知指导机器人解决任务。SayPlan [292]构建了一个3D场景图作为行动空间，并为3D模拟设计了多个动作和工具，然后使用LLMs作为规划者来调用这些动作或工具进行机器人任务规划。此外，现在在不同领域中，真实场景中的专业应用也不断增加。例如，在外科机器人领域，[715]提出了一个多模态LLM框架，用于机器人辅助吸血，实现高层次任务推理，从而实现自主外科手术子任务。一些自动驾驶系统[716,717]也将视觉-语言模型与车辆控制工具集成，实现可解释的导航。总的来说，与其他任务相比，物理世界的应用是面临最大挑战的，但也提供了最大的工业价值。因此，未来仍需要我们继续探索物理环境中基于行动学习和工具集成的先进技术。"
            },
            {
              "type": "text",
              "content": "·Scientifc:Scientific tools have played atransformativerole in advancing AIagents acrossdisciplines,enabling them to learn, adapt, and execute tasks while integrating foundational models with frameworks that drive innovation and address complex challnges. In materials science, HoneyComb [696] exemplifies tool-driven advancements with its ToolHub. General Tools provide dynamic access to real-time information and the latest publications, efectively bridging gaps in static knowledge bases. Material Science Tools are designed for computationally intensive tasks,leveraging a Python REPL environment to dynamically generate and execute code for precise numerical analysis. Similarly,ChemCrow [697] demonstrates the transformative power of tools in chemistry by integrating GPT-4 with 18 expert-designed tools to automate complex tasks such as organic synthesis, drug discovery, and materials design.These tools include OPSIN for IUPAC-to-structure conversion, calculators for precise numerical computations, and other specialized chemistry software that enables accurate reaction predictions and molecular property evaluations. Similarly, SciToolAgent [698] showcases how multi-tool integration can revolutionize scientific research.Designed to addressthe limitations of existing systems, SciToolAgent integrates over 50O tools (e.g., Web API, ML models,function calls, databases, and soon).Finaly, SciAgent [699] exemplifies a multi-agent framework that integrates ontological knowledge graphs with specialized agents for hypothesis generation and critical analysis,emphasizing the power of modular, tool-driven systems to accelerate discovery in materials science and beyond. These examples underscore the transformative potential of integrating specialized tools into AIframeworks to address domain-specific challenges effectively.",
              "index": 4,
              "part": 0,
              "translated_content": "·科学：科学工具在推动AI代理在各个学科领域取得进步方面发挥了转变性作用，使它们能够学习、适应和执行任务，同时将基础模型与推动创新并解决复杂挑战的框架相结合。在材料科学领域，HoneyComb [696]通过其ToolHub展示了由工具驱动的进步。通用工具提供动态访问实时信息和最新出版物，有效地弥合了静态知识库中的差距。材料科学工具旨在为计算密集型任务设计，利用Python REPL环境动态生成和执行代码，用于精确数值分析。类似地，ChemCrow [697]通过将GPT-4与18个专家设计的工具集成，展示了化学领域工具的转变力量，以自动化有机合成、药物发现和材料设计等复杂任务。这些工具包括OPSIN用于IUPAC结构转换，计算器用于精确数值计算，以及其他专业化化学软件，实现准确的反应预测和分子性质评估。类似地，SciToolAgent [698]展示了多工具集成如何革新科学研究。设计用于解决现有系统的局限性，SciToolAgent集成了超过500种工具（例如Web API、ML模型、函数调用、数据库等）。最后，SciAgent [699]展示了一个集成本体知识图与专门代理用于假设生成和批判性分析的多代理框架，强调模块化、工具驱动系统在加速材料科学及其他领域的发现中的潜力。这些例子突显了将专业工具整合到AI框架中以有效解决特定领域挑战的转变潜力。"
            },
            {
              "type": "text",
              "content": "Tool learning Inspired byhuman evolution [718],the integration of tools in Al involves three key aspects: Tool Discovery (identifying suitable tools),Tool Creation (developing newtools)and Tool Usage (effectivelyemploying tools). We also systematically review existing literature and summarize them in the following:\n\n1. Tool Discovery: Inreal-world environments,there is a wide range of tools from thedigital to the physical world.Finding the most appropriate tools for user instructions can be challenging.Therefore,the processof tool discovery is to identify and select the appropriate tools that AI agents can operate on to achieve their objectives. This stage also requires the world models in AI agents to have a profound understanding of any complex user instructions and world knowledge of diferent tools.Moreover, the versatility of AI agents is also correlated with its abilityto operate diverse tool systems.Generall,tool discoverycan becategorized into two mainstream paradigms: retrieval-based and generative-based methods. Retrieval-based methods aim to select the most relevant tools from the tool library. For example, HuggingGPT [152] introduces a framework in which LLMs act as controlers,orchestrating task planning and then invoking suitable models from platforms such as Hugging Face to fulfill user intention. In generative-based approaches, we often fine-tune LLMs to learn how to use and select tools based on various user instructions.For instance, ToolFormer[689] collcts a massive corpus with the corresponding API calls (e.g.,calculator, QA system, search engines,translation, and calendar)for training. ToolLLM[690] collct tool instructions based on solution paths and then fine-tune Llama models to generate better API calls for tool utilization.",
              "index": 5,
              "part": 0,
              "translated_content": "受人类进化启发的工具学习[718]，将工具集成到人工智能中涉及三个关键方面：工具发现（识别合适的工具）、工具创建（开发新工具）和工具使用（有效利用工具）。我们还系统地审查了现有文献，并总结如下：\n\n1. 工具发现：在现实环境中，从数字到物理世界存在各种各样的工具。为用户指令找到最合适的工具可能具有挑战性。因此，工具发现的过程是识别和选择AI代理可以操作以实现其目标的适当工具。这一阶段还要求AI代理的世界模型对任何复杂的用户指令和不同工具的世界知识具有深刻理解。此外，AI代理的多功能性也与其操作不同工具系统的能力相关。一般来说，工具发现可以分为两种主流范式：基于检索和基于生成的方法。基于检索的方法旨在从工具库中选择最相关的工具。例如，HuggingGPT [152]引入了一个框架，其中LLMs充当控制器，编排任务规划，然后调用来自Hugging Face等平台的合适模型以实现用户意图。在基于生成的方法中，我们通常对LLMs进行微调，以学习如何根据各种用户指令使用和选择工具。例如，ToolFormer[689]收集了大量语料库，其中包括相应的API调用（例如计算器、问答系统、搜索引擎、翻译和日历）进行训练。ToolLLM[690]根据解决路径收集工具指令，然后对Llama模型进行微调，以生成更好的API调用以利用工具。"
            },
            {
              "type": "text",
              "content": "2.Tool Creation In addition to using existing tools,theability to create new tools plays acrucialrole in human civilization.Forlanguage agents,a widely adopted approach is to use LLMs to generate functions as executable programs, which consist of both the code and documentation.For example,PAL[701] generates programs as intermediate reasoning steps to solve problems, LATM[702] or Creator [703] use LLMs to create code for user intentions,and tofurther design a verifier to validate thecreated tools.SciAgent[699]notonly integrates multiple scientific tools but also crafts new tools for scientific discovery.More details on tool creation from an optimization perspective can be found in Section 9.4.2.",
              "index": 6,
              "part": 0,
              "translated_content": "2. 工具创建\n除了使用现有工具外，创造新工具的能力在人类文明中起着至关重要的作用。对于语言代理，一种被广泛采用的方法是利用LLMs生成作为可执行程序的函数，这些函数包括代码和文档。例如，PAL[701]生成程序作为解决问题的中间推理步骤，LATM[702]或Creator[703]使用LLMs为用户意图创建代码，并进一步设计验证器来验证所创建的工具。SciAgent[699]不仅集成了多个科学工具，还为科学发现创造新工具。关于工具创建的更多细节，可在第9.4.2节中找到从优化角度的讨论。"
            },
            {
              "type": "text",
              "content": "3.Tool Usage After collecting or creating tools,the effective use of tools constitutes the cornerstone of the capabilities of AI agents,alowing applications that bridge virtual and physical worlds.Modern AI agents increasingly employ tools to tackle complex tasks across diverse domains, with three key dimensions of expansion: 1) Vertical Specialization: Agents leverage domain-specific tools to achieve professional-grade performance in complex fields such as robotics,science, and healthcare; 2) Horizontal Integration: Systems combine multiple toolkits across modalities (vision, language,control)for multimodal problem-solving; 3) Embodiment: Agents physically interact with environments through robotic tools and sensors.",
              "index": 7,
              "part": 0,
              "translated_content": "3. 工具使用\n在收集或创建工具之后，有效利用工具构成了AI代理能力的基石，使得能够构建虚拟和物理世界之间的应用程序。现代AI代理越来越多地利用工具来处理跨越各种领域的复杂任务，具有三个关键的扩展维度：1) 垂直专业化：代理利用特定领域的工具在复杂领域（如机器人技术、科学和医疗保健）中实现专业水平的性能；2) 水平整合：系统跨越多种形式（视觉、语言、控制）结合多模态工具包进行问题解决；3) 具体化：代理通过机器人工具和传感器与环境进行物理交互。"
            },
            {
              "type": "text",
              "content": "Summary Toollearning and action learning constitutethetwo most important components of the action system in AI agents.Toollearning canbeconsideredasakindofaction touse external states for problem-solving.Toollearning enables AIagents tosubstantiallbroaden theirrangeoftasks,pushing the boundaries beyond the scopeoffoundation models.For example,empowered by APIorfunctioncall,language modelscandirectlyreuse the capabilityof existing models (e.g.retrieval,coding,web search)to generate answers,ratherthan next-token prediction[719].Toollearning also involves multiplechallnging stages,including how todetermine the tool space,howtodiscoverand select tools, and howtocreate anduse tools.Overall,tollearning playsa pivotalrole inbuildinganomnipotent AIagentframework to solve complex tasks in different domains.",
              "index": 8,
              "part": 0,
              "translated_content": "总结 工具学习和行动学习构成了AI代理行动系统中最重要的两个组成部分。工具学习可以被视为利用外部状态进行问题解决的一种行动方式。工具学习使得AI代理能够显著拓展他们的任务范围，将边界推动至基础模型的范围之外。例如，通过API或函数调用的支持，语言模型可以直接重复利用现有模型的能力（例如检索、编码、网络搜索）来生成答案，而不是进行下一个标记的预测。工具学习还涉及多个具有挑战性的阶段，包括如何确定工具空间、如何发现和选择工具，以及如何创建和使用工具。总体而言，工具学习在构建一个全能的AI代理框架中发挥着至关重要的作用，以解决不同领域中的复杂任务。"
            }
          ],
          "raw_title": "Tool-Based Action Paradigm",
          "type": null,
          "children": [],
          "translated_title": "8.3.3 工具支持的行动范式"
        }
      ],
      "translated_title": "8.3 主体行动系统的范式"
    },
    {
      "title": "8.4 Action and Perception: “Outside-In\" or “Inside-out\"",
      "number": "8.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "A central debate in cognitive science and neuroscience concerns whether action or perception stands at the root of causa flow in intelligent systems.Figure 8.5 presents different perspectives.The traditional“outside-in”view insists that causalinfluence begins withexternalstimuli.The environment excites peripheralreceptors,these signals propagate inward, and eventually producebehavior.This perspective portrays the organism—or agent—as essentially reactive:the external worldcauses sensory changes,and the agent's actions represent adownstream effect of those changes. In contrast,Buzsaki's“inside-out\"framework[18]proposes that it is the agent's own actions that shape the meaning and consequences of incoming signals. Such a view implies an active agent, one which continuously generates predictions and motor commands, while sending“corollary discharg\"or“action copies\"to sensory areas. These internally generated signals serve asreferences thatinform the agent which sensorychanges are self-initiated ratherthan imposedbytheoutside world.Inthis manner,cause shifts fromanexternalevent toan internalllaunched initiative,leavingexternalstimulitoplayaconfirmatoryorcorrectiverole.This reversalhas significant implications for how we interpret perception's purpose and function:it isnot an end initself,buta means of updatingandrefiningthe agent's own action-driven hypotheses about the environment.",
          "index": 0,
          "part": 0,
          "translated_content": "认知科学和神经科学中的一个核心争论是关于行动或感知在智能系统中的因果流中起到根本作用。图8.5呈现了不同的观点。传统的“从外到内”的观点坚持认为，因果影响始于外部刺激。环境刺激外围感受器，这些信号向内传播，最终产生行为。这种观点描绘了生物体或代理人本质上是被动的：外部世界导致感觉变化，代理人的行为代表了这些变化的下游效应。相比之下，Buzsaki的“从内到外”的框架[18]提出，正是代理人自己的行为塑造了传入信号的意义和后果。这种观点意味着一个主动的代理人，不断产生预测和运动命令，同时向感觉区域发送“副产生放电”或“行动副本”。这些内部产生的信号作为参考，告知代理人哪些感觉变化是自发启动的，而不是外部世界强加的。通过这种方式，因果从外部事件转变为内部发起的倡议，使外部刺激扮演确认或纠正角色。这种逆转对我们如何解释感知的目的和功能有重要影响：感知并非自身目的，而是更新和完善代理人关于环境的基于行动的假设的手段。"
        },
        {
          "type": "text",
          "content": "From an evolutionary perspective, possessing the abilityto move without relying on sophisticated sensory analysis can yield immediate survival benefits. Even simple organisms profit from periodic motion that stirs up food in nutrient-richwater,long before elaborate perceptualcapacities evolve.Inother words,movement precedes advanced sensing inevolutionary time,suggesting that thecapacity to act is not merely the efect of external stimuli but can",
          "index": 1,
          "part": 0,
          "translated_content": "从进化的角度来看，拥有在不依赖复杂感知分析的情况下移动的能力可以带来即时的生存益处。即使是简单的生物体也能从定期运动中获益，这种运动搅动了富含营养的水域中的食物，早在复杂的感知能力进化之前。换句话说，在进化的时间尺度上，运动先于高级感知，这表明行动能力不仅仅是外部刺激的结果，而是可以"
        }
      ],
      "raw_title": "Action and Perception: “Outside-In\" or “Inside-out\"",
      "type": null,
      "children": [],
      "translated_title": "8.4 活动与感知：由外而内还是由内而外"
    },
    {
      "title": "Brain from Outside-ln",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "figure",
          "src": "images/34387808fd61693c71bcf3ae8a21530f13f589892fd8dbdadf3e42bdbb71da00.jpg",
          "alt": "",
          "caption": "Figure 8.5:(a)Compare the brain from“outside-in\"and\"inside-out\".(b)Ilustrationof the schematicofthe corollary discharge mechanism.Amotor command (efferent signal) travels from motor areas tothe eye muscles,while acorollary discharge(dashed arrow)is routedto acomparator in the sensorysystem.The comparator uses this internal signal to modulate or subtract external (exafferent) input.Additionally, tension feedbackfrom the muscles (reafferentsignal) exerts adelayed effectonperception.Direct projections from motor to sensorycortices underliethis architecture in all mammals. Part (b) is adapted from the original figure in [18].",
          "index": 0,
          "part": 0,
          "translated_caption": "图8.5：(a) 从“外部到内部”和“内部到外部”比较大脑。(b) 描述了余留放电机制的示意图。一个运动命令（传出信号）从运动区域传输到眼部肌肉，同时一个余留放电（虚线箭头）被路由到感觉系统中的比较器。比较器使用这个内部信号来调节或减去外部（外来）输入。此外，来自肌肉的张力反馈（再传入信号）对感知产生延迟影响。所有哺乳动物的大脑皮层中都存在从运动到感觉皮层的直接投射，这种结构是该架构的基础。部分(b)改编自参考文献[18]中的原始图。"
        },
        {
          "type": "text",
          "content": "itself be the driving cause of subsequent perceptual development. It is precisely when action mechanisms become sufficiently establishedthat the agent benefits from additionalsensors,which guidethose movements more strategically. This developmentalsequence grounds perception inutility,tying sensory discrimination to the practical outcomesof movement.\n\nDisruptions in the normalinterplay of action and perceptionilluminate the intricate cause-effect loop.During sleep paralysis,the brain's motorcommands temporarily fail to reach the muscles; external stimuli stillbombardthe senses, but the usual action-to-perception calibration is lost. As a result, the individual experiences a heightened sense of unreality because the brain lacks internally generated reference signals to interpret sensory input. Similarly,if one externally manipulates the eye without the brain issuing a motor command,the visual scene appears to move, highlighting how perception alone—devoid of a preceding,self-initiated action—risks confusion.Neurophysiological data further support the inside-out model.Many neurons in areas once deemed“purely sensory\"track not only changes in external stimuli but also self-generated movements—sometimes more strongly so.This indicates that“cause\"in the brain frequently emerges from within,guiding both the magnitude and meaning ofexternal signals.Without these internal correlates, raw sensory data can become ambiguous or even useless to the system.",
          "index": 1,
          "part": 0,
          "translated_content": "在行动机制足够建立之时，动作本身成为随后感知发展的推动因素。正是额外传感器的加入使代理受益，这些传感器更有策略地指导这些运动。这种发展顺序将感知基础于效用，将感官辨别与运动的实际结果联系在一起。\n\n行动和感知正常互动的中断揭示了错综复杂的因果循环。在睡眠瘫痪期间，大脑的运动命令暂时无法到达肌肉；外部刺激仍然轰击感官，但通常的行动到感知的校准丢失了。因此，个体会经历一种加剧的虚幻感，因为大脑缺乏内部产生的参考信号来解释感官输入。同样，如果在大脑没有发出运动命令的情况下外部操纵眼睛，视觉场景似乎会移动，突显了单纯感知——缺乏先前、自发行动的情况下——可能导致混淆。神经生理数据进一步支持内在-外在模型。许多曾被视为“纯粹感觉”的区域的神经元不仅追踪外部刺激的变化，还追踪自发的运动——有时甚至更强烈。这表明大脑中的“原因”经常源自内部，指导外部信号的幅度和意义。没有这些内部相关因素，原始的感官数据可能对系统而言变得模棱两可，甚至无用。"
        },
        {
          "type": "text",
          "content": "Implications for IntelligentAgents The inside-out perspectiveofers potent insights for modernresearch on intelligent agents.Most contemporary AI systems—and many LLM agents—stillfunction predominantly in a reactive mode, awaiting user input and generating responses based on statisticalcorelations learned from vast datasets.Such pasivity resembles an“outside-in\"framework,where the agent'sroleislimited toresponding,notinitiating.Yetfanagent were to be active,continuously forming and testing hypotheses via self-initiated behaviors (physicalorrepresentational), it might ground its own“perceptual\" inputs—be they sensory streams or linguistic prompts—and thereby reduce ambiguity.Forinstance,anLLM-basedagent that interjects questions orverifies itsownstatements againstaknowledge base could better discern which inferences are self-caused from those demanded by external data.Bytracking these self-initiatedcontributions (analogous tocorollarydischarge),the modelcouldimprovecoherence,lessn erors known as “hallucinations\", and refine its internal state through iterative cause-effect loops.",
          "index": 2,
          "part": 0,
          "translated_content": "智能代理的启示内部-外部视角为现代智能代理研究提供了深刻的见解。大多数当代人工智能系统——以及许多LLM代理——仍然主要以一种被动模式运作，等待用户输入，并根据从大量数据集中学习到的统计相关性生成响应。这种被动性类似于一种“外部-内部”框架，代理的角色受限于响应而非主动发起。然而，如果一个代理是主动的，通过自发行为（物理或表征性）持续形成和测试假设，它可能会基于自身的“感知”输入——无论是感官流或语言提示——从而减少歧义。例如，一个基于LLM的代理，插入问题或根据知识库验证自己的陈述，可以更好地区分哪些推论是自身引起的，哪些是外部数据要求的。通过跟踪这些自发贡献（类似于余流放电），模型可以改善连贯性，减少称为“幻觉”的错误，并通过迭代的因果循环优化其内部状态。"
        },
        {
          "type": "text",
          "content": "A proactive stance also encourages more data-effcient andcontext-aware learning.Instead of passvely waiting for labeled examples,anagent can explore,provoke feedback, and incorporate self-generatedexperiences intoitstraining. Over time,this tight coupling between action and perception may bolster the agent's ability tohandle complex tasks, adapt to unanticipated challenges,and generalize more robustly.The shift from anoutside-in toan inside-out model reframes perception as causally downstream of action.Intellgent systems-whether biological or artificial—stand to benefit from recognizing that purposeful movement,or proactive conversational steps in thecase of LLMs,can actively create,shape, and interpret the signalsthatflow back in.Byacknowledging thecause-efect power of action and strivingto buildactiveratherthanmerelyreactive agents,we mayapproach adeperunderstanding of both natural cognition and the next generation of AI.",
          "index": 3,
          "part": 0,
          "translated_content": "积极的立场还鼓励更具数据效率和上下文感知的学习。代理不再 passively 等待标记示例，而是可以探索、引发反馈，并将自动生成的经验纳入其训练中。随着时间的推移，行动和感知之间的紧密耦合可能增强代理处理复杂任务、适应意外挑战并更稳健地泛化的能力。从外部到内部模型的转变将感知重新定位为行动的因果下游。智能系统——无论是生物还是人工的——都有望从认识到有目的的运动，或者在LLM的情况下是积极的对话步骤，可以积极地创造、塑造和解释回流信号中的信号中受益。通过承认行动的因果关系能力，并努力构建积极而非仅仅是反应性代理，我们可能会更深入地理解自然认知和下一代人工智能。"
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td>Dimension</td><td>Human Brain / Cognition</td><td>LLM Agent</td><td>Remarks</td></tr><tr><td>Perception</td><td>- Integrates multiple sen- sory channels (vision, hear- ing, smell, touch, taste). - Perception closely tied to - Perception depends on ex- emotions, endocrine system, and physical state. - Highly sensitive, capable of- Lacks real-time coupling detecting subtle differences.</td><td>- Primarily language-based with some multimodal capa- bilities. ternal sensors and models with limited integration. with physical states.</td><td>Perception differences lead to varying ways of under- standing reality. Embodied AI attempts to bridge this gap but still faces both hardware and software challenges.</td></tr><tr><td> tion</td><td>Unified Representa- - Simultaneously processes multimodal inputs: vision, hearing, language, motion, and emotions. laborate to create unified spa- tiotemporal and semantic un- derstanding.</td><td>- Primarily text-based. Some multimodal models can pro- cess images or audio but with low integration. - Different brain regions col- - No fully unified spatiotem- poral modeling like the hu- man brain.</td><td>Even advanced multimodal models  lack  the human brain's holistic,unified representation capacity. Hardware and algorithmic challenges remain.</td></tr><tr><td>Granularity in Task Switching</td><td>- Flexible in shifting between macro and micro cognitive tasks. - Can plan at a high level - Cannot autonomously real- and shift focus to finer details locate attention between task when needed. - Adjusts task priority and - May get stuck in a spe- focus dynamically based on context and working mem- ory.</td><td>- Relies heavily on prompt engineering for granularity control. layers. cific level of abstraction in absence of guided prompts.</td><td>Humans can dynamically adjust cognitive granular- ity based on situational de- mands, while LLMs require explicit instruction to switch task focus effectively.</td></tr><tr><td> Action</td><td>Goal-oriented process drives multiple sensory to make decisions. - Real-time Learning from the  experience via  the environmental interaction. - Encompass both physical activities and mental pro- cesses.</td><td>- Action space need to be de- fined in advance. - Unable to support actions in continuous spaces. - Relies on online training to optimize the decision- capability. making process in the envi- ronment.</td><td>Humans are capable of actively learning new actions and performing continuous actions, whereasLLM agents currently lack this</td></tr></table></body></html>",
          "caption": "Table 8.2: Comparing the perception and action of human and AI agents.",
          "index": 4,
          "part": 0,
          "translated_caption": "表8.2：比较人类和人工智能代理的感知和行动。"
        }
      ],
      "raw_title": "Brain from Outside-ln",
      "type": null,
      "children": [],
      "translated_title": "外源性大脑"
    },
    {
      "title": "8.5 Summary and Discussion",
      "number": "8.5",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Traditionally,actionrepresents the behaviorsof the human cognition system based onthe interactive feedback fromthe environment.Itendows humans with thecapability tothink,reason,speak,run,and perform anycomplex manipulations. Based on the action system,humans can iteratively evolve thebrain intelligenceby enhancing their perception and actions fromthe world,andformaclosedloop tofurther create newcivilization and innovationin the world.Similarlyto a humancognitionsystem,theaction systemplus the tool system alsoplayan importantrole forAIagents.Integrating action systems allows AIagents tosystematicaly plan,execute, andadjusttheir behaviors,facilitating more adaptable and robust performance in dynamiccontexts. Inthis section, we systematically examine and summarizethe impact of the action module on AI agents, focusing on both action systems and tool systems.",
          "index": 0,
          "part": 0,
          "translated_content": "传统上，行为代表了基于与环境的互动反馈的人类认知系统的行为。它赋予人类思考、推理、言语、奔跑和执行任何复杂操控的能力。基于行为系统，人类可以通过增强他们对世界的感知和行动来逐步发展大脑智能，并形成一个闭环以进一步在世界中创造新的文明和创新。类似于人类认知系统，行为系统加上工具系统对AI代理也发挥着重要作用。整合行为系统使AI代理能够系统地规划、执行和调整他们的行为，促进在动态环境中更具适应性和鲁棒性的表现。在本节中，我们系统地审查和总结了行为模块对AI代理的影响，重点关注行为系统和工具系统。"
        },
        {
          "type": "text",
          "content": "Action System In our studies,we briefly describe the action system from three perspectives:action space, action learning,andtoollearning. Inanaction system,action space usually serves as the most important component, which determies the upper bound of AI agents in solving downstream tasks. Itformulates which actions can be selected and performed byAIagents during interactions withreal-worldenvironments. For action space,there are alsovarious diffculties depending ondata types,ranging from discrete tocontinuous data. Withthe growing demandforAI agents, there is alsoarising expectationforAIagents tohandle more sophisticatedtasks,particularlythose involvingreal-world applications. Therefore,how to buildrobust and generalaction space issillan ongoingchallenge inaction systems. On the basis of action space, action learning is another crucial component in enabling agents to interact efectively with theexternal world and withhumans. Action learning represents the process of an AIagent to learn and optimize its policy during interaction with real-world environments. Based on diffrent foundation models,it also derives diferent action learning paradigms,from zero-shot learming (e. g.",
          "index": 1,
          "part": 0,
          "translated_content": "行为系统 在我们的研究中，我们从三个角度简要描述了行为系统：行为空间、行为学习和工具学习。在行为系统中，行为空间通常作为最重要的组成部分，决定了AI代理在解决下游任务时的上限。它规定了AI代理在与真实世界环境互动时可以选择和执行哪些动作。对于行为空间，根据数据类型的不同，从离散到连续数据，也存在各种困难。随着对AI代理的需求不断增长，人们对AI代理处理更复杂任务的期望也在增加，特别是涉及真实应用的任务。因此，如何构建稳健和通用的行为空间仍然是行为系统中的一个持续挑战。在行为空间的基础上，行为学习是使代理能够有效与外部世界和人类互动的另一个关键组成部分。行为学习代表了AI代理在与真实世界环境互动过程中学习和优化其策略的过程。基于不同的基础模型，它也衍生出不同的行为学习范式，例如零样本学习。"
        },
        {
          "type": "text",
          "content": ", prompt engineeing)to supervised training and reinforcement learning. In action learning,it isessential tothoroughly understandthetask,including how to devise system prompts,how todetermine thepre-trained orfine-tuned datasets,and the reward signals oroptimization polices during the training. Despite notable progressin action learning to advance AIagent frameworks, numerous questions remain to be addressd. Specifically, the ICL paradigm requires specific prior knowledge fora proper prompt design. Additionally,combining pre-training and post-training for supervised training necessitates high-quality and diverse data, which oftenrequires meticulous data processng and significanthuman efort. Furthermore,the unstable nature of reinforcement learming poses difficulties inits application inlarge-scale training scenarios. Moreover, the designof action systems playsacrucialrole in maximizingthebenefitsof toolintegration. By incorporating aneffective action system,AIagents can seamlessly engage with various tools,execute complex user intents,and transform external data into meaningful outcomes. This synergy between action systems and tools not only mitigates the limitations of memorizationandreduces the riskof hallucinations[714]but also enhances the expertise androbustness of the system. For instance, an AI agent equipped with a robust action system can dynamicall select and employ the most appropriate tols fora giventask,ensuring bothaccuracy and effciency in its responses.",
          "index": 1,
          "part": 1,
          "translated_content": "在行为学习中，深入了解任务是至关重要的，包括如何设计系统提示、如何确定预训练或微调数据集，以及在训练过程中的奖励信号或优化策略。尽管在推动AI代理框架方面在行为学习方面取得了显著进展，但仍有许多问题有待解决。具体来说，ICL范式需要特定的先验知识来进行适当的提示设计。此外，将预训练和后训练结合进行监督训练需要高质量和多样化的数据，这通常需要细致的数据处理和大量人力投入。此外，强化学习的不稳定性在大规模训练场景中应用时存在困难。此外，行为系统的设计在最大化工具集成的好处方面起着至关重要的作用。通过整合有效的行为系统，AI代理可以无缝地与各种工具互动，执行复杂的用户意图，并将外部数据转化为有意义的结果。行为系统与工具之间的协同作用不仅可以减轻记忆限制并降低幻觉风险，还可以增强系统的专业知识和稳健性。例如，配备强大行为系统的AI代理可以动态选择并使用最适合特定任务的工具，从而确保其响应的准确性和效率。"
        },
        {
          "type": "text",
          "content": "Furthermore,action systems facilitatehierarchicalreasoning processs,enabling agentstoorchestrate intricateworkflows thatalignclosely with userobjectives. This alignment is essential fortasks requiring precise execution andreal-timedecision-making, thereby bridging the gap between foundational modelcapabilities and practical applicationdemands. Additionally, the transparencyandinterpretabilityprovidedbytoolexecutionprocessesenhanceusertrust andfacilitate effective humanmachinecolaboration. Consequently, the combination of specialized tools and robust action systems significantly elevates the performance, reliability, and applicability of AI agents in diverse and dynamic environments.",
          "index": 1,
          "part": 2,
          "translated_content": "此外，行为系统促进了层次推理过程，使代理能够组织复杂的工作流程，与用户目标紧密契合。这种对齐对于需要精确执行和实时决策的任务至关重要，从而弥合了基础模型能力与实际应用需求之间的差距。此外，工具执行过程提供的透明度和可解释性增强了用户信任，并促进了有效的人机协作。因此，专业工具与强大行为系统的结合显著提升了AI代理在多样化和动态环境中的性能、可靠性和适用性。"
        },
        {
          "type": "text",
          "content": "In summary,action systemscan significantlyestablish thefoundationforthe problem-solvingcapabilitiesof AIagen frameworks, enabling them to tackle a broader range of complex tasks beyond foundation models.\n\nruture Directions Nonetheless, building an effective action system for agents requires solutions to a numberof :hallenges, as we summarize in the following:\n\n1.Effciency presents a significant hurdle,particularly in real-time applications where swift and precise responses are critical.Thecomplexity involved in action system can lead to unacceptable latency,hindering the practical deployment of AI systems in scenarios like fraud detection or real-time decision-making.To mitigate these efficiency issues, strategies such as filtering out irrelevant or redundant information, employing zero-shot prompting to streamline reasoning processes, and utilizing high-speed storage solutions for caching pertinent knowledge are imperative. These approaches help in maintaining high performance while reducing response times.",
          "index": 2,
          "part": 0,
          "translated_content": "总之，行为系统可以显著建立AI代理框架的解决问题能力的基础，使它们能够处理更广泛的复杂任务，超越基础模型的范围。\n\n未来方向然而，为代理构建有效的行为系统需要解决一系列挑战，我们在以下总结：\n\n1.效率是一个重要障碍，特别是在需要快速和精确响应的实时应用中。行为系统涉及的复杂性可能导致不可接受的延迟，阻碍AI系统在欺诈检测或实时决策等场景中的实际部署。为了缓解这些效率问题，必须采取策略，如过滤掉不相关或冗余信息，利用零提示来简化推理过程，并利用高速存储解决方案缓存相关知识。这些方法有助于保持高性能的同时减少响应时间。"
        },
        {
          "type": "text",
          "content": "2. Evaluation is also a important factor in action system, including action learning and toollearning. In the real-world environment, there exists massive actions from different sources. Therefore,how to determine the correct action or tools from disparate sources to avoid conflicting information is stilla significant challenge in AI Agent.To alleviate these problems, how to build an effective and robust evaluation system to measure action system is essential to maintain the accuracy and reliability of responses.Developing robust evaluation system, verification protocols and creating transparent methods are crucial to reduce incorectness in action prediction.Besides,exposing the decision-making processes of foundation models also help us understand which action is better and how to coordinate with various actions or tools to provide trustworthy outputs.",
          "index": 3,
          "part": 0,
          "translated_content": "2. 评估在行为系统中也是一个重要因素，包括行为学习和工具学习。在现实世界的环境中，来自不同来源的大量行为存在。因此，如何确定来自不同来源的正确行为或工具，以避免冲突信息，仍然是AI代理中一个重要挑战。为了缓解这些问题，建立有效和稳健的评估系统来衡量行为系统是至关重要的，以保持响应的准确性和可靠性。开发稳健的评估系统、验证协议，并创建透明的方法对于减少行为预测中的不正确性至关重要。此外，揭示基础模型的决策过程也有助于我们理解哪种行为更好，以及如何与各种行为或工具协调，提供可信赖的输出。"
        },
        {
          "type": "text",
          "content": "3. Multi-modality Action learming has achieve remarkable progresses in LLM-based autonomous agent, due to the success of large language models.However,how to understand and invoke action beyond the language instructions (e.g.,GUI operations or embodied tools) stillremain challnges. In real-world scenarios, humans can develop or learn to use new skills through any kinds of instructions (e.g.,language, image, videos or human guidance). Therefore, enabling AI agents to develop or learn actions through diverse modalities is crucial to advance the capability of AI Agent in solving practicaltasks from the real-world scenarios.In other words, it is necessary forus to explore how to reduce the gap between human and AI agents in tool utilization, facilitating the design of advanced agent frameworks for the future.",
          "index": 4,
          "part": 0,
          "translated_content": "3. 基于大型语言模型的自主代理中，多模态行为学习取得了显著进展。然而，如何理解和调用超出语言指令范围的行为（例如GUI操作或具体工具）仍然是挑战。在现实场景中，人类可以通过任何形式的指令（例如语言、图像、视频或人类引导）开发或学习新技能的使用。因此，使AI代理能够通过多样的模态开发或学习行为对于提升AI代理在解决来自现实场景的实际任务中的能力至关重要。换句话说，我们有必要探索如何减少人类和AI代理在工具利用方面的差距，促进未来先进代理框架的设计。"
        },
        {
          "type": "text",
          "content": "4. Privacy is a critical concern in the field of generative AI, especially using LLMs.As a consequence, maintaining the privacy of sensitive user data and preventingthe disclosure of user behaviors are essential in tool utilization[720]. To address these privacy concerns,some safe techniques like federated learning can be used to enable models to be trained on decentralized data sources without exposing sensitive information directly. Additionally, model distilation is often necessary to ensure models maintain high performance while safeguarding data integrity.These methods enable the efective training of models while preserving the confidentiality of user data.\n5. Safty Moreover, the ethicalimplications of human-model collaboration and the safety concerns associated with models interacting with physical environments necesitate careful consideration. Ensuring that human dignity and agency are preserved when integrating human labor with AIsystems is critical. Establishing ethical guidelines, promoting fair working conditions, and fostering interdisciplinary collaboration are necessary to address these concerns.Additionally,developing robust safety mechanisms to prevent erroneous or malicious actions by AI systems interacting with physical tools or actions is imperative to safeguard against potential risks.",
          "index": 5,
          "part": 0,
          "translated_content": "4. 隐私在生成式人工智能领域尤为关键，特别是在使用大型语言模型时。因此，在工具利用中，维护敏感用户数据的隐私和防止用户行为的披露至关重要。为了解决这些隐私问题，可以采用一些安全技术，如联邦学习，使模型能够在分散的数据源上进行训练，而不直接暴露敏感信息。此外，通常需要进行模型蒸馏，以确保模型在维护数据完整性的同时保持高性能。这些方法能够有效地训练模型，同时保护用户数据的保密性。\n\n5. 此外，人类与模型协作的伦理影响以及模型与物理环境交互所涉及的安全问题需要认真考虑。在将人类劳动与人工智能系统整合时，确保人类尊严和自主权得到保护至关重要。建立伦理准则，促进公平的工作条件，促进跨学科合作是解决这些问题的必要条件。此外，发展强大的安全机制，以防止人工智能系统与物理工具或行为交互时发生错误或恶意行为，对于防范潜在风险至关重要。"
        },
        {
          "type": "text",
          "content": "In additiontotheabovechallenges,there alsoremainopen problemsforthe action system.Forexample,howto achieve an optimal balance between the foundation models and external tools,deciding on the appropriate timing to use the former versus thelatter,remainsunanswered.Specifically,although tool systems canoffer flexibilityandextensibility for foundation models,there is an increasing trend toenhance the intrinsiccapabilityoffoundation models.Therefore, balancing between foundation models and tool systems is essential for developing versatile and effcient AIagents.",
          "index": 6,
          "part": 0,
          "translated_content": "除了上述挑战之外，行动系统仍然存在一些未解决的问题。例如，如何在基础模型和外部工具之间实现最佳平衡，确定何时使用前者而非后者的适当时机，仍然没有答案。具体来说，尽管工具系统可以为基础模型提供灵活性和可扩展性，但越来越多地增强基础模型的内在能力是一个增长的趋势。因此，在基础模型和工具系统之间取得平衡对于开发多功能且高效的人工智能代理至关重要。"
        }
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": [],
      "translated_title": "8.5 总结与讨论"
    },
    {
      "title": "Self-Evolution in Intelligent Agents",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "figure",
          "src": "images/7eb2110a388d0c61ba0b70305ed0f1721a096440997d77ce36c9e16143055348.jpg",
          "alt": "",
          "caption": "Figure 8.6: Structures of self-evolution in LLM agents.",
          "index": 0,
          "part": 0,
          "translated_caption": "图8.6：LLM代理中的自我进化结构。"
        },
        {
          "type": "text",
          "content": "In the history of machine learning research, manually designed AI systems have gradually been replaced by more efcient,learned solutions[756]For instance,before the advent ofdeeplearning,features were typicallyhandcrafted by experts[757,758],butthese were eventuallsupersededbyfeatures extractedthrough neuralnetworks.As neural networks have become increasingly complex,various techniques for automated design-such as neural architecture search-haveemerged,further replacingthe needfor manualldesignednetwork structures[759]. Similarly,Agentic systems initiallrelied heavilyon manual design,with behaviorrules anddecision-making strategies explicitlyrafted by developers.Althoughfullautomationofagentself-evolutionhas not yet been achieved,itisanticipatedand deemed necessary for future progress.A successfulprecedentfor such automationcan already be seen inautomated machine learning(AutoML)[712,760,761,762,204]whichhas automatedvarious components of traditional machine leaing pipelines. In particular, AutoML streamlines theselection and configuration of machine learming algorithm pipelines while incorporating advanced techniques for hyperparameter optimization[763,764,765,766,767].Among the most notable applicationsof AutoML is NAS [768,769], which automates the design of neuralnetwork architectures to enhance model performance.Drawing inspiration from this successful transition towards automation in traditional machine learning, we propose extending similar principles to the domain of agentic AI systems.",
          "index": 1,
          "part": 0,
          "translated_content": "在机器学习研究的历史中，手动设计的人工智能系统逐渐被更高效的学习解决方案取代。例如，在深度学习出现之前，特征通常是由专家手工设计的，但最终被神经网络提取的特征所取代。随着神经网络变得越来越复杂，各种自动设计技术，如神经架构搜索，已经出现，进一步取代了需要手动设计网络结构的需求。同样，代理系统最初严重依赖手动设计，行为规则和决策策略是由开发人员明确制定的。虽然代理自我进化的完全自动化尚未实现，但预计并被认为是未来进展所必需的。这种自动化的成功先例已经可以在自动化机器学习（AutoML）中看到，它自动化了传统机器学习流程的各个组件。特别是，AutoML简化了机器学习算法流水线的选择和配置，同时结合了高级的超参数优化技术。在AutoML的最显著应用中，NAS自动设计神经网络架构以增强模型性能。受传统机器学习中成功向自动化转变的启发，我们提议将类似原则扩展到代理人工智能系统的领域。"
        },
        {
          "type": "text",
          "content": "Akeycounterintuitive issue inmuchofcurrentagentresearchisthat,while theultimategoalofdevelopingorimproving agentic AIsystems isto automatehuman efforts, the processofcreating these systemsremains,forthe time being, beyondthereachof fullautomation.Therefore, we argue that all manuallydesignedagenticAIsystems willeventually be replaced by learnable and self-evolvingsystems,which couldultimately place the development and improvement of agentic AI into an autonomous,self-sustaining loop.Enabling self-evolution mechanism in LLM agents has the following benefits:",
          "index": 2,
          "part": 0,
          "translated_content": "当前代理研究中一个关键的反直觉问题是，虽然开发或改进代理人工智能系统的最终目标是自动化人类工作，但创建这些系统的过程目前仍然超出了完全自动化的范围。因此，我们认为，所有手动设计的代理人工智能系统最终将被可学习和自我进化的系统所取代，这可能最终将代理人工智能的开发和改进置于一个自主、自我维持的循环中。在LLM代理中启用自我进化机制具有以下好处："
        },
        {
          "type": "text",
          "content": "1. Scalability: While LLM-based agents have demonstrated remarkable performance, their improvement stll heavily depends on the underlying LLMs.However, upgrading these models iscostly, and scaling performance through the inclusion of additional real-world data requires extensive retraining on large datasets, which poses significant resource constraints.Self-evolving agentic systems,in contrast,can optimize agent behavior without necessitating modifications to the underlying LLMs,offering a more eficient and scalable solution.",
          "index": 3,
          "part": 0,
          "translated_content": "1. 可扩展性：尽管基于LLM的代理展现出了卓越的性能，但它们的改进仍然严重依赖于基础的LLM。然而，升级这些模型是昂贵的，并且通过包含额外的现实世界数据来扩展性能需要在大型数据集上进行大量的重新训练，这带来了重大的资源限制。相比之下，自我进化的代理系统可以优化代理行为，而无需修改基础的LLM，提供了更高效和可扩展的解决方案。"
        },
        {
          "type": "text",
          "content": "2. Reduction in Labor Costs: Manually designing agentic systems is a complex and labor-intensive process that requires developers to engage deeply with intricate technical details.Traditional methods often involve building these systems from scratch, demanding significant expertise and efort.By contrast, self-evolving agentic systems can automate much of this process, significantly reducing the need for manual intervention and lowering development costs.\n\n3. Aligned with Natural Intellgence Development: Just as humans continuously improve themselves through learning and adaptation,equipping LLM agents with self-improvement capabilities is a necessary step toward the development of truly autonomous agents. This enables them to refine their performance, adapt to new challenges, and evolve without direct human intervention.",
          "index": 4,
          "part": 0,
          "translated_content": "2. 降低劳动成本：手动设计代理系统是一个复杂而劳动密集的过程，需要开发人员深入研究复杂的技术细节。传统方法通常涉及从头开始构建这些系统，需要相当多的专业知识和努力。相比之下，自我进化的代理系统可以自动化这个过程的大部分内容，显著减少了对手动干预的需求，降低了开发成本。\n\n3. 与自然智能发展一致：正如人类通过学习和适应不断改进自己一样，为LLM代理配备自我改进能力是迈向真正自主代理的发展的必要步骤。这使它们能够优化性能，适应新挑战，并在没有直接人类干预的情况下进化。"
        },
        {
          "type": "figure",
          "src": "images/86dc6d35cbbd558ddde70ccc61d38f275c453e795c4bc71d463de1e3fcc953e2.jpg",
          "alt": "",
          "caption": "Figure 8.7:An illustrationofkey concepts discussed in this section,including optimization spaces,the optimizer, and the optimizing objective.The optimizer iterativelyrefinescomponents withintheoptimization spaces to enhance agentic systems untilasatisfactory outcome is achieved,thereby achieving self-improvement inthe LLMagent systems.",
          "index": 5,
          "part": 0,
          "translated_caption": "图8.7: 本节讨论的关键概念示意图，包括优化空间、优化器和优化目标。优化器在优化空间内迭代地改进组件，以增强代理系统，直到达到满意的结果，从而实现LLM代理系统的自我改进。"
        },
        {
          "type": "text",
          "content": "To achieve the goal of automating human eforts,numerous studies have proposed leveraging LLMs as the driving engine to enable self-evolution inagentic systems.In particular,LLMs provide aneffcient alternative totraditional optimization methods,such as gradient-based[770]and reinforcement learning-based approaches[771].They extend the optimization space from numerical values to more diverse domains, with naturallanguage serving as a universal bridge. An LLM is capable of optimizing complex, heterogeneous parameters,such as instructions [732]and tool implementations[772],andcan operate across arange of LLMs,including both open-source andclosed-source models. A notableexampleofthisapproach isAFLOW[773],whichautomatesthe generationandoptimizationofentire agentic system workflows.This system employs Monte Carlo TreeSearch toleverage thecomprehensive capabilities of LLMs. In this framework,traditionallyhandcrafted agentic systems are replaced byalgorithmically generated ones, marking a kind of paradigm shift.Additionall,agrowing bodyofresearch explores similar methodologies,furtheradvancing the field.",
          "index": 6,
          "part": 0,
          "translated_content": "为了实现自动化人类努力的目标，许多研究提出了利用LLMs作为推动引擎，实现代理系统自我进化的方法。特别是，LLMs为传统优化方法（如基于梯度和基于强化学习的方法）提供了一种高效的替代方案。它们将优化空间从数值值扩展到更多样化的领域，其中自然语言作为一种通用桥梁。LLM能够优化复杂、异构的参数，如指令和工具实现，并可以跨越各种LLMs，包括开源和闭源模型。这种方法的一个显著例子是AFLOW，它自动化生成和优化整个代理系统工作流程。该系统利用蒙特卡洛树搜索来发挥LLMs的全面能力。在这个框架中，传统手工制作的代理系统被算法生成的系统所取代，标志着一种范式转变。此外，越来越多的研究探索类似的方法，进一步推动了该领域的发展。"
        },
        {
          "type": "text",
          "content": "This partisstructuredasfollows:First,we introduce variousoptimization spaces explored inrecentresearchonagentic systems,including prompts,tools, and workflows. In the subsequent section, we review optimization algorithms, discussing both traditionaloptimization paradigms and meta-optimization, where the optimization processalso affcts the underlying optimization algorithms themselves.Wethen explore the self-evolution scenarios,categorizing them into twotypes:onlineoptimizationandoffineoptimization.Following this,wediscussthe applicationof large language model (LLM)agent self-improvement techniques,particularly inknowledge discovery within the AI-for-science domain. Finally, we discuss the security concerns associated with agent self-evolution technologies.",
          "index": 7,
          "part": 0,
          "translated_content": "本部分结构如下：首先，我们介绍了最近研究中探索的各种代理系统优化空间，包括提示、工具和工作流程。在接下来的部分中，我们回顾了优化算法，讨论了传统优化范式和元优化，其中优化过程也影响基础优化算法本身。然后，我们探讨了自我进化场景，将其分类为在线优化和离线优化两种类型。在此之后，我们讨论了大型语言模型（LLM）代理自我改进技术的应用，特别是在人工智能科学领域的知识发现中。最后，我们讨论了与代理自我进化技术相关的安全问题。"
        }
      ],
      "raw_title": "Self-Evolution in Intelligent Agents",
      "type": null,
      "children": [],
      "translated_title": "智能体中的自我进化"
    },
    {
      "title": "Optimization Spaces and Dimensions for Self-evolution",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The optimization of autonomous agents represents acomplexchallenge that encompases multiple levelsof abstraction. In this section,we first establish prompt optimizationas the foundationallayer, upon whichthreedistinct branches of optimization emerge: agentic workflow optimization, tool optimization,and comprehensively autonomous agent optimization.",
          "index": 0,
          "part": 0,
          "translated_content": "自主代理的优化代表着一个复杂的挑战，涵盖了多个抽象层面。在本节中，我们首先将即时优化确立为基础层，之上涌现出三个不同的优化分支：代理工作流优化、工具优化和全面自主代理优化。"
        }
      ],
      "raw_title": "Optimization Spaces and Dimensions for Self-evolution",
      "type": null,
      "children": [],
      "translated_title": "自我演化的优化空间和维度"
    },
    {
      "title": "9.1 Overview of Agent Optimization",
      "number": "9.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "ExistingLLM-based agent optimization can be conceptualized in terms ofatwo-tieredarchitecture.Atthe foundation lies prompt optimization,whichfocuses onenhancing the basic interaction patterns of Language Modelnodes.Building upon this foundation,threeparalelbranches emerge:i)workfow-leveloptimization,which focuses onthecoordination and interaction patterns between multiple LM nodes; i) tooloptimization, where agents evolve by developing and improving tools to adapt tonew tasksand leverage past data; and i)comprehensive autonomous agent optimization, which aims at the holistic enhancement of agent capabilities by considering multiple dimensions.",
          "index": 0,
          "part": 0,
          "translated_content": "现有基于LLM的智能代理优化可以在一个两层架构中进行概念化。在基础层面上是提示优化，专注于增强语言模型节点的基本交互模式。在此基础上，出现了三个并行分支：i) 工作流水平优化，重点关注多个LM节点之间的协调和交互模式；ii) 工具优化，代理通过开发和改进工具来适应新任务并利用过去的数据而进化；iii) 全面自主代理优化，旨在通过考虑多个维度来全面增强代理的能力。"
        },
        {
          "type": "text",
          "content": "Similarly tooptimization paradigms in AutoML,agent optimization can be categorized as either single-objective or multi-objective.Contemporary agent optimization primarilycenters on threecanonicalmetrics: performance,inference cost,and latency.Performancemeasures theeffctiveness ofthe agent incompleting itsassignedtasks,while inference cost quantifies the computationalresources required for agent operation. Latency represents the time taken forthe agent to respond and complete tasks.These objectivescan vary depending on the specific optimization modality. For instance,in prompt-level optimization, aditionalconstraints such as prompt length may become relevant objectives. This multi-faceted nature of optimization objectivesreflects the complexity of agent systems andthe need to balance multiple competing requirements.",
          "index": 1,
          "part": 0,
          "translated_content": "类似于AutoML中的优化范式，智能代理优化可以被归类为单目标或多目标。当代智能代理优化主要集中在三个经典的度量标准上：性能、推理成本和延迟。性能衡量了代理完成其指定任务的有效性，而推理成本则量化了代理运行所需的计算资源。延迟表示代理响应并完成任务所花费的时间。这些目标根据具体的优化模式可能会有所不同。例如，在提示级别的优化中，额外的约束条件，如提示长度，可能成为相关的目标。这种多方面的优化目标的性质反映了代理系统的复杂性以及平衡多个竞争需求的必要性。"
        }
      ],
      "raw_title": "Overview of Agent Optimization",
      "type": null,
      "children": [],
      "translated_title": "9.1 代理优化概述"
    },
    {
      "title": "9.2 Prompt Optimization",
      "number": "9.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Prompt optimization plays the mostcriticalrole in LLM-based agent optimization. When optimizing an agent, beyond model-leveloptimizations,task-specificor model-specific promptoptimization directly impacts theagent's peformance, latency, and cost. Given a task $T=(Q,G_{t})$ ，where $Q$ denotes the input query and $G_{t}$ represents the optional ground truth, the objective of prompt optimization is to generate a task-specific prompt $P_{t}^{*}$ that maximizes performance:",
          "index": 0,
          "part": 0,
          "translated_content": "大型语言模型(LLMs)基于代理的优化中，提示优化在优化中起着至关重要的作用。在优化代理时，除了模型级别的优化之外，任务特定或模型特定的提示优化直接影响代理的性能、延迟和成本。给定一个任务$T=(Q,G_{t})$，其中$Q$表示输入查询，$G_{t}$代表可选的地面实况，提示优化的目标是生成一个任务特定的提示$P_{t}^{*}$，以最大化性能："
        },
        {
          "type": "formula",
          "content": "$$ \nP^{*}=\\underset{P\\in\\mathcal{P}}{\\arg\\operatorname*{max}}\\mathbb{E}_{T\\sim\\mathcal{D}}[\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,P),T)]\n $$",
          "index": 1,
          "part": 0
        },
        {
          "type": "text",
          "content": "where $\\mathcal{P}$ represents the space of possible prompts, $\\phi_{\\mathrm{exe}}$ denotes the execution function, and $\\phi_{\\mathrm{eval}}$ represents the evaluation function. This optimization is typically implemented through three fundamental functions: $\\phi_{\\mathrm{opt}},\\phi_{\\mathrm{exe}}$ , and $\\phi_{\\mathrm{eval}}$ . The Optimize function $\\phi_{\\mathrm{opt}}$ refines existing prompts based on optimization signals, the Execute function $\\phi_{\\mathrm{exe}}$ invokes the current prompt to obtain output $O$ , and the Evaluation function $\\phi_{\\mathrm{eval}}$ assesses current outputs to generate evaluation signals $S_{\\mathrm{eval}}$ and optimization signals $S_{\\mathrm{opt}}$ . The evaluation signals are used to select effective prompts, while the optimization signals assist the Optimize function in performing optimization.",
          "index": 2,
          "part": 0,
          "translated_content": "其中，$\\mathcal{P}$代表可能提示的空间，$\\phi_{\\mathrm{exe}}$表示执行函数，$\\phi_{\\mathrm{eval}}$表示评估函数。这种优化通常通过三个基本函数实现：$\\phi_{\\mathrm{opt}}$、$\\phi_{\\mathrm{exe}}$和$\\phi_{\\mathrm{eval}}$。优化函数$\\phi_{\\mathrm{opt}}$根据优化信号改进现有提示，执行函数$\\phi_{\\mathrm{exe}}$调用当前提示以获取输出$O$，评估函数$\\phi_{\\mathrm{eval}}$评估当前输出以生成评估信号$S_{\\mathrm{eval}}$和优化信号$S_{\\mathrm{opt}}$。评估信号用于选择有效的提示，而优化信号则帮助优化函数执行优化。"
        }
      ],
      "raw_title": "Prompt Optimization",
      "type": null,
      "children": [
        {
          "title": "9.2.1 Evaluation Functions",
          "number": "9.2.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "At the core of prompt optimization lies the evaluation function $\\phi_{e v a l}$ ， which serves as the cornerstone for deriving optimization signals and guiding the evolutionary trajectory of prompts.This function orchestrates a sophisticated interplay between evaluation sources,methodologies,and signal generation,establishing afeedbackloopthat drives continuous improvement. The evaluation function $\\phi_{e v a l}$ processes evaluation sources as input, and employs various evaluationmethods to generate diferent types of signals,whichsubsequently guide theoptimizationprocessHere,we define the dimensions of sources, methods, and signal types to establishthe foundationfor prompt optimization.",
              "index": 0,
              "part": 0,
              "translated_content": "在提示优化的核心是评估函数 $\\phi_{eval}$，它作为导出优化信号和引导提示演化轨迹的基石。该函数在评估来源、方法论和信号生成之间进行复杂的协调，建立一个反馈循环，推动持续改进。评估函数 $\\phi_{eval}$ 处理评估来源作为输入，并采用各种评估方法生成不同类型的信号，随后指导优化过程。在这里，我们定义了来源、方法和信号类型的维度，以建立提示优化的基础。"
            },
            {
              "type": "text",
              "content": "Evaluation Sources Evaluation sources primarily consist of LLM Generated Output $G_{l l m}$ and task-specific Ground Truth $G_{t}$ .Existing workssuchas[730,774,728,775,732,00]predominantlyleveragecomparisonsbetween $G_{l l m}$ and $G_{t}$ as evaluation sources. Some approaches [776, 721, 777] utilize only $G_{l l m}$ as the evaluation source. For instance, PROMST [721] assesses prompt effectiveness by comparing $G_{l l m}$ against human-crafted rules; SPO [778] employs pairwise comparisons of outputs from different prompts to determine relative effectiveness.",
              "index": 1,
              "part": 0,
              "translated_content": "评估来源主要包括LLM生成的输出$G_{llm}$和任务特定的基准$G_{t}$。现有工作主要利用$G_{llm}$和$G_{t}$之间的比较作为评估来源。一些方法仅利用$G_{llm}$作为评估来源。例如，PROMPT通过将$G_{llm}$与人工设计的规则进行比较来评估提示的有效性；SPO则利用来自不同提示的输出的成对比较来确定相对有效性。"
            },
            {
              "type": "text",
              "content": "Evaluation Methods Evaluation Methods can be broadly categorized into three approaches: benchmark-based evaluation, LLM-as-a-Judge, and human feedback.Benchmark-based evaluationremains the most prevalent method in prompt optimization[730,774,721,732,300].This approachrelies on predefined metrics orrules to provide numerical feedbackasevaluation signals.While itoffersanautomated evaluation process,itsefectivenessultimatelydependson how well the benchmark design aligns with human preferences.\n\nThe introduction of LLM-as-a-Judge represents a significant advancement in automated evaluation and preference alignment. Leveraging LLMs\"inherent alignment with human preferences and carefull designed judging criteria, this approach [589] can assess task completion quality based on task descriptions and prompt outputs $G_{l l m}$ , providing reflective textual gradientfeedback.Notable implementations include ProteGi[779]TextGrad[728],Semantic Search [775] and Revolve[780]. Furthermore,LLM-as-a-judge enables comparative evaluation between ground truth $G_{t}$ and output $G_{l l m}$ with specific scoring mechanisms [724]. The effectivenessof this method hinges on both the design of judger prompts andtheunderlying model's alignment with human preferences.As a specialized extension,Agent-as-aJudge[781] refines this paradigm by employing dedicated agents for providing process evaluation on complex tasks, while maintaining high alignment with human preferences at significantly reduced evaluation costs.",
              "index": 2,
              "part": 0,
              "translated_content": "评估方法\n评估方法可以大致分为三种方法：基准评估、LLM作为评判者和人类反馈。基准评估仍然是提示优化中最普遍的方法。这种方法依赖预定义的指标或规则来提供数值反馈作为评估信号。虽然它提供了自动化的评估过程，但其有效性最终取决于基准设计与人类偏好的一致性程度。\n\nLLM作为评判者的引入代表了自动化评估和偏好对齐的重大进展。利用LLM与人类偏好的内在一致性和精心设计的评判标准，这种方法可以根据任务描述和提示输出$G_{llm}$来评估任务完成质量，提供反映性的文本梯度反馈。显著的实施包括ProteGi、TextGrad、Semantic Search和Revolve。此外，LLM作为评判者还可以通过特定评分机制在地面真相$G_{t}$和输出$G_{llm}$之间进行比较评估。这种方法的有效性取决于评判提示的设计以及基础模型与人类偏好的一致性。作为一种专门的扩展，Agent-as-a-Judge通过利用专门的代理人在复杂任务上提供过程评估，同时以显著降低的评估成本保持与人类偏好的高度一致性，进一步完善了这一范式。"
            },
            {
              "type": "text",
              "content": "Humanfeedbackrepresents thehighest levelof intellgence integrationintheevaluationprocessAs humans remain the ultimate arbitersof prompt efectiveness,direct human feedback can rapidly and substantially improve prompt quality. However,this appoach introducessignificantresource overhead.APOHF[777]demonstrates that incorporating human fedback can achieve robust prompt optimization with minimal computational resources, particularly excelling in open-ended tasks such as user instructions, prompt optimization for text-to-image generative models, and creative writing.Nevertheless,therequirement forhuman intervention somewhatcontradicts the goal ofautomated evolution.",
              "index": 3,
              "part": 0,
              "translated_content": "人类反馈代表了评估过程中智能整合的最高水平。由于人类仍然是提示效果的最终裁决者，直接的人类反馈可以快速且显著地提高提示的质量。然而，这种方法引入了显著的资源开销。APOHF[777]表明，整合人类反馈可以在最小的计算资源下实现强大的提示优化，特别擅长于开放式任务，如用户指导、文本到图像生成模型的提示优化以及创意写作。然而，对人类干预的需求在一定程度上与自动演化的目标相矛盾。"
            },
            {
              "type": "text",
              "content": "Signal Types Feedback generated by evaluation methods manifests in three distinct forms,each serving different optimization needs.Numerical feedback[730,774,721,732,300] quantifies performance through scalar metrics, compatible with rules,ground truth,human assessment,and LLM judgments.While widely applicable,this approach requires substantialsamplesforstatisticalreliability,potentiallyoverlooking instance-specificdetailsthatcouldguide optimization.Textual feedback[728,775,780]provides detailed,instance-specific guidancethrough analysis and concrete suggestions. This sophisticated approach requires intelligent participation, either from human experts or advanced language models,enabling targeted improvements in prompt design through explicit recommendations. However, itsreliance on sophisticated intelligence sources impacts itsscalability.Ranking feedback[778]establishes relative quality ordering through either comprehensive ranking or pairwise comparisons. This approach uniquely circumvents the need for absolute quality measures or predefined criteria,requiring only preference judgments.It proves particularly valuable when absolute metrics are difficult to define or when optimization primarily concerns relative improvements.",
              "index": 4,
              "part": 0,
              "translated_content": "信号类型评估方法生成的反馈以三种明显形式呈现，每种形式都满足不同的优化需求。数值反馈通过标量度量指标量化性能，与规则、基准真相、人类评估和LLM判断兼容。虽然广泛适用，但这种方法需要大量样本以获得统计可靠性，可能忽视可指导优化的特定实例细节。文本反馈通过分析和具体建议提供详细的、特定实例的指导。这种复杂方法需要智能参与，可以是来自人类专家或先进语言模型，通过明确建议实现对提示设计的有针对性改进。然而，它对复杂智能来源的依赖影响了其可扩展性。排名反馈通过全面排名或成对比较建立相对质量排序。这种方法独特地避免了对绝对质量度量或预定义标准的需求，只需要偏好判断。当难以定义绝对度量或优化主要涉及相对改进时，这种方法尤为有价值。"
            }
          ],
          "raw_title": "Evaluation Functions",
          "type": null,
          "children": [],
          "translated_title": "9.2.1 评估函数"
        },
        {
          "title": "9.2.2 Optimization Functions",
          "number": "9.2.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "The design ofoptimization functions is crucial in determining the quality of generated prompts in each iteration of prompt optimization.Through efective signalguidance,prompt self-evolutioncan achieve faster convergence.Current optimization approaches primarily rely on two types of signals: evaluation signals $S_{e v a l}$ that identify the most effective existing prompts, and optimization signals $S_{o p t}$ that provide detailed guidance for improvements.\n\nOptimize via Evaluation Signals When optimizing with evaluation signals,the processbegins by selecting the most effective prompts based on $\\phi_{e v a l}$ assessments. Rather than directly learning from past errors, some methods adop1 heuristic exploration andoptimizationstrategies.SPO[778] iterativelyrefines prompts based onthe outputs ofcurrent best-performing ones,leveraging the language model's inherent ability to align withtask requirements. Similarly, Evoprompt[723] employs evolutionary algorithms with LLMs serving as evolution operators for heuristic prompt combination. PromptBreeder[732] advances this approach further by comparing score variations between mutated prompts while simultaneously modifying both meta-prompts and prompts throughthe LLM's inherent capabilities.",
              "index": 0,
              "part": 0,
              "translated_content": "优化函数的设计在确定每次提示优化迭代中生成质量的关键作用。通过有效的信号引导，提示自我演化可以实现更快的收敛。当前的优化方法主要依赖于两种类型的信号：评估信号$S_{eval}$，用于识别最有效的现有提示，以及优化信号$S_{opt}$，提供改进的详细指导。\n\n通过评估信号进行优化时，该过程从基于$\\phi_{eval}$评估的最有效提示的选择开始。一些方法采用启发式探索和优化策略，而非直接从过去的错误中学习。SPO[778]根据当前表现最佳提示的输出迭代地优化提示，利用语言模型固有的与任务要求对齐的能力。类似地，Evoprompt[723]采用进化算法，LLMs作为启发式提示组合的进化操作符。PromptBreeder[732]通过比较变异提示之间的分数变化，同时通过LLM的固有能力修改元提示和提示，进一步推进了这种方法。"
            },
            {
              "type": "text",
              "content": "Optimize via Optimization Signals While optimization methods based solely on evaluation signals require extensive search to findoptimal solutions invast search spaces throughtrialand error,an alternative approach leverages explicit optimization signals to guide theoptimization direction and improve effciency.Existing methods demonstrate various ways to utilize these optimization signals. OPRO [730] extracts common patterns from high-performing prompt solutions to guide subsequent optimization steps.ProTegi[779]employslanguage models to analyze failure cases and predict error causes, using these insights as optimization guidance.TextGrad[728] extends this approach further by transforming promptreflections into“textual gradients\",applying this guidance acrossmultiple prompts within agentic systems.Revolve[780] further enhances optimization by simulating second-order optimization,extending previous first-order feedback mechanisms to modelthe evolving relationship betweenconsecutive prompts andresponses.This allows the system to adjust based onhow previous gradients change, avoiding stagnation in suboptimal patterns and enabling more informed, long-term improvements in complex task performance.",
              "index": 1,
              "part": 0,
              "translated_content": "在优化信号的指导下进行优化 虽然仅基于评估信号的优化方法需要进行广泛搜索以在庞大的搜索空间中找到最佳解决方案，但另一种方法利用明确的优化信号来引导优化方向并提高效率。现有方法展示了利用这些优化信号的各种方式。OPRO[730]从高性能提示解决方案中提取共同模式，以指导后续优化步骤。ProTegi[779]采用语言模型来分析失败案例并预测错误原因，利用这些见解作为优化指导。TextGrad[728]进一步扩展了这一方法，将提示反映转化为“文本梯度”，将这一指导应用于代理系统中的多个提示。Revolve[780]通过模拟二阶优化进一步增强了优化，将先前的一阶反馈机制扩展为建模连续提示和响应之间不断演变关系的方式。这使系统能够根据先前梯度的变化进行调整，避免停滞在次优模式中，并实现更具见解的、长期的复杂任务性能改进。"
            }
          ],
          "raw_title": "Optimization Functions",
          "type": null,
          "children": [],
          "translated_title": "9.2.2 优化函数"
        },
        {
          "title": "9.2.3 Evaluation Metrics",
          "number": "9.2.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "The efectiveness of prompt optimization methods can be evaluated across multiple dimensions. Performance metrics[782,778,730]forCloseTasksserveasthemostdirectindicatorsofaprompt'sinherent peformance,encompassing measures such as pass $\\ @1$ , accuracy, F1 score, and ROUGE-L. These metrics enable researchers to assess the stability, effectiveness,and convergence rate of prompt optimization processes.Another crucial dimension is Effciency metrics [778].While some prompt optimization approaches achieve outstanding results,they often demand substantial computationalresources,larger sample sizes,andextensive datasets.Incontrast,other methods achieve moderate results with lower resource requirements,highlighting the trade-offs between performance and effciency in agent evolution. The third dimension focuses on qualitative metrics that assesspecific aspects of agent behavior:consistency[776] measures output stability across multiple runs,fairness[783]evaluates theability to mitigate the language model's inherent biases,andconfidence[784,785]quantifies theagent'scertainty inits predictions.When these behavioral aspects are treated asdistinctobjectives,prompt optimizationframeworks providecorresponding metricsforevaluation.",
              "index": 0,
              "part": 0,
              "translated_content": "可以评估提示优化方法的有效性，跨多个维度进行评估。对于CloseTasks的性能指标[782,778,730]作为提示固有性能的最直接指标，包括通过率@1、准确性、F1分数和ROUGE-L等指标。这些指标使研究人员能够评估提示优化过程的稳定性、有效性和收敛速度。另一个关键维度是效率指标[778]。虽然一些提示优化方法取得了出色的结果，但它们通常需要大量的计算资源、更大的样本量和广泛的数据集。相反，其他方法在资源需求较低的情况下取得了中等结果，突显了代理演化中性能和效率之间的权衡。第三个维度关注评估代理行为特定方面的定性指标：一致性[776]衡量了多次运行中输出的稳定性，公平性[783]评估了减轻语言模型固有偏见的能力，信心[784,785]量化了代理对其预测的确定性。当将这些行为方面视为不同的目标时，提示优化框架提供相应的评估指标。"
            }
          ],
          "raw_title": "Evaluation Metrics",
          "type": null,
          "children": [],
          "translated_title": "9.2.3 评估指标"
        }
      ],
      "translated_title": "9.2 提示优化"
    },
    {
      "title": "9.3 Workflow Optimization",
      "number": "9.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "While prompt-level optimization has shown promising results in enhancing individual LLM capabilities, modern AI systems often require the coordination of multiple LLM components to tackle complex tasks. This necessitates a more comprehensive optimization domain-the agentic workfow space. At its core, an agentic workflow consists of LLM-invoking nodes, where each node represents a specialized LLMcomponent designed for specific sub-tasks within the larger system.\n\nAlthough this architecture bearssimilarities to multi-agent systems,itis important todistinguishagenticworkflows from fully autonomous multi-agent scenarios.In agentic workflows, nodes operate under predetermined protocols and optimization objectives,rather than exhibiting autonomous decision-making capabilities.Many prominent systems, such as MetaGPT[626] AlphaCodium[786] can be categorized under this framework.Moreover, agentic workflows can serve as executable components within larger autonomous agent systems, making theiroptimization crucial for advancing both specialized task completion and general agent capabilities.",
          "index": 0,
          "part": 0,
          "translated_content": "尽管在增强单个LLM能力方面，提示级别的优化显示出了令人鼓舞的结果，但现代人工智能系统通常需要协调多个LLM组件来解决复杂任务。这需要一个更全面的优化领域——智能工作流空间。在其核心，智能工作流包括LLM调用节点，其中每个节点代表专门设计用于较大系统中特定子任务的LLM组件。\n\n尽管这种架构与多代理系统相似，但重要的是要将智能工作流与完全自主的多代理情景区分开来。在智能工作流中，节点根据预定的协议和优化目标操作，而不是展示自主决策能力。许多著名系统，如MetaGPT[626]和AlphaCodium[786]，可以归类为这种框架。此外，智能工作流可以作为更大的自主代理系统中的可执行组件，使它们的优化对于推进专门任务完成和一般代理能力的发展至关重要。"
        },
        {
          "type": "text",
          "content": "Following the formalization proposed by GPTSwarm[651] and AFLOW[773],his sectionfirst establishes a formal definition of agentic workflows and their optimization objectives.We then examine the core components of agentic workflows—nodes and edges—analyzing their respective search spaces and discussing existing representation approaches in the literature.",
          "index": 1,
          "part": 0,
          "translated_content": "在GPTSwarm[651]和AFLOW[773]提出的形式化基础上，本节首先建立了智能工作流及其优化目标的形式化定义。然后，我们将研究智能工作流的核心组件——节点和边缘——分析它们各自的搜索空间，并讨论文献中现有的表示方法。"
        }
      ],
      "raw_title": "Workflow Optimization",
      "type": null,
      "children": [
        {
          "title": "9.3.1 Workflow Formulation",
          "number": "9.3.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "An agentic workflow $\\kappa$ can be formally represented as:",
              "index": 0,
              "part": 0,
              "translated_content": "一个代理工作流 $\\kappa$ 可以形式化表示为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathcal{K}=\\{(N,E)|N\\in\\mathcal{N},E\\in\\mathcal{E}\\}\n $$",
              "index": 1,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\mathcal{N}=\\{N(M,\\tau,P,F)|M\\in\\mathcal{M},\\tau\\in[0,1],P\\in\\mathcal{P},F\\in\\mathcal{F}\\}$ represents the set of LLM-invoking nodes, with $M$ $\\tau,\\mathcal{P}$ , and $\\mathcal{F}$ denoting the available language models, temperature parameter, prompt space, and output format space respectively. $E$ indicates the edges between different LLM-invoking nodes. This formulation encapsulates both the structural components and operational parameters that define an agentic workflow's behavior.\n\nGiven a task $T$ and evaluation metrics $L$ , the goal of workflow optimization is to discover the optimal workflow $K^{*}$ that maximizes performance:",
              "index": 2,
              "part": 0,
              "translated_content": "其中$\\mathcal{N}=\\{N(M,\\tau,P,F)|M\\in\\mathcal{M},\\tau\\in[0,1],P\\in\\mathcal{P},F\\in\\mathcal{F}\\}$代表了调用LLM节点的集合，其中$M$，$\\tau$，$\\mathcal{P}$和$\\mathcal{F}$分别表示可用的语言模型、温度参数、提示空间和输出格式空间。$E$表示不同LLM调用节点之间的边。这种表述包含了定义代理工作流行为的结构组件和操作参数。\n\n给定一个任务$T$和评估指标$L$，工作流优化的目标是发现最大化性能的最优工作流$K^{*}$："
            },
            {
              "type": "formula",
              "content": "$$ \nK^{*}=\\underset{K\\in\\mathcal{K}}{\\arg\\operatorname*{max}}L(K,T)\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $K$ is the search space of workflow, and $L(K,T)$ typically measures multiple aspects including task completion quality,computationaleficiency,andexecution latency.Thisoptimizationobjectivereflects the practicalchallenges in deploying agentic workflows, where we must balance effectiveness with resource constraints.",
              "index": 4,
              "part": 0,
              "translated_content": "其中$K$是工作流的搜索空间，$L(K,T)$通常衡量多个方面，包括任务完成质量、计算效率和执行延迟。这种优化目标反映了在部署代理工作流时面临的实际挑战，我们必须在效果和资源约束之间取得平衡。"
            }
          ],
          "raw_title": "Workflow Formulation",
          "type": null,
          "children": [],
          "translated_title": "9.3.1 工作流程制定"
        },
        {
          "title": "9.3.2 Optimizing Workflow Edges",
          "number": "9.3.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "The edge space $\\mathcal{E}$ defines the representation formalism for agentic workflows. Current approaches primarily adopt three distinct representation paradigms: graph-based, neural network-based, andcode-based structures.Each paradigm ofers unique advantages and introduces specific constraints on the optimization process.\n\nGraph-basedrepresentations enable theexpressonof hierarchical, sequential,andparalelrelationships between nodes. This approach naturally accommodates complex branching patterns and facilitates visualization of workflow topology, making it particularly suitable for scenarios requiring explicit structural manipulation.For example,GPTSwarm[651] demonstrated the effctivenessof graph-based workflow representation in coordinating multiple LLM components through topology-aware optimization. Neural network architectures provide another powerfulrepresentation paradigm that excels in capturing non-linear relationships between nodes. Dylan [725] showed that neural network-based workflowscan exhibit adaptive behavior through learnableparameters,making themespeciallyeffective for scenarios requiring dynamic adjustment based on input and feedback.Code-based representationoffers the mostcomprehensive expressivenessamong current approaches.AFLOW[773] and ADAS[741] established that representing workflows as executablecode supportslinear sequences,conditionallogic,loops,andthe integrationofboth graphand network structures. This approach provides precise control over workflow execution and leverages LLMs'inherent code generation capabilities.",
              "index": 0,
              "part": 0,
              "translated_content": "边缘空间 $\\mathcal{E}$ 定义了智能工作流的表示形式。目前的方法主要采用三种不同的表示范式：基于图形、基于神经网络和基于代码结构。每种范式都具有独特的优势，并对优化过程引入特定的约束。\n\n基于图形的表示使得节点之间的层次、顺序和并行关系得以表达。这种方法自然地适应复杂的分支模式，并便于可视化工作流拓扑结构，特别适用于需要明确结构操作的场景。例如，GPTSwarm[651] 在通过具有拓扑感知优化的图形化工作流表示协调多个LLM组件方面展示了其有效性。神经网络架构提供了另一种强大的表示范式，擅长捕捉节点之间的非线性关系。Dylan [725] 表明，基于神经网络的工作流通过可学习参数可以表现出自适应行为，因此在需要根据输入和反馈进行动态调整的场景中特别有效。基于代码的表示在当前方法中提供了最全面的表达性。AFLOW[773] 和 ADAS[741] 显示，将工作流表示为可执行代码支持线性序列、条件逻辑、循环以及图形和网络结构的整合。这种方法可以精确控制工作流的执行，并利用LLMs固有的代码生成能力。"
            },
            {
              "type": "text",
              "content": "Thechoice ofedgespace representationsignificantly influences boththe searchspace dimensionalityandtheapplicable optimization algorithms.[728]focused solelyon prompt optimization while maintaining afixed workflow topology, enabling the use of textual feedback-based optimization techniques.In contrast,[651] developed reinforcement learning algorithms for joint optimization of individual node prompts andoveralltopology.[773]leveragedcode-based representation to enable direct workflowoptimization by language models,while recent advances by[787] and[788] introduced methods for problem-specific topology optimization.",
              "index": 1,
              "part": 0,
              "translated_content": "边缘空间表示形式的选择显著影响了搜索空间的维度和适用的优化算法。[728] 专注于即时优化，同时保持固定的工作流拓扑结构，从而可以利用基于文本反馈的优化技术。相比之下，[651] 开发了强化学习算法，用于联合优化单个节点提示和整体拓扑结构。[773] 利用基于代码的表示形式，通过语言模型直接实现工作流优化，而[787] 和 [788] 的最新进展则引入了针对特定问题的拓扑优化方法。"
            }
          ],
          "raw_title": "Optimizing Workflow Edges",
          "type": null,
          "children": [],
          "translated_title": "9.3.2 优化工作流边缘"
        },
        {
          "title": "9.3.3 Optimizing Workflow Nodes",
          "number": "9.3.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "The node space $N$ consists of four key dimensions that influence node behavior and performance.The output format space $F$ significantly impacts performance by structuring LLM outputs, with formats like XML and JSON enabling more precise control over response structure. The temperature parameter $\\tau$ controls output randomness, affecting the stability-creativity tradeoff in node responses. The prompt space $P$ inherits the optimization domain from prompt-level optimization, determining the core interaction patterns with LLMs. The model space $M$ represents available LLMs, each with distinct capabilities and computational costs.",
              "index": 0,
              "part": 0,
              "translated_content": "节点空间$N$包含四个关键维度，影响节点行为和性能。输出格式空间$F$通过构建LLMs的输出格式，如XML和JSON，显著影响性能，使响应结构更具精确控制。温度参数$\\tau$控制输出的随机性，影响节点响应中稳定性与创造性的权衡。提示空间$P$继承了来自提示级别优化的优化领域，决定了与LLMs的核心交互模式。模型空间$M$代表可用的LLMs，每个LLM都具有独特的能力和计算成本。"
            },
            {
              "type": "text",
              "content": "For single-node optimization,existing researchhas primarilyfocused on specific dimensions within this space.[773] concentrated exclusivelyonpromptoptimization, while[741] extended the search space to include both prompts and temperature parameters.Taking a diferent approach,[789] fixed prompts while exploring model selection across different nodes. Output format optimization, though crucial, remains relatively unexplored [790].\n\nCompared to edge space optimization,node space optimization poses unique scalabilitychallnges due tothetypically large number of nodes in agentic workflows.The dimensionality of the search space grows multiplicatively with each additional node,necessitating effcient optimization strategies thatcan effectively handle this complexity while maintaining reasonable computational costs.",
              "index": 1,
              "part": 0,
              "translated_content": "针对单节点优化，现有研究主要集中在该空间内的特定维度上。[773] 专注于提示优化，而[741] 将搜索空间扩展到包括提示和温度参数。采用不同方法，[789] 固定提示同时探索跨不同节点的模型选择。尽管输出格式优化至关重要，但仍相对未被深入探讨。\n\n与边缘空间优化相比，节点空间优化由于代理工作流程中通常存在大量节点，面临独特的可扩展性挑战。随着每个额外节点的增加，搜索空间的维度呈乘法增长，需要高效的优化策略，能够有效处理这种复杂性同时保持合理的计算成本。"
            }
          ],
          "raw_title": "Optimizing Workflow Nodes",
          "type": null,
          "children": [],
          "translated_title": "9.3.3 优化工作流节点"
        }
      ],
      "translated_title": "9.3 工作流优化"
    },
    {
      "title": "9.4 Tool Optimization",
      "number": "9.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Unlike conventional usage of LLMs that typically operate in asingle-turn manner, agents are equipped with advanced multi-turn planning capabilities andthe ability to interact with the external world via various tools.These unique atributes make the optimization of tool usage acriticalcomponent in enhancing an agent'soverallperformance and adaptability.Tool optimization involves systematically evaluating andrefining how an agent selects,invokes,and intgrates availabletools tosolve problems with highereffciencyand lower latency.Key performance metrics in this context include decision-making accuracyretrievalefficiency,selection precision,task planning,andrisk management. Central to this optimization are two complementary strategies: tool learning and tool creation.",
          "index": 0,
          "part": 0,
          "translated_content": "与通常单轮操作LLMs的传统用法不同，代理配备了先进的多轮规划能力，并具有通过各种工具与外部世界进行交互的能力。这些独特的特性使得优化工具使用成为增强代理整体性能和适应性的关键组成部分。工具优化涉及系统地评估和完善代理如何选择、调用和整合可用工具以更高效、更低延迟地解决问题。在这一背景下，关键的性能指标包括决策准确性、检索效率、选择精度、任务规划和风险管理。在这一优化过程中，两种互补策略至关重要：工具学习和工具创建。"
        }
      ],
      "raw_title": "Tool Optimization",
      "type": null,
      "children": [
        {
          "title": "9.4.1 Learning to Use Tools",
          "number": "9.4.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Unlike prompting-based methods thatleverage frozen foundation modelsin-contextlearning abilities,raining-based methods optimize the model that backs LLM agents with supervision. Drawing inspiration from developmental psychology,tollearningcanbecategorizedintotwoprimary streams: learning from demonstrations and learning from feedback[714].The other way toelicit the power of LLMs(agents)using tools is byusing prompt-basedorin-context learning methods for better reasoning abilities.\n\nLearning from demonstrations involves training models backed LLM agents to mimic expert behaviors through imitation learning.Techniques such as behavior cloning alow models to learn policies in a supervised manner by replicating human-annotated tool-use actions. Formally, given a dataset $D=\\{(q_{i},a_{i}^{*})\\}_{i=0}^{N-1}$ ,where $q_{i}$ is a user query and $a_{i}^{*}$ is the corresponding human demonstration, the controller's parameters $\\theta_{C}$ are optimized as:",
              "index": 0,
              "part": 0,
              "translated_content": "与基于提示的方法不同，利用冻结的基础模型在上下文学习能力方面，基于训练的方法通过监督来优化支持LLM代理的模型。受发展心理学启发，基于示范的学习可以分为两个主要流派：从示范学习和从反馈学习中学习。利用基于示范的学习方法训练支持LLM代理的模型，通过模仿学习专家行为。诸如行为克隆等技术使模型能够通过复制人类注释的工具使用动作以监督方式学习策略。形式上，给定数据集 $D=\\{(q_{i},a_{i}^{*})\\}_{i=0}^{N-1}$，其中 $q_{i}$ 是用户查询，$a_{i}^{*}$ 是相应的人类示范，控制器的参数 $\\theta_{C}$ 被优化为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\theta_{C}^{*}=\\arg\\operatorname*{max}_{\\theta_{C}}\\mathbb{E}_{(q_{i},a_{i}^{*})\\in D}\\prod_{t=0}^{T_{i}}p_{\\theta_{C}}(a_{i,t}^{*}\\mid x_{i,t},H_{i,t},q_{i})\n $$",
              "index": 1,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $a_{i,t}^{*}$ is the human annotation at timestep $t$ for query $q_{i}$ , and $T_{i}$ is the total number of timesteps.\n\nLearning from feedback leverages reinforcementlearning to enable models to adapt based on rewards derived from environment or human feedback. The optimization objective for the controller's parameters $\\theta_{C}$ is:",
              "index": 2,
              "part": 0,
              "translated_content": "其中 $a_{i,t}^{*}$ 是查询 $q_{i}$ 在时间步 $t$ 的人类标注，$T_{i}$ 是时间步的总数。\n\n从反馈学习中学习利用强化学习，使模型能够根据来自环境或人类反馈的奖励进行调整。控制器参数 $\\theta_{C}$ 的优化目标是："
            },
            {
              "type": "formula",
              "content": "$$ \n\\theta_{C}^{*}=\\arg\\operatorname*{max}_{\\theta_{C}}\\mathbb{E}_{q_{i}\\in Q}\\mathbb{E}_{\\{a_{i,t}\\}_{t=0}^{T_{i}}}\\left[R\\left(\\{a_{i,t}\\}_{t=0}^{T_{i}}\\right)\\right]\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $R$ represents the reward function based on the sequence of actions $\\{a_{i,t}\\}$\n\nIntegrating toollearning intotheoptimizationframeworkenhances the system'sabilityto generalize toolusage across diverse tasks and environments.By incorporating both demonstration-based and feedback-based learning,the model can iteratively improve its tool invocation strategies, selection policies, and execution accuracy.\n\nOptimization Reasoning Strategies for Tool Using Optimizing the aforementioned metrics for beter LLM agents' abilitie requires acombination of advanced retrieval models,fine-tuned reasoning strategies,and adaptive learning mechanisms. Reasoning strategies,such as Chain-of-Thought (CoT)[46], Tree-of-Thought [72],and Depth-First Search Decision Trees (DFS-DT)[690],facilitate more sophisticated decision-making processes regarding tool usage. Fine-tuning themodel's understanding of tools,including parameter interpretationand action execution,enables more precise andeffective toolinteractions.Additionally,learming fromthemodel'soutputsallowsforbeter post-processing and analysis, further refining tool utilization efficacy.",
              "index": 4,
              "part": 0,
              "translated_content": "其中 $R$ 代表基于动作序列 $\\{a_{i,t}\\}$ 的奖励函数。\n\n将工具学习整合到优化框架中增强了系统在不同任务和环境中泛化工具使用能力。通过结合基于演示和基于反馈的学习，模型可以迭代改进其工具调用策略、选择策略和执行准确性。\n\n为了优化上述指标以提高LLM代理的能力，需要结合先进的检索模型、精细调节的推理策略和自适应学习机制。推理策略，如思维链 (CoT) [46]、思维树 [72] 和深度优先搜索决策树 (DFS-DT) [690]，促进了关于工具使用的更复杂的决策过程。通过优化模型对工具的理解，包括参数解释和动作执行，可以实现更精确和有效的工具交互。此外，从模型的输出中学习可以实现更好的后处理和分析，进一步提升工具利用效率。"
            }
          ],
          "raw_title": "Learning to Use Tools",
          "type": null,
          "children": [],
          "translated_title": "9.4.1 学习使用工具"
        },
        {
          "title": "9.4.2 Creation of New Tools",
          "number": "9.4.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Beyond theoptimization ofexisting tools,theability tocreatenew toolsdynamically[703,702,772]basedon a deep understanding of tasks andcurrent toolusage can significantlyenhance the LLM Agent framework's adaptability and efficiency. In recent work,severalcomplementary approaches have been proposed.ToolMakers[702]establishes a closed-loop framework where atool-making agent iterativelyexecutes three phases: (1)）Proposing Python functions via programming-by-example using three demonstrations, (2)Verifying functionality through automated unit testing (3 validation samples)withself-debuging oftest cases,and (3)Wrapping validated tools with usage demonstrations for downstream tasks.This rigorous process ensures reliability while maintaining fullautomation. CREATOR[703] adoptsa four-stage lifecycle:Creation of task-specific tools through abstract reasoning, Decision planning for tool invocation,Execution of generated programs,and Rectificationthrough iterative toolrefinementemphasizing tool diversity,separation ofabstract/concretereasoning,anderorrecovery mechanisms.Incontrast,CRAFT[77]employs an offlineparadigmthat distill domain-specificdata into reusable, atomictools (e.g.object color detection)through GPT-4 prompting,validation, and deduplication. Its training-freeapproachcombines human-inspectablecode snippets with compositional problem-solving,enabling explainable toolchains while avoiding modelfine-tuning—particularly effective when decomposing complex tasks into modular steps.",
              "index": 0,
              "part": 0,
              "translated_content": "除了优化现有工具之外，基于对任务和当前工具使用的深刻理解，动态地创造新工具可以显著增强LLM智能代理框架的适应性和效率。在最近的工作中，提出了几种互补的方法。ToolMakers建立了一个闭环框架，其中一个制造工具的代理通过三个演示迭代地执行三个阶段：（1）通过示范进行编程示范提出Python函数，（2）通过自动化单元测试验证功能（3个验证样本）并进行自我调试测试用例，（3）为下游任务包装经验证的工具并提供使用演示。这一严格的过程确保了可靠性同时保持了全自动化。CREATOR采用了四阶段生命周期：通过抽象推理创建特定任务的工具，决策规划工具调用，执行生成的程序，通过迭代工具细化强调工具多样性、抽象/具体推理的分离和错误恢复机制的纠正。相比之下，CRAFT采用了离线范式，通过GPT-4提示、验证和去重将领域特定数据提炼为可重复使用的原子工具（例如对象颜色检测）。其无需训练的方法将人可检查的代码片段与组合问题解决相结合，实现可解释的工具链，同时避免模型微调，特别适用于将复杂任务分解为模块化步骤时。"
            },
            {
              "type": "text",
              "content": "The integration of these complementary approaches presents richresearchopportunities.Hybrid systems could merge CRAFT's pre-made toolrepositories with ToolMakers'on-demand generation, using functional caching to balance effciency and adaptability.Future frameworks might implement multi-tier tool hierarchies where primitive operations from CRAFTfeed into ToolMakers'composite tools,while CREATOR-stylerectification handles edge cases.Advances in self-supervised toolevaluation metrics andcross-domain generalization could further automate the tollifecycle. Notably,the interplay between tool granularity (atomic vs.composite)and reusability paterns warrants systematic investigation—fine-grainedtools enableflexible composition but increase orchestration complexity.As agents evolve, bidirectional tool-task co-adaptation mechanisms may emerge, where toolsreshape task representations while novel tasks drive tool innovation, ultimately enabling self-improving AI systems.",
              "index": 1,
              "part": 0,
              "translated_content": "这些互补方法的整合为丰富的研究机会提供了可能。混合系统可以将CRAFT的预制工具库与ToolMakers的按需生成相结合，利用功能缓存来平衡效率和适应性。未来的框架可能实现多层工具层次结构，其中来自CRAFT的基本操作输入到ToolMakers的复合工具中，而CREATOR风格的纠正处理边缘情况。自监督工具评估指标和跨领域泛化的进展可能进一步自动化工具生命周期。值得注意的是，工具粒度（原子 vs. 复合）和可重复使用模式之间的相互作用需要进行系统研究，细粒度工具可以实现灵活的组合，但会增加编排复杂性。随着代理的发展，双向工具-任务相互适应机制可能会出现，其中工具重塑任务表示，而新任务推动工具创新，最终实现自我改进的人工智能系统。"
            }
          ],
          "raw_title": "Creation of New Tools",
          "type": null,
          "children": [],
          "translated_title": "9.4.2 新工具的创建"
        },
        {
          "title": "9.4.3 Evaluation of Tool Effectiveness",
          "number": "9.4.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "The evaluation metrics and benchmarks discussed below offra comprehensive basis for quantifying an agent's tool usage capabilities.Byassessingaspectssuchastoolinvocatin,selectionaccuracy,retrievaleficiencyand planning for complextasks,these benchmarks notonly measure current performancebut alsoprovide clear,concrete objectives for optimizing tool usage.Such metrics are instrumentalin guiding both immediate performance enhancements and long-term strategic improvements inagent-based systems.In thefollowing sections,we firstreview the evolution of agent tool use benchmarks and then consolidatethe key evaluation metrics that serve as targets for further tool optimization.",
              "index": 0,
              "part": 0,
              "translated_content": "下面讨论的评估指标和基准为量化代理工具使用能力提供了全面的基础。通过评估工具调用、选择准确性、检索效率和复杂任务规划等方面，这些基准不仅衡量当前性能，而且为优化工具使用提供了明确的具体目标。这些指标在引导代理系统的即时性能提升和长期战略改进方面起着重要作用。在接下来的几节中，我们首先回顾了代理工具使用基准的演变，然后整合了作为进一步工具优化目标的关键评估指标。"
            },
            {
              "type": "text",
              "content": "Tool Evaluation Benchmarks Recent eforts in LLM-as-Agent research have spawned diverse benchmarks and frameworks for evaluating tool-use capabilities.Earlystudies such as Gorilla[727] and API-Bank[791]pioneered large-scale datasets and methods fortesting LLMinteractions withexternalAPIs,shedding light on issueslike argument accuracyand hallucination.Subsequent works like T-Bench[792]and ToolBench [690] introduced more extensive task suites and stressed the importanceof systematic data generation fortool manipulation.StableToolBench[793]further extendedthisline of inquirybyhighlightingtheinstabilityofreal-world APIs,proposingavirtual APIserver for more consistent evaluation.Meanwhile,ToolAlpaca[794] investigated the feasibilityof achieving generalized tool-use in relatively smaller language models with minimal in-domain training.Additional efforts like ToolEmu[795]assessed the safetyandrisk aspects of tool-augmented LMagents through emulated sandboxenvironments.MetaTool[796] then introduced a new benchmark focused on whether LLMs know whento use tools andcancorrectly choose which tools to employ. It providesadataset named ToolEthatcovers single-tooland multi-tool usage scenarios,encouraging research into tool usage awareness and nuanced tool selection. ToolEyes[797]pushed the evaluation further by examining real-world scenarios and multi-step reasoning across a large tool library. Finally, $\\tau$ -bench [798] introduced a humanin-the-loop perspective,emphasizing dynamic user interactions and policy compliance in agent-based conversations. Together, these benchmarks and frameworks underscore the evolving landscape of tool-augmented LLM research, marking a shift from isolated reasoning tasks to comprehensive, real-world agent evaluations.",
              "index": 1,
              "part": 0,
              "translated_content": "工具评估基准\n最近LLM作为代理研究的努力产生了多样化的基准和框架，用于评估工具使用能力。早期研究，如Gorilla和API-Bank，开创了大规模数据集和测试LLM与外部API交互的方法，揭示了诸如参数准确性和幻觉等问题。随后的作品，如T-Bench和ToolBench，引入了更广泛的任务套件，并强调了系统性数据生成对工具操作的重要性。StableToolBench通过强调现实世界API的不稳定性，提出了虚拟API服务器，以实现更一致的评估。同时，ToolAlpaca调查了在相对较小的语言模型中通过最少领域内训练实现通用工具使用的可行性。ToolEmu评估了通过模拟沙盒环境对工具增强的LLM代理的安全性和风险方面。MetaTool引入了一个新的基准，重点关注LLM是否知道何时使用工具，并能正确选择要使用的工具。它提供了一个名为ToolE的数据集，涵盖了单工具和多工具使用情景，鼓励研究工具使用意识和细致的工具选择。ToolEyes通过检查真实场景和跨大型工具库的多步推理进一步推动了评估。最后，$\\tau$ -bench引入了人机协作的视角，强调了基于代理对话中的动态用户交互和政策遵从。总的来说，这些基准和框架突显了工具增强LLM研究不断发展的格局，标志着从孤立推理任务转向全面的、真实世界代理评估。"
            },
            {
              "type": "text",
              "content": "Metrics forToolInvocation Deciding whether to invokeanexternaltolisacritical step thatcan significantly affct both theefficiencyandtheeffectivenessofasystem.In many scenarios,the modelmust determineif itsownreasoning is suffcienttoansweraqueryor if aditionalexternalknowledge(orfunctionality)providedbyatoolisrequired.To formalize this process, we introduce a labeled dataset",
              "index": 2,
              "part": 0,
              "translated_content": "工具调用的度量标准决定是否调用外部工具是一个至关重要的步骤，它可以显著影响系统的效率和有效性。在许多场景中，模型必须确定自身推理是否足以回答查询，或者是否需要工具提供的额外外部知识（或功能）。为了形式化这一过程，我们引入了一个带标签的数据集。"
            },
            {
              "type": "formula",
              "content": "$$ \nD_{\\mathrm{inv}}=\\{(q_{i},y_{i})\\}_{i=0}^{N-1},\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $q_{i}$ represents the $i$ -th user query and $y_{i}\\in\\{0,1\\}$ is a binary label indicating whether tool invocation is necessary $\\langle y_{i}=1\\rangle$ )or not ( $(y_{i}=0)$ ). Based on this dataset, the model learns a decision function $d(q_{i})$ defined as:",
              "index": 4,
              "part": 0,
              "translated_content": "其中，$q_{i}$表示第$i$个用户查询，$y_{i}\\in\\{0,1\\}$是一个二进制标签，指示是否需要调用工具（$\\langle y_{i}=1\\rangle$）或不需要（$(y_{i}=0)$）。基于这个数据集，模型学习一个决策函数$d(q_{i})$，定义如下："
            },
            {
              "type": "formula",
              "content": "$$ \nd(q_{i})={\\left\\{\\begin{array}{l l}{1,}&{{\\mathrm{if~}}P_{\\theta}(y=1\\mid q_{i})\\geq\\tau,}\\\\ {0,}&{{\\mathrm{otherwise}},}\\end{array}\\right.}\n $$",
              "index": 5,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $P_{\\theta}(y=1\\mid q_{i})$ denotes the predicted probability (from a model parameterized by $\\theta$ ） that a tool should be invoked for query $q_{i}$ , and $\\tau$ is a predetermined threshold.\n\nIn addition tothisdecisionrule,severalmetricscanbeusedtoevaluate the model'sabilitytocorrctlydecideontool invocation. For example, the overall invocation accuracy $A_{\\mathrm{inv}}$ can be computed as:",
              "index": 6,
              "part": 0,
              "translated_content": "其中，$P_{\\theta}(y=1\\mid q_{i})$表示模型（由参数$\\theta$参数化）预测在查询$q_{i}$中调用工具的概率，$\\tau$是预先确定的阈值。\n\n除了这个决策规则，还可以使用几种指标来评估模型正确决定是否调用工具的能力。例如，可以计算整体调用准确率$A_{\\mathrm{inv}}$如下："
            },
            {
              "type": "formula",
              "content": "$$ \nA_{\\mathrm{inv}}=\\frac{1}{N}\\sum_{i=0}^{N-1}\\mathbf{1}\\{d(q_{i})=y_{i}\\},\n $$",
              "index": 7,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Other metrics such as precision, recal, and Fl score are also applicable. Moreover, if $C_{\\mathrm{inv}}$ represents the cost incurred by invoking a tool and $R(q_{i})$ the benefit or reward obtained when a tool is correctly used, one can define a net benefit score:",
              "index": 8,
              "part": 0,
              "translated_content": "其中，$\\mathbf{1}\\{\\cdot\\}$是指示函数。还可以使用其他指标，如精确率、召回率和F1分数。此外，如果$C_{\\mathrm{inv}}$表示调用工具产生的成本，$R(q_{i})$表示正确使用工具时获得的收益或奖励，则可以定义净收益分数："
            },
            {
              "type": "formula",
              "content": "$$ \n{\\cal B}_{\\mathrm{inv}}=\\sum_{i=0}^{N-1}\\left({\\bf1}\\{d(q_{i})=1\\}\\cdot R(q_{i})-C_{\\mathrm{inv}}\\right).\n $$",
              "index": 9,
              "part": 0
            },
            {
              "type": "text",
              "content": "This formulation not only emphasizes accuracy but alsoconsiders the cost-effectiveness of invoking external tools.\n\nTool Selection Among Candidates Once the decision to invoke atoolis made,the next challenge is to select the most appropriate tool from a pool of candidates. Let the candidate toolset be represented as:",
              "index": 10,
              "part": 0,
              "translated_content": "这种表述不仅强调准确性，还考虑了调用外部工具的成本效益。\n\n在决定调用工具之后，下一个挑战是从候选工具池中选择最合适的工具。假设候选工具集表示为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathcal{T}=\\{t_{1},t_{2},\\dots,t_{M}\\}.\n $$",
              "index": 11,
              "part": 0
            },
            {
              "type": "text",
              "content": "For a given query $q_{i}$ , assume that the optimal tool (according to ground truth) is $t_{i}^{*}$ and the model selects $\\hat{t}_{i}$ .The simplest measure of selection performance is the tool selection accuracy $A_{S}$",
              "index": 12,
              "part": 0,
              "translated_content": "对于给定查询$q_{i}$，假设最佳工具（根据地面实况）为$t_{i}^{*}$，模型选择了$\\hat{t}_{i}$。最简单的选择性能度量是工具选择准确率$A_{S}$。"
            },
            {
              "type": "formula",
              "content": "$$ \nA_{S}=\\frac{1}{|Q|}\\sum_{q_{i}\\in Q}\\mathbf{1}\\{\\hat{t}_{i}=t_{i}^{*}\\}.\n $$",
              "index": 13,
              "part": 0
            },
            {
              "type": "text",
              "content": "However, many scenarios involve ranking multiple candidate tools by their relevance.In such cases,ranking-based metrics such as Mean Reciprocal Rank (MRR) and normalized Discounted Cumulative Gain (nDCG) offer a more nuanced evaluation. [690] use those two when evaluating the tool retriever system.\n\nTool Retrieval Efficiency and Hierarchical Accuracy Toolretrieval involves both the speed of identifying a suitable toolandtheaccuracyof thatselection.Effcientretrievalmethodsreducelatencyandcomputationaloverhead,whilehigh retrieval accuracyensures thatthemost relevant tolisidentifiedforthetask.Toevaluate toolusagecomprehensively, we adopta hierarchical framework that distinguishes between retrieval accuracy and selection accuracy. Retrieval accuracy $(A_{R})$ reflects how precisely the system retrieves the correct tool from the repository,typically measured by metricssuch as ExactMatch(EM)and Fl score,whichcapture both complete and partial matches.Incontrast,selection accuracy $(A_{S})$ assesses the system's ability to choose the optimal tool from a set of candidates, again using similar metrics. Overall tool usage awareness is further evaluated by accuracy, recall, precision, and Fl score.",
              "index": 14,
              "part": 0,
              "translated_content": "然而，许多场景涉及通过相关性对多个候选工具进行排名。在这种情况下，诸如平均倒数排名（MRR）和归一化折现累积增益（nDCG）之类的基于排名的指标提供了更为细致的评估。[690]在评估工具检索系统时使用了这两种指标。\n\n工具检索效率和层次准确性 工具检索涉及识别合适工具的速度以及选择的准确性。高效的检索方法可以减少延迟和计算开销，而高检索准确性则确保为任务识别出最相关的工具。为了全面评估工具使用，我们采用了一个层次框架，区分了检索准确性和选择准确性。检索准确性（$A_{R}$）反映了系统从存储库中精确检索正确工具的能力，通常通过诸如精确匹配（EM）和F1分数之类的指标来衡量，这些指标捕捉了完全匹配和部分匹配。相反，选择准确性（$A_{S}$）评估了系统从一组候选工具中选择最佳工具的能力，同样使用类似的指标。总体工具使用意识进一步通过准确度、召回率、精确度和F1分数进行评估。"
            },
            {
              "type": "text",
              "content": "The overall retrieval efficiency $E_{R e t}$ is thus can be expressed as:",
              "index": 15,
              "part": 0,
              "translated_content": "因此，总体检索效率$E_{Ret}$可以表示为："
            },
            {
              "type": "formula",
              "content": "$$ \nE_{R e t}=\\frac{A_{R}\\times A_{S}\\times A_{P}\\times A_{U}}{C_{R}}\n $$",
              "index": 16,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $C_{R}$ is the cost associated with retrieval. Optimization strategies may involve training embedding models with feedback mechanisms to enhance both efficiency and each hierarchical component of accuracy.\n\nFor a more nuanced evaluation of tool selection, Metatool [796] introduces the Correct Selection Rate (CSR),which quantifies the percentage of queries for which the model selects the expected tool(s).This evaluation framework addresses four aspects:selecting thecorrect toolamong similar candidates,choosing appropriate tools in contextspecific scenarios,ensuring reliabilityby avoiding the selectionof incorrect or non-existent tools,andhandling multi-tool queries.Together,these metrics and sub-tasks providearobust measureof both the effciency and precision in tool retrieval and selection.",
              "index": 17,
              "part": 0,
              "translated_content": "其中$C_{R}$是与检索相关的成本。优化策略可能涉及使用反馈机制训练嵌入模型，以增强效率和每个层次组件的准确性。\n\n为了更细致地评估工具选择，Metatool [796]引入了正确选择率（CSR），用于量化模型选择预期工具的查询百分比。这种评估框架涵盖了四个方面：在类似候选工具中选择正确的工具，在特定上下文场景中选择适当的工具，通过避免选择不正确或不存在的工具来确保可靠性，并处理多工具查询。这些指标和子任务共同提供了工具检索和选择中效率和精度的稳健度量。"
            },
            {
              "type": "text",
              "content": "Tool Planning for Complex Tasks Complextasks often require thesequential application of multiple tools to reach an optimal solution. A tool plan can be represented as an ordered sequence",
              "index": 18,
              "part": 0,
              "translated_content": "复杂任务的工具规划\n复杂任务通常需要依次应用多个工具才能达到最佳解决方案。工具规划可以表示为一个有序序列。"
            },
            {
              "type": "formula",
              "content": "$$ \n\\Pi=[t_{1},t_{2},\\dots,t_{K}],\n $$",
              "index": 19,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $K$ is the number of steps.The quality of such a plan is typicall evaluated by balancing its task effectiveness (e.g., via a metric $R_{\\mathrm{task}}(\\Pi))$ against the plan's complexity (or length). This balance can be captured by a composite planning score of the form",
              "index": 20,
              "part": 0,
              "translated_content": "其中$K$是步骤的数量。这样的计划质量通常通过平衡任务效果（例如，通过度量$R_{\\mathrm{task}}(\\Pi)$）与计划的复杂性（或长度）来评估。这种平衡可以通过形式为的综合规划得分来捕捉。"
            },
            {
              "type": "formula",
              "content": "$$ \nS_{\\mathrm{plan}}=\\alpha\\cdot R_{\\mathrm{task}}(\\Pi)-\\beta\\cdot K,\n $$",
              "index": 21,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\alpha$ and $\\beta$ are coeficients that adjust the trade-of between the benefits of high task performance and the cost associated with plan complexity. When ground truth plans $\\Pi^{*}$ are available, similarity metrics such as BLEU or ROUGE can be used to compare the predicted plan $\\Pi$ with $\\Pi^{*}$ , and an overall planning efficiency metric can be defined accordingly.\n\nIn addition,recent worksuch as ToolEyes[797]highlightsthe importance of behavioralplanning intoolusage.Beyond selecting toolsand parameters,it iscrucialforLLMs toconciselysummarize acquired information andstrategicallyplan subsequent steps.Inthiscontext, thebehavioralplanningcapabilityisevaluatedalong twodimensions.First,the score $S_{b}$ validity $\\in[0,1]$ is computed by assessing (1) the reasonableness of summarizing the current state, (2) the timeliness of planning forthe next sequence ofactions,and (3)thediversityof planning.Second, thescoe $S_{b\\mathrm{-integrity}}\\in[0,1]$ .s calculated byevaluating(1)grammaticalsoundness,(2)logicalconsistency,and(3)theabilitytocorrectthinking.The composite behavioral planning score is then determined as",
              "index": 22,
              "part": 0,
              "translated_content": "其中$\\alpha$和$\\beta$是调整高任务性能收益和计划复杂性成本之间权衡的系数。当存在地面真值计划$\\Pi^{*}$时，可以使用BLEU或ROUGE等相似度指标将预测计划$\\Pi$与$\\Pi^{*}$进行比较，然后相应地定义整体规划效率指标。\n\n此外，最近的工作（如ToolEyes[797]）强调了将行为规划引入工具使用的重要性。除了选择工具和参数外，LLMs还需要简洁总结获取的信息并策略性地规划后续步骤。在这种情况下，行为规划能力沿两个维度进行评估。首先，通过评估当前状态总结的合理性、为下一个动作序列规划的及时性以及规划的多样性来计算得分$S_{b}$，其有效性$\\in[0,1]$。其次，通过评估语法正确性、逻辑一致性和纠正思维能力来计算得分$S_{b\\mathrm{-integrity}}\\in[0,1]$。然后，将综合行为规划得分确定为"
            },
            {
              "type": "formula",
              "content": "$$ \nS_{B P}=S_{b\\mathrm{-validity}}\\cdot S_{b\\mathrm{-integrity}},\n $$",
              "index": 23,
              "part": 0
            },
            {
              "type": "text",
              "content": "providingaholistic measure of the model'splanningcapability.This integrated framework ensures that tool planning forcomlex tasks not onlyfocuses onthe selection and ordering of tols but alsoon maintaining coherent,eective, and strategically sound planning processes.\n\nIn summaryoptimizing tool performance within anAgent systemnecesstatesacomprehensive approachthat balances decision-makingaccuracy,retrievalefficiency,hierarchicalselection precision,strategicplanning,rigorous riskanagement,androbust toollearning mechanisms.By implementing targeted optimization and learning strategies,itis possible to enhance both the effectiveness and efficiency of tool-assisted machine learning workflows.",
              "index": 24,
              "part": 0,
              "translated_content": "提供了对模型规划能力的整体度量。这一综合框架确保了针对复杂任务的工具规划不仅侧重于工具的选择和排序，还注重维持连贯、高效和具有战略性的规划过程。\n\n总之，在Agent系统中优化工具性能需要一个综合方法，平衡决策准确性、检索效率、层次选择精度、战略规划、严密风险管理和强大的工具学习机制。通过实施有针对性的优化和学习策略，可以提高工具辅助机器学习工作流程的效果和效率。"
            }
          ],
          "raw_title": "Evaluation of Tool Effectiveness",
          "type": null,
          "children": [],
          "translated_title": "9.4.3 工具效果评估"
        }
      ],
      "translated_title": "9.4 工具优化"
    },
    {
      "title": "9.5 Towards Autonomous Agent Optimization",
      "number": "9.5",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "In addition to optimizing individual modules in agent evolution,such as prompts,tols,and workflows—which are susceptible tolocal optimathatcancompromise theoverallperformance of the agenticsystem,a significant body of researchfocuses on optimizing multiplecomponents within the entire agentic systems.This holistic approach enables large language model(LLM) agents to evolve more comprehensively.However,optimizing the entire system imposes higher requirements. The algorithm must not only account for the impact of individual components on the agentic system but also consider the complex interactions between different components.",
          "index": 0,
          "part": 0,
          "translated_content": "除了优化智能代理演化中的个体模块，如提示、工具和工作流程——这些容易受到局部优化的影响，可能损害智能系统整体性能外，大量研究致力于优化整个智能系统内的多个组件。这种整体方法使大型语言模型（LLM）代理能够更全面地演化。然而，优化整个系统提出了更高的要求。算法不仅必须考虑个体组件对智能系统的影响，还必须考虑不同组件之间的复杂相互作用。"
        },
        {
          "type": "text",
          "content": "ADAS[741] isone of the most representative works that firstformalydefines the research problem of automated design inagentic systems.Itintegrates multiple agenticsystemcomponents intothe evolutionary pipeline.Specifically, ADAS itroduces a meta-agentcapable ofiteratively designing the agenticsystem's workflow, prompts,and potential tools withinthe overalloptimization process.Asdemonstrated in the experiments,the automaticallydesigned agentic systems outperform state-of-the-art hand-designed baselines.",
          "index": 1,
          "part": 0,
          "translated_content": "ADAS[741]是最具代表性的工作之一，首次正式定义了智能系统中自动设计问题。它将多个智能系统组件整合到进化管道中。具体来说，ADAS引入了一个元代理，能够在整体优化过程中迭代地设计智能系统的工作流程、提示和潜在工具。正如实验所证明的那样，自动设计的智能系统胜过了最先进的手工设计基准。"
        },
        {
          "type": "text",
          "content": "Additionall,[726]proposes an agent symbolic learning frameworkfor training language agents,inspired byconnectionist learning principles used in neural networks.By drawing an analogy between agent pipelines andcomputational graphs,the framework introduces a language-based approach to backpropagation and weight updates. It defines a prompt-based lossfunction,propagates language lossthrough agent trajectories, and updatessymbolic components accordingly.This method enables structured optimizationof agentic workflows and naturally extends to multi-agent systems by treating nodes as independent agents or allowing multiple agents to act within a single node.",
          "index": 2,
          "part": 0,
          "translated_content": "此外，[726]提出了一种代理符号学习框架，用于训练语言代理，灵感来自神经网络中使用的连接主义学习原理。通过将代理管道与计算图进行类比，该框架引入了一种基于语言的反向传播和权重更新方法。它定义了基于提示的损失函数，通过代理轨迹传播语言损失，并相应地更新符号组件。该方法实现了代理工作流的结构化优化，并通过将节点视为独立代理或允许多个代理在单个节点中执行而自然地扩展到多代理系统。"
        },
        {
          "type": "text",
          "content": "[799] proposes an approach tooptimize both prompts andthe agent'sowncode,enabling self-improvement.This aligns with the concept of self-reference,where a system cananalyze and modify itsown structureto enhance performance.\n\nSimilarly,[773],[787],[800]and[788]focus onoptimizing both the workflow and prompts within agentic systems.In particular,[285] introduces anapproachthat trains additionallarge language models (LLMs)to generate prompts and workflows, enabling the automated design of agentic system architectures.\n\nIn summary,optimizing the workflow of an entire agentic system is not merely a straightforward aggregationof individualcomponent optimizations.Instead,it requires carefully designed algorithms that account for complex interdependencies among components.This makes system-wide optimization a significantly more challenging task, necessitating advanced techniques to achieve effective and comprehensive improvements.",
          "index": 3,
          "part": 0,
          "translated_content": "[799]提出了一种优化提示和代理自身代码的方法，实现自我改进。这符合自指的概念，即系统可以分析和修改自身结构以增强性能。\n\n类似地，[773]、[787]、[800]和[788]专注于优化代理系统内的工作流程和提示。特别是，[285]提出了一种方法，训练额外的大型语言模型（LLMs）生成提示和工作流程，实现代理系统架构的自动化设计。\n\n总之，优化整个代理系统的工作流程并不仅仅是对各个组件优化的简单聚合。相反，它需要考虑到组件之间复杂的相互依赖关系的精心设计算法。这使得系统范围的优化成为一项更具挑战性的任务，需要先进的技术来实现有效和全面的改进。"
        }
      ],
      "raw_title": "Towards Autonomous Agent Optimization",
      "type": null,
      "children": [],
      "translated_title": "9.5 迈向自主代理优化"
    },
    {
      "title": "Large Language Models as Optimizers",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "In this chapter,we present and discussexisting works thatconceptualizeLLMs asoptimizers.First, wenote that most existing studies focus on the promptoptimization problem defined in Equation(9.1),asoptimizing other components of agentic workflows remainsanemerging researcharea.To proceed, wedraw parallels withclasscal iterativealgorithms and examine their integration into modern optimization workflows.",
          "index": 0,
          "part": 0,
          "translated_content": "在本章中，我们介绍并讨论将LLMs概念化为优化器的现有研究。首先，我们注意到大多数现有研究集中于Equation(9.1)中定义的提示优化问题，因为优化代理工作流的其他组件仍然是一个新兴的研究领域。为了继续，我们将类比于经典的迭代算法，并研究它们如何融入现代优化工作流中。"
        }
      ],
      "raw_title": "Large Language Models as Optimizers",
      "type": null,
      "children": [],
      "translated_title": "大型语言模型作为优化器"
    },
    {
      "title": "Online and Offline Agent Self-Improvement",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "In the pursuitof self-improvement,intellgent agents leverageoptimizationasbothamechanism forrefining individual components—such as prompt design, workflow orchestration,tool utilization,reward function adaptation,and even theoptimization algorithms themselves—and as a strategic framework that ensures these individualimprovements are aligned toward coherent performance enhancement.Forinstance,optimizing the reward function and prompt design in isolation might yield conflicting outcomes,but a strategicapproach coordinates these optimizations to maintain coherence and maximize overalleffectiveness.We categorize self-evolution into two primary paradigms:online and ofine self-improvement.Aditionally, weexplore hybrid optimization strategies that integrateboth approaches to maximize efficiency and adaptability.",
          "index": 0,
          "part": 0,
          "translated_content": "在自我完善的追求中，智能代理利用优化作为一种机制，用于精细调节个体组件，如提示设计、工作流编排、工具利用、奖励函数调整，甚至优化算法本身，并作为一个战略框架，确保这些个体改进朝向一致的性能增强。例如，在孤立情况下优化奖励函数和提示设计可能会产生冲突的结果，但战略方法协调这些优化以保持一致性并最大化整体有效性。我们将自我进化分为两个主要范式：在线和离线自我改进。此外，我们探讨了集成这两种方法以最大化效率和适应性的混合优化策略。"
        }
      ],
      "raw_title": "Online and Offline Agent Self-Improvement",
      "type": null,
      "children": [],
      "translated_title": "在线和离线代理自我提升"
    },
    {
      "title": "11.1 Online Agent Self-Improvement",
      "number": "11.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Online self-improvement refers toreal-time optimization in which an agent dynamicaly adjusts its behavior basedon immediate feedback.This paradigm ensures that agents remain responsive to evolving environments bycontinuously optimizing key performance metrics—such as task success,latency,cost, and stability-inan iterative feedback loop. Online self-improvement is particularly effective in applications that require dynamic adaptability,such asreal-time decision-making, personalized user interactions, and automated reasoning systems. Key optimization strategies in online self-improvement can be clasified intothe following fourcategories: IterativeFeedback and Self-Reflection, Active Exploration in Multi-Agent Systems, Real-Time Reward Shaping, and Dynamic Parameter Tuning.",
          "index": 0,
          "part": 0,
          "translated_content": "在线自我改进是指代理根据即时反馈动态调整其行为的实时优化。这一范式确保代理通过连续优化关键性能指标（如任务成功率、延迟、成本和稳定性）在迭代反馈循环中保持对不断变化的环境的响应能力。在线自我改进在需要动态适应性的应用中特别有效，如实时决策制定、个性化用户交互和自动推理系统。在线自我改进中的关键优化策略可分为以下四类：迭代反馈与自我反思、多代理系统中的主动探索、实时奖励塑造和动态参数调整。"
        },
        {
          "type": "text",
          "content": "Iterative Feedback and Self-Reflection These methodologies[48,67,72,70,847,47]focus on enabling agents to critique andrefinetheirwnutputs iteratively.Reflexion[48],Self-Refine[67],andTreeof Thoughts[72]introduce self-critique loops,where the model identifies errors and proposes revisions in real-time.ReAct[70] combines chain-of-thought“reasoning\"with“acting\",allowing the model torevise steps iteratively after observing external feedback.Inaddition,othermethodseitherrelyonself-consistency[78]toselectthe mostcoherentsolutionorleverage a process reward model(PRM)Lightman et al.[847]tochoose the best solution fromthecandidates.Collctively, these frameworks reduce eror propagation and support rapid adaptation without requiringa separate offine fine-tuning cycle.",
          "index": 1,
          "part": 0,
          "translated_content": "迭代反馈与自我反思 这些方法学旨在使代理能够迭代地批判和优化其自身输出。自我反思和自我完善以及思维树引入了自我批判循环，模型在实时中识别错误并提出修订。ReAct将“推理”的思路与“行动”相结合，允许模型在观察外部反馈后迭代修订步骤。此外，其他方法要么依赖于自洽性来选择最连贯的解决方案，要么利用过程奖励模型（PRM）（Lightman等，2019）从候选方案中选择最佳解决方案。总的来说，这些框架减少了错误传播，并支持在不需要单独的离线微调周期的情况下进行快速适应。"
        },
        {
          "type": "text",
          "content": "Active Exploration in Multi-Agent Systems These approaches [626,848,627,152]actively explore and dynamically search for novel paterns and workflow improvements in multi-agent systems.MetaGPT [626],CAMEL [848],and ChatDev[627]showcase multi-roleor multi-agent ecosystems thatinteract inreal-time,exchanging continuous feedback to refine each other'scontributions.Similarly,HuggingGPT[152]coordinates specialized models(hosted on Hugging Face)through a central LLM controller, which dynamically routes tasks and gathers feedback.These collaborative strategies further highlight how online updates among agents can incrementaly refine collective outcomes.",
          "index": 2,
          "part": 0,
          "translated_content": "多智能体系统中的主动探索 这些方法[626,848,627,152]积极探索并动态搜索多智能体系统中的新模式和工作流改进。MetaGPT[626]、CAMEL[848]和ChatDev[627]展示了多角色或多智能体生态系统，在实时互动中交换持续反馈以完善彼此的贡献。类似地，HuggingGPT[152]通过中央的LLM控制器协调托管在Hugging Face上的专门模型，动态路由任务并收集反馈。这些协作策略进一步凸显了智能体之间的在线更新如何逐步完善集体结果。"
        },
        {
          "type": "text",
          "content": "Real-Time Reward Shaping Rather than relying on fixed or purely offine reward specifications, some frameworks[731,91,05,849]integrate immediate feedback signalsnotonly tocorrect errors,but alsoto adapt intenal reward functions and policies.This enablesself-adaptive reward calibration that balances trade-offs between performance,computational cost, and latency,alowing agents to optimize reward mechanisms dynamically in response to user interactions.",
          "index": 3,
          "part": 0,
          "translated_content": "实时奖励塑造 一些框架[731,91,05,849]不仅依赖于固定或纯离线奖励规范，还整合了即时反馈信号，用于纠正错误，同时也用于调整内部奖励函数和策略。这使得自适应奖励校准成为可能，平衡了性能、计算成本和延迟之间的权衡，使智能体能够根据用户交互动态优化奖励机制。"
        },
        {
          "type": "figure",
          "src": "images/48c2dee600c375a3465e5c88add4a12af8fea68f2a03e929c894d12eb5b02990.jpg",
          "alt": "",
          "caption": "Figure 1l.l:Anilustrationofself-improvementunder threedifferent utilization scenarios,including Online,Ofline, and Hybrid self-improvement.",
          "index": 4,
          "part": 0,
          "translated_caption": "图1.1：展示了在三种不同利用场景下的自我改进示意图，包括在线、离线和混合自我改进。"
        },
        {
          "type": "text",
          "content": "Dynamic Parameter Tuning In this category,agents autonomously update their internal parameters (including prompt templates,toolinvocationthresholds,searchheuristics,etc.)inrealtime,leveraging gradient-freeor approximated gradientmethods.These updates optimize both computationaleficiency and decision accuracy,allowing for seamless adaptation to evolving contexts. Self-Stering Optimization (SSO)[850] eliminates the need for manual annotation and maintains signalaccuracy while keeping training on-policy by autonomously generating preference signals during iterative training.",
          "index": 5,
          "part": 0,
          "translated_content": "动态参数调整：在这一类别中，智能体通过使用无梯度或近似梯度方法，在实时环境中自主更新其内部参数（包括提示模板、工具调用阈值、搜索启发式等）。这些更新优化了计算效率和决策准确性，使其能够无缝适应不断变化的环境。自我优化（SSO）[850]消除了手动标注的需求，通过在迭代训练过程中自动生成偏好信号，在保持训练在线策略的同时保持信号准确性。"
        },
        {
          "type": "text",
          "content": "Online self-improvement fosters a continuously evolving agent framework where learning is embedded within task execution,promotingenhancedreal-timeadaptability,user-centric optimization,androbust problem-solvingcapabilities.",
          "index": 6,
          "part": 0,
          "translated_content": "在线自我优化促进了一个不断演化的智能体框架，其中学习嵌入在任务执行中，促进了增强的实时适应性、以用户为中心的优化和强大的问题解决能力。"
        }
      ],
      "raw_title": "Online Agent Self-Improvement",
      "type": null,
      "children": [],
      "translated_title": "11.1 在线代理自我提升"
    },
    {
      "title": "11.2 Offline Agent Self-Improvement",
      "number": "11.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Offline self-improvement,incontrast,leverages structured,batchbasedoptimization.This paradigm utilizesscheduled trainingsessionswithhigh-qualitycurateddatasets tosystematicallimprove the agent'sgeneralizationcapabilities[851, 667,852, 853,854]. Unlike online approaches, offline approaches accommodate more computationall intensive methodologies,including Batch Parameter Updates and Fine-Tuning, Meta-Optimization, and Systematic Reward Model Calibration.\n\nBatch Parameter Updates and Fine-Tuning In this category,agents undergo extensive fine-tuning using supervised learning or reinforcementlearning (RL)techniques,optimizing performance across large-scale datasets over multiple training epochs. Retrieval-augmented generation (RAG) is often integrated to enhancecontextual understanding and long-term memoryretrieval[740,741].Such methods allow agentstooptimizeretrievalstrategies,therebyimproving reasoning over extensive knowledge corpora.\n\nMeta-Optimization of Agent Components Here ofline training is not limited to improving task performance but extends to refining optimization algorithms themselves.Meta-learning strategies that optimize hyperparameters or even restructure the optimization process dynamically have demonstrated promising outcomes[731, 91]. These meta-optimization approaches enable agents to discover the most efective learning parameters for new problem domains.\n\nSystematic Reward ModelCalibrationOfine settings facilite the precisecalibration ofreward models, incorporating hierarchical orlistwise reward integration frameworks (e.g.,LIRE[855])toalign agent behavior withlong-term objectives through gradient-basedreward optimization.Such calibration ensures that reward functions reflectreal-world task complexity, thereby mitigating bias and enhancing generalization.\n\nThe structured nature of offline optimization results in a robust agent baseline, whose performance is fine-tuned to optimize stability,effciency, and computationalcost before real-world deployment.Offline trainingallows for high-fidelity model refinement and is essential for misson-criticalapplications requiring predictable performance guarantees.",
          "index": 0,
          "part": 0,
          "translated_content": "离线自我改进相反，利用结构化的基于批处理的优化。这种范式利用预定的训练会话和高质量策划数据集，系统地提升代理的泛化能力。与在线方法不同，离线方法适用于更加计算密集的方法，包括批处理参数更新和微调、元优化以及系统性奖励模型校准。\n\n批处理参数更新和微调：在这个类别中，代理通过使用监督学习或强化学习（RL）技术进行广泛微调，优化在大规模数据集上跨多个训练周期的性能。检索增强生成（RAG）通常被整合以增强上下文理解和长期记忆检索。这些方法允许代理优化检索策略，从而改善对广泛知识语料库的推理能力。\n\n代理组件的元优化：这里的离线训练不仅限于提升任务性能，还延伸到优化算法本身。优化超参数或甚至动态重构优化过程的元学习策略已经展现出有前途的成果。这些元优化方法使代理能够为新问题领域发现最有效的学习参数。\n\n系统性奖励模型校准：离线设置有助于精确校准奖励模型，整合了分层或列表式奖励集成框架（例如，LIRE），通过基于梯度的奖励优化来使代理行为与长期目标保持一致。这种校准确保奖励函数反映真实世界任务的复杂性，从而减轻偏见并增强泛化能力。\n\n离线优化的结构化特性导致了一个强大的代理基线，其性能经过微调以优化稳定性、效率和计算成本，然后再进行真实世界部署。离线训练允许进行高保真度的模型细化，对于需要可预测性能保证的关键任务应用至关重要。"
        }
      ],
      "raw_title": "Offline Agent Self-Improvement",
      "type": null,
      "children": [],
      "translated_title": "11.2 离线Agent自我提升"
    },
    {
      "title": "11.3 Comparison of Online and Offline Improvement",
      "number": "11.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Online and ofline optimizationoffer complementary benefits,each excelling indifferent aspectsofself-improvement. Online optimization thrives indynamic environments,where real-time feedback enables continuous adaptation. It is well-suited foralications thatrequireimmediateresponsiveness,suchas interactiveagents,real-timedecsionmaking, andreinforcementlearningsystems.However,frequent updates may introduce instabilityor drift,requiring mechanisms to mitigate performance degradation over time.\n\nIn contrast,offine optimization emphasizesstructured,high-fidelitytraining using pre-collecteddatasets,ensuring robust and stable performance before deployment. By leveraging computationaly intensive learning methods such as batchtraining,fine-tuning,andmeta-optimizationofline approaches provide strong generalization andlong-term consistency.However,theylack the agilityofonlinelearming andmay struggle toadapt effcientlytonovel scenarios without additional retraining. Table 11.1 summarizes the key distinctions between these two paradigms.",
          "index": 0,
          "part": 0,
          "translated_content": "在线和离线优化提供了互补的好处，各自在自我提升的不同方面表现出色。在线优化在动态环境中蓬勃发展，在这种环境中，实时反馈使得持续适应成为可能。它非常适用于需要即时响应的应用，如交互式代理、实时决策和强化学习系统。然而，频繁的更新可能会引入不稳定性或漂移，需要机制来减轻随时间推移而带来的性能下降。\n\n相比之下，离线优化强调使用预先收集的数据集进行结构化、高保真度的训练，在部署之前确保强大且稳定的性能。通过利用计算密集型的学习方法，如批量训练、微调和元优化，离线方法提供了强大的泛化能力和长期一致性。然而，它们缺乏在线学习的灵活性，并且可能在没有额外重新训练的情况下难以有效地适应新领域。表11.1总结了这两种范式之间的关键区别。"
        },
        {
          "type": "table",
          "content": "<html><body><table><tr><td>Feature</td><td>Online Optimization</td><td>Offline Optimization</td></tr><tr><td>Learning Process</td><td>Continuous updates based on real-time feedback</td><td>Batch updates during scheduled training phases</td></tr><tr><td>Adaptability</td><td>High, capable of adjusting dynamically</td><td>Lower, adapts only after retraining</td></tr><tr><td>Computational Effi- ciency</td><td>More efficient for incremental updates</td><td>More resource-intensive due to batch training</td></tr><tr><td>Data Dependency</td><td>Requires real-time data streams</td><td>Relies on curated, high-quality datasets</td></tr><tr><td>Risk of Overfitting</td><td>Lower due to continuous learning</td><td>Higher if training data is not diverse</td></tr><tr><td>Stability</td><td>Potentially less stable due to frequent updates</td><td>More stable with controlled training set- tings</td></tr></table></body></html>",
          "caption": "Table 1l.1: Comparison of Online vs. Offline Optimization Strategies in Self-Improvement Agents.",
          "index": 1,
          "part": 0,
          "translated_caption": "表1.1：自我增强代理中在线与离线优化策略的比较。"
        },
        {
          "type": "text",
          "content": "While bothapproaches have inherent strengths andtrade-offs, modern intellgentsystems increasingly integrate them through hybrid optimization strategies.These hybrid frameworks leverage the stability of ofline training while incorporating real-time adaptability,enabling agents tomaintain long-termrobustnesswhilecontinuouslyrefining their performance in dynamic environments.",
          "index": 2,
          "part": 0,
          "translated_content": "尽管这两种方法都具有固有的优势和权衡，但现代智能系统越来越多地通过混合优化策略将它们整合起来。这些混合框架利用了离线训练的稳定性，同时融合了实时适应性，使代理能够在动态环境中保持长期稳健性，同时不断完善其性能。"
        }
      ],
      "raw_title": "Comparison of Online and Offline Improvement",
      "type": null,
      "children": [],
      "translated_title": "11.3 线上与线下改进的比较"
    },
    {
      "title": "11.4 Hybrid Approaches",
      "number": "11.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Recognizing that both online and ofline methods have inherentlimitations, manycontemporary systems adopt hybrid optimization strategies.These hybrid methods integrate structured offlineoptimization withresponsive online updates to achieve continuous incremental agent enhancement.\n\nHybrid optimization explicitly supportsself-improvement byempowering agents toautonomously evaluate,adapt,and enhance their behaviors through distinct yet interconnected stages:\n\n·Offline Pre-Training: In this foundational stage, agents acquire robust baseline capabilities through extensive offline training oncurateddatasets.Thisstage establishes essentialskills,suchasreasoning and decision-making, required for initialautonomous performance.For instance,frameworks such as the one introduced by Schrittwieser et al.[856]illustrate howofflinepretrainingsystematicallyenhances initialagent capabilities,ensuring subsequent online improvements are built upon a stable foundation. ·Online Fine-Tuning for Dynamic Adaptation: Agents actively refine theircapabilities byautonomously evaluating their performance,identifying shortcomings, and dynamically adjusting strategies based on real-time feedback. This adaptive fine-tuning stage directly aligns with the agent self-improvement paradigm by allowing real-time optimization of agent-specific workflows and behaviors,exemplified by Decision Mamba-Hybrid (DM-H) [857], where agents efficiently adapt to complex, evolving scenarios.",
          "index": 0,
          "part": 0,
          "translated_content": "认识到在线和离线方法都具有固有的局限性，许多当代系统采用混合优化策略。这些混合方法将结构化的离线优化与响应式的在线更新相结合，以实现对代理的持续增量增强。\n\n混合优化明确支持自我改进，通过使代理能够自主评估、调整和增强其行为，通过独特而相互关联的阶段实现：\n\n·离线预训练：在这个基础阶段，代理通过对策划数据集的广泛离线训练获得强大的基准能力。这个阶段建立了初步自主表现所需的基本技能，如推理和决策能力。例如，Schrittwieser等人介绍的框架系统显示了离线预训练如何系统地增强初始代理能力，确保随后的在线改进建立在稳定的基础之上。 ·在线微调以实现动态调整：代理通过自主评估其表现、识别不足之处，并根据实时反馈动态调整策略，积极完善其能力。这种自适应微调阶段与代理自我改进范式直接契合，允许实时优化代理特定的工作流程和行为，Decision Mamba-Hybrid (DM-H)等实例展示了代理如何有效适应复杂、不断变化的场景。"
        },
        {
          "type": "text",
          "content": "·Periodic Offline Consolidation for Long-Term Improvement: periodic offline consolidation phases, agents systematically integrateandsolidifyimprovements identifiedduring onlineinteractions.This ensures thatincremental, online-acquired skill and improvements are systematically integrated into the agent'score models, maintaining long-term stability and efectivenessThe Uni-O4 framework[858] exemplifies how this processenables seamless transitions between offline knowledge consolidation and online adaptive improvements.",
          "index": 1,
          "part": 0,
          "translated_content": "·定期离线巩固以实现长期改进：定期的离线巩固阶段，代理系统地整合和巩固在线交互中识别出的改进。这确保了增量式、在线获得的技能和改进被系统地整合到代理的核心模型中，保持长期稳定性和有效性。Uni-O4框架展示了这一过程如何实现离线知识巩固和在线自适应改进之间的无缝过渡。"
        },
        {
          "type": "text",
          "content": "Hybrid optimization thus explicitly supports autonomous,continuous evolution by seamlessly interweaving structured offline learning with proactive,real-time online adaptation.Thiscyclicalapproachequips agents withboth immediate responsiveness and stable long-term improvement, making it idealy suited for complex,real-world scenarios such as autonomous robotics, personalized intelligent assistants, and interactive systems.",
          "index": 2,
          "part": 0,
          "translated_content": "混合优化因此通过将结构化的离线学习与主动的实时在线适应相无缝地交织在一起，明确支持自主、持续的演化。这种循环方法为代理系统提供了即时的响应能力和稳定的长期改进，使其非常适用于复杂的现实场景，如自主机器人、个性化智能助手和交互系统。"
        }
      ],
      "raw_title": "Hybrid Approaches",
      "type": null,
      "children": [],
      "translated_title": "11.4 混合方法"
    },
    {
      "title": "Scientific Discovery and Intelligent Evolution",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "In previouschapters,we primarilydiscussedtheevolutionofagenticsystems fromatechnicalperspective,focusingon how to develop systems that can effctivelyperform well-efined tasks traditionally executed by humans.However,a fundamentalandimportant question remains:canthese agents drivea self-sustaining innovationcyclethat propels both agent evolution and human progress?\n\nScientific knowledge discovery is acompelling example of self-evolution in intelligent beings,as it helps them adapt to the world ina sustainable way.Agentscapableof discoveringscientificknowledge at differentlevels of autonomy and in asafe manner willalso play important roles in technological innovation for humanity. In this section, we survey progressin autonomous discovery using agentic workfows and discuss thetechnologicalreadiness toward fully autonomous,self-evolving agents.Withinthis scope,thegoalofthe agent istouncover, validate,andintegrate data, insights,and principles toadvance anobjectivescientificunderstandingofnatural phenomena.Insteadofalteringthe world, theagent seeks tobetterunderstandnatureasaScientist AI[859]andassist humans inextending theboundaries of knowledge.",
          "index": 0,
          "part": 0,
          "translated_content": "在前几章中，我们主要从技术角度讨论了智能代理系统的演变，重点关注如何开发能够有效执行传统由人类执行的明确定义任务的系统。然而，一个根本而重要的问题仍然存在：这些代理能否推动自我持续创新循环，推动代理演化和人类进步？\n\n科学知识的发现是智能生物自我演化的一个引人注目的例子，因为它帮助它们以可持续的方式适应世界。能够以不同程度的自主性和安全方式发现科学知识的代理也将在人类技术创新中扮演重要角色。在这一部分中，我们调查了使用代理工作流进行自主发现的进展，并讨论了朝着完全自主、自我演化代理的技术准备情况。在这个范围内，代理的目标是揭示、验证和整合数据、见解和原则，以推进对自然现象客观科学理解的进展。代理不是试图改变世界，而是试图作为一种“科学家AI”更好地理解自然，并协助人类拓展知识的边界。"
        },
        {
          "type": "text",
          "content": "We firstdefinetheconceptof knowledge andintelligence toclarifyourdiscussion,thenintroducethree typical scenarios where agents and scientificknowledge interact.Wealsohighlight existing successes andexamples of self-enhancing agents applied totheoretical,computational, andexperimentalscientificresearch.Lastly,we summarizethe current challenges for a future outlook.",
          "index": 1,
          "part": 0,
          "translated_content": "我们首先定义知识和智能的概念，以澄清我们的讨论，然后介绍了代理和科学知识相互作用的三种典型场景。我们还强调了已有成功案例，以及应用于理论、计算和实验科学研究的自我增强代理。最后，我们总结了未来展望中的当前挑战。"
        }
      ],
      "raw_title": "Scientific Discovery and Intelligent Evolution",
      "type": null,
      "children": [],
      "translated_title": "科学发现与智能进化"
    },
    {
      "title": "Collaborative and Evolutionary Intelligent Systems",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The concepts of collaboration and evolution lie at the heart of intelligent multi-agent systems (MAS).Inspired by biologicalecosystems andhuman societal dynamics,these systems leverage collective intellgence to solve complex challenges that exceed the capabilities of individual agents[914]. Human societies exemplify how cooperation, specialization,anddistributeddecision-making significantlyenhancecollective problem-solvingeffctiveness.Similarly, MAS adopts these strategies,integrating specialized agents toaddress intricatetaskscollaboratively.The foundational principle ofcollective intelligence-the“Wisdom of Crowds\"by[915]-suggests diverse, independent agents often yield superior decisions compared to solitary experts,directly underpinning the design philosophy of MAS.Cognitive theories,such as Minsky'ssocietyof mind[17]andthetheoryofmind[916,917],further reinforcethis paradigmby proposing that intelligence emerges from structured interactions among specialized units.",
          "index": 0,
          "part": 0,
          "translated_content": "合作与进化的概念是智能多代理系统（MAS）的核心。受生物生态系统和人类社会动态的启发，这些系统利用集体智慧来解决超出个体代理能力范围的复杂挑战。人类社会展示了合作、专业化和分布式决策如何显著增强集体问题解决效率的例子。同样，MAS采用这些策略，整合专门代理以协作解决复杂任务。集体智慧的基本原则——“众人的智慧”——表明，多样化、独立的代理通常比孤立的专家产生更优越的决策，直接支持MAS的设计理念。认知理论，例如明斯基的“心智社会”和心灵理论，进一步强调了这一范式，提出智能是从专门单位之间的结构化互动中产生的。"
        },
        {
          "type": "text",
          "content": "Recently, advancements in large language models (LLMs)have introduced new possibilities for collaborative and evolutionary multi-agent systems (LLM-MAS).Benefiting from powerful reasoning, planning,and decision-making capabilities, these models enable the creation of sophisticated MAS architectures mirroring the cooperative and adaptive characteristics found in human societies. Agents within LLM-MAS often assume distinct identities and roles,reflecting human-like division of labor and specialized collaboration.Byembracing structured communication, dynamic knowledge sharing, and coordinated decision-making,these systems emulate human social dynamics to achieve common goals. Moreover, LLM-MAS is inherently evolutionary; agents continuously adapt and improve through interactions,feedback, and iterative learning,resulting inenhanced system performance over time.Roadmap In this chapter,we systematically survey the emerging fieldof LLM-based multi-agent systems,focusingspecifically on their colaborative mechanisms and evolutionary capabilities.We first examine how distinct system objectives shape agent roles,behavior patterns,and collaborative strategies in Chapter 13.Next, inChapter 14 we analyze various communication structures,including interaction protocols thatfacilitateeffective agent-agent and human-agent communication.Additionall,we explorecollaborative decision-making methodologies andhow agents leverage their unique expertise and perspectives inChapter 15,and discussthe collective intelligence and evolution mechanism in Chapter 16. FinallyinChapter 17, we discuss evolutionary processes,highlighting adaptive learning methods, continuous knowledge sharing,and mechanisms foriterative improvement thatcollectively enhanceMAS performance. Through this comprehensive survey, we identif current achievements, discuss existing challenges,and highlight promising research directions for collaborative and evolutionary intelligent systems.",
          "index": 1,
          "part": 0,
          "translated_content": "最近，大型语言模型（LLMs）的进步为协作和进化的多代理系统（LLM-MAS）引入了新的可能性。借助强大的推理、规划和决策能力，这些模型使得能够创建复杂的MAS架构，反映了人类社会中发现的合作和适应性特征。LLM-MAS中的代理通常扮演不同的身份和角色，反映了类似人类的分工和专业化合作。通过采用结构化通信、动态知识共享和协调决策，这些系统模拟人类社会动态以实现共同目标。此外，LLM-MAS在本质上是进化的；代理通过互动、反馈和迭代学习不断适应和改进，从而随着时间的推移提高系统性能。在本章中，我们系统地调查了基于LLM的多代理系统这一新兴领域，重点关注它们的协作机制和进化能力。首先，我们在第13章中研究了不同的系统目标如何塑造代理的角色、行为模式和协作策略。接下来，在第14章中，我们分析了各种通信结构，包括促进有效代理-代理和人-代理通信的交互协议。此外，我们探讨了协作决策方法以及代理如何利用其独特的专业知识和观点，在第15章中，并在第16章讨论了集体智慧和进化机制。最后，在第17章中，我们讨论了进化过程，重点介绍了适应性学习方法、持续知识共享以及促进迭代改进的机制，共同提升MAS性能。通过这一全面调查，我们确定了当前的成就，讨论了现有的挑战，并强调了协作和进化智能系统的有前途的研究方向。"
        },
        {
          "type": "figure",
          "src": "images/14ea00640f8414245fcf2dd5fabe6e70c5a081ee5396b68cb13812b2c2726d08.jpg",
          "alt": "",
          "caption": "Figure 12.3: Taxonomy of LLM-based Multi-Agent Systems.",
          "index": 2,
          "part": 0,
          "translated_caption": "图12.3：基于LLM的多智能体系统分类。"
        }
      ],
      "raw_title": "Collaborative and Evolutionary Intelligent Systems",
      "type": null,
      "children": [],
      "translated_title": "1. 协作与进化智能系统"
    },
    {
      "title": "Design of Multi-Agent Systems",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "In the contextof LLM-based multi-agent systems (LLM-MAS),collaboration goals and collaboration norms serve asfoundational elements that shape system behavior, interaction patterns,and overalleffectiveness.Collaboration goals specifytheexplicit objectives agents aim to achieve-whether individuall,collectively,or competitivelywhile colaboration norms define the rules,constraints,and conventions that govern agent interactions within the system.Together, these components establish arobustframework guiding effectivecommunication,coordination,and cooperation among agents.",
          "index": 0,
          "part": 0,
          "translated_content": "在基于LLM的多智能体系统（LLM-MAS）的背景下，协作目标和协作规范作为塑造系统行为、交互模式和整体有效性的基础要素。协作目标指明智能体旨在实现的明确目标 - 无论是个体的、集体的还是竞争性的，而协作规范则定义了规则、约束和惯例，指导系统内智能体之间的互动。这些组成部分共同确立了一个健壮的框架，引导智能体之间有效的沟通、协调和合作。"
        },
        {
          "type": "text",
          "content": "This section categorizes LLM-MAS into three broad classes based on distinct combinations of collaboration goals and norms:strategic learning,modeling and simulation, and collborative task solving.Although not exhaustive, these categories cover a wide spectrum of LLM-MAS designs andclearly reflect how system objectives shape agent interactions and outcomes.\n\n·Strategic Learning systems embed agents within a game-theoretic context, where agents pursue individual or partially conflicting goals.The interactions can be cooperative,competitive,or mixed, guided explicitly by predefined game rules and interaction norms.This seting often aligns with non-cooperative (strategic) and cooperative concepts in traditional game theory. Please refer to Section 13.1 for details. ·Modeling and Simulation contexts focus on agents acting independently,driven by diverse environmental or social factors. Here, interactions emerge organically without necessarily converging on common goals. reflecting the complex dynamics seen inlarge-scale social or economic simulations. Please refer to Section 13.2 for details. ·Collaborative Task Solving emphasizes systematic cooperation among agents to achieve explicitly shared objectives.Agents typically adopt structured workflows,clear roledefinitions, and highly predefined collaboration norms to synchronize their actions toward collctive goals.Please refer to Section 13.3 for details.",
          "index": 1,
          "part": 0,
          "translated_content": "本节将LLM-MAS分为三类，基于不同的协作目标和规范组合：战略学习、建模与仿真以及协作任务解决。虽然不是穷尽的，这些类别涵盖了LLM-MAS设计的广泛领域，清晰地反映了系统目标如何塑造智能体的相互作用和结果。\n\n- 战略学习系统将智能体嵌入到博弈论背景中，智能体追求个体或部分冲突的目标。交互可以是合作的、竞争的或混合的，明确受预定义的博弈规则和互动规范指导。这种设置通常与传统博弈论中的非合作（战略）和合作概念相一致。详情请参见第13.1节。\n- 建模与仿真背景侧重于智能体独立行动，受到各种环境或社会因素驱动。在这里，交互会自然产生，不一定会收敛于共同目标，反映了大规模社会或经济仿真中看到的复杂动态。详情请参见第13.2节。\n- 协作任务解决强调智能体之间的系统合作，以实现明确定义的共享目标。智能体通常采用结构化工作流程、清晰的角色定义和高度预定义的协作规范，以同步他们的行动朝向集体目标。详情请参见第13.3节。"
        },
        {
          "type": "text",
          "content": "Ithe remainder of this chapter, weelaborate oneach category,examining how LLMs enable,influence,and enhance gent behaviors, interactions, and collective intelligence within our scope.\n\nIn the following, weexamine these categories indetail, highlighting how each leverages the capabilities of large language models to shape agent behaviors and interactions.",
          "index": 2,
          "part": 0,
          "translated_content": "在本章的其余部分，我们将详细阐述每一类别，探讨LLMs如何在我们的范围内实现、影响和增强智能体的行为、相互作用和集体智能。\n\n接下来，我们将详细研究这些类别，重点介绍每一类别如何利用大型语言模型的能力来塑造智能体的行为和相互作用。"
        }
      ],
      "raw_title": "Design of Multi-Agent Systems",
      "type": null,
      "children": [],
      "translated_title": "多智能体系统设计"
    },
    {
      "title": "13.1 Strategic Learning: Cooperation vs. Competition",
      "number": "13.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Strategic learning refers to agentscapabilities todynamically anticipate,interpret,andinfuence the actions of other agents within game-theoretic settings—whether competitive,cooperative,or mixed[949].Agents iterativelyadjust their strategies based on new information,commonly modeled using foundational concepts such as Nash equilibria [950], Bayesian games[951,914,952]orrepeatedinteractions[953,954].With LLMs enabling nuanced linguistic reasoning, strategiclearning increasingly integrates“soft\"signals-including dialogue,persuasion,andimplicit negotiation-thus enriching traditional game-theoretic reasoning frameworks [952, 955, 956, 957].",
          "index": 0,
          "part": 0,
          "translated_content": "战略学习指的是智能代理在博弈理论环境中动态地预测、解释和影响其他代理的行动能力——无论是竞争性的、合作性的还是混合性的[949]。代理根据新信息迭代调整他们的策略，通常使用基本概念，如纳什均衡[950]、贝叶斯博弈[951, 914, 952]或重复互动[953, 954]进行建模。随着LLMs实现了细致的语言推理，战略学习越来越多地整合了“软”信号，包括对话、说服和隐含协商，从而丰富传统的博弈论推理框架[952, 955, 956, 957]。"
        },
        {
          "type": "text",
          "content": "In economic applications, multi-agent strategic simulations provide valuable insights into market behaviors and negotiation tactics,highlighting both competitive andcooperative dynamics.For example,[958] and[951] demonstrate how LLM-empowered agents can simulate hiring processes,exhibit rational decision-making in controlled economic experiments,and even forecast stockmovements.[959]introduces a GPT-4-based competitiveenvironment to illustrate how restaurant and customer agents compete to optimize profits and satisfaction, showcasing realistic bidding and pricing strategies.Meanwhile,[960] investigate Buyer-Seller bargaining in LLM-based negotiations, while[961]use ultimatum game simulations to illuminate policymaking decisions grounded in human-like strategic behavior.",
          "index": 1,
          "part": 0,
          "translated_content": "在经济应用中，多智能体战略模拟为市场行为和谈判策略提供了宝贵的见解，突出了竞争性和合作性动态。例如，[958]和[951]展示了LLM增强的代理如何模拟招聘流程，在受控经济实验中展现理性决策，甚至预测股票走势。[959]引入基于GPT-4的竞争环境，展示餐厅代理和顾客代理如何竞争以优化利润和满意度，展示了现实的竞标和定价策略。与此同时，[960]研究了LLM基础谈判中的买方-卖方讨价还价，而[961]利用最后通牒博弈模拟揭示了以类人战略行为为基础的政策制定决策。"
        },
        {
          "type": "figure",
          "src": "images/5c6e4dc7b40dcadaa908b43a194b0c87cf4dd1f0d03bc8b93ccc5ac7ea254fb3.jpg",
          "alt": "",
          "caption": "Figure 13.1: An overview of three major collaboration types inLLM-based MAS: Modeling & Simulation,Strategic Learning,and Collaborative Task Solving. Each category is distinguished by how agents' goals and norms are set (independent vs. divergent vs. shared) and how they coordinate.",
          "index": 2,
          "part": 0,
          "translated_caption": "图13.1：基于LLM的多代理系统中三种主要协作类型的概述：建模与仿真、战略学习和协作任务解决。每个类别的区别在于代理的目标和规范的设定方式（独立 vs. 分歧 vs. 共享）以及它们如何协调。"
        },
        {
          "type": "text",
          "content": "Beyondconventionalmarkets,strategic learningapplies broadly whereverresource allocation,allancesorcompetitivecooperative trade-offs are present.Examples include multi-commodity competitions [962,959], in which agents stratgicallynegotiateterms tomaximizeindividualbenefits,orsustainability-focusedcontexts where agentscoordinate resource consumption[963].In gaming,social deduction games such as Werewolf, Chameleon, Avalon,and Jubensha require agents tomanagethecomplex interplaybetween deceptionandcollaboration[964,965,966,153,919,967,968, 969,970].Studies by[97,965]highlight LLM-based agents thatexcelatorchestratingsubtledeceit andcollabation, while[967,972,968,969]emphasize adaptive,multi-round strategy in Avalon.[970]further pushes this boundary by showcasing autonomous,multi-agent interactions inthe Jubensha murder mystery genre,re-creatingcomplexnarratives. Similarly,diplomatic simulations ([973]and[974])employ LLM-based agents to emulate sophisticated geopolitical negotiation and alliance formation dynamics at global scales.",
          "index": 3,
          "part": 0,
          "translated_content": "在传统市场之外，战略学习广泛适用于资源分配、联盟或竞争合作权衡存在的各个领域。例如，多商品竞争[962,959]中，代理商战略性地协商条款以最大化个人利益，或者在以可持续发展为重点的情境中，代理商协调资源消耗[963]。在游戏中，社交推理游戏如《狼人杀》、《变色龙》、《阿瓦隆》和《局本射》需要代理商管理欺骗和合作之间复杂的相互作用[964,965,966,153,919,967,968,969,970]。[97,965]的研究强调了基于LLM的代理商在精心策划微妙欺骗和合作方面的出色表现，而[967,972,968,969]强调了《阿瓦隆》中自适应的、多轮策略。[970]进一步推动了这一边界，展示了在《局本射》谋杀悬疑类型中的自主多代理互动，重新创造了复杂的叙事。类似地，外交模拟([973]和[974])利用基于LLM的代理模拟全球范围内复杂的地缘政治谈判和联盟形成动态。"
        },
        {
          "type": "text",
          "content": "SummaryA key advantage of LLM-driven strategic learning lies in effectivelycombining rigorous game-theoretic logic with naturallanguage reasoning.Thisfusion enables agents tointerpret sophisticated instructions,engage in persuasive dialogue,and adapt more flexibly to novel orunstructured settings.Consequently,LLM-based strategic agent hold significant promise for accurately modeling complex real-world interactions- spanning economic competition, social negotiation, and geopolitical strategy-far more efectively than conventionalrule-based or numeric-only approaches.",
          "index": 4,
          "part": 0,
          "translated_content": "总结LLM驱动的战略学习的一个关键优势在于有效地将严谨的博弈论逻辑与自然语言推理相结合。这种融合使代理商能够解释复杂的指令，进行有说服力的对话，并更灵活地适应新颖或非结构化的环境。因此，基于LLM的战略代理商在准确建模复杂的现实世界互动方面具有重要潜力，远比传统基于规则或仅数字化的方法更有效地跨越经济竞争、社会谈判和地缘政治战略领域。"
        }
      ],
      "raw_title": "Strategic Learning: Cooperation vs. Competition",
      "type": null,
      "children": [],
      "translated_title": "13.1 战略学习：合作与竞争"
    },
    {
      "title": "13.2 Modeling Real-World Dynamics",
      "number": "13.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Modeling and simulation represents another crucial area of application for LLM-based multi-agent systems (LLMMAS),aiming toreplicatecomplex social,economic,andpolitical phenomenaat scale.By utilizingLLMs'sophisticated language understanding and contextualreasoning,these simulations can feature highly heterogeneous agents whose evolving behaviors mirror real-world dynamism.Unlike strategic learning environments that emphasize explicit competitive orcooperative goals,agents in modeling and simulation scenarios operate independently,guided by their domain-specific roles, preferences, and interactions with the simulated environment [975].",
          "index": 0,
          "part": 0,
          "translated_content": "基于LLM的多智能体系统（LLMMAS）的建模和仿真代表另一个关键的应用领域，旨在在规模上复制复杂的社会、经济和政治现象。通过利用LLMs的复杂语言理解和情境推理，这些仿真可以呈现高度异质的智能体，其不断演化的行为反映了现实世界的动态性。与强调明确竞争或合作目标的战略学习环境不同，在建模和仿真场景中，智能体独立运作，受其领域特定角色、偏好和与模拟环境的交互指导。"
        },
        {
          "type": "text",
          "content": "In healthcare,for example,[921] introduces Agent Hospital, where LLM-powered doctor agents iteratively refine treatmentstrategies through realistic interactions with virtual patients.This enables researchers totest management protocols,raining paradigms, and“what-if\"scenarios in acontrolled yetrealistic setting.Similarly, in economic contexts,[976]present EconAgents,leveraging LLM-driven agents torealisticall modelindividual-levelbehaviors such as employment decisions,consumption patterns,and savings strategies.These agents facilitateexpressive macroeconomic simulations,surpassing traditional numeric or strictlyrule-based methods inadaptability and realism[977]. In addition,politicalscience applications alsobenefitfrom this approach.Forexample,[978]and[977]successfully simulate election proceses andpolicymaking dynamics,revealing how publicdiscourse,candidate strategies, and voter interactions shape real-world political outcomes.",
          "index": 1,
          "part": 0,
          "translated_content": "在医疗保健领域，例如，引入了Agent Hospital，其中由LLM驱动的医生智能体通过与虚拟患者的真实互动反复完善治疗策略。这使研究人员能够在受控而又逼真的环境中测试管理协议、培训范式和“假设”场景。类似地，在经济背景下，提出了EconAgents，利用LLM驱动的智能体逼真地模拟个体层面的行为，如就业决策、消费模式和储蓄策略。这些智能体促进了富有表现力的宏观经济模拟，超越了传统的数值或严格基于规则的方法在适应性和逼真性方面的表现。此外，政治科学应用也受益于这种方法。例如，成功地模拟了选举过程和政策制定动态，揭示了公共话语、候选人策略和选民互动如何塑造现实世界的政治结果。"
        },
        {
          "type": "text",
          "content": "Beyondeconomics and politics,LLM-based simulation accommodates a variety of social and cultural phenomena.For example,[979]and[255] use simulations of linguistic and emotional propagation in social networks to investigatehow opinions,beliefs,or sentiment clusters form online.Researchby[980] explores how opinion dynamics evolve under various topological and interaction paterns,while[981] examines the conditions under which fake news spreads or stalls in heterogeneous agent populations.Large-scale simulation platforms such as GenSim[982]and OASIS [936] pushthe boundary furtherby scaling totens of thousands orevenmillons ofuser agents,thus enabling the study of emergent group behaviors and systemiceffcts-suchasviralinformationdiffusion,echo-chamber formation,or group polarization—under realistic constraints.",
          "index": 2,
          "part": 0,
          "translated_content": "除了经济学和政治学，基于LLM的模拟还涵盖了各种社会和文化现象。例如，[979]和[255]使用社交网络中语言和情绪传播的模拟来研究在线观点、信念或情绪集群是如何形成的。[980]的研究探讨了在不同拓扑结构和交互模式下观点动态是如何演变的，而[981]则研究了假新闻在异质智能体群体中传播或停滞的条件。诸如GenSim[982]和OASIS [936]等大规模模拟平台进一步拓展了边界，将规模扩展到数万甚至数百万用户智能体，从而使得能够研究新兴群体行为和系统效应，如病毒式信息传播、同质化信息茧房形成或群体极化等，都在现实约束条件下得以实现。"
        },
        {
          "type": "text",
          "content": "Summary The strength of LLM-based simulation lies in capturing both the structural dynamics(e.g.,network topology or institutionalrules and the cognitive orlinguistic nuances thatdrive real-world behavior.Byembedding languagebased reasoning into agent models,researchers can examine complexsocial processes—like persuasion,framing,or cultural transmission-that would be difficult to capture through purely numeric or rule-based approaches.",
          "index": 3,
          "part": 0,
          "translated_content": "总结：LLM模拟的优势在于捕捉结构动态（例如，网络拓扑或制度规则）和推动现实行为的认知或语言细微差别。通过将基于语言的推理嵌入到代理模型中，研究人员可以研究复杂的社会过程，如说服、框架构建或文化传播，这些过程通过纯粹的数字或基于规则的方法很难捕捉。"
        }
      ],
      "raw_title": "Modeling Real-World Dynamics",
      "type": null,
      "children": [],
      "translated_title": "13.2 建模现实世界动态"
    },
    {
      "title": "13.3 Collaborative Task Solving with Workflow Generation",
      "number": "13.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Collaborativetask solving orchestrates multiple agents toward aclearlydefinedobjectivethroughstructured workflows. In contrast to strategic learning (which may involve competing interests)or open-ended modeling and simulation (where agents actindependently),collaborative agentsfunction aspart ofaunified problem-solving pipeline.Agents typically folow clearly defined roles (e.g.“Planner\",Implementer\",or“Evaluator\")and stage-based processes to ensure efficient and accurate task completion.\n\nSystems such as MetaGPT[626],CAMEL[848],Communicative Agents [983],and frameworks described in [924] exemplify how clearly defined roles,responsibilities, and decision flows allow LLM-based agents to coordinate effctively.A typical workflow might involve one agent analyzing a problem statement, another proposing a solution outline,athird implementing partial solutions, andafourth verifying correctnessCommunication among these agents is oftencarriedoutthroughiterativeroundsof naturallanguage“dialogue\",leveragingthe inherentlanguage-generation strengthsofLLMs.Thisstructuredapproachalso proves beneficialforscaling tomore ambitious projects, as sub-tasks can be delegated to specialized agents with domain-specific prompts or training.",
          "index": 0,
          "part": 0,
          "translated_content": "协作任务解决通过结构化工作流程协调多个代理朝着明确定义的目标前进。与涉及竞争利益的战略学习（可能涉及）或独立行动的开放建模和仿真相比，协作代理作为统一的问题解决流程的一部分。代理通常遵循明确定义的角色（如“规划者”、“执行者”或“评估者”）和阶段性流程，以确保任务的高效和准确完成。\n\nMetaGPT[626]、CAMEL[848]、交际代理[983]以及[924]中描述的框架等系统展示了明确定义的角色、责任和决策流程如何使基于LLM的代理能够有效协调。典型的工作流可能涉及一个代理分析问题陈述，另一个提出解决方案概要，第三个实施部分解决方案，第四个验证正确性。这些代理之间的沟通通常通过自然语言“对话”的迭代轮次进行，利用LLM固有的语言生成能力。这种结构化方法对于扩展到更雄心勃勃的项目也是有益的，因为子任务可以委托给具有特定领域提示或训练的专门代理。"
        },
        {
          "type": "text",
          "content": "Recently,collaborativetask-solving systems have been explored extensively insoftware developmentscenarios (e.g., multi-agent coding,debugging,and testing).However,scientific discovery represents a particularly prominent and compelling aplication.For example,the Agent Laboratory[746]employs agents in structured scientific workflows: proposing hypotheses,designing experiments,analyzing results,andrefining subsequent inquiries,whicheffectively mirrors the iterative nature of thescientific investigation.Similar multi-agent designscan be adapted to tasks such as literaturereview,policydrafting,orlarge-scaledataanalysis, using well-definedprotocols tomaintaincoherence and avoid duplication of effort.",
          "index": 1,
          "part": 0,
          "translated_content": "最近，在软件开发场景中广泛探讨了协作任务解决系统（例如，多代理编码、调试和测试）。然而，科学发现代表了一个特别突出和引人注目的应用。例如，Agent Laboratory利用代理在结构化科学工作流程中：提出假设、设计实验、分析结果和完善后续探究，有效地反映了科学探究的迭代性质。类似的多代理设计可以应用于文献综述、政策起草或大规模数据分析等任务，利用明确定义的协议来保持连贯性，避免工作重复。"
        },
        {
          "type": "text",
          "content": "Summary Compared to other LLM-based multi-agent paradigms,collaborative task-solving inherently prioritizes clarity and predictability:Each agent's role andobjective are predefined, limiting emergent orchaotic behaviors.This structureis particularlyadvantageous in domains requiring precision,accountability,or sequential decision-making. Atthe same time,research isongoing tostriketherightbalancebetween structure and flexibilitywhichensures that agents have enoughautonomy tocreatively contribute solutions while adhering to asharedworkflow thatultimately guarantees reliable, high-quality task completion.",
          "index": 2,
          "part": 0,
          "translated_content": "相较于其他基于LLM的多代理范式，协作任务解决固有地优先考虑清晰性和可预测性：每个代理的角色和目标都是预先定义的，限制了新兴或混乱行为。这种结构在需要精确性、责任制或顺序决策的领域特别有优势。同时，正在进行研究以在结构和灵活性之间取得平衡，确保代理有足够的自主权创造性地提供解决方案，同时遵循共享工作流程，最终确保可靠、高质量的任务完成。"
        },
        {
          "type": "text",
          "content": "Discussion The aforementioned three dimensions—strategic learning, modeling and simulation,and collaborative task solving—reflect thebreadth of LLM-based multi-agent systems.Eachcategory addresses distinct research questions and real-world applications,leveraging language-basedreasoning totacklechallenges that extendbeyond thecapabilitiesof conventional, purely numeric, or rule-driven agent designs.",
          "index": 3,
          "part": 0,
          "translated_content": "讨论：前述三个维度——战略学习、建模与仿真以及协作任务解决——展示了基于LLM的多代理系统的广度。每个类别都涉及不同的研究问题和现实应用，利用基于语言的推理来解决超越传统、纯数值或规则驱动代理设计能力范围之外的挑战。"
        }
      ],
      "raw_title": "Collaborative Task Solving with Workflow Generation",
      "type": null,
      "children": [],
      "translated_title": "13.3 协作任务解决与工作流生成"
    },
    {
      "title": "13.4 Composing AI Agent Teams",
      "number": "13.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "In MAS,agentsarethecoreunits that interact within the systemandarecriticaltoitsfunctionality.Theseagentscanbe categorized as either homogeneous orheterogeneous,depending onwhether they share identicalor differing personas, capabilities, and action spaces.\n\nHomogeneous Homogeneous agents that share identicalcapabilities,action spaces,and observation spaces.Compared to single-agentsystems,the primaryadvantagelies intask parallelization,allowingmultipleagents tohandledifferent parts of atasksimultaneously and improve overalleffciency.They are often used insimpler,coordinated tasks where uniformity across agents can drive improved performance.\n\nSeveral studies have applied homogeneous agents to simulate teamwork in games ike Overcooked and Minecraft, as well as real-worldtasks such as householdlabor division.[924]proposed acognitive-inspired modularframework that enables LLM-based agents tocommunicatethrough natural language to perform labor division, request asistance from one another, and collaborativelycompleteobject transportation tasks.[984]introduced prompt-based organizational structures intotheframework,reducingcommunicationcosts betweenagents andimproving teameficiency inhousehold task suchas preparing afternoon tea, washing dishes,and preparing a meal.Furthermore,several studies[926,925] have employed multiple LLM-based agents in popular games such as Overcooked and Minecraft to experiment with their ability tocooperate and complete tasks.According tothe game settings,these agents are also homogeneous.",
          "index": 0,
          "part": 0,
          "translated_content": "在多智能体系统中，智能体是系统内相互交互的核心单元，对系统的功能至关重要。这些智能体可以根据它们是否共享相同的个性、能力和行动空间而被分类为同质或异质。\n\n同质智能体共享相同的能力、行动空间和观测空间。与单一智能体系统相比，其主要优势在于任务并行化，允许多个智能体同时处理任务的不同部分，提高整体效率。它们通常用于较为简单、协调的任务，智能体之间的统一性可以推动性能的提升。\n\n一些研究已经将同质智能体应用于模拟游戏中的团队合作，如《Overcooked》和《Minecraft》，以及现实世界任务，比如家庭劳动分工。[924]提出了一个受认知启发的模块化框架，使基于LLM的智能体能够通过自然语言进行通信，执行劳动分工，相互请求帮助，并协作完成物体运输任务。[984]将基于提示的组织结构引入框架中，减少了智能体之间的沟通成本，在家庭任务（如准备下午茶、洗碗和准备餐点）中提高了团队效率。此外，一些研究[926,925]在流行游戏中（如《Overcooked》和《Minecraft》）使用多个基于LLM的智能体，探索它们合作完成任务的能力。根据游戏设置，这些智能体也是同质的。"
        },
        {
          "type": "text",
          "content": "Heterogeneous Agent diversity plays acrucialrole in improving collaboration outcomes.Researchshows that heterogeneity among agentscanenhanceproblem-solvingcapabilities,asdiverseagents bringvariedperspectivesand skil to the task athand[985986].Heterogeneitycontributes toricher problem-solvingstrategies andimprovesoverallcollaboration in MAS.Theheterogeneouscharacteristicsof agentscanbereflected inthefollowingdimensions: personas-level heterogeneityobservation-space heterogeneityandaction-space heterogeneityNotethatthese heterogeneitieaenot mutually exclusive—a heterogeneous agent may exhibit one or more of these characteristics.",
          "index": 1,
          "part": 0,
          "translated_content": "异质智能体的多样性在改善协作结果中起着至关重要的作用。研究表明，智能体之间的异质性可以增强问题解决能力，因为不同的智能体为手头的任务带来了不同的观点和技能。异质性有助于丰富问题解决策略，提高多智能体系统中的整体协作。智能体的异质特征可以在以下维度中体现：个性层面的异质性、观测空间的异质性和行动空间的异质性。需要注意的是，这些异质性并不是相互排斥的——一个异质智能体可能表现出其中一个或多个特征。"
        },
        {
          "type": "text",
          "content": "·Personas-level heterogeneity.Refers to diversity in agent profiles, which influences how agents approach problem-solving and interact with one another.Most current LLM-based heterogeneous multi-agent systems fall into this category [987, 627, 50, 970].For example,in sofware development, agents may take on personas such as programmers, product managers,or testers. In medical diagnostics,agents may represent cardiologists,oncologists,or paediatricians,each with distinct areas of expertise.The distinct perspectives and expertise of each persona contribute to more robust decision-making.While these heterogeneous agents may share the same action space—such as writing documents [626] (e.g., code, requirement reports, or test reports)or providing diagnostic advice [922]—their personas influence the outcomes of these actions, where role-specific enhancements within multi-agent architectures have shown to significantly streamline and optimize task execution. For instance, a product manager performing the action of writing a document would produce a requirements report, whereas a programmer performing the same action would produce software implementation code[626].This diversity leads to beter decision-making and innovation,especially in complex, multidisciplinary tasks.",
          "index": 2,
          "part": 0,
          "translated_content": "·个性层面的异质性。指代智能体配置文件的多样性，影响智能体如何解决问题并相互交互。目前大多数基于LLM的异质多智能体系统属于这一类别[987, 627, 50, 970]。例如，在软件开发中，智能体可能扮演程序员、产品经理或测试人员等角色。在医学诊断中，智能体可能代表心脏病专家、肿瘤学家或儿科医生，每个人都有不同的专业领域。每个角色的独特观点和专业知识有助于更加健壮的决策制定。尽管这些异质智能体可能共享相同的行动空间——比如撰写文件[626]（例如代码、需求报告或测试报告）或提供诊断建议[922]——但他们的人设影响了这些行动的结果，多智能体架构中的角色特定增强已被证明可以显著简化和优化任务执行。例如，执行撰写文档动作的产品经理会生成需求报告，而执行相同动作的程序员会生成软件实现代码[626]。这种多样性有助于更好的决策制定和创新，特别是在复杂的跨学科任务中。"
        },
        {
          "type": "text",
          "content": "·Observation-space heterogeneity. In MAS, the ability of agents to perceive and interpret their environment can vary.Observation-space heterogeneity refers to these diferences in what agents can observe or perceive within their environment. For example, in the game Werewolf, some agents, like werewolves,can see the identities of their teammates,and the seer can obtain the identity of a designated player, while others, like villagers,cannot seethe true identityof any player[971]. Similarly,in the Avalon game, different roles have distinct observation spaces [919,972],thus influencing the strategies and communications of the players. In these settings,each agent's perceptual ability or observation space is directly linked to their role in the system. In a multi-agent system, this variation in what agents can observe often influences their decision-making, communication, and coordination with other agents.",
          "index": 3,
          "part": 0,
          "translated_content": "·观测空间的异质性。在多智能体系统中，智能体感知和解释环境的能力可能存在差异。观测空间的异质性指的是智能体在其环境中能够观察或感知的差异。例如，在狼人游戏中，一些智能体（如狼人）可以看到队友的身份，而预言家可以获取指定玩家的身份，而其他一些智能体（如村民）则无法看到任何玩家的真实身份。类似地，在阿瓦隆游戏中，不同的角色具有不同的观测空间，从而影响玩家的策略和沟通。在这些情境中，每个智能体的感知能力或观测空间与其在系统中的角色直接相关。在多智能体系统中，智能体能够观察到的差异通常会影响它们的决策、沟通和与其他智能体的协调。"
        },
        {
          "type": "text",
          "content": "·Action-space heterogeneity.On the other hand,this refers tofundamental differences inthe actions agents can perform due to physical or functional constraints.This is particularly relevant in both virtual and physical environments where agents may have diffrent capabilities based on their design or purpose.In the virtual environments of games like Werewolf [965,971,966] and Avalon [919,967], diffrent roles have distinct abilities or skills [971, 919,972]. For example, in Werewolf, while werewolves may have the ability to communicate secretly with each other, villagers might be limited to voting or observing only. This dynamic requires agents to collaborate based on their uniquecapabilities and promotes the learning of strategies such as teamwork, trust, and deception in their interactions.Meanwhile,in robotics,agents may exhibit diverse physical capabilities.For instance, as described in [988],some robots lack mobility and can only manipulate objects,while others are specialized for movement butcannot manipulate objects. In such cases,agents with different action spaces must divide tasks effectively,leveraging their specific abilities to take on the parts of the task they are suited for, ultimately collaborating to complete the overalltask.This type of heterogeneity requires agents to collaborate and coordinate their actions eficiently, often dividing tasks based on their individual strengths.",
          "index": 4,
          "part": 0,
          "translated_content": "·行动空间的异质性。另一方面，这指的是由于物理或功能约束而导致的智能体可以执行的行动的根本差异。这在虚拟环境和实际环境中尤为重要，因为智能体可能根据其设计或目的而具有不同的能力。在类似《狼人》和《阿瓦隆》等游戏的虚拟环境中，不同的角色具有独特的能力或技能。例如，在《狼人》中，狼人可能有秘密交流的能力，而村民可能仅限于投票或观察。这种动态要求智能体根据其独特的能力进行合作，并促进学习策略，如团队合作、信任和欺骗。同时，在机器人领域，智能体可能表现出多样化的物理能力。例如，正如[988]中所描述的，一些机器人缺乏机动性，只能操纵物体，而另一些专门用于移动，但无法操纵物体。在这种情况下，具有不同行动空间的智能体必须有效地分配任务，利用其特定能力来承担适合自己的任务部分，最终合作完成整体任务。这种异质性要求智能体有效地协作和协调它们的行动，通常根据它们各自的优势来分配任务。"
        },
        {
          "type": "text",
          "content": "Homogeneity to Heterogeneous Evolution In some LLM-based multi-agent systems, agents have the ability to evolve autonomously and continuously adapt through interactions with their environment.Due to the inherent randomness in both LLM models and theenvironment, the evolution ofthese agentsoften followsdifferent trajectories.This can lead to heterogeneous behaviors emerging over multiple simulations,even when agents initialy havehomogeneous personas and action spaces.Forexample,as shown in[989],agents with identicalaction spaces and personas atthe start developed diffrentiatedroles after multiple rounds of interactions withthe environment andother agents.Some agents,forinstance,specializedinfood gathering,whileothers focused oncrafting weapons.Similarly,[990]bserved that initiallyhomogeneousagents developed distinctlanguage usage patterns,emotionalexpressions,and personalities after group interactions.These emergent behaviors demonstrate the possibility of transitions from homogeneous to heterogeneous systems.",
          "index": 5,
          "part": 0,
          "translated_content": "从同质到异质的演变在一些基于LLM的多智能体系统中，智能体具有通过与环境互动自主演化和持续适应的能力。由于LLM模型和环境中固有的随机性，这些智能体的演化通常会遵循不同的轨迹。即使智能体最初具有同质的人设和行动空间，也可能在多次模拟中出现异质行为。例如，正如[989]所示，起初具有相同行动空间和人设的智能体在与环境和其他智能体多轮互动后发展出不同的角色。一些智能体专注于食物采集，而其他人则专注于制造武器。类似地，[990]观察到最初同质的智能体在群体互动后发展出不同的语言使用模式、情感表达和个性。这些新兴行为展示了从同质到异质系统的转变的可能性。"
        }
      ],
      "raw_title": "Composing AI Agent Teams",
      "type": null,
      "children": [],
      "translated_title": "13.4 组建人工智能代理团队"
    },
    {
      "title": "13.5 Agent Interaction Protocols",
      "number": "13.5",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "In this section,there willinitially be classificationof typicalkindsof messages,providing aclear view regarding the contnt and exchange modes for agent interactions.Next, agent-environment, agent-agent, and agent-human communications interface designs will beaddressed.Architecturalissues and protocol specifications fortransparent informationexchange willalsobe addressed.Interface standardizationwillhavea specialfocus,which isessentialfor providing interoperability,scalability, andeffciencyfor multi-agent systems.The section willend with unification of communication protocol discussions, where agent-environment or agent-user interacting design principles and requirements are addressed, as wellas providingclarityconsistency,andfunctionalcoherence forariousapplications for LLM-based systems.",
          "index": 0,
          "part": 0,
          "translated_content": "在本节中，首先将对典型消息类型进行分类，提供有关智能体相互作用的内容和交换模式的清晰视图。接下来，将讨论智能体与环境、智能体与智能体以及智能体与人类之间的通信界面设计。将讨论透明信息交换的架构问题和协议规范。接口标准化将受到特别关注，这对于为多智能体系统提供互操作性、可扩展性和效率至关重要。该部分将以统一通信协议讨论结束，其中将讨论智能体与环境或智能体用户交互设计原则和要求，同时为基于LLM系统的各种应用提供清晰、一致和功能上的连贯性。"
        }
      ],
      "raw_title": "Agent Interaction Protocols",
      "type": null,
      "children": [
        {
          "title": "13.5.1 Message Types",
          "number": "13.5.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Structured: Structured messages,either in JSON ([991,992]),XML([993,636]),or as acode ([626,627,994]),are a crucial aspect of multi-agent system communication with LLM.The primary advantages of structured messages aretheir syntactically and semanticaly defined structure,enabling unambiguous understanding and straightforward parsing.Withtheir lackof ambiguity,theyfacilitateunerrant information extractionand processing with much less overhead on computation and greater system dependability.For example,JSON and XML can represent specific-task configuration parameters or facilitate data exchange asa machine-readable mode,and messages writen asacode can even be executable several times directly, which makes workflow and automation simpler.",
              "index": 0,
              "part": 0,
              "translated_content": "结构化消息，无论是JSON（[991, 992]），XML（[993, 636]），还是作为代码（[626, 627, 994]），是与LLM进行多代理系统通信的关键方面。结构化消息的主要优势在于它们具有语法和语义上定义的结构，使得理解明确且解析直观。由于它们缺乏歧义，它们促进了信息提取和处理的准确性，计算开销更小，系统可靠性更高。例如，JSON和XML可以表示特定任务的配置参数或以机器可读模式促进数据交换，而作为代码编写的消息甚至可以直接多次执行，使工作流程和自动化更简单。"
            },
            {
              "type": "text",
              "content": "Structured messages are particularly well-suited for high-efficiency,deterministic applications.They are useful for sub-task decomposition,sub-task assignment,andcoordination among agents forcooperativemulti-agent architecture because they explicitly stateoperationalcommands.Moreover,as structured messages have a prescribed form,retrieving data as wellas storing data is facilitated and system optimization and longitudinal analysis are also feasible.\n\nUnstructured: Incontrast,unstructured mesages,e.g.naturaltext([979719]),isual data,e.g.images,vides, and audio signals,e.gspeech,ambient sounds([995,996,762]),havehigher informationdensity andrepresentational capability.Such modalities are best suited for communication with nuanced andcontext-dependent information. Images, for instance,communicate spatialrelationships,illumination,andfacialexpressons,andvideoscommunicate dynamic temporally-organized sequences,e.g.,state orbehaviorchanges overtime.Similarly,audio signalsalsocommunicate not justlinguistic informatinbutalsoparalinguisticinformation,e.g.tone,emotion,andintonation,whicharecritical for natural and context-aware interactions.",
              "index": 1,
              "part": 0,
              "translated_content": "结构化消息特别适用于高效、确定性的应用程序。它们对于子任务分解、子任务分配以及协作多代理架构中的代理之间协调非常有用，因为它们明确陈述操作命令。此外，由于结构化消息具有规定的形式，检索数据以及存储数据变得更加便利，系统优化和纵向分析也变得可行。\n\n与之相反，非结构化消息，例如自然文本、视觉数据（如图像、视频）和音频信号（如语音、环境声音），具有更高的信息密度和表现能力。这些模态最适合传达细微且与上下文相关的信息。例如，图像可以传达空间关系、光照和面部表情，而视频可以传达动态的时间组织序列，例如随时间变化的状态或行为。同样，音频信号不仅传达语言信息，还传达语言外信息，例如语调、情感和语调，这对于自然且上下文感知的互动至关重要。"
            },
            {
              "type": "text",
              "content": "Unstructured messages are well-adapted forambiguitytasks,as wellas forcomplex,real-world settings.The fact that they can expressabstract ideas as wellasaffective subtlety,orimplicitcontextual suggestions, makesunstructured messages wellsuited for creative, as wellas discovery-oriented, problem spaces.Unstructured data'scomplexity, however,callsforadvanced processing techniques,for example,feature extraction based ondeep learning,forone to tap intotheir full potential.Advances with pre-trained LLMs as wellas multi-modallarge language models have alleviated these complexities to alarge extent, enabling novelapplications for unstructured communication within multi-agent systems [533, 513,997].",
              "index": 2,
              "part": 0,
              "translated_content": "非结构化消息非常适用于模糊任务，以及复杂的真实世界环境。它们能够表达抽象思想，以及情感微妙或隐含的上下文建议，使非结构化消息非常适合创造性和以发现为导向的问题空间。然而，非结构化数据的复杂性需要高级处理技术，例如基于深度学习的特征提取，才能充分发挥其潜力。预训练的LLM（大型语言模型）以及多模态大型语言模型的进展在很大程度上减轻了这些复杂性，使得在多代理系统中实现非结构化通信的新应用成为可能。"
            },
            {
              "type": "text",
              "content": "Summary: Unstructured and structured messages have complementary roles for multi-agent communication with LLM-based.Whilestructured messages oferaccuracy,consistency, andcomputationeffciencyand are appropriate for operational and deterministic operations,unstructured messages offer rich,contextualized representations enabling agents to negotiate vague,creative,highly dynamic situations.Together,these modesoffer afoundation for adaptive, effective multi-agent cooperation.",
              "index": 3,
              "part": 0,
              "translated_content": "摘要：基于LLM的多代理通信中，非结构化消息和结构化消息具有互补的作用。结构化消息提供准确性、一致性和计算效率，适用于操作性和确定性操作；而非结构化消息提供丰富的、具有上下文的表示，使代理能够处理模糊、创造性和高度动态的情境。这两种模式共同为自适应、有效的多代理合作奠定了基础。"
            }
          ],
          "raw_title": "Message Types",
          "type": null,
          "children": [],
          "translated_title": "13.5.1 消息类型"
        },
        {
          "title": "13.5.2 Communication Interface",
          "number": "13.5.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Agent-EnvironmentInterface LLM-based agents willtypicallyhave to act on their environment once or severaltimes in order toperformarange ofoperations.Fromthe agent's point of view,itsoutput intotheenvironment is something that it would prefer, e.g.,a UIclick, webrequest,ora move foracomputer graphic'scharacter.Environments differ withregard to whatactions they willaccept,andsoasnothave its actions not getexecuted,the agent must findout what actions are foraspecific environment that it is acting within and performactions that arefora specific task as wellas validfora specificenvironment.After the agent outputs itschosen action,the agent willhave areturn from the environment.It willconsistofobservations ifsuccessful,orafeedbackonerrorif therewasone.Theagent will have to actonthis feedback.There are nowadays various typesofenvironments where anagentcan act,e.g.perating systems,computer games,database, ande-commerce websites.To make agent-environment interfaces share acommon interface and have agents trained on various LLMs plug into various environments with minimal further adaption, various frameworks have been proposed. These frameworks make for easier tests on agents'capability on various executable environments [706].",
              "index": 0,
              "part": 0,
              "translated_content": "基于LLM的智能代理通常需要在环境中执行一次或多次动作，以执行一系列操作。从代理的角度来看，其输出到环境中的是它所偏好的内容，例如UI点击、Web请求或计算机图形角色的移动。不同环境会因其接受的动作而异，因此为了确保其动作能够执行，代理必须了解在其所处的特定环境中哪些动作是有效的，并执行适用于特定任务和特定环境的动作。代理输出其选择的动作后，将从环境中获得反馈。如果成功，将包括观察结果；如果出现错误，则会得到错误反馈。代理需要根据这些反馈采取行动。如今有各种类型的环境，代理可以在其中执行动作，例如操作系统、电脑游戏、数据库和电子商务网站。为了使代理-环境接口具有共同的接口，并使经过各种LLM训练的代理能够在各种环境中进行最小程度的进一步适应，提出了各种框架。这些框架使得更容易测试代理在各种可执行环境中的能力。"
            },
            {
              "type": "text",
              "content": "Agent-Agent Communication In MAS, communication through natural language is predominant.This is likely because large language models possess strong linguistic capabilitiesdue to pretraining on massive naturallanguage corpora.Another possiblereason isthatfor many tasks, naturallanguagecommunication isalready suffcientto meet the requirements.Based on the type of information exchanged, multi-agent systems can be categorized as follows: Natural Language-Based Systems Among LLM-based multi-agent systems utilizing natural language, text-based communication is the most common[922,924,987,970,998].There are also some systems that use voice as the mediumofcommunication[996,762,999,100].In these systems, agents engage in behaviors such as discussions, negotiations,persuasion,or critique through naturallanguage to achieve theirobjectives.Structured InformationBased Systems Compared to naturallanguage,structured information has characteristics such as higher consistency, lower parsing complexity,and reduced ambiguity, making it more suitable for efficient and low-costcommunication between agents [626]. In some implementations,the information exchanged between agents is structured into distinct components tofacilitate easier parsing and utilization bythereceiving agent.For instance,theexchanged information might include fields specifying the sender,receiver, message type, and instructions on how the recipient should parse or use the content [929].",
              "index": 1,
              "part": 0,
              "translated_content": "在多Agent系统中，通过自然语言进行通信是主导方式。这可能是因为大型语言模型具有强大的语言能力，这是由于在大规模自然语言语料库上进行预训练。另一个可能的原因是，对于许多任务来说，自然语言通信已经足以满足需求。根据交换的信息类型，多Agent系统可以被分类如下：基于自然语言的系统在利用自然语言的LLM多Agent系统中，基于文本的通信是最常见的。还有一些系统使用语音作为通信媒介。在这些系统中，代理通过自然语言进行行为，如讨论、谈判、说服或批评，以实现它们的目标。基于结构化信息的系统与自然语言相比，结构化信息具有更高的一致性、更低的解析复杂度和减少的歧义性，使其更适合于代理之间的高效、低成本通信。在一些实现中，代理之间交换的信息被结构化为不同的组件，以便接收代理更容易解析和利用。例如，交换的信息可能包括指定发送者、接收者、消息类型以及接收方应如何解析或使用内容的字段。"
            },
            {
              "type": "text",
              "content": "Human-Agent Communication The purpose of developing multi-agent systems is to expand the boundaries of human capabilities andcognition,ultimately serving human wellbeing.While in some social simulation multi-agent systems, humans primarilyexist asobservers [50,0o1], most multi-agent systems allow human participation in various forms. During this participation,humans need tocommunicate with agents,andthiscommunicationcantake the formof either natural language or structured information[924,930]. When human-to-agent communication primarilyrelies on natural language,asingle LLMoften acts asa hub to parse human naturallanguage into structuredinformationthat agents can process more efectively for subsequent operations.This hub LMcan either exist within the multi-agent system or function independently of it.To save time and enhance communication effciency,humans can also use structured information to communicate with the multi-agent system through programming or similar methods.By following predefinedcommunication protocols,humans can send messages containing therequired datato the multi-agent system. The system willthen process the messages and data according to its internallogic and return the results. [931]",
              "index": 2,
              "part": 0,
              "translated_content": "人-代理通信\n开发多代理系统的目的是拓展人类能力和认知的边界，最终为人类福祉提供服务。在一些社会仿真多代理系统中，人类主要作为观察者存在，但大多数多代理系统允许人类以各种形式参与。在这种参与过程中，人类需要与代理进行交流，这种交流可以采用自然语言或结构化信息的形式。当人类与代理之间的交流主要依赖自然语言时，一个单一的LLM通常充当中心枢纽，将人类的自然语言解析为结构化信息，以便代理可以更有效地进行后续操作。这个中心LM可以存在于多代理系统内部，也可以独立于系统运作。为了节省时间并提高通信效率，人类还可以使用结构化信息通过编程或类似方法与多代理系统进行通信。通过遵循预定义的通信协议，人类可以向多代理系统发送包含所需数据的消息。系统将根据内部逻辑处理消息和数据，并返回结果。"
            }
          ],
          "raw_title": "Communication Interface",
          "type": null,
          "children": [],
          "translated_title": "13.5.2 通信接口"
        },
        {
          "title": "13.5.3 Next-Generation Communication Protocols",
          "number": "13.5.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "The fieldof LLM-basedagents is stillin its infancy.Developers typically design agent architectures andcommunication mechanisms tailored to specific domains ortasks,including agent-to-environment, agent-to-human,and inter-agent interactions.However, most existing systems lack a unifiedcommunicationframework,resulting in fragmented,siloed ecosystems.Multi-agent systems,tools,environments,anddatasourcesoftenoperateindependently,makingit difiult for agents tointeroperate orsharecapabilities.Furthermore,the burden oflearning andimplementing bespoke protocols falls on humans,and almost allcurrent protocols are manually designed—alabor-intensive process that often lacks semantic flexibility or scalability.",
              "index": 0,
              "part": 0,
              "translated_content": "基于LLM的智能代理领域仍处于起步阶段。开发者通常设计针对特定领域或任务的代理架构和通信机制，包括代理与环境、代理与人类以及代理之间的交互。然而，大多数现有系统缺乏统一的通信框架，导致碎片化、孤立的生态系统。多智能体系统、工具、环境和数据源通常独立运作，使得代理难以互操作或共享能力。此外，学习和实施定制协议的负担落在人类身上，几乎所有当前的协议都是手工设计的——这是一个劳动密集型过程，通常缺乏语义灵活性或可扩展性。"
            },
            {
              "type": "text",
              "content": "To address these issues,several new agent communication protocols have been proposed, each targeting differeni aspects of the protocol design stack.\n\nInternet of Agents(IoA)[933]introducesan internet-inspired,instant-messaging-likecommunication architecture that supports dynamic team formation and task-driven collaboration. Agents register with a central coordination server, which handles identity management and discovery.Communication fows are orchestrated using FSM (Finite State Machine)-based dialogue templates. IoA supports multiple message types, including discussion, task assignment, andtriggering mechanisms, and provides structured fields for controlling speaker turns,nested groupformation, and maximum dialogue length. This alows agents to select and adapt message formats to match specific coordination phases, offering flexibility within a fixed schema.",
              "index": 1,
              "part": 0,
              "translated_content": "为了解决这些问题，已经提出了几种新的智能体通信协议，每种协议针对协议设计堆栈的不同方面。\n\n智能体互联网（IoA）引入了一种受互联网启发的即时通讯架构，支持动态团队形成和任务驱动的协作。代理注册到一个负责身份管理和发现的中央协调服务器。通信流程使用基于FSM（有限状态机）的对话模板进行编排。IoA支持多种消息类型，包括讨论、任务分配和触发机制，并为控制发言者轮次、嵌套组形成和最大对话长度提供了结构化字段。这使代理能够选择和调整消息格式以匹配特定的协调阶段，在固定模式内提供灵活性。"
            },
            {
              "type": "text",
              "content": "Model Context Protocol (MCP)[931],developed by Anthropic,focuses on enabling LLM agents to accessstructured tools and data.Itadopts afullycentralized approach basedon OAuth identityauthentication,and interactions are constraied to JSON-RPC 2.0 messages.While itlacks a meta-protocollayer or semantic negotiation capabilities,its simple andrigid architecture makes it a practical choice for tooluse cases with well-defined APIs.However, MCP sacrifices flexibility and extensibility, requiring manual registration of supported functions.",
              "index": 2,
              "part": 0,
              "translated_content": "由Anthropic开发的模型上下文协议（Model Context Protocol，MCP）[931]专注于使LLM代理能够访问结构化工具和数据。它采用基于OAuth身份验证的完全集中化方法，交互受限于JSON-RPC 2.0消息。虽然它缺乏元协议层或语义协商能力，但其简单而严格的架构使其成为具有明确定义API的工具用例的实际选择。然而，MCP牺牲了灵活性和可扩展性，需要手动注册支持的函数。"
            },
            {
              "type": "text",
              "content": "Agent Network Protocol (ANP)[1002] aims to achieve fulldecentralization.Agents identify themselves through W3C-compliant decentralized identifiers (DIDs)and communicate over encrypted pee-to-peer channels.The protocol includes a meta-protocollayer that enables agents to negotiate which application-level protocol toadopt,supporting semantic protocol selection based onagent capabilities.ANPalsoallowsfor multi-protocol support atthe application layer (e.g.,HTTP,JSON-RPC, naturallanguage),providing strong extensibility anddecentralization but does notet explicitly support public protocol reuse.",
              "index": 3,
              "part": 0,
              "translated_content": "Agent Network Protocol（ANP）[1002]旨在实现完全去中心化。代理通过符合W3C标准的去中心化标识（DIDs）进行身份识别，并通过加密的点对点通道进行通信。该协议包括一个元协议层，使代理能够协商采用哪种应用级协议，支持基于代理能力的语义协议选择。ANP还允许在应用层支持多种协议（例如HTTP、JSON-RPC、自然语言），提供了强大的可扩展性和去中心化，但并未明确支持公共协议重用。"
            },
            {
              "type": "text",
              "content": "Agora[932] offers a highly flexible and language-driven protocol mechanism. Instead of registering pre-defined APIs,agentscan generate and share ProtocolDescriptions (PDs),which are freetext descriptions ofcommunication semantics.Using alarge language model,agentscan dynamicaly interpret and execute any PDatruntime.This allows protocols tobecreated,deployed, andused entirelythrough language,without anymanualregistration orconfiguration. Agora avoids centralized registries and supports decentralized protocol sharing:agents may publish or retrieve PDs from peer-distributed repositories to enable cumulative learning and interoperability across systems.",
              "index": 4,
              "part": 0,
              "translated_content": "Agora提供了一种高度灵活且以语言驱动的协议机制。代替注册预定义的API，代理可以生成并共享协议描述（PDs），这些描述是通信语义的自由文本描述。利用大型语言模型，代理可以在运行时动态解释和执行任何PD。这使得协议可以完全通过语言创建、部署和使用，无需任何手动注册或配置。Agora避免了集中式注册表，并支持去中心化的协议共享：代理可以从对等分布式存储库发布或检索PD，以实现系统间的累积学习和互操作性。"
            },
            {
              "type": "text",
              "content": "Summary: As shown in Table 13.1, next-generation agent communication protocols differ along key dimensions suchas identity and security mechanisms, meta-protocol negotiation capabilities,application-layer flexibilityand the degree of centralization.Aunified, secure,scalable, and dynamic protocolinfrastructurewhere agents can negotiate andco-create protocolson the fyiscriticalforenablinglarge-scale, interoperable agent ecosystems.While current frameworks such as MCP,ANP,Agora, and IoA represent early but promising steps,protocol design remains arapidly evolving frontier in the development of intelligent agent systems.",
              "index": 5,
              "part": 0,
              "translated_content": "总结：如表13.1所示，下一代代理通信协议在身份和安全机制、元协议协商能力、应用层灵活性以及中心化程度等关键维度上存在差异。一个统一的、安全的、可扩展的和动态的协议基础设施，代理可以在其中协商和共同创建协议，对于实现大规模、互操作的代理生态系统至关重要。尽管当前的框架如MCP、ANP、Agora和IoA代表了早期但有前景的步骤，但协议设计仍然是智能代理系统发展中快速发展的前沿领域。"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td> Layer</td><td> MCP</td><td>ANP</td><td>Agora</td><td>IoA</td></tr><tr><td>Identity & Security</td><td>OAuth-based centralized identity authentication.</td><td>DID-based decentralized identity with encrypted channels.</td><td>No centralized registra- tion.Identity derived from PD hash.</td><td>Agents register with a central server for identity and discovery.</td></tr><tr><td>Meta-Protocol Layer</td><td>No meta-protocol layer; relies on pre-defined in- terfaces.</td><td>Uses DID document to negotiate and select ap- propriate protocol via se- mantics.</td><td>LLM interprets PD text to automatically negoti- ate and deploy communi- cation protocols.</td><td>A centralized discovery mechanism combined with FSM-based  dia- logue flow control.</td></tr><tr><td>col Layer</td><td>Application  Proto- Supports only  JSON- Supports multiple proto- RPC 2.0.</td><td>cols such as HTTP and natural language.</td><td>Allows  arbitrary PD- driven protocols with high flexibility.</td><td>Task-driven protocol coordination supporting multiple message for- mats.</td></tr><tr><td>ization</td><td>Degree of Central- Highly centralized archi- Fully decentralized. tecture.</td><td></td><td>Decentralized: no regis- tration or fixed ID, with optional peer-to-peer PD sharing.</td><td>Highly centralized archi- tecture with a central co- ordination server.</td></tr><tr><td>Protocol Flexibility</td><td>Fixed and rigid; hard Highly flexible with se- RPC.</td><td>to adapt beyond JSON-mantic negotiation.</td><td>Extremely flexible; any PD can define a new pro- tocol dynamically.</td><td>Moderately high flexibil- ity; agents can select and adapt message formats based on task phases and coordination needs.</td></tr></table></body></html>",
              "caption": "Table 13.1: Comparison of four agent communication protocols (MCP, ANP, Agora, IoA)across identity, negotiation, and execution layers. $\\mathbf{PD}=$ Protocol Description; DID:Decentralized Identifier; LLM:Large Language Model; FSM:Finite State Machine",
              "index": 6,
              "part": 0,
              "translated_caption": "表13.1：跨身份、谈判和执行层比较四种代理通信协议（MCP、ANP、Agora、IoA）。 $\\mathbf{PD}=$ 协议描述；DID：去中心化标识符；LLM：大型语言模型；FSM：有限状态机"
            },
            {
              "type": "table",
              "content": "<html><body><table><tr><td>Paper</td><td> System Design</td><td colspan=\"3\">Communication</td><td colspan=\"2\">Collaboration</td><td>Evolution</td></tr><tr><td></td><td>Category</td><td>Typology</td><td>Interface</td><td>Agent TypeInteraction</td><td></td><td>Decision</td><td>Type</td></tr><tr><td>Agent Hospital [921]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>T/M, C-O</td><td>Dict</td><td>Ind</td></tr><tr><td>Welfare Diplomacy [934]</td><td>M&S</td><td>S-L</td><td>Code, JSON, Text</td><td>Hom</td><td>CL</td><td>Voting</td><td>CI</td></tr><tr><td>MEDCO[923]</td><td>M&S</td><td>S-L</td><td>Text</td><td>Het</td><td>T/M, C-O</td><td>Dict</td><td>Ind</td></tr><tr><td>MedAgents[922]</td><td>M&S</td><td>S-L</td><td>Text</td><td>Hom</td><td>T-O</td><td>Dict</td><td>CI</td></tr><tr><td>Generative Agents [50]</td><td>M&S</td><td>S-D</td><td>Visual</td><td>Hom</td><td>CL</td><td>Dict</td><td>Ind</td></tr><tr><td>RECONCILE [918]</td><td>SL</td><td>S-D</td><td>Text</td><td>Hom</td><td>CL</td><td>D-B</td><td>CI</td></tr><tr><td>Agent Laboratory [746]</td><td>CTS</td><td>S-L</td><td>Code, Text</td><td>Het</td><td>C-O, T-O</td><td>Dict</td><td>Ind</td></tr><tr><td>CoELA[924]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Hom</td><td>T-O</td><td></td><td></td></tr><tr><td>The virtual lab [752]</td><td>CTS</td><td>S-L</td><td>Text</td><td>Het</td><td>C-O, CL</td><td>Dict</td><td>Ind</td></tr><tr><td>SciAgents [743]</td><td>CTS</td><td>S-L</td><td>Text</td><td>Het</td><td>T-O</td><td>Dict</td><td>CI</td></tr><tr><td>S-Agents [927]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>T-O, CL</td><td>Dict</td><td></td></tr><tr><td>GPT-Bargaining [1003]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td>CI</td></tr><tr><td>FORD[1004]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td>CI</td></tr><tr><td>MADRA [1005]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td></td></tr><tr><td>Multiagent Bench [948]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Hom</td><td>T-O, CL</td><td>D-B</td><td>CI, Ind</td></tr><tr><td>OASIS [936]</td><td>M&S</td><td>D</td><td>Text</td><td>Het</td><td>C-O</td><td></td><td></td></tr><tr><td>S [255]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td></td><td></td></tr><tr><td>FPS [981]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td></td><td></td></tr><tr><td>GPTSwarm[1006]</td><td>CTS</td><td>D</td><td>Code, JSON, Text</td><td>Hom</td><td>T-O</td><td>Dict</td><td>CI, Ind</td></tr><tr><td>ChatEval[1007]</td><td>CTS</td><td>D</td><td>Text</td><td>Hom</td><td>T-O</td><td>Voting</td><td>CI</td></tr><tr><td>MetaGPT [626]</td><td>CTS</td><td>S-L</td><td>Code, JSON, Text, Visual</td><td>Het</td><td>T-O</td><td>Dict</td><td>CI</td></tr><tr><td>AutoAgents [1008]</td><td>CTS</td><td>D</td><td>Text</td><td>Het</td><td>T-O</td><td>C-O</td><td>CI</td></tr><tr><td>SWE-agent [628]</td><td>CTS</td><td>D</td><td>Text</td><td>Hom</td><td>T-O</td><td>Dict</td><td>Ind</td></tr><tr><td>AgentCoder [994]</td><td>CTS</td><td>D</td><td>Code, Text</td><td>Het</td><td>T-O</td><td>D-B</td><td>CI</td></tr><tr><td>MASTER[1009]</td><td>CTS</td><td>S-L</td><td>Text</td><td>Hom</td><td>T-O</td><td>D-B</td><td>CI</td></tr><tr><td>Reflexion [48]</td><td>CTS</td><td>D</td><td>Text</td><td>Het</td><td>T-O</td><td>D-B</td><td>Ind</td></tr><tr><td>MACM[1010]</td><td>CTS</td><td>D</td><td>Text, Code</td><td>Het</td><td>T-O</td><td>D-B</td><td>CI</td></tr><tr><td>Debate[985]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td>CI</td></tr></table></body></html>",
              "caption": "Table 13.2:Classification framework for LLM-based multi-agent systems,highlighting diffrent aspects of system design,communication,collaboration, and evolution. Below are our abbreviations,for ease of reference: $\\mathbf{M}\\&\\mathbf{S}=$ Modeling & Simulation, $\\mathbf{CTS}=$ Collaborative Task Solving， $\\mathrm{SL}=$ Strategic Learning, $\\mathbf{S-D=}$ StaticDecentralized, $\\mathbf{S-L}=\\mathbf{S}$ Static-Layered, ${\\mathrm{Hom}}=$ Homogeneous, Het $\\mathbf{\\tau}=\\mathbf{\\tau}$ Heterogeneous, $\\mathbf{T}/\\mathbf{M}=$ Teaching/Mentoring, $C{\\mathrm{-}}0=\\left.\\left\\langle\\right.$ Consensus-Oriented, $\\mathbf{\\partial}_{\\mathbf{T}-\\mathbf{O}}=$ Task-Oriented, ${\\mathrm{CL}}=$ Collaborative Learning, Dict $\\mathbf{\\tau}=\\mathbf{\\tau}$ Dictatorial, $\\mathbf{D-B}=$ DebateBased, $\\mathrm{CI}=$ Collective Intelligence, Ind $\\c=$ Individual.",
              "index": 7,
              "part": 0,
              "translated_caption": "表13.2：基于LLM的多智能体系统的分类框架，突出系统设计、通信、协作和演化的不同方面。以下是为方便参考而使用的缩写：$\\mathbf{M}\\&\\mathbf{S}=$ 建模与模拟，$\\mathbf{CTS}=$ 协作任务解决，$\\mathrm{SL}=$ 战略学习，$\\mathbf{S-D}=$ 静态分散式，$\\mathbf{S-L}=\\mathbf{S}$ 静态分层，${\\mathrm{Hom}}=$ 同质的，Het $\\mathbf{\\tau}=\\mathbf{\\tau}$ 异质的，$\\mathbf{T}/\\mathbf{M}=$ 教学/指导，$C{\\mathrm{-}}0=\\left.\\left\\langle\\right.$ 以共识为导向，$\\mathbf{\\partial}_{\\mathbf{T}-\\mathbf{O}}=$ 任务导向，${\\mathrm{CL}}=$ 协作学习，Dict $\\mathbf{\\tau}=\\mathbf{\\tau}$ 独裁的，$\\mathbf{D-B}=$ 辩论型，$\\mathrm{CI}=$ 集体智能，Ind $\\c=$ 个体。"
            }
          ],
          "raw_title": "Next-Generation Communication Protocols",
          "type": null,
          "children": [],
          "translated_title": "13.5.3 下一代通信协议"
        }
      ],
      "translated_title": "13.5 代理交互协议"
    },
    {
      "title": "14.1 System Topologies",
      "number": "14.1",
      "level": 2,
      "content": [
        {
          "type": "figure",
          "src": "images/4b9d903ce673fb44aaa5ff3593d7df89375449cc30355b75b9753253afd5d141.jpg",
          "alt": "",
          "caption": "Figure 14.1: Different types of topological structure for multi-agent collaboration.",
          "index": 0,
          "part": 0,
          "translated_caption": "图14.1：多智能体协作的不同拓扑结构类型。"
        },
        {
          "type": "figure",
          "src": "images/4b9b4f0dc754b2f05dab6a1d19a8f8cf8e14c4937ecc8b6c842b6ac4c491b69e.jpg",
          "alt": "",
          "caption": "Figure 14.2: Collaborative and competitive agents.",
          "index": 1,
          "part": 0,
          "translated_caption": "图14.2：协作和竞争代理。"
        },
        {
          "type": "text",
          "content": "This section examines the interaction typology in LLM-based multi-agent systems (MAS)and its impact on communication,collaboration,and task execution. We first analyze static topologieswhereconnectivity patterns are fixed by domain knowledge—and then explore dynamic(adaptive)topologies that adjust inter-agent connections based on performance metrics,workload variations,or strategicconstraints.Weconclude with a discussion of scalability challenges and trade-offs inbalancing systemcost, performance,androbustness,drawing onrecent research indistributed processing, self-organization, and emergent collaborative behaviors.",
          "index": 2,
          "part": 0,
          "translated_content": "本节探讨了基于LLM的多智能体系统（MAS）中的交互类型学及其对通信、协作和任务执行的影响。我们首先分析了静态拓扑结构，其中连接模式由领域知识固定——然后探讨了根据性能指标、工作量变化或战略约束调整智能体间连接的动态（自适应）拓扑结构。最后，我们讨论了在平衡系统成本、性能和鲁棒性方面的可伸缩性挑战和权衡，借鉴了分布式处理、自组织和新兴协作行为的最新研究成果。"
        }
      ],
      "raw_title": "System Topologies",
      "type": null,
      "children": [
        {
          "title": "14.1.1 Static Topologies",
          "number": "14.1.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Static topologies are definedby predeterminedstructural paterns that remainlargely unchanged during system execution. In these configurations,connections among agents—or between agents andacentralcoordinator—are established using fixed rules and heuristics,ensuring predictable communication flows and simplified coordination.Three canonical forms are typically considered: layered (hierarchical), decentralized, and centralized architectures.\n\nLayered (Hierarchical)Structures Layered topologies arrange agents hierarchically, with high-level agents coordinating or supervising lower-level ones.This approach mirrors traditional management frameworks—such as Standard\n\nOperating Procedures (SOP)or the Waterfallmodel-where tasks are decomposed into sequential, well-defined stages. For instance, the AutoAgents [1o08] framework assigns roles (e.g., Planner, Agent Observer, and Plan Observer) to synthesize execution plans,while ChatDev[983] leverages hierarchical task decomposition to streamline software development[626,921,627].Although hierarchical structures facilitate debugging,performance monitoring, and modularity, they can create bottlenecks when upper-tier agents are overloaded[10ll]. Recent studies in storytelling[10] 14] and data science applications including data cleaning [1015,1016], visualization[1017, 1018] and automachinelearning [1019,020],highlight the trade-off between consistency and theemergenceof adaptive real-time behaviors.",
              "index": 0,
              "part": 0,
              "translated_content": "静态拓扑结构由预先确定的结构模式定义，在系统执行过程中基本保持不变。在这些配置中，代理之间的连接，或者代理与中央协调器之间的连接，是通过固定规则和启发式方法建立的，确保可预测的通信流和简化的协调。通常考虑三种经典形式：分层（层次化）、分散式和集中式架构。\n\n分层（层次化）结构\n分层拓扑将代理按层次排列，高层代理协调或监督低层代理。这种方法类似于传统的管理框架，例如标准操作规程（SOP）或瀑布模型，其中任务被分解为顺序、明确定义的阶段。例如，AutoAgents框架分配角色（例如，规划者、代理观察者和计划观察者）以综合执行计划，而ChatDev利用分层任务分解来简化软件开发。尽管分层结构有助于调试、性能监控和模块化，但当上层代理超负荷时可能会产生瓶颈。最近在叙事和数据科学应用领域，包括数据清洗、可视化和自动机器学习等方面的研究，突显了一致性与自适应实时行为之间的权衡。"
            },
            {
              "type": "text",
              "content": "Decentralized Structures In decentralized topologies, agents interact on a peer-to-peer basis without a central coordinator,forming networks that areoften modeledaschains,rings,smallworldorrandom graphs[121,971].This structure enhances fault tolerance since the failure of a single agent does notcompromise the network.For example, [1022]show that distributing graphreasoning tasksamong multiple agents enables scalability beyond the contextlength limits of individual LLMs.Additionally,[1023]propose decomposition strategies that allow an orchestrating LLM to delegate subtasks efectively.However, maintaining acoherent global state in decentralized systems necesstates sophisticated consensus and synchronization protocols.",
              "index": 1,
              "part": 0,
              "translated_content": "分散式结构\n在分散式拓扑中，代理在对等基础上相互作用，没有中央协调器，形成的网络通常被建模为链、环、小世界或随机图[121,971]。这种结构增强了容错性，因为单个代理的故障不会危及网络。例如，[1022]表明，将图推理任务分布在多个代理之间可以实现超出单个LLM上下文长度限制的可伸缩性。此外，[1023]提出了分解策略，允许编排LLM有效地委派子任务。然而，在分散式系统中保持一致的全局状态需要复杂的共识和同步协议。"
            },
            {
              "type": "text",
              "content": "Centralized Structures Centralized topologies rely on a master coordinator that gathers information and directs peripheral agents hierarchically.Such a setup allows for better controlover handling resources and sharing a global view,such as withculture parksand Lyfe Agents[1024,025]. With additional agents,however, a bottleneck atthe center node may occur, with increased communication overhead and susceptibility to failures. Current studies on coordinator-agentconfigurations[971] andresearchon ensuring autonomy forcentralizedconfigurations [1026] point out problems with scalabilitywithconsistency.While consistency is guaranteed forcentralized architectures,there may not necessarily be flexibility for dynamic adaptation.",
              "index": 2,
              "part": 0,
              "translated_content": "集中式结构\n\n集中式拓扑依赖于一个主协调器，该协调器收集信息并按层次指导外围代理。这种设置允许更好地控制资源处理和共享全局视图，例如文化公园和Lyfe代理。然而，随着额外代理的增加，中心节点可能出现瓶颈，通信开销增加，对故障的敏感性也增加。当前关于协调器-代理配置的研究以及确保集中式配置的自治性的研究指出了与一致性和可伸缩性相关的问题。虽然集中式架构可以保证一致性，但不一定具备动态适应性的灵活性。"
            },
            {
              "type": "text",
              "content": "Briefly, static topologies have advantages of determinismand predefinition.With pre-defined structural patterns, these systems have predictable communication patterns and effctive coordination among agents. Topologies of thesestructures aretypicallydefined onstructuralknowledgeorstatic rules,and,as such,theysuit domains where workflowforthe tasks is static,there are predefinedroles,and systemrequirements are welldefined.The second primary advantage is design,implementation, and maintenance ease.With structure predefined, design as well as execution procedures are made simpler, and,asaresult, maintenance isa simplerprocessResourcehandling as wellas modularization gets simpler due to well-defined, static structure.",
              "index": 3,
              "part": 0,
              "translated_content": "简而言之，静态拓扑具有确定性和预定义的优势。通过预定义的结构模式，这些系统具有可预测的通信模式和有效的代理间协调。这些结构的拓扑通常基于结构知识或静态规则定义，因此适用于任务工作流程静态、有预定义角色以及系统需求明确定义的领域。第二个主要优势是设计、实施和维护的简便性。由于结构预定义，设计和执行程序变得更简单，因此维护过程更简单。由于静态结构明确定义，资源处理和模块化变得更简单。"
            },
            {
              "type": "text",
              "content": "However,statictopologies themselves are nonflexible,groundedon pre-specified patterns ofconnectivitythat do not respond toreal-timechanges.Wellsuitedforaspecific purpose atdesigntimebutentirelylackingflexibilityforreacting to unforeseen challenges,including sudden agent breakdown,varying degrees of task complexity, and system goal modification,static topologies do nothave real-time response flexibility potential.Real-time response inflexibility inhibits runtime system reconfiguration anddecreases system effectiveness in dynamic settings where circumstances occur.Failureto self-organize and morph according toemergingconditions may equate to inefficiency as well as low system performance, particularly where dynamic or emergent setings are at hand.",
              "index": 4,
              "part": 0,
              "translated_content": "然而，静态拓扑本身缺乏灵活性，基于预先指定的连接模式，无法响应实时变化。虽然在设计时适用于特定目的，但完全缺乏对未预见挑战的灵活性，包括突发代理故障、任务复杂度不同以及系统目标修改。静态拓扑缺乏实时响应灵活性潜力，这种实时响应的不灵活性抑制了运行时系统重配置，并降低了在动态环境中系统的有效性。未能根据新出现的情况进行自组织和变形可能导致低效以及系统性能低下，特别是在动态或新兴环境下。"
            }
          ],
          "raw_title": "Static Topologies",
          "type": null,
          "children": [],
          "translated_title": "14.1.1 静态拓扑结构"
        },
        {
          "title": "14.1.2 Dynamic and Adaptive Topologies",
          "number": "14.1.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "While static topologies providedeterminism and predictabilityillustrated by statictopologies such as hierarchical or centralized ones performing well with stable-task domains and well-defined roles—static topologies do not fit open-ended or novel domains.Real domains,from real-time collaborative plan,to dynamic social simulations, often demand that agents make changes on their patterns of interaction as work continues,available resources vary,or feedback fromthe environment is received. Such structural tension with adaptative maleability generates dynamic topologies,which,atruntime,recast inter-agentrelationshipsasaresponse tofeedbackon performance,workload,or strategic constraints, striking a balance between consistency and responsiveness.",
              "index": 0,
              "part": 0,
              "translated_content": "尽管静态拓扑结构提供了确定性和可预测性，例如在稳定任务领域和定义明确角色中表现良好的分层或中心化的静态拓扑结构所展示的，但静态拓扑结构并不适用于开放式或新领域。真实领域，从实时协作计划到动态社会模拟，通常要求代理根据工作进行更改其互动模式，可用资源变化，或接收到来自环境的反馈。这种与适应性可塑性的结构张力产生了动态拓扑结构，这些结构在运行时根据对性能、工作负载或战略约束的反馈，重新构建代理之间的关系，以在一致性和响应性之间取得平衡。"
            },
            {
              "type": "text",
              "content": "For example,DyLAN framework[725] supports inference-time agent selection througha two-step processa forwardbackward team optimization step with unsupervised Agent Importance Scores,follwed by dynamic team reformulation at runtime.Similarly, OPTIMA[1027] optimizes inter-agent connectivity iteratively through a generate-rank-select-train framework,utilizing reward functions asa means for determining abalance among task quality, token efficiency, and readability, with communication actions further optimized through strategies such as Direct Prefernce optimization. The MAD framework [649] ilustrates flexibility through a joint optimization among three prompt phases and structure,with dynamic role assignment(such as verifers and debate participants)within pruned spaces for structure.",
              "index": 1,
              "part": 0,
              "translated_content": "例如，DyLAN框架通过两步流程支持推理时代理选择，即通过无监督代理重要性评分进行前向后向团队优化步骤，随后在运行时进行动态团队重组。类似地，OPTIMA通过生成-排名-选择-训练框架迭代地优化代理之间的连接，利用奖励函数作为确定任务质量、代币效率和可读性之间平衡的手段，通过诸如直接偏好优化等策略进一步优化通信行为。MAD框架通过三个提示阶段和结构的联合优化展示了灵活性，通过动态角色分配（例如验证者和辩论参与者）在结构的修剪空间内进行。"
            },
            {
              "type": "text",
              "content": "Topological control also becomes tractable through technological advancements.GPTSwarm [651] conceptualizes agents ascomputation graphs and uses evolutionarystrategies andreinforcement learningforadjusting adjacency matrices for optimizing nodes based on feedback for the task.MACNET[1028] uses a directed acyclic graph architecture with supervisory instructors managing edges and executive assistants managing nodes for more complex coordination domains,facilitating adaptivecommunication through topologicalordering and sensitive propagation of output. Application-specificversions alsoemphasize architecture diversity.Open-world environments have DAMCS [1029], which couples hierarchical knowledge graphs (A-KGMS)with structured communication schemes (S-CS) for cooperative planning as a function of messages passed based on context.AutoAgents [1030] leverages a dynamic drafting-execution pipeline with pre-defined agents jointlysketching out expert teams,adesignthat's highlyefctive for creative applications such as novel generation through parallel processing and internal supervision. Noticeably, small-world development within large-scale MACNET[1028] systems corresponds with graph reasoning ideas shown in[1022], where distributed architecture bypasses local limitations of LLM through structured collaboration.In terms ofcollaborative task solving,several paradigms have emerged that emphasize therole of dynamic topologies. These paradigms include search-based methodologies,LLM-based generation,and configurations utilizing external parameters.",
              "index": 2,
              "part": 0,
              "translated_content": "随着技术的进步，拓扑控制也变得可行。GPTSwarm[651]将代理构想为计算图，并利用进化策略和强化学习来根据任务反馈调整邻接矩阵以优化节点。MACNET[1028]采用有向无环图架构，监督教练管理边缘，执行助手管理节点，用于更复杂的协调领域，通过拓扑排序和输出的敏感传播促进自适应通信。特定应用版本也强调架构多样性。开放世界环境中有DAMCS[1029]，它将分层知识图（A-KGMS）与结构化通信方案（S-CS）相结合，基于上下文传递的消息进行协作规划。AutoAgents[1030]利用动态起草-执行流水线，预定义代理共同勾画专家团队，这种设计对于创意应用（如通过并行处理和内部监督进行新颖生成）非常有效。值得注意的是，在大规模MACNET[1028]系统中的小世界发展与[1022]中展示的图推理思想相符，分布式架构通过结构化协作规避了LLM的局部限制。在协作任务解决方面，出现了几种强调动态拓扑作用的范式。这些范式包括基于搜索的方法、基于LLM的生成，以及利用外部参数的配置。"
            },
            {
              "type": "text",
              "content": "Search-based Methods A number of works adopt search-based methodologies to iteratively optimize communication structures.For example,ADAS [741] employs a Meta Agent Search algorithm that iteratively generates and tests new agent designs within acode space, archiving superior configurations and thereby updating subsequent generation strategies.Similarly,Afow[773] models each LM callas a node inagraph and utilizes Monte Carlo Tree Search (MCTS)to dynamicall extend and refine the workflow.Other frameworks,such as MAD[1031] and OPTIMA [1027], integrate iterative generate-rank-select-train paradigms thatecho MCTS principles to balance task performance with efficiency.",
              "index": 3,
              "part": 0,
              "translated_content": "基于搜索的方法\n一些作品采用基于搜索的方法来迭代优化通信结构。例如，ADAS[741]采用元代理搜索算法，该算法在代码空间内迭代生成和测试新的代理设计，存档优越配置，从而更新后续的生成策略。类似地，Afow[773]将每个LM调用建模为图中的一个节点，并利用蒙特卡洛树搜索（MCTS）动态扩展和优化工作流程。其他框架，如MAD[1031]和OPTIMA[1027]，整合了迭代生成-排名-选择-训练范式，这些范式与MCTS原则相呼应，以平衡任务性能和效率。"
            },
            {
              "type": "text",
              "content": "LLM-based Methods Complementing search-based methods, severalrecent works leverage the generative capacity of LLMs to construct and adapt dynamic topologies.Dylan [725] introduces a temporal feed-forward network (T-FFN) model that treats each communication step as a network layer,using forward-backward propagation to compute Agent Importance Scores for dynamic team selection. Inrelated work,DAMCS[1029],AutoAgents[1030],and TDAG[1032] dynamically generate specialized sub-agents orupdate hierarchicalknowledge graphs,enablingcooperative planning and task decomposition.Further,frameworks such as AutoFlow[773]and Flow[1033]represent task workflows in natural language programs oractivity vertex graphs (AOV),allowing continuous refinement through reinforcement learning signals.ScoreFlow[788]complements these approaches by applying gradient-based (loss-gradient)optimization to continuously reconfigure agent workflows.",
              "index": 4,
              "part": 0,
              "translated_content": "基于LLM的方法\n为了补充基于搜索的方法，一些最近的作品利用LLM的生成能力来构建和调整动态拓扑结构。Dylan[725]引入了一个称为时间前馈网络（T-FFN）的模型，将每个通信步骤视为一个网络层，利用前向-后向传播计算代理重要性分数，用于动态团队选择。在相关工作中，DAMCS[1029]、AutoAgents[1030]和TDAG[1032]动态生成专门的子代理或更新分层知识图，实现协作规划和任务分解。此外，AutoFlow[773]和Flow[1033]等框架将任务工作流表示为自然语言程序或活动顶点图（AOV），通过强化学习信号进行持续的细化。ScoreFlow[788]通过应用基于梯度的（损失梯度）优化来连续重新配置代理工作流程，对这些方法进行了补充。"
            },
            {
              "type": "text",
              "content": "External Parameters Given that fine-tuning LLM-based agents is often resource-intensive,a considerable number of researchers advocate configuring inter-agent topologies by training parameters independent of the LLM-agent.This approach is initiatedbyGPTSwarm[651],in whichthe inter-agent topologies are representedasadirectedacyclic graph (DAG),with edge weights serving as the sole trainable component of the system.Further advancing this paradigm, AgentPrune provides a unified modeling framework fromthe spatial-temporal graph perspective for mainstream MAS, where communication redundancy,ie.,unnecessary edges,is identified and prunedthrough magnitude-based pruning. Follow-upworks in this lineof research include G-Safeguard[1034],which similarly trains GNN outside of the MAS to detect andeliminate malicious communication paths.Although these methods are parameter-effcient,their relatively small parameter space and low coupling with LLM-agents often result in performancelimitations to some extent.",
              "index": 5,
              "part": 0,
              "translated_content": "外部参数 鉴于微调基于LLM的代理通常需要大量资源，许多研究者主张通过训练与LLM代理独立的参数来配置代理间拓扑结构。这一方法由GPTSwarm[651]发起，其中代理间拓扑结构被表示为有向无环图（DAG），边权重作为系统唯一可训练的组件。在进一步推进这一范式的过程中，AgentPrune从时空图的角度为主流多主体系统提供了一个统一的建模框架，通过基于幅值的修剪识别和修剪通信冗余，即不必要的边。在这一研究线的后续作品包括G-Safeguard[1034]，该作品类似地训练GNN在多主体系统之外，以检测和消除恶意通信路径。尽管这些方法在参数效率上表现出色，但由于其相对较小的参数空间和与LLM代理的低耦合，往往在一定程度上会导致性能限制。"
            },
            {
              "type": "text",
              "content": "Discussion Dynamic topologies extend beyond task-solving and play a crucial role in simulating complex social interactions.As detailed inarecent survey[975],LLM-basedagent modelscanevolve inter-agent links tocapture real-timechanges inautonomy,socialbehaviors,and environmental feedback across variousdomains, including cyber, physical,and mixed environments.Systems such as[50],OASI[936] and ProjectSid[989] simulate dynamic social networks.[50]employs generative naturallanguage memory retrievalto adjust socialties based on agents'experiences, while OASIS constructs a real-time social media environment with continuously updated user relationships and informationflows.Project Sid[989]introduces thePIANO(ParallInformation Agregation via NeuralOrchestration) architecture,enablingover lautonomous AIagents tointeract inreal-timewithina Minecraft environment,leading to the emergence ofcomplexsocietalstructures such asspecializedroles,collctive rule adherence,andculturaland religiou transmission.Additionally,architectures like AgentScope-scability[1035] and Social Survey [975]support large-scale multi-agent simulations,enablingstudiesofculturaldissemination,colectivedecision-making,andemergent group dynamics in environments with hundreds orthousands ofinteracting agents.Additionaly,dynamic topologies are also tailored to specific application domains such as medical and open-domain embodied AI. In the medical field, AI hospital[1036] and agent hospital[921] simulatereal medical workflows, where iterative cycles of diagnosis, treatment, and feedback continuously reshape communication patterns among various roles,such as intern doctors, patients,examiners,and supervising physicians.These frameworks dynamically adjust inter-agent communication to optimizecolaboration and decision-making.Similarly, in open-domain and embodied AI applications,frameworks like IOA[9]support heterogeneous,cross-device agent interactions,facilitating dynamic team formation and task allocation in real-world scenarios.",
              "index": 6,
              "part": 0,
              "translated_content": "讨论 动态拓扑结构不仅限于解决任务，在模拟复杂社会互动中发挥着关键作用。正如最近的一项调查所详细介绍的[975]，基于LLM的代理模型可以演化出代理间的联系，以捕捉自主性、社会行为和环境反馈的实时变化，涵盖了包括网络、物理和混合环境在内的各种领域。诸如[50]、OASI[936]和Project Sid[989]等系统模拟了动态社交网络。[50]利用生成式自然语言记忆检索来根据代理经验调整社交关系，而OASIS构建了一个实时社交媒体环境，其中用户关系和信息流持续更新。Project Sid[989]引入了PIANO（通过神经编排实现并行信息聚合）架构，使超过百个自治AI代理在Minecraft环境中实时交互，导致出现诸如专业角色、集体规则遵守、文化和宗教传播等复杂社会结构。此外，像AgentScope-scability[1035]和Social Survey[975]这样的架构支持大规模多代理模拟，实现了对文化传播、集体决策和新兴群体动态的研究，使得在涉及数百或数千个互动代理的环境中成为可能。此外，动态拓扑结构还针对特定应用领域进行了定制，如医疗和开放领域的具体应用。在医疗领域，AI医院[1036]和代理医院[921]模拟真实的医疗工作流程，其中诊断、治疗和反馈的迭代周期不断重塑各种角色之间的通信模式，如住院医生、患者、检查者和主治医师。这些框架动态调整代理间的沟通，以优化协作和决策。同样，在开放领域和具体应用的AI中，像IOA[9]这样的框架支持异构、跨设备的代理交互，促进了真实场景中动态团队形成和任务分配。"
            },
            {
              "type": "text",
              "content": "Although the aforementioned dynamic multi-agent topologies have made substantial progressin performance metrics, they stillfacethefollowingthreelimitations,whichwebelieveshouldbethefocalpointsforfutureresearchondynamic topologies:\n\n(1) Generalizability. Current MAS topologies are typically optimized for a single-task domain. For example, AFlow[773] is dedicated to search and optimization within math or code benchmarks, producing a fixed workflow that is difficult to adapt to newtask domains.Other dynamic topologies,such as ADAS[741],GPTSWarm[651],and AgentPrune,face the samechallenge.We argue thatMAS should becapableof lifelong learning,wherein the system generalizes across different task domains with minimal resources (e.g., API call, FLOPs, GPU hours).",
              "index": 7,
              "part": 0,
              "translated_content": "尽管前述动态多代理拓扑在性能指标上取得了实质性进展，但仍面临以下三个限制，我们认为这应成为未来动态拓扑研究的重点：\n\n(1) 泛化能力。当前多代理系统拓扑结构通常针对单一任务领域进行优化。例如，AFlow专注于在数学或代码基准中进行搜索和优化，生成难以适应新任务领域的固定工作流程。其他动态拓扑结构，如ADAS、GPTSwarm和AgentPrune，面临相同挑战。我们认为多代理系统应具备终身学习能力，在其中系统能够跨不同任务领域进行泛化，且资源消耗最小（例如API调用、FLOPs、GPU时数）。"
            },
            {
              "type": "text",
              "content": "(2)Resource Efficiency.Present dynamic topologies oftentend tooptimize forcomplex,resource-intensive structures. Their training processes aretypicallexorbitantlycostly,asexemplified byADAS[74l],wheretrainingwithGPT-3.5 incurs a cost of approximately $\\$300$ per session. Such expenses severely constrain their large-scale applicability in real-world scenarios.Future developments should focus on achieving beter test-time topology optimization with significantly reduced costs.\n\n(3) Inference Efficiency.As MaAS[787]has incisively observed, multi-agent topologies of excessive complexity, while capableofconsistentlydelivering satisfactory performance,arelamentablydeficient intask adaptability.That is to say,theyareunabletodynamically allocatereasoningresources (i..,tols,thenumberofagents,andreasoning steps)inresponse tothe diffcultyof agiven task.Consequently,this may lead toacertain lack of effciency inthe inference process. Although MaAS has, to acertain extent, achievedtask dynamism through the designed agentic supernet, their applicability and scalability in large-scale deployment still remain to be tested.",
              "index": 8,
              "part": 0,
              "translated_content": "(2) 资源效率。目前的动态拓扑结构往往优化复杂、资源密集的结构。它们的训练过程通常成本高昂，ADAS[74]便是一个例子，使用GPT-3.5进行训练每次会产生大约300美元的成本。这些费用严重限制了它们在实际场景中的大规模适用性。未来的发展应该专注于实现更好的测试时拓扑优化，并显著降低成本。\n\n(3) 推理效率。正如MaAS[787]所指出的，过度复杂的多代理拓扑结构虽然能够持续提供令人满意的性能，但在任务适应性方面存在明显不足。也就是说，它们无法根据特定任务的难度动态分配推理资源（例如工具、代理数量和推理步骤）。因此，这可能导致推理过程中某种程度的效率缺失。尽管MaAS在一定程度上通过设计的代理超网络实现了任务动态性，但它们在大规模部署中的适用性和可扩展性仍有待测试。"
            }
          ],
          "raw_title": "Dynamic and Adaptive Topologies",
          "type": null,
          "children": [],
          "translated_title": "14.1.2 动态和自适应拓扑结构"
        }
      ],
      "translated_title": "14.1 系统拓扑结构"
    },
    {
      "title": "14.2 Scalability Considerations",
      "number": "14.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Scalability is acriticalchallenge inLM-basedmulti-agent systems (MAS),especiallas the number of agents grows. In fully connectednetworks,the numberofcommunication pathsgrowsquadratically,leading toacommunication explosion that increases token usage andcomputationalcosts[1037,626].Centralized andlayered topologiescanexperience synchronization bottlenecks if supervisory nodes are inundated by messages,whereas decentralized networks—while more fault tolerant—necessitate complex consensus algorithms to achieve a coherent global state.",
          "index": 0,
          "part": 0,
          "translated_content": "在基于大型语言模型的多智能体系统（MAS）中，可伸缩性是一个关键挑战，特别是在代理数量增加的情况下。在完全连接的网络中，通信路径的数量呈二次增长，导致通信量激增，增加了令牌使用和计算成本。如果监督节点被消息淹没，集中式和分层拓扑结构可能会经历同步瓶颈，而去中心化网络虽然更具容错性，但需要复杂的共识算法来实现一致的全局状态。"
        },
        {
          "type": "text",
          "content": "Recent worksuchas[1028]demonstrates that when multi-agent collaboration is structured as adirected acyclic graph (DAG),the systemcan scaleeffcently tohandle large graphs-upto1O0 nodesor more-without significant performance degradation.Similarly[1022] shows that distributing graph reasoning tasks among many agents circumvents the limitations imposed by long textualinputs and context-length constraints.Moreover,studies on self-organized agents[1038]revealthat dynamic multiplication andtaskdistribution allow the system tomaintain aconstant workload per agent while increasing overallprocessng capacity.Finally, the multi-dimensional taxonomy proposed by [1039] providesa valuable framework foranalyzing trade-off between agent autonomy and alignment,offering insight into how to balance centralized control with decentralized flexibility to optimize scalability.",
          "index": 1,
          "part": 0,
          "translated_content": "最近的研究（如[1028]）表明，当多智能体协作被构建为有向无环图（DAG）时，系统可以高效扩展以处理大规模图，甚至可以达到100个节点或更多，而不会出现显著的性能下降。类似地，研究（如[1022]）表明，将图推理任务分配给多个代理可以规避长文本输入和上下文长度限制所施加的限制。此外，关于自组织代理的研究（如[1038]）揭示了动态复制和任务分配使系统能够在增加整体处理能力的同时保持每个代理的恒定工作负载。最后，[1039]提出的多维分类法为分析代理自治和协调之间的权衡提供了有价值的框架，为如何平衡集中控制与分散灵活性以优化可伸缩性提供了见解。"
        },
        {
          "type": "text",
          "content": "In addition tothese foundational studies,recent advances in practical multi-agent platform designfurtherenrich the scalablity discussion.Forexample,AgentScope[105]offers adeveloper-centric platformthat leverages an actor-based distributed framework to enable seamless migration between local and distributed deployments.Its unified workflow and automatic paralleloptimization significantlyreduce the communication overhead and synchronization challenges that typicallemerge as agent numbers increase.By incorporating fault-tolerance mechanisms and intellgent mesage filtering,AgentScope illustrates how system-level supportscan be designed to maintain performance even in dynamic and heterogeneous deployment environments.",
          "index": 2,
          "part": 0,
          "translated_content": "除了这些基础研究外，最近在实际多智能体平台设计方面取得的进展进一步丰富了可伸缩性讨论。例如，AgentScope提供了一个面向开发者的平台，利用基于actor的分布式框架，实现在本地和分布式部署之间的无缝迁移。其统一工作流程和自动并行优化显著减少了通信开销和同步挑战，这些挑战通常会随着代理数量的增加而出现。通过整合容错机制和智能消息过滤，AgentScope展示了如何设计系统级支持，以在动态和异构的部署环境中保持性能。"
        },
        {
          "type": "text",
          "content": "Another complementary approach is presented in Project Sid[989], which explores scalability within the realmof simulating agent civilizations.Here,the focusshifts from isolated task solving tothe simulationofcomplexsocietal dynamics. The proposed PIANO (Parallel Information Aggregation via Neural Orchestration)architecture allows agents to operate concurrently by decoupling slower cognitive processes from rapid reactive modules.A dedicated cognitive controllr is introduced to ensure coherence among multiple paralleloutputs.This design not only enables scalability fromsmallgroupstosimulations involving overathousand agentsbutalsoeffctivelyaddresses the inherent coordination challenges arising from high-frequency interactions.",
          "index": 3,
          "part": 0,
          "translated_content": "在模拟智能体文明领域，另一种互补方法在Sid项目中提出，该项目探讨了可伸缩性。在这里，重点从孤立任务解决转移到模拟复杂社会动态。所提出的PIANO（通过神经编排实现并行信息聚合）架构允许代理同时运行，通过将较慢的认知过程与快速的反应模块解耦。引入专用认知控制器以确保多个并行输出之间的一致性。这种设计不仅使得从小团体扩展到涉及超过一千个代理的模拟成为可能，而且有效地解决了由高频交互引起的固有协调挑战。"
        },
        {
          "type": "text",
          "content": "Taking scalability toaneven larger scale,AgentSociety[1040] demonstrates acomprehensive framework for simulating realistic social environments with up to10,O0O agents.By integrating LLM-driven social generative agents within a realistic urban,social, and economic setting,AgentSociety employs distributed computing and a high-performance messaging system (e.g., MQTT) to support millons of daily interactions.This platform exemplifies how emerging hybrid architectures can support macro-level phenomena—such as economic market dynamics,opinion diffusion, and urban planning simulations—by effectively managing the trade-offs between communication cost, coordination overhead, and emergent behavior fidelity.",
          "index": 4,
          "part": 0,
          "translated_content": "将可伸缩性提升到更大规模，AgentSociety[1040]展示了一个全面的框架，用于模拟最多10,000个智能体的现实社会环境。通过在现实的城市、社会和经济环境中整合基于LLM的社会生成智能体，AgentSociety利用分布式计算和高性能消息传递系统（例如MQTT）来支持数百万次日常互动。这个平台展示了新兴混合架构如何通过有效管理通信成本、协调开销和新兴行为的准确性之间的权衡，来支持宏观现象，例如经济市场动态、舆论传播和城市规划模拟。"
        },
        {
          "type": "text",
          "content": "Despite the theoretical advantages of scaling upagent populations,it is imperative to question whether pursuit of large-scale agent deployments is inherentlyvaluable for alltask-solving scenarios.Although thetotalcomputational capacity scales with the number of agents,when memoryoverhead and inter-agent communication costs arefactored in,the marginal utility of adding additional agents may demonstrate diminishing returns. This phenomenon arises from the fundamentalconstraintthat, whiletheoverallworkloadis the product of individual taskcomplexity andthe degree of labor division,coordinationcosts tend to increase super-linearly with agentcount.Therefore,for many bounded problem domains,there is likely an optimal agent population size beyond which performance plateaus—or even deteriorates—due to excessive coordination overhead.",
          "index": 5,
          "part": 0,
          "translated_content": "尽管在理论上扩大智能体群体规模具有优势，但必须质疑追求大规模智能体部署是否在所有任务解决场景中都具有内在价值。虽然总计算能力随着智能体数量的增加而扩展，但考虑到内存开销和智能体间通信成本，增加额外智能体的边际效用可能呈现递减。这种现象源于基本约束，即虽然整体工作量是个体任务复杂度和劳动分工程度的乘积，但协调成本往往随着智能体数量呈超线性增长。因此，在许多有界问题领域，可能存在一个最佳的智能体群体规模，超过该规模性能将达到平稳状态，甚至由于过多的协调开销而恶化。"
        },
        {
          "type": "text",
          "content": "Conversely,in simulation scenarios where theobjective is to modelcomplexsocial dynamics,emergent behaviors,or large-scalecollective intelligence,scaling tonumerous agents becomes not merelybeneficial but essential. Inthese contexts,theresearchfocus shiftsfromoptimizingcomputationaleffciencyfortask solving toaccuratelyreproducingor predicting macro-level patterns emerging from micro-level agent interactions.Such simulations—covering domains like economic marketbehavior,social network evolution,and urban infrastructure planningoften requirethecomputational overhead of managing vast agent populations in order to capture realistic population-level phenomena.",
          "index": 6,
          "part": 0,
          "translated_content": "相反，在模拟场景中，如果目标是模拟复杂的社会动态、 emergent 行为或大规模集体智能，扩展到众多智能体不仅有益而且是必不可少的。在这些情境下，研究重点从优化任务解决的计算效率转向准确复制或预测由微观智能体相互作用产生的宏观模式。这种模拟——涵盖经济市场行为、社交网络演化和城市基础设施规划等领域——通常需要管理庞大智能体群体的计算开销，以捕捉逼真的群体层面现象。"
        },
        {
          "type": "text",
          "content": "Hybrid architectures that combine centralized oversight with decentralized sub-teams offra promising solution to these scalabilitychallnges[921,18].Inthesedesigns,supervisoryagents handle globalobjectivesandcoordination, while worker agents focus on executing specific subtasks.This hierarchicalorganizationhelps to mitigate information overload at any single node andalows for dynamic adjustment of agent team sizes based on task demands, thereby optimizing resource utilization.Furthermore, advanced techniques such as graph search algorithms,reinforcement learning-basedupdates,andevolutionary methods arecriticalforiterativelyrefningthe network structureasthe system scales.Intelligent message filtering,prioritization,andaggregation mechanisms can significantlyreduce communication overhead without sacrificingthequalityofinter-agentcollaboration.Inaition,asynchronouscommunicationprotocols and partial knowledge sharing strategies show promise in minimizing coordination bottenecks while maintaining sufficient global awareness among agents.",
          "index": 7,
          "part": 0,
          "translated_content": "将集中式监督与分权的子团队相结合的混合架构为这些可扩展性挑战提供了有前途的解决方案[921,18]。在这些设计中，监督代理处理全局目标和协调，而工作代理专注于执行特定子任务。这种分层组织有助于减轻任一节点的信息过载，并允许根据任务需求动态调整代理团队大小，从而优化资源利用。此外，高级技术，如图搜索算法、基于强化学习的更新以及演化方法，在系统扩展时对网络结构进行迭代优化至关重要。智能消息过滤、优先级设定和聚合机制可以显著减少通信开销，而不牺牲智能体间协作的质量。此外，异步通信协议和部分知识共享策略显示出在最小化协调瓶颈的同时保持智能体之间充分全局意识的潜力。"
        },
        {
          "type": "text",
          "content": "Concluding Remarks on Scalability Overall,the study of system topology and scalability in LLM-based MAS reveals a spectrum of design choices—from staticconfigurations that ofer simplicity and predictabilityto dynamic architecures that provide flexibility and adaptability.While foundational works (e.g.,[1028],[1038])emphasize scalable graph structures and self-organizing principles,thepracticaladvancesdemonstrated by AgentScope,Project Sid,andAgentSocietyilustratehowintegrateddistributedframeworks,concurentprocessingandrealisticenviment simulations can collectively address the challenges of scaling multi-agent systems.Thecontext-dependent nature of scalability requirements—contrasting between task-solving and simulation scenarios—highlights the importance of purpose-specific design in multi-agent architectures.As research continues to evolve, the development of more sophisticated adaptive algorithms,distributed architectures, and multi-dimensional evaluation frameworks will be essential for advancing the scalability and practical viability of LLM-based multi-agent systems.",
          "index": 8,
          "part": 0,
          "translated_content": "关于可扩展性的总结 总的来说，基于LLM的多Agent系统中对系统拓扑结构和可扩展性的研究揭示了一系列设计选择——从提供简单性和可预测性的静态配置到提供灵活性和适应性的动态架构。虽然基础性作品（例如[1028]，[1038]）强调可扩展的图结构和自组织原则，但AgentScope、Project Sid和AgentSociety所展示的实际进展表明，集成的分布式框架、并行处理和逼真的环境模拟共同应对了扩展多Agent系统的挑战。可扩展性需求的依赖于上下文的特性——在任务解决和模拟场景之间的对比突显了多Agent架构中目的特定设计的重要性。随着研究的不断发展，更复杂的自适应算法、分布式架构和多维度评估框架的发展将对推进基于LLM的多Agent系统的可扩展性和实际可行性至关重要。"
        }
      ],
      "raw_title": "Scalability Considerations",
      "type": null,
      "children": [],
      "translated_title": "14.2 可扩展性考虑"
    },
    {
      "title": "Collaboration Paradigms and Collaborative Mechanisms",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "In this chapter, weofferadetailed exploration of thesepurposefulinteractions,examining how one agentinfluences collaboration within MAS.Wereference the diverse interaction behaviors that emerge from human social structures, further explaining multi-agentcollaboration through interaction purposes,interaction forms,andtherelationshipsthat form.\n\nMulti-Agent Systems (MAS)comprise multiple agents that interact in a shared environment, autonomously making decisions to accomplish tasks colaboratively orcompete with eachother[1041].In our context, we focus on collaborative phenomenons because they widely appeared in most practical applications.Basicalleach agent in MAS is equipped with different roles and initial knowledge and its own set of goals.\n\nWhen engaged inproblem solving orcommunication,agents interact withotheragentsortheenvironment tocollect and process information, independentlymaking decisions based on theirobjectives,existing knowledge,andobservations, and subsequentlyexecuting actions[975,1041,1042,1043].Knowledge, memory,and environmentalobservations form the agents'beliefs,while varying motivations influence their approach to tasks and decision making [1041]. Consequently,effctive problem solving requires diverse purposeful interactions,including agent-agent and agentenvironment.These interactions may involve multiple rounds and occur in various directions,dependingonthe system design.",
          "index": 0,
          "part": 0,
          "translated_content": "在本章中，我们对这些有目的的相互作用进行了详细探讨，研究了一个代理如何影响多代理系统内的协作。我们参考了从人类社会结构中产生的多样化互动行为，进一步通过互动目的、互动形式和形成的关系来解释多智能体协作。\n\n多智能体系统（MAS）包括在共享环境中相互作用的多个智能体，它们自主做出决策以协作完成任务或相互竞争[1041]。在我们的背景下，我们关注协作现象，因为它们广泛出现在大多数实际应用中。基本上，MAS中的每个智能体都配备有不同的角色和初始知识，以及自己的一套目标。\n\n在解决问题或进行通信时，智能体与其他智能体或环境进行互动，收集和处理信息，根据其目标、现有知识和观察独立做出决策，然后执行行动[975,1041,1042,1043]。知识、记忆和环境观察形成了智能体的信念，而不同的动机影响了它们对任务和决策的方法[1041]。因此，有效的问题解决需要多样化的有目的互动，包括智能体之间的互动和智能体与环境的互动。这些互动可能涉及多轮次，并且根据系统设计以不同方向发生。"
        }
      ],
      "raw_title": "Collaboration Paradigms and Collaborative Mechanisms",
      "type": null,
      "children": [],
      "translated_title": "合作范式与协作机制"
    },
    {
      "title": "15.1 Agent-Agent collaboration",
      "number": "15.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Considering thecategorizations of MAScollaborations,we focus on moredetailson thegranularity neededto capture the nuanceddynamics incomplexmulti-agent interactions.Sepecifically,wecategorize inter-agent interactions into four types,nspired by sociologicalinsights from human-to-humaninteraction patterns and applying them to agent-agent interactions in MAS.Sociological theories on human interaction, which include consensus building,skillarning, teaching, and task division collaboration,providea more refined way of clasifying agents.interactions.These interactions form collaborative paradigms, which enable diverse intellgent agents to work together effctively in solving complex problems, and they are shaped by various forms of goals,contexts and outcomes.Each paradigm addresses unique challenges related tocooperation,competition,coordination, anddecision-making.Additionally,MAS implementations involveagents with diferenttypes of interactions,ratherthanasingle typeorunidirectional process, formingcomplexinteraction networksthatevolveover time.Incollaborativesoftwaredevelopment[626,627],asenior developer agent may interact task-wise with an architect agent, guide junior agents through multi-round dialogues. They work together oncodereviews fordecision-making andlearn witha testingexpert agent toimprove test coverage. Examining theobjectivesandresultsofthese interactions revealsthecrucial techniques andtechnologies shaping agent behavior and decision-making, thereby enhancing our comprehension of multi-agent dynamics.",
          "index": 0,
          "part": 0,
          "translated_content": "考虑到多智能体系统（MAS）协作的分类，我们更详细地关注捕捉复杂多智能体相互作用中微妙动态所需的粒度。具体而言，我们将智能体间的相互作用分为四种类型，受到人际互动模式的社会学洞见的启发，并将其应用于MAS中的智能体间相互作用。人际互动的社会学理论，包括共识建立、技能学习、教学和任务分工合作，提供了一种更精细的分类智能体相互作用的方式。这些相互作用形成了协作范式，使各种智能体能够有效地共同解决复杂问题，并受到各种目标、背景和结果的影响。每种范式都涉及与合作、竞争、协调和决策相关的独特挑战。此外，MAS实现涉及具有不同类型相互作用的智能体，而不是单一类型或单向过程，形成随时间演变的复杂相互作用网络。在协作软件开发中，一位资深开发人员智能体可能会按任务与架构师智能体进行互动，通过多轮对话指导初级智能体。他们共同进行代码审查以做出决策，并与测试专家智能体学习以提高测试覆盖率。审视这些相互作用的目标和结果揭示了塑造智能体行为和决策的关键技术和技术，从而增强我们对多智能体动态的理解。"
        },
        {
          "type": "text",
          "content": "Consensus-oriented Interaction Consensus-oriented interactions concentrate on harmonizing the MAS's final target via negotiation,voting,and socialchoice frameworks[1044].Thisinteraction is significant for incorporating diverse knowledge and ensuring agents shift their views towards a unified understanding to achieve consensus [1045]. In this interaction,agents integrate knowledge to establish aunified understanding,whichlargely helps joint decisionmaking in complex problem-solving situations that demand diffrent viewpoints.For instance, MedAgents [922], MDAgents [1046], and AI Hospital[1036] demonstrate how collaborative dialogue among multidisciplinary agents improves problem solving by sharpening reasoning skills and accessing inherent knowledge.",
          "index": 1,
          "part": 0,
          "translated_content": "共识导向互动\n共识导向互动集中于通过谈判、投票和社会选择框架来协调多智能体系统（MAS）的最终目标[1044]。这种互动对于整合不同知识、确保智能体转变观点以达成共识至关重要[1045]。在这种互动中，智能体整合知识以建立统一理解，这在需要不同观点的复杂问题解决情境中极大地有助于联合决策。例如，MedAgents [922]、MDAgents [1046] 和 AI Hospital [1036] 展示了跨学科智能体之间的协作对问题解决的改进，通过提升推理技能和获取固有知识来提高问题解决能力。"
        },
        {
          "type": "figure",
          "src": "images/490ece6305c26c4dff52c55417e33b54872fb4347d02379d38b7859d56a45e4e.jpg",
          "alt": "",
          "caption": "Figure 15.1: Anoverview offour agent-agentcollboration types in LLM-based MAS:Consensus-oriented,Collborative Learning,Teaching/Mentoring,and Task-oriented.Each type is described alongfour keydimensions: information flow, collaboration purpose, knowledge integration, and output focus.",
          "index": 2,
          "part": 0,
          "translated_caption": "图15.1：基于LLM的MAS中四种代理-代理协作类型的概述：以共识为导向、协作学习、教学/指导和任务为导向。每种类型都沿着四个关键维度进行描述：信息流动、协作目的、知识整合和输出重点。"
        },
        {
          "type": "text",
          "content": "These dialogues allow agents to ensemble expertise intocoherent outcomes,frequentlyoutperforming conventional methods like zero-shot orfew-shotreasoning.The importance ofconsensus-driven teamwork is particularly evident in scientificenvironments,where addressing complexchallengesrequires diverse perspectives and meticulous validation Agent Laboratory[746],serves as an example where PhD and postdoctoral agents colaborate to agree on research objectives,interpret experiments,andconsolidate research findings.Similarly, VirutalLab[752]organizeaseriesof team toconducts scientificresearch, where allagents discussa scientific agenda, and individual meetings,where an agent accomplishes a specific task.",
          "index": 3,
          "part": 0,
          "translated_content": "这些对话使智能体将专业知识整合到一致的结果中，通常优于传统方法，如零-shot或少-shot推理。共识驱动的团队合作的重要性在科学环境中尤为明显，在这些环境中，解决复杂挑战需要多元视角和细致验证。Agent Laboratory[746]就是一个例子，博士和博士后智能体合作达成研究目标，解释实验，并整合研究结果。类似地，VirtualLab[752]组织了一系列团队进行科学研究，所有智能体讨论科学议程，并进行个体会议，其中一个智能体完成特定任务。"
        },
        {
          "type": "text",
          "content": "Method for multi-agent consensus typicall include severalapproaches,including Discussing,debating, negotiating, reflecting, and voting.Common methods for reaching consensus encompass an array of structured techniques. The primary mechanisms involved are discussing,debating,negotiating,reflecting,and voting.Debates allow agent to obtain competing hypotheses,while negotiation helps resolveconflicting priorities andresource limitations.Specific frameworks have been created to support these consensus-building activities.During these processes,agents gather output from peers tackling thesame issue,and include environmentalfeedback asnumericaldataandcontextual details. These interactions enable agents to share viewpoints,assumptions, and progressively achieveacommon understanding.",
          "index": 4,
          "part": 0,
          "translated_content": "多智能体共识方法通常包括几种途径，包括讨论、辩论、协商、反思和投票。达成共识的常见方法包括一系列结构化技术。涉及的主要机制包括讨论、辩论、协商、反思和投票。辩论使智能体获得竞争性假设，而协商有助于解决冲突的优先事项和资源限制。已经创建了特定框架来支持这些共识建立活动。在这些过程中，智能体收集来自解决相同问题的同行的输出，并将环境反馈作为数值数据和情境细节包括其中。这些互动使智能体能够分享观点、假设，并逐步达成共同理解。"
        },
        {
          "type": "text",
          "content": "For example, GPTSwarm[651] formulates the collaboration between agents with graph design, that the information fow and edge connections build the basicgroupdiscusson.In GPTSwarm,ifanagentconsistently provides incoect opinions, it willbe excluded.RECONCILE[918] uses a round-table discussion format with several discussion cycles and voting systems based onconfidence levels. It integrates reflection by learning from past discussons, using confidence metrics and human insights to improve their responses.Furthermore,debates are quite important for achieving agreement, reducing hallucinations and also addressing complex issues [985,1047,1031,1003]. In GOVSIM[1048],agents collaborate toachievea balance, and it suggests using a sharedresource andconserving it for future needs. The negotiations went beyond simple information exchange and relationship-focused interactions. The Multi-Agent Debate (MAD)framework [1031] promotes creative thinking by having agents deliver arguments in a“tit-for-tat\"patern, with a judge overseeing the process tofinalizeasolution.The Formal Debate framework (FORD)[1004] enhances consistency among language models through organized debates,enabling stronger models to steer consensus,while weakerones adjust their perspectives.Similarly,AutoAgents[30]defineacollaborative refinement action inwhicheachagentupdates itschatrecord.Inthe process,italsoappends the previous statementsof the other agent and refines its action to achieve consensus.",
          "index": 5,
          "part": 0,
          "translated_content": "例如，GPTSwarm使用图形设计来规定智能体之间的协作，信息流和边缘连接构建了基本的团体讨论。在GPTSwarm中，如果一个智能体持续提供错误意见，它将被排除在外。RECONCILE采用圆桌讨论形式，包括多个讨论周期和基于信心水平的投票系统。它通过学习过去讨论的经验，利用信心度量和人类见解来改进其响应，融入了反思。此外，辩论对于达成一致、减少幻觉以及解决复杂问题非常重要。在GOVSIM中，智能体合作以实现平衡，并建议使用共享资源并为未来需求保存。谈判超越了简单的信息交流和关系为中心的互动。多智能体辩论（MAD）框架通过让智能体以“以牙还牙”的模式提出论点来促进创造性思维，由一位评委监督该过程以最终确定解决方案。正式辩论框架（FORD）通过组织辩论增强了语言模型之间的一致性，使更强大的模型引导共识，而较弱的模型则调整其观点。类似地，AutoAgents定义了一种协作的改进行动，其中每个智能体更新其聊天记录。在这个过程中，它还附加了另一个智能体的先前陈述，并完善其行动以达成共识。"
        },
        {
          "type": "text",
          "content": "Collaborative Learning Interaction In collaborative learning,interaction usually happens among similar agents. Although architecturaly alike, accumulate distinctmemoriesand experiences duetotheir unique behaviors and varied environmental interactions.By solving problems together, these agents share experiences to boost their strategy learning,task-solving,and skillaquisitioncapabilities.Overtime,each agent enhances itsskillsthrough ongoing interaction,leading totheevolution of individuals.Thekeydiffrence betweencollaborativelearningandconsensusoriented interactions lies in their fundamental goals and processes.Whileconsensus-oriented interaction focuses on knowledge integration andbeliefalignmentthroughsynthesizing diverse viewpoints toreachagreement,collaborative learning interaction emphasizes peer knowledge construction and experience sharing,prioritizing mutualimprovement and individual growth.When engaged in colaborative learning interaction,agents update their context or memory from observing others'behavior.For example, agents can learn optimal strategies by observing the deliveration from peers,adapting their own approach based on these observations without necessarily agreeing ona single“best\" strategy[961,962,963,971,965,967,972968,969].As highlighted in[966],the efective discussion tactics significantly impactlearning outcomesamong agents. In these interactions,agents collaborate tolearnand address problems,focusing on mutual understanding and enhancement rather than reaching unanimous decisions.This method refines personal responses and knowledge via ongoing feedback.",
          "index": 6,
          "part": 0,
          "translated_content": "在协作学习中，交互通常发生在相似的智能体之间。尽管在架构上相似，但由于其独特的行为和不同的环境交互，它们积累了不同的记忆和经验。通过共同解决问题，这些智能体共享经验，以提升它们的策略学习、任务解决和技能习得能力。随着时间的推移，每个智能体通过持续的互动增强其技能，导致个体的进化。协作学习和以共识为导向的互动之间的关键区别在于它们的基本目标和过程。共识导向的互动侧重于通过合成不同观点以达成一致来实现知识整合和信念调整，而协作学习互动强调同行知识构建和经验分享，优先考虑相互改进和个体成长。参与协作学习互动时，智能体会通过观察他人的行为更新其背景或记忆。例如，智能体可以通过观察同行的讨论学习到最佳策略，并根据这些观察调整自己的方法，而不一定要达成单一的“最佳”策略。正如在文献中所强调的，有效的讨论策略显著影响智能体之间的学习结果。在这些互动中，智能体合作学习并解决问题，侧重于相互理解和增强，而不是达成一致的决定。这种方法通过持续的反馈完善个人的响应和知识。"
        },
        {
          "type": "text",
          "content": "The methods commonly employed in collborative learning interaction include:1). Experience sharing. , Agents exchange personalinsights and best practices. As described in[303],iterative experience refinement enables LLM agents to achieve adaptive improvement in software development via continual acquisition and utilization of team experience in successve pattern and the cumulative pattern. Furthermore, MAS-CTC[301l is a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions andcommunicate with their insights in across-team collaboration environment. Itenables different teams to concurrently propose various task-oriented decisionsas insights,and then communicate for insights interchange in important phases (multi-team aggregation).",
          "index": 7,
          "part": 0,
          "translated_content": "在协作学习互动中常用的方法包括：1). 经验分享。智能体交换个人见解和最佳实践。正如[303]中所描述的，通过迭代经验的完善，LLM智能体通过连续获取和利用团队经验在软件开发中实现自适应改进，形成连续模式和累积模式。此外，MAS-CTC[301]是一个可扩展的多团队框架，使得编排的团队能够共同提出各种决策，并在跨团队协作环境中交流见解。它使不同团队能够同时提出各种面向任务的决策作为见解，并在重要阶段进行见解交换（多团队聚合）。"
        },
        {
          "type": "text",
          "content": "Dierent agent teams utilize a greedy pruning mechanism andaggregation mechanisms to eliminate low-quality content, thus improve the performance in software development. Differently, in MOBA[1049], a novel MLLM-based mobile multi-agent system,globalagentreflects onlocalagent executionresults to support adaptive planning toalign withthe environment. AutoAgents [1030] employs a knowledge sharing mechanism where agents exchange execution results to enhance communication and feedback, where agents can obtain long-term,short-term and dynamic memory from others. 2).",
          "index": 7,
          "part": 1,
          "translated_content": "不同的智能体团队利用贪婪修剪机制和聚合机制来消除低质量内容，从而提高软件开发的性能。与此不同，在MOBA[1049]中，一种基于MLLM的新型移动多智能体系统，全局智能体根据本地智能体执行结果进行反思，以支持与环境对齐的自适应规划。AutoAgents [1030]采用知识共享机制，智能体交换执行结果以增强沟通和反馈，智能体可以从其他智能体获取长期、短期和动态记忆。"
        },
        {
          "type": "text",
          "content": "Peer discussions. Peer discussions allow agentsto articulate their reasoning processes and learn from others\" approaches. MEDCO[923]create a dynamic environment where clinical reasoning and decision-making skillsare strengthened through collborative problem-solving among student agents. Moreover, In[105o],agents engage in structured peer discussions after initializing their output,reviewing each other's reasoning step by step. Through feedback exchange andconfidence scoring,agents refine their decision-making,learn from diverse approaches, and iterativelyenhancetheir reasoning accuracy,fostering collaborative knowledge acquisition. 3). Observational learning. Observationallearning occurs when agents monitor others'behaviors and outcomes to inform their own strategies. AgentCourt [1051]develops lawyer agents that participate incourt debates andimprove through accumulated experiences, demonstrating improved reasoning and consistency through experiential learning.",
          "index": 7,
          "part": 2,
          "translated_content": "同行讨论。同行讨论使智能体能够表达其推理过程并学习他人的方法。MEDCO[923]创建了一个动态环境，通过学生智能体之间的协作问题解决来加强临床推理和决策技能。此外，在In[1050]中，智能体在初始化其输出后参与结构化的同行讨论，逐步审查彼此的推理过程。通过反馈交流和信心评分，智能体完善其决策制定，从不同方法中学习，并通过迭代增强其推理准确性，促进协作知识获取。3)。观察学习。观察学习发生在智能体监测他人行为和结果以指导其自身策略的情况下。AgentCourt [1051]开发了参与法庭辩论的律师智能体，并通过积累经验不断改进，展示了通过经验学习获得改进的推理和一致性。"
        },
        {
          "type": "text",
          "content": "In iAgents [1046], the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novelagent reasoning mechanism, InfoNav, to navigate agents'communication towards effective information exchange. Together with InfoNav,iAgentsorganizes human information in a mixed memory to provide agents with accurate andcomprehensive information for exchange. Additional experimental phenomenon indicates diffculty of certain tasks making agents continuously refine their strategies in pursuit of the required information. MARBLE[948]designs acognitive evolve planning combining the“expectation'of the agent andits actualactionresults toupdate the overallplanning experience for better planning in the next round.",
          "index": 7,
          "part": 3,
          "translated_content": "在iAgents[1046]中，人类社交网络在智能体网络中得到了反映，智能体主动交换人类任务解决所需的信息，从而克服信息不对称。iAgents采用了一种新颖的智能体推理机制InfoNav，将智能体的通信引导向有效的信息交换。与InfoNav一起，iAgents将人类信息组织在混合记忆中，为智能体提供准确和全面的信息以进行交换。额外的实验现象表明，某些任务的困难使智能体不断完善他们的策略，以追求所需的信息。MARBLE[948]设计了一个认知演进规划，结合智能体的“期望”和实际行动结果来更新下一轮规划的整体经验，以实现更好的规划。"
        },
        {
          "type": "text",
          "content": "Despite itsbenefits,collaborativelearning interaction faces severalchallenges.These include ensuring equitable knowledge exchange among agents with varying capabilities,preventingthepropagationof errorsor biases acrossthe system, maintaining agent diversity while facilitating learning,and developing efective mechanisms for agents toselectively incorporate others'knowledgebased onrelevance andreliability.Overcoming thesechallenges requires the meticulous creation of interaction frameworks and learning strategies.And it should balance individual advancement with the broaderdevelopment of the system.Although issues suchasknowledge fairnes,bias propagation,and scalability present diffculties, there is great potential to improve MAS,particularly indynamic and complex environments.By using iterative learning processes andproviding opportunities,collaborative learning enables agents todevelop richer knowledge bases and more refined problem-solving abilities.",
          "index": 8,
          "part": 0,
          "translated_content": "尽管协作学习交互具有诸多优势，但也面临一些挑战。其中包括确保在具有不同能力的智能体之间实现公平的知识交流，防止错误或偏见在系统中的传播，保持智能体多样性同时促进学习，并开发有效的机制，使智能体能够基于相关性和可靠性有选择性地吸收他人的知识。要克服这些挑战，需要精心创建交互框架和学习策略，并在个体进步与系统整体发展之间取得平衡。尽管知识公平、偏见传播和可扩展性等问题存在困难，但在多样化和复杂环境中，提升多智能体系统的潜力是巨大的。通过使用迭代学习过程并提供机会，协作学习使智能体能够发展更丰富的知识库和更精细的问题解决能力。"
        },
        {
          "type": "text",
          "content": "Teaching/Mentoring Interaction To tackle these challenges,it is important tocarefully develop interaction protocols and learning frameworks that harmonize individual development with overall system progress. In thecontext of MAS, teaching andmentoring interactions arefundamentalmechanisms incollaborativeenvironmentsespeciallinscenarios where knowledge transfer is essential for growth andcollective intelligence.Unlike collaborative learning,where knowledge is exchanged reciprocall among agents,eaching and mentoring interactions focus on the unidirectional flow of knowledge from an experienced agent to a less experienced one.The mechanisms and methods used in teaching/mentoring interactions include several key strategies:",
          "index": 9,
          "part": 0,
          "translated_content": "教学/指导交互 为了解决这些挑战，重要的是精心制定交互协议和学习框架，使个体发展与整体系统进步相协调。在多智能体系统背景下，教学和指导交互是协作环境中的基本机制，特别是在知识传递对于增长和集体智能至关重要的场景中。与协作学习不同，其中知识在智能体之间相互交换，教学和指导交互侧重于知识从经验丰富的智能体向经验较少的智能体的单向流动。教学/指导交互中使用的机制和方法包括几个关键策略："
        },
        {
          "type": "text",
          "content": "·Criticism and Feedback.The mentor agent evaluates the learner's performance and provides corrective or constructive feedback.This helps the learner refine their knowledge and skills through a feedback loop where they update their internal knowledge based on the feedback received.\n·Evaluation. Mentors assess the learner's capabilities or progress through performance reviews and clear assessment criteria, providing valuable insights for development.\n·Instruction and Teaching. Mentors convey targeted knowledge, guidelines, or techniques using direct instruction which allow learners to pose questions and receive clarifications.",
          "index": 10,
          "part": 0,
          "translated_content": "·批评与反馈。导师智能体评估学习者的表现并提供纠正性或建设性反馈。这有助于学习者通过反馈循环完善他们的知识和技能，他们会根据收到的反馈更新内部知识。\n·评估。导师通过绩效评估和清晰的评估标准评估学习者的能力或进展，为发展提供宝贵的见解。\n·指导和教学。导师使用直接教导传达有针对性的知识、指导或技巧，使学习者能够提出问题并获得澄清。"
        },
        {
          "type": "text",
          "content": "Iterative Teaching and ReinforcementTeaching is typicaly progressive,where each phase provides opportunities for thelearner tocomplete tasks and get feedback. For example,inthe MEDCO system[923],student agents improve their professionalskillthroughacyclic practice-orientedlearning approachdirected byexpert mentors,inadditionto engaging in peer discussions.These expertagents conduct ongoing assessments and provide real-time guidance on clinical competencies,focusing on patient interaction skill and diagnostic reasoning.[921] shows that an agentic doctor can continually improve their diagnosis by merelyinteracting with agentic patients in a simulated hospital and can transfer its learned knowledge of real-world cases.",
          "index": 11,
          "part": 0,
          "translated_content": "迭代式教学与强化教学通常是渐进的，每个阶段都为学习者提供完成任务并获得反馈的机会。例如，在MEDCO系统中，学生智能体通过循环实践导向学习方法来提高其专业技能，这一方法由专家导师指导，同时还参与同行讨论。这些专家智能体进行持续评估，并就临床能力提供实时指导，重点关注患者互动技能和诊断推理。研究表明，一个智能医生可以通过与仿真医院中的智能患者互动来持续改进诊断，并将其学到的知识应用于真实案例中。"
        },
        {
          "type": "text",
          "content": "Thisinteractiontype can be categorized basedonthedirection of knowledge transfer into two primary types: unidirectional and interactive.Unidirectionalisrooted intraditionalteaching models where knowledge fowsfromthe teacher to the student.This approach emphasizes thetransmission offacts andconcepts,often involving lectures and direct instructions [923].\n\nTask-oriented Interaction. Task-orientedcollaborations involve agents working togetherto achieve common objectives through effectivecoordination and task decomposition strategies,aswellas ahighdegree of cooperation and cordination.Agents interact primarily by processing upstream output and generating results for downstream agents following established task dependencies rather than engaging in complex discussions or debates.\n\nRecent frameworks demonstrate diverse implementations of this interaction patern: (1) software development frameworks such as MetaGPT[626] and ChatDev[627],agents operate ina structured pipeline that mirrorsthe software development lifecycle.Forexample,architect agents process requirements to generatetechnicalspecifications, which development agents then use to producecode,followed bytesting agents who validate the implementations; (2) Collaborative reasoning frameworks like Exchange-of-Thought (EoT)[1052],GPTSwarm[651], MACNET [1028] involve structuring agents ina specific format (e.g.,rng,tree, directedacrylic graphs,optimizable graphs),which mitigates context expansion risks by ensuring onlyoptimized solutions progressthrough the sequence,and enforcing multiple agents tocollaborate together towards solvingcomplex mathematical or knowledge reasoning tasks; In (3)ML applications[05319],agentsadhere tostringent workflowstructures,eachfulfiling specifictasks in proceses. For more complex tasks such as VideoQA,the TraveLER framework[1054] showcases modular task breakdown across structured phases(Traverse, Locate,Evaluate, and Replan),with aPlanner agent managing interactions and improving strategies based on iterative agent inputs.",
          "index": 12,
          "part": 0,
          "translated_content": "这种互动类型可以根据知识传递方向分为两种主要类型：单向和互动式。单向类型植根于传统教学模式，其中知识从教师传递给学生。这种方法强调事实和概念的传递，通常涉及讲座和直接指导。\n\n任务导向互动。任务导向的合作涉及代理通过有效的协调和任务分解策略共同实现共同目标，以及高度的合作和协调。代理主要通过处理上游输出并根据已建立的任务依赖关系为下游代理生成结果来互动，而不是进行复杂的讨论或辩论。\n\n最近的框架展示了这种互动模式的多样实现：(1) 软件开发框架，如MetaGPT和ChatDev，代理在一个结构化的流水线中运行，这反映了软件开发的生命周期。例如，架构代理处理需求以生成技术规范，然后开发代理使用这些规范生成代码，随后测试代理验证实现；(2) 协作推理框架，如Exchange-of-Thought（EoT）、GPTSwarm、MACNET，涉及以特定格式（例如，图、树、有向丙烯酸图、可优化图）组织代理，通过确保只有优化的解决方案通过序列，减轻了上下文扩展风险，并强制多个代理共同协作解决复杂的数学或知识推理任务；在(3) 机器学习应用中，代理遵循严格的工作流结构，每个代理在流程中执行特定任务。对于更复杂的任务，如视频问答（VideoQA），TraveLER框架展示了跨结构化阶段的模块化任务分解（遍历、定位、评估和重新规划），其中规划代理管理互动并根据迭代代理输入改进策略。"
        },
        {
          "type": "text",
          "content": "These handoff relyon explicit deliverables instead of direct agent negotiations.Inspired by GPTSwarm[651]-alike graph agentic systems, MACNET [1028] structures agents into directed acyclic graphs (DAG).Here, supervisory figures issuedirectives while executors implementsolutions.By ensuring only optimized solutions progress through the sequence,thissetupmitigatescontext expansionrisks.InMLapplications[053,019]agents adhere to stringent workflow structures,eachfulfiling specifictasks inprocesses.For more complextaskssuchasVideoQA,theTraveLER framework[1054] showcases modular task breakdown acrosstructured phases (Traverse, Locate, Evaluate,and Replan), with a Planner agent managing interactions and improving strategies based on iterative agent inputs.",
          "index": 13,
          "part": 0,
          "translated_content": "这些交接依赖于明确的交付成果，而不是直接的代理协商。受到类似GPTSwarm的图代理系统的启发，MACNET将代理结构化为有向无环图（DAG）。在这里，监督人员发布指令，执行者实施解决方案。通过确保只有优化的解决方案通过序列，这种设置减轻了上下文扩展风险。在机器学习应用中，代理遵循严格的工作流结构，每个代理在流程中执行特定任务。对于更复杂的任务，如视频问答（VideoQA），TraveLER框架展示了跨结构化阶段的模块化任务分解（遍历、定位、评估和重新规划），其中规划代理管理互动并根据迭代代理输入改进策略。"
        },
        {
          "type": "text",
          "content": "Beyond organized development, task-driven interactions have been shown in open-ended contexts such as Minecraft game,in where agents adjust to ever-changing environments.In[927],leader agents manage workflows by breaking down complexobjectives intospecifictasks,while executoragents perform actionsike gathering resources.Coordination mechanisms are important forensuring agentscollaborate effectivelytowardsfinal goal, includingcommunication protocols,synchronization strategies,andresource-sharing techniques.The interactionof agents inMASfortaskexecution has garmered significant interest, notably through utilizing LLMs forhandling intricate tasks and workfows.The collaboration ofagents are vitalfortaskcompletion,particularly inever-changingsettings likesoftware development and project management [626, 630].",
          "index": 14,
          "part": 0,
          "translated_content": "除了有组织的发展之外，在开放式环境中如Minecraft游戏中已经展示了任务驱动的交互，代理根据不断变化的环境进行调整。在[927]中，领导代理通过将复杂目标分解为具体任务来管理工作流程，而执行代理执行动作如收集资源。协调机制对确保代理有效协作达到最终目标非常重要，包括通信协议、同步策略和资源共享技术。在多智能体系统中，代理的相互作用对任务执行引起了极大兴趣，特别是通过利用LLMs处理复杂任务和工作流程。代理之间的合作对任务完成至关重要，特别是在诸如软件开发和项目管理等不断变化的环境中[626, 630]。"
        }
      ],
      "raw_title": "Agent-Agent collaboration",
      "type": null,
      "children": [],
      "translated_title": "15.1 代理-代理协作"
    },
    {
      "title": "15.2 Human-AI Collaboration",
      "number": "15.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "To unlock the potential of MAS in meeting human objectives,people often work alongside them using three primary methods: one-of task delegation, multi-turn interactive instruction, and immersive human-agent collaboration\n\nIn one-off task delegation,humans delegate single-instance tasks toMAS,suchas posing a question toaQ&A platform or assigning acoding task[1055,626].Without additional input, theagenthandlesthetaskautonomously,delivering a complete response or solution ina singlereply.This is presentlytheprevalent wayhumanscolaborate withLM-based agents [922, 627, 31].\n\nFor multi-turn interactive instruction,humans engage in iterative interactions with LLM-based agent systems to refine andexplore solutions untilasatisfactoryresult is achieved.This typeof interaction iswidelyseen in creative applications,suchas image editing or writingedit[938].Forinstance,auser might ask the system to add an object to a specific location in an image,replace an element,changethe background,orreviseapart in asentence.These interactions often span multiple rounds, with users continuously refining their requests until the desired outcome is reached.Moreover,certain other LLM-based agent systems may require human approval or clarification during multi-turn interactions before proceeding to the next step[1056,930]. Under human guidance,these LLM-based agent systems can complete household tasks as well as software development tasks.",
          "index": 0,
          "part": 0,
          "translated_content": "为了发挥多智能体系统（MAS）在实现人类目标方面的潜力，人们通常使用三种主要方法与它们共同工作：一次性任务委派、多轮交互式指导和沉浸式人-智能体协作。\n\n在一次性任务委派中，人们将单个任务委派给MAS，例如向问答平台提问或分配编码任务。在没有额外输入的情况下，智能体自主处理任务，在单次回复中提供完整的回答或解决方案。这是目前人类与基于LM的智能体合作的主要方式。\n\n在多轮交互式指导中，人们与基于LM的智能体系统进行迭代交互，直到达到满意的结果为止。这种类型的交互在创意应用中广泛存在，例如图像编辑或编辑写作。例如，用户可能要求系统在图像的特定位置添加一个对象，替换一个元素，更改背景，或修改句子中的某一部分。这些交互通常跨越多个轮次，用户不断完善他们的请求，直到达到期望的结果。此外，在多轮交互过程中，某些其他基于LM的智能体系统可能需要人类批准或澄清才能继续下一步。在人类指导下，这些基于LM的智能体系统可以完成家庭任务以及软件开发任务。"
        },
        {
          "type": "text",
          "content": "Immersive human-agent colaboration features LLM-based agents simulating human behaviors to serve as partners. For instance,inan immersive setting,humanstreatthese agents as teammates,achievingcommon objectives.Instances include agents representing humans in meetings orhelp solve tasks likechores orprojects.This strategy highlights effective integration and teamwork in dynamic contexts [937, 924].\n\nTo assess Human-AI collaboration quantitatively, several frameworks have been suggested. Co-Gym[1057],for instance,measures the communication, situational awareness, and personalizationof LLM-based agents in tasks such as travel planning, writing related work, and tabular analysis.\n\nIn summary, as LLM-based agent systems have advanced,Human-AIcollaboration has diversified to address challenges across domains.This ranges from simple command-based AI interactions for questions,to multi-turn dialogues for design and development, and partnering with human daily tasks.\n\nWith advancements inLLM-based agent systems,they are expected tointegrate more into daily life,streamlining tasks and boosting effciency.Atthe same time, humans willrefine and adapt their waysof interacting with AI, leading to more effectivecollaboration.Webelieve this shift willdrive fundamentalchanges inboth social productivity and the socialrelations ofproduction,reshaping how work is organized and how humans and AIcooperate in the large language models era.",
          "index": 1,
          "part": 0,
          "translated_content": "沉浸式人-智能体协作特点是基于大型语言模型(LLM)的智能体模拟人类行为，充当合作伙伴。例如，在沉浸式环境中，人们将这些智能体视为队友，共同实现目标。实例包括代表人类参加会议或帮助解决家务或项目任务。这种策略突出了在动态环境中的有效整合和团队合作。\n\n为了定量评估人工智能与人类的协作，提出了几种框架。例如，Co-Gym测量了LLM智能体在旅行规划、撰写相关工作和表格分析等任务中的沟通、情境意识和个性化能力。\n\n总之，随着基于LLM的智能体系统的进步，人工智能与人类的协作已经多样化，以解决跨领域的挑战。从简单的基于命令的人工智能问答交互，到用于设计和开发的多轮对话，再到与人类日常任务合作。\n\n随着基于LLM的智能体系统的进步，人们预计它们将更多地融入日常生活，简化任务并提高效率。与此同时，人类将不断完善和调整与人工智能互动的方式，从而实现更有效的协作。我们相信这种转变将推动社会生产力和生产关系的根本变革，重塑工作组织方式以及人类和人工智能在大型语言模型时代的合作方式。"
        }
      ],
      "raw_title": "Human-AI Collaboration",
      "type": null,
      "children": [],
      "translated_title": "15.2 人工智能与人类的协作"
    },
    {
      "title": "15.3 Collaborative Decision-Making",
      "number": "15.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Collaborative decision-making processes are crucial for ensuring theeffcient operation of MAS and the successful completion of tasks.Although collboration itself is a core feature, the approaches of decision-making directly determines theeffectivenessofcollaborationandtheoverallperformanceofthe system.Recentresearchhashighlighted thecriticalroleofcollborative decision-making.[1037]showedthat diverse decision-making methodscan significantly enhance the collaborative eficiency of the system.[649]emphasized that arationaldecision-making mechanismcan stimulate the emergence of intelligence within a system.",
          "index": 0,
          "part": 0,
          "translated_content": "协作决策过程对于确保多代理系统的高效运行和成功完成任务至关重要。尽管协作本身是一个核心特征，但决策方法直接决定了协作的有效性和系统整体性能。最近的研究突出了协作决策的关键作用。[1037]表明，多样化的决策方法可以显著增强系统的协作效率。[649]强调，合理的决策机制可以激发系统内部智能的出现。"
        },
        {
          "type": "text",
          "content": "Froma broader perspective,the collaborative decision-making process can be divided into two majorcategories based on their architectural characteristics: Dictatorial Decision-Making and Collective Decision-Making [1037].\n\nDictatorial Decision-Making. Dictatorial Decision-Making is a process where decision-making relies on a single agent in aMAS.In this paradigm, allagents send their state information orlocalobservations tothisdictatorialagent. The dictatorialagent isresponsibleforassembling this data,studying the core problems,andestablishing definitive decision guidelines.The key principle for such anapproachis toleverage aglobalmindset in moving towards improved decision-making,hence paving the reliabilityof thesystem performancealong withthesuccessfulachievement of task goals.[1031,058,046] demonstratedthe single-agent decision-making process with a single LLM, who synthesized various views on the same problem to make decision-making even more objective and comprehensive.Furthermore, [134,1059]suggestedthe weightedintegration methodthroughranking,scoring orchecklist,enhancing therobustness of decision-making procedures. In addition,beyond the explicit inclusion of perspectives,[1030,06o] proposed architectures where acentralagent breaksdowncomplex tasks into simpler sub-tasks and assigns them to specialized agents grouped bytheirfunctionalities.Moreover,in[651,28],itiscommon that the last nodes agent works inan environment to assemble the past information and deducea conclusion according tothe topological structure,rather than by a central agent.",
          "index": 1,
          "part": 0,
          "translated_content": "从更广泛的角度来看，协作决策过程可以根据其架构特征分为两类主要类别：独裁式决策和集体决策[1037]。\n\n独裁式决策。独裁式决策是多代理系统中决策依赖于单个代理的过程。在这种范式中，所有代理将它们的状态信息或局部观察发送给这个独裁代理。独裁代理负责汇总这些数据，研究核心问题，并建立明确的决策指导方针。这种方法的关键原则是利用全局思维方式，朝着改进决策制定，从而提高系统性能的可靠性以及成功实现任务目标的方向[1031,058,046]。证明了单一代理决策过程中单个LLM的作用，LLM综合了对同一问题的各种观点，使决策更加客观和全面。此外，[134,1059]建议通过排名、评分或清单的加权综合方法，增强决策程序的稳健性。另外，除了明确包含不同观点外，[1030,06o]提出了一种架构，其中一个中心代理将复杂任务分解为更简单的子任务，并将它们分配给按功能分组的专门代理。此外，在[651,28]中，通常最后一个节点代理在环境中工作，根据拓扑结构汇总过去的信息并得出结论，而不是由一个中心代理。"
        },
        {
          "type": "text",
          "content": "Collective Decision-Making.Collective Decision-Making involves agents collaborating to reach decisions without a central authority,relying onlocaldata and interactions like voting or negotiation.This method shares decision-making power among agents,allowing the system to adapt according tochanges while maintaining robustnessand scalability.\n\n·Voting-based Decision MakingVoting systems are important for collective decision-making, providing a framework for reaching consensus.A conclusive majority is achieved through voting as described by [1045, 968]. Moreover, the GEDI electoral module[1037] enables multiple voting methods. This method largely improve reasoning and fault-tolerance while avoiding complex system designs.\n\n· Debate-based Decision MakingIn comparison with voting-based methods, debate-based decision-making focuses on organized interactions between agents, in order to obtain the best result. In[1031,1061],agents participate in guided discussion, where they articulate and proposals in an atempt to resolve disagreements and reconcile points of view. Simultaneously, [1050,1062] practice restraint stance, using communication channels among agents for consensus-building through repeated discussions.To tackle the issue of“cognitive islands,certain systems would employ a common retrieval knowledge base to enable agents to be aware of the same knowledge throughout debates[1005].By mimicking human dialogue, these systems alowed agents to exchange perspectives and make more informed decisions.",
          "index": 2,
          "part": 0,
          "translated_content": "集体决策。集体决策涉及代理之间协作达成决策，没有中央权威，依赖于本地数据和像投票或谈判这样的互动。这种方法在代理之间分享决策权力，允许系统根据变化进行调整，同时保持稳健性和可扩展性。\n\n·基于投票的决策制定。投票系统对于集体决策制定至关重要，提供了达成共识的框架。如[1045,968]所述，通过投票实现确定性多数。此外，GEDI选举模块[1037]实现了多种投票方法。这种方法在提高推理和容错能力的同时，避免了复杂的系统设计。\n\n·基于辩论的决策制定。与基于投票的方法相比，基于辩论的决策制定侧重于代理之间的有组织互动，以获得最佳结果。在[1031,1061]中，代理参与引导性讨论，在那里他们阐述和提出建议，试图解决分歧并调和观点。同时，[1050,1062]采取克制立场，利用代理之间的通信渠道通过反复讨论进行共识建立。为了解决“认知孤岛”问题，某些系统将采用共同的检索知识库，使代理在辩论过程中意识到相同的知识[1005]。通过模仿人类对话，这些系统使代理能够交换观点并做出更明智的决策。"
        },
        {
          "type": "text",
          "content": "Discussionand Future Work Collaboration inmulti-agent systems (MAS)stil faces numerous challenges that require further research.Current methods are largely based on contextually dependent interactions; however,they do not include a specificframework fortraining andoptimizing cooperative actions.This heavy dependence onlarge language models (LLMs)hassomelimitations,astheir effctiveness is inherentlytiedtothe sizeoftheLLM'scontextualwindow andits native reasoning capabilities.WhileLLMs provide asolid foundation forenabling interactions,these systems are still limited by the inherent limitations of context-dependent communication.",
          "index": 3,
          "part": 0,
          "translated_content": "讨论与未来工作在多智能体系统（MAS）中的合作仍然面临着许多需要进一步研究的挑战。当前的方法主要基于情境依赖性互动；然而，它们并没有包括针对训练和优化合作行动的具体框架。对大型语言模型（LLMs）的严重依赖具有一些局限性，因为它们的有效性与LLM的上下文窗口大小及其固有的推理能力密切相关。虽然LLMs为促进互动提供了坚实基础，但这些系统仍然受限于依赖上下文的沟通的固有限制。"
        },
        {
          "type": "text",
          "content": "Future studies should focus onfinding frameworks that inspire agents for activelearning with regard tooptimal timing and information dissemination methodologies. Using methodologies from multi-agent reinforcement learning (MARL), there is a growing requirement for strategies that willhelp agents determine appropriate moments for information sharing,aswellas what information should be shared through whatchannels.Thiscalls for not just devising novel interactionprotocols but alsoincorporating training methodologies that willconstantlyoptimize these protocols with each improvement.",
          "index": 4,
          "part": 0,
          "translated_content": "未来的研究应该专注于找到激励智能体积极学习的框架，特别是关于最佳时机和信息传播方法。利用多智能体强化学习（MARL）的方法，越来越需要帮助智能体确定适当的信息分享时机以及通过哪些渠道分享哪些信息的策略。这不仅需要设计新颖的互动协议，还需要整合培训方法，以便随着每一次改进不断优化这些协议。"
        }
      ],
      "raw_title": "Collaborative Decision-Making",
      "type": null,
      "children": [],
      "translated_title": "15.3 协作决策"
    },
    {
      "title": "Collective Intelligence and Adaptation",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The concept ofcollective intelligence is centraltothedevelopment ofmulti-agent systems(MAS),drawing inspiration from biological and societalcooperation.An inherent concept within collective intellgence isthe“Wisdom of Crowds\" by [915],which asserts that independent communities often make better decisions asa whole than any one person. Cognitive theoreticalmodels like the Society of Mind[17]and itsrelated theorymind[916,917]further supportthe paradigm,suggesting that intelligence springs from a synergy among primary, specialistcomponents.Moreover,In human societies,individualscollaborate,divide labor, and engage incolective problem-solving toaddresscomplex challenges.MAS adopt similar strategies where specialized agents to participate in solving complex problems and collective decision-making [914].",
          "index": 0,
          "part": 0,
          "translated_content": "集体智能的概念对多智能体系统（MAS）的发展至关重要，从生物和社会合作中汲取灵感。集体智能内在的概念是“群体智慧”，它认为独立的群体往往比任何一个人做出更好的决策。认知理论模型如“心智社会”及其相关理论进一步支持这一范式，表明智能源于主要和专业组件之间的协同作用。此外，在人类社会中，个体合作、分工，参与集体问题解决以解决复杂挑战。MAS采用类似的策略，专门的代理参与解决复杂问题和集体决策。"
        },
        {
          "type": "text",
          "content": "Theemrgenceof collective intelligence within MAS is a dynamic and iterative processThroughcontinuous interaction, agents develop a shared understanding andcollective memory progressvely.The interaction dynamics are strengthened by heterogeneity among individual agents,environmental feedback, and agent-agent interactions [914],which are allimportant for the emergenceof complex social networks and improving decision-making strategies.It is worth highlighting thatcolective intellgence is not merelythe summation of individualcapability,butrefers toemergent behavior beyond individualagent capacity.beyond individual agent capacity.Individualagent development is deeply linkedwithcollectiveintellgencegrowth.Withongoinginvolvementwithcollectivetasksandself-reflectionnshaed contexts,agents increasingly developreasoning and decision-making capabilities.The evolution ofindividual agents is closelyrelated tocollective intelligence evolution.Through continuous interaction in joint activities and critical examination of shared contexts, agents continuously refine their reasoning and decision-making abilities.",
          "index": 1,
          "part": 0,
          "translated_content": "多智能体系统内集体智能的出现是一个动态和迭代的过程。通过持续的互动，代理逐渐形成共享理解和集体记忆。个体代理之间的异质性、环境反馈和代理-代理互动加强了互动动态，这些因素对于复杂社会网络的出现和改进决策策略至关重要。值得强调的是，集体智能不仅仅是个体能力的总和，而是指超越个体代理能力的新兴行为。个体代理的发展与集体智能的增长密切相关。通过持续参与集体任务和自我反思共享背景，代理逐渐发展出推理和决策能力。个体代理的演化与集体智能的演化密切相关。通过在联合活动中持续互动和对共享背景的批判性审视，代理不断完善他们的推理和决策能力。"
        },
        {
          "type": "text",
          "content": "In parallcomplexand diversebehavior among agents emerges.These include beyond-restricted-protocolbehaviors, suchas advancedsocialinteractions,includingtrust,strategic deceptionadaptivecamouflage,andemergentcoperation, evoking a shiftfromreactive intocooperative strategies,as wellas deeper social dynamics.With achain of recursive interactions,agents necessarilyformcooperative strategies,which eventuallyturn intosocialcontracts,organizational hierarchies,and divisions oflabor.Social phenomenanecessrilyemerge through recursive interactions among agents, coupled with their adjustment with thechanging environment. It marks a transition from fundamental cooperative behavior into complex social constructs, leading to cultural norms and conventions.",
          "index": 2,
          "part": 0,
          "translated_content": "在多智能体系统中出现了复杂和多样化的行为。这些行为包括超越受限协议的行为，如高级社交互动，包括信任、战略欺骗、适应性伪装和新兴合作，引发了从被动到合作策略的转变，以及更深层次的社会动态。通过一系列递归互动，代理必然形成合作策略，最终演变成社会契约、组织等级和分工。社会现象必然是由代理之间的递归互动以及它们与不断变化的环境的调整相结合而产生的。这标志着从基本合作行为向复杂社会构建的过渡，导致文化规范和惯例的形成。"
        }
      ],
      "raw_title": "Collective Intelligence and Adaptation",
      "type": null,
      "children": [],
      "translated_title": "集体智慧与适应性"
    },
    {
      "title": "16.1 Collective Intelligence",
      "number": "16.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The concept of collective intelligence, which refers tothe ability of a groupof agents to exhibit problem-solving capabilities that surpassthose of individual agents.This phenomenon is often characterized by emergent behaviors, sophisticated decision-making,and higher-orderreasoning abilities thatarise frominteractions among agents,leading to enhanced performance incollaborative decision-making scenarios and socialsimulations[975].[917] demonstrate that LLM-based agents can exhibit collborativebehaviors and high-order Theory of Mindcapabilities,whichare crucial for understandingthe perspectives ofother agents ina shared environment.Theirfindingssuggestthatthe integration of LLMs into MAScan facilitate more sophisticated forms of collective intellgence,thereby improving the overall efficacy of collaborative decision-making.",
          "index": 0,
          "part": 0,
          "translated_content": "集体智能的概念指的是一组智能体表现出超越个体智能体的问题解决能力。这种现象通常以新兴行为、复杂决策和高阶推理能力为特征，这些能力是由智能体之间的相互作用产生的，导致在协作决策场景和社会模拟中表现出增强的性能。研究表明，基于LLM的智能体可以表现出协作行为和高阶心理理论能力，这对于理解共享环境中其他智能体的视角至关重要。他们的研究结果表明，将LLMs整合到多智能体系统中可以促进更复杂形式的集体智能，从而提高协作决策的整体效力。"
        },
        {
          "type": "text",
          "content": "Improved System Performance A primary advantage ofcollective intellgence in MAS is that collaboration leads to superior problem-solving capabilities.Collective intelligence can be encouraged to overcome“groupthink\"and individual cognitive bias in order to allow acolective to cooperate on one process-while achieving enhanced intellectual performance.When individualagents share information and coordinate actions, the system can achieve better results than any single agent operating independently[626,922,1046,1031,1063].Collective intelligence is therefore shared or group intelligence that emergesfrom the collaboration,collectiveefforts,and competitionof many individuals andappears in consensusdecision making.Collective intelligence stronglycontributes to the shiftof knowledge andpowerfromthe individualtothecollective.[924]demonstratedthis throughtheir CooperativeEmbodied Language Agent (CoELA), which achieved a $40\\%$ improvement in efficiency over traditional planning methods in ThreeDWorld multi-agent transport tasks.This substantialimprovement stems from the system's ability toeffectively utilize LLMs for planning and communication in multi-agent settings,providing compelling evidence for enhanced colaborative decision-making capabilities.As previously discussed, the inherentdiversity andinterdisciplinary nature of LLM-based multi-agentsystems,along with various inter-agent interaction, which provide internalfeedback and enrichedcontext for individual decision-making,hence reduce bias and improve the consistency of solution [918].",
          "index": 1,
          "part": 0,
          "translated_content": "多智能体系统中集体智能的改进系统性能的主要优势在于协作导致卓越的问题解决能力。集体智能可以被鼓励以克服“群体思维”和个体认知偏见，从而使集体能够在一个过程上进行合作，同时实现增强的智力表现。当个体智能体分享信息并协调行动时，系统可以取得比任何单个智能体独立操作更好的结果。因此，集体智能是从许多个体的协作、集体努力和竞争中产生的共享或群体智慧，体现在共识决策中。集体智能大大促进了知识和权力从个体到集体的转移。他们通过他们的合作体现语言智能体（CoELA）展示了这一点，在ThreeDWorld多智能体运输任务中，CoELA比传统规划方法提高了40%的效率。这一显著改进源于系统在多智能体环境中有效利用LLM进行规划和沟通的能力，为增强的协作决策能力提供了令人信服的证据。正如前面讨论的，基于LLM的多智能体系统固有的多样性和跨学科性，以及各种智能体之间的互动，提供了内部反馈和丰富的背景信息，从而减少偏见并提高解决方案的一致性。"
        },
        {
          "type": "text",
          "content": "Emergent BehaviorsOne of the most intriguing aspects of collective intellgence is the emergenceof new,complex behaviors that arise spontaneously from agent interactions.These behaviors are notexplicitly programmed but emerge from leaning and adaptation.Asdiscussd invarious studies[971,965,966],agentsdeveloped strategicbehaviors, including trust-building,adversarialtactics,deception,andleadershipduringthegame.Thecollectivebehaviorevolved through experience sharing,where village-aligned agents learned cooperation and strategic alliance formation, and wolf-aligned agents improved deception through“information confusion\"tactics.Moreover,agents optimized voting patternsanddeceptionstrategies without explicit training,which indicates the group intellgenceemerged over multiple roundsofinteractions.SimilarlyintheAvalongame[68]esearchersobservedthatagentsbecamebeteratidentifing and countering deceptive information.Individuals adapted todeceptive environments andrefinedtheir decision-making using first-and second-order perspective shifts.Furthermore,agents demonstrated adaptive cooperation and ad hoc teamwork,despite nopredefinedcollaboration protocols[969].Thesefindings highlighttheabilityofLLM-basedagents to developsophisticated behaviors through interaction andlearning,showcasing the potentialforemergentbehaviors in collective intellgence scenarios.Notably,these emergent behaviors relyon memory andreflective mechanisms.Agents retrieve andreflectonhistoricalinformation togenerateacompact context,enhancingtheirreasoningcapabilities[239]. In MAS,shared context and environmental informationsignificantly boost agents'usable memory.This enables agents to build on past interactions,refine strategies, and adapt more effectively to dynamic environments [1064].",
          "index": 2,
          "part": 0,
          "translated_content": "新兴行为\n集体智能中最引人注目的一个方面是新兴的复杂行为，这些行为是由智能体相互作用自发产生的。这些行为并不是明确编程的，而是通过学习和适应而出现的。正如在各种研究中所讨论的[971,965,966]，智能体发展了战略行为，包括建立信任、对抗策略、欺骗和领导力。集体行为通过共享经验而演变，村庄对齐的智能体学会了合作和战略联盟形成，狼对齐的智能体通过“信息混淆”策略改进了欺骗。此外，智能体在没有明确训练的情况下优化了投票模式和欺骗策略，这表明群体智能是在多轮互动中出现的。同样，在Avalon游戏中[68]，研究人员观察到智能体在识别和对抗欺骗信息方面变得更加优秀。个体适应了欺骗环境，并利用一级和二级视角转变来完善决策。此外，尽管没有预定义的协作协议，智能体展示了自适应合作和临时团队合作[969]。这些发现凸显了基于LLM的智能体通过互动和学习发展复杂行为的能力，展示了集体智能场景中新兴行为的潜力。值得注意的是，这些新兴行为依赖于记忆和反思机制。智能体检索和反思历史信息以生成紧凑的背景，增强他们的推理能力[239]。在多智能体系统中，共享的背景和环境信息显著提升了智能体的可用内存。这使智能体能够建立在过去互动的基础上，完善策略，并更有效地适应动态环境[1064]。"
        },
        {
          "type": "text",
          "content": "Social Evolution One of the most significant findings in the fieldof generativeagent societies isthe spontaneous emergence of social norms.[1o65] demonstrated that agents,throughcontinuous interaction, arecapable ofcreating, representing,spreading,evaluating,andcomplying with social norms.These norms serve asthe foundation for social order, reducing conflicts and improving coordination among agents, thereby leading to more stable and organized societies.Interestingly, thestudyfound that agentsdevelop norms morerapidly intheir beliefs than theydo in their behaviors.This suggests thatwhile agents may quickly internalizecertain norms,the translation of these norms into consistent actions takes longer. Over time,these norms tend to synthesize into more general principles,resulting in more concise and effective personal norm sets.Furthermore, the Project Sid simulation[989] models large-scale agent societies and providesfurtherevidence of the emergenceofsocialnorms androle specialization.In this study, agents were observed to autonomouslyform specialized socialroles.These roles were not predefined but emerged naturally as agents interacted within their environment and developedcollective rules.The simulation alsohighlighted the importanceof democratic processes inthe adherence and modificationofthese collectiverules.Agents were found to engage incultural and religious transmission,spreading ideas and doctrines across communities.This processof norm creationandrole specializationleads tobeterorganization,reducedconflict,andadaptive governance structures within the societyTheevolution ofculturalandreligious beliefs inmulti-agent societies is alsoobservedin[106], which occurs through agent-driven selection of ideas, mirroring real-world societalchanges.Additionall,the[936], which simulates socialinteractions among one millon agents,provides valuable insights into culturaltransmission and group polarization.Cultural memes and belief systems propagate naturally among agent societies.Agents exhibit herd behavior,conforming to prevailing opinions even whentheseopinions are irrational.This leads to the emergence of group polarization,where agents reinforce extreme views through repeated interactions.This finding highlights the significant impact of group size on the dynamics of cultural evolution and social behavior.",
          "index": 3,
          "part": 0,
          "translated_content": "社会演化\n在生成式智能体社会领域最重要的发现之一是社会规范的自发出现。研究表明，通过持续互动，智能体能够创造、表达、传播、评估和遵守社会规范。这些规范作为社会秩序的基础，减少冲突，提高智能体之间的协调，从而导致更加稳定和有组织的社会。有趣的是，研究发现智能体在其信念中比在行为中更快地发展规范。这表明，虽然智能体可能会迅速内化某些规范，但将这些规范转化为一致的行动需要更长的时间。随着时间的推移，这些规范往往会合成更一般的原则，导致更简明有效的个人规范集。此外，Project Sid模拟模型展示了大规模智能体社会中社会规范和角色专业化的出现，并提供了进一步的证据。在这项研究中，观察到智能体自主形成了专业化的社会角色。这些角色并非预定义，而是在智能体在其环境中互动并发展集体规则的过程中自然出现。该模拟还突出了民主过程在遵守和修改这些集体规则中的重要性。发现智能体参与了文化和宗教传播，将想法和信条传播到社区中。这种规范创造和角色专业化的过程导致了更好的组织、减少冲突和社会内适应性治理结构的出现。多智能体社会中文化和宗教信仰的演化也在研究中观察到，这是通过智能体驱动的思想选择，反映了现实世界社会变化。此外，模拟了一百万智能体之间社会互动的研究提供了有关文化传播和群体极化的宝贵见解。文化模因和信仰系统在智能体社会中自然传播。智能体表现出群体行为，遵循主流观点，即使这些观点是不理性的。这导致了群体极化的出现，智能体通过反复互动加强极端观点。这一发现突显了群体规模对文化演化和社会行为动态的重要影响。"
        }
      ],
      "raw_title": "Collective Intelligence",
      "type": null,
      "children": [],
      "translated_title": "16.1 集体智慧"
    },
    {
      "title": "16.2 Individual Adaptability",
      "number": "16.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "In multi-agent systems (MAS),individualadaptabilityrefers to an agent'sabilitytoadjust its behavior anddecisionmaking strategiesbased on previous interactions andexperiences.This isalsodefined asself-evolving,where agents can dynamically self-evolve by modifying themselves,such as altering their initial goals and planning strategies,and training themselves based on feedback orcommunicationlogs[38].This adaptability isfacilitatedbytheintegrationof large language models (LLMs), which support dynamic monitoring and adaptation processes[1067], as wellas the agents\"memorycapabilities andinformation exchange.These modules arecrucialtoensure thatagents cancontinuously improve their performance,respond effectively todynamic environments,andoptimizetheir decision-making processes. We categorize the mechanisms contributing to individualadaptability into memory-based learning and parameter-based learning, where there are training-free and training-based approaches.",
          "index": 0,
          "part": 0,
          "translated_content": "在多智能体系统（MAS）中，个体适应性指的是一个智能体根据先前的互动和经验调整其行为和决策策略的能力。这也被定义为自我进化，智能体可以通过修改自身，如改变其初始目标和规划策略，并根据反馈或通讯日志对自己进行训练，从而动态自我进化。这种适应性得到大型语言模型（LLMs）的支持，这些模型支持动态监控和适应过程，以及智能体的记忆能力和信息交换。这些模块对于确保智能体能够持续改进其性能，有效应对动态环境，并优化其决策过程至关重要。我们将有助于个体适应性的机制归类为基于记忆的学习和基于参数的学习，其中包括无需训练和基于训练的方法。"
        },
        {
          "type": "text",
          "content": "Memory-based learning Memory andreflective mechanisms significantly enhance individual adaptability in LLMbased multi-agent systems byleveraging historicalrecords andexperiences to inform decision-making [221,1068,50]. By maintaining and utilizing individual memory of past interactions,decisions,andoutcomes,the agent can refine its decision-making process over time.This memoryserves as arepository ofexperiences thatthe agentcan drawon when making future decisions. Using this stored knowledge,individual agent isable torefine its decision-making process, learning from previous successes and failures [921,1051]. For example, in clinical simulation,doctor agents can kep improving treatment performance over time by accumulating experience from both successful and unsuccessful cases [921].Insocialbehavior simulation,agentscan improvetheir adaptabilitybyengaging in more complexscenarios and utilizing scenario memories to enhance performance [50].",
          "index": 1,
          "part": 0,
          "translated_content": "基于记忆的学习和反思机制通过利用历史记录和经验来指导决策，显著增强了基于大型语言模型（LLM）的多智能体系统中个体的适应性。通过保持和利用过去互动、决策和结果的个体记忆，智能体可以随着时间的推移优化其决策过程。这种记忆作为经验的仓库，智能体在做出未来决策时可以借鉴。利用这些存储的知识，个体智能体能够优化其决策过程，从以往的成功和失败中学习经验。例如，在临床模拟中，医生智能体可以通过积累来自成功和失败案例的经验，不断提高治疗表现。在社会行为模拟中，智能体可以通过参与更复杂的场景并利用场景记忆来提升性能，从而改善其适应性。"
        },
        {
          "type": "text",
          "content": "Shared memory-based learning In contrast, shared memory-based learning extends thisconcept by enabling multiple agents to exchange information and insights derived fromtheir respective experiences.Rather than relying solelyon individual memory,agentscan benefitfrom the collective knowledge of the group.Bysharing data,strategies,and feedback,agents enhancetheir abilitytocooperateandoptimize their decisionscollaboratively.Shared memory-based learning is particularly valuable in environments where agents need tocooperate,exchange tasks,or work toward common goals[919,967,968].For instance, ProAgent [1069] anticipates teammates'decisions and dynamically adjusts each agent's strategies based on the communication logs betweenagents,facilitating mutualunderstanding and improving collaborative planning capability.",
          "index": 2,
          "part": 0,
          "translated_content": "基于共享记忆的学习相比之下，通过使多个智能体交换彼此经验所得信息和见解，扩展了这一概念。智能体不再仅依赖个体记忆，而是可以从群体的集体知识中受益。通过共享数据、策略和反馈，智能体增强了他们协作的能力，并共同优化他们的决策。在需要智能体合作、交换任务或共同努力实现共同目标的环境中，基于共享记忆的学习尤为重要。例如，ProAgent预测队友的决策，并根据智能体之间的通信日志动态调整每个智能体的策略，促进相互理解，提高协作规划能力。"
        },
        {
          "type": "text",
          "content": "Parameter-based learning. Beyond memory-based learming in textual form, many MAS employ parameter-based leaning,whichevolves agentsindividualadaptabilitythrough post-training techniques.For instance,[1o70]discusses thea Larning through Communication (LTC) paradigm, whereusing communication logs between agents are leveraged to constructt generate datasets fortotraining or fine-tuninge LLMs.The integration of symbolic and connectionist paradigms within LLM-powered agents enhances botheir reasoning and adaptability. More recently, research has increasingly focusedon multi-agent(co-)fine-tuning,which improvescollaboration andreasoning capabilitiesthrough cooperative trajectories.Examples include multi-agent debate fine-tuning [1071] and SiruiS [1072]. Additionally, Sweet-RL[1073]employs reinforcement learning toenhance the critic model within MAS,fostering bettercollaborative reasoning.However, despite their promising performance,future parameter-based learning paradigms may need to addressthe balance between agents'general capabilities andtheir specialization for specific roles within MAS.This hybrid approach allowsagents tohandle both structured and unstructureddata,improving theirability tomake decisions in dynamic environments [1074, 1075].",
          "index": 3,
          "part": 0,
          "translated_content": "基于参数的学习。在文本形式的基于记忆的学习之外，许多多智能体系统采用了基于参数的学习，通过后期训练技术提高智能体的个体适应性。例如，[1070]讨论了学习通过通信（LTC）范式，其中利用智能体之间的通信日志构建生成数据集，用于训练或微调LLMs。在LLM驱动的智能体中整合符号和连接主义范式增强了它们的推理和适应性。最近的研究越来越多地集中在多智能体的（共同）微调上，通过合作轨迹提高协作和推理能力。例如，多智能体辩论微调[1071]和SiruiS [1072]。此外，Sweet-RL [1073]采用强化学习增强MAS中的评论模型，促进更好的协作推理。然而，尽管它们表现出有希望的性能，未来基于参数的学习范式可能需要解决智能体的一般能力与它们在MAS中特定角色的专业化之间的平衡。这种混合方法使智能体能够处理结构化和非结构化数据，提高它们在动态环境中做出决策的能力。 [1074, 1075]。"
        }
      ],
      "raw_title": "Individual Adaptability",
      "type": null,
      "children": [],
      "translated_title": "16.2 个体适应能力"
    },
    {
      "title": "Evaluating Multi-Agent Systems",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The transition from single-agent to multi-agent systems, and specifically Large Language Model(LLM)-based systems, requires a paradigm change inthe evaluation paradigm.In contrast to single-agent evaluation,in whichthe immediate concern is performance on a particular task, evaluation of LLM-based multi-agent systems must be understood in terms of inter-agent dynamics asa whole,such as collaborative planning and communication effectiveness.Both task-orintedreasoning andholisticcapability evaluation are addressed in this chapter,reflecting the nuance of such evaluations.Ingreater detail, thereare two main areas that we examine for evaluation.First, there is task-solving Multi-Agent Systems (MAS),where we examine benchmarks assessing and enhancing LLM reasoning for coding, knowledge,and mathematical problem-solving tasks.These tests alsoaccentuate theutility of distributed problem solving,achieved through organized workflows,specialisationamong agents,iterative improvement, and call for additional tools.Enhanced reasoning,primarily because of agent-agent decision-making cooperation and multi-round communications, is shownfor MAS compared with agent-based individual ones.Folowing that,there is a general evaluationof MASabilities,extendingbeyondone-task-oriented achievement,toagent iteractions atahighlyadvanced level. It involvesa move away fromone-dimensional measurements into multi-dimensionalframeworksfordocumenting achievements atcollborations,reasoningabilities,systemeffcencyandflexibility.Wecategorize such measurements into collaboration-oriented and competition-oriented measurements and have identified effciency,decision-making quality,quality ofcollaboration, and flexibility as primary measure domains.These measurementscapture various aspects of agent behavior,including communication effectiveness,resource distribution,andresponse to dynamic situations.",
          "index": 0,
          "part": 0,
          "translated_content": "从单一代理到多代理系统的转变，特别是基于大型语言模型（LLM）的系统，需要在评估范式上进行范式转变。与单一代理评估相比，在单一任务表现上的即时关注，LLM基础的多代理系统的评估必须从整体上理解为代理间动态，例如协作规划和通信效果。本章讨论了任务导向推理和整体能力评估，反映了这些评估的微妙之处。更详细地，我们在评估中考虑了两个主要领域。首先是解决任务的多代理系统（MAS），我们审查了评估和增强LLM在编码、知识和数学问题解决任务中推理能力的基准。这些测试也强调了通过组织工作流程、代理间的专业化、迭代改进实现的分布式问题解决的效用，并呼吁使用额外工具。相较于基于单个代理的个体系统，由于代理间决策协作和多轮通信，MAS表现出了增强的推理能力。接着，对MAS能力进行了一般评估，超越了单一任务导向的成就，转向高级别代理互动。这涉及从一维测量转向多维框架，以记录协作、推理能力、系统效率和灵活性方面的成就。我们将这些测量分类为面向协作和面向竞争的测量，并确定了效率、决策质量、协作质量和灵活性作为主要测量领域。这些测量捕捉了代理行为的各个方面，包括通信效果、资源分配和对动态情况的响应。"
        }
      ],
      "raw_title": "Evaluating Multi-Agent Systems",
      "type": null,
      "children": [],
      "translated_title": "评估多Agent系统"
    },
    {
      "title": "Building Safe and Beneficial AI Agents",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "TherapiddevelopmentofLLM-basedagents introducesanew setof safetychallnges that gobeyondthose of traditional LLMs.Equipped with advanced reasoning,planning,and tool-using capabilities, these agents are designed to perform tasks autonomously and interact with theirenvironments[34].However,this autonomy alsoexpands the attack surface, creating new vulnerabilitiesthat demand careful research and atention[1129,40].Inthis part,we first establish a comprehensive framework for understanding agent safety,examining both internal and external safety threats to AI agents.We willexplore the various attack vectors associated with these threats and propose potential mitigation strategies. This framework is organized into two key areas:",
          "index": 0,
          "part": 0,
          "translated_content": "基于LLM的智能代理的快速发展引入了一系列超越传统LLM的安全挑战。这些代理配备了先进的推理、规划和使用工具的能力，旨在自主执行任务并与其环境进行交互。然而，这种自主性也扩大了攻击面，制造了新的漏洞，需要谨慎研究和关注。在这部分中，我们首先建立了一个全面的框架，以理解智能代理的安全性，审视人工智能代理面临的内部和外部安全威胁。我们将探讨与这些威胁相关的各种攻击向量，并提出潜在的缓解策略。该框架分为两个关键领域："
        },
        {
          "type": "text",
          "content": "(1) IntrinsicSafety threats stem from vulnerabilities in the agent'score components,which include the LLM“brain\" as wellas the perception and action modules.Each ofthese components has unique weaknesses thatcan be exploited by adversaries:\n\n·Brain is the LLM itself,responsible for key decision-making tasks such as reasoning and planning.It is guided by a knowledge module that provides essential contextual information.\n·Perception consists of sensors that interpret theexternalenvironment, where malicious manipulation of external objects can lead to erroneous perceptions.\n·Action isresponsible for tool usage and downstream applications, which are also susceptible to exploitation.\n\n(2)Extrinsic Safety threats arise from interactions between the agent and externaloften untrusted,entities.These include:\n\n·Agent-Memory Interactions: The agent frequently accesses and interacts with memory storage, which serves as an external database for decision-making and contextual information retrieval. Recent research highlights vulnerabilities in the agent-memory interface that could be exploited to manipulate the agent's actions. ·Agent-Agent and Agent-Environment Interactions:These refer to the interactions betweenthe agent and other agents (e.g.,other agents or human operators),aswell as its environment, which includes task-related objects or dynamic systems.The complexity of these interactions further compounds the agent's exposure to external threats.",
          "index": 1,
          "part": 0,
          "translated_content": "(1) 内在安全威胁源于智能代理核心组件中的漏洞，包括LLM“大脑”以及感知和行动模块。这些组件中的每一个都有独特的弱点，可以被对手利用：\n\n- 大脑是LLM本身，负责诸如推理和规划等关键决策任务。它由一个知识模块引导，提供必要的上下文信息。\n- 感知包括解释外部环境的传感器，对外部对象的恶意操纵可能导致错误的感知。\n- 行动负责工具使用和下游应用，也容易受到利用。\n\n(2) 外部安全威胁源于代理与外部通常是不受信任的实体之间的交互。这些包括：\n\n- 代理-内存交互：代理经常访问和与存储在内存中的信息交互，内存作为决策和上下文信息检索的外部数据库。最近的研究突出了代理-内存接口中的漏洞，可以被利用来操纵代理的行为。\n- 代理-代理和代理-环境交互：这些是指代理与其他代理（例如其他代理或人类操作员）以及其环境之间的交互，包括任务相关对象或动态系统。这些交互的复杂性进一步增加了代理面临外部威胁的风险。"
        },
        {
          "type": "text",
          "content": "As illustratedinFigure17.l,theserisksarebroadlycategorized into intrinsicandextrinsic safety,helping toclarif their origin and nature.In addition to identifying threats,we also provide arigorous, mathematical foundation for understanding atacks such as jailbreaking, prompt injection, and data poisoning.Moreover, we present practical, actionable solutions, tracing the development of safety measures from early LLM safeguards to comprehensive strategies that protect the entire agent system.This includes exploring guardrails,advanced alignment techniques (such as superalignment),and the crucialbalance between safety and helpfulness. Finall,we analyze the“scaling law of AIsafety\"-thecomplex relationship between an agent'scapabilities and its potentialrisks—and the essential trade-offs that must be made.This part provides aclear understanding of thechallenges,theoreticalfoundations,and practical strategiesnecessary todevelopeffective and trustworthy AIagents thatcan be safely andefectivelydeployed in real-world scenarios.",
          "index": 2,
          "part": 0,
          "translated_content": "如图17.1所示，这些风险被广泛分类为内在安全和外部安全，有助于澄清它们的来源和性质。除了识别威胁，我们还提供了一个严谨的数学基础，用于理解诸如越狱、提示注入和数据污染等攻击。此外，我们提出了切实可行的解决方案，追溯了从早期LLM安全措施到保护整个代理系统的全面策略的发展过程。这包括探讨护栏、高级对齐技术（如超对齐）以及安全性和帮助性之间的关键平衡。最后，我们分析了“AI安全的扩展定律”——代理能力与潜在风险之间的复杂关系，以及必须做出的基本权衡。这部分内容提供了对挑战、理论基础和实际策略的清晰理解，这些策略对于开发能够在现实场景中安全有效部署的AI代理是必要的。"
        },
        {
          "type": "text",
          "content": "This part isorganizedasfollws:First, we examine intrinsic safety risks (Chapter 18)focusing on threats to the LLM “brain,\"as wellas vulnerabilities inthe agent's perception and action components (Chapter 19). Next, we explore extrinsic safetythreats relatedto agent-memory,agent-agent,and agent-environment interactions(Chapter 20).Finally, we investigate superalignment techniques aimed atensuring the safety of agent behaviors,whileaddressingthebroader challenge of balancing safety with performance.This includes exploringhow safety measures scale with the increasing capabilities of AI systems and examiningthetrade-offs involved indesigning secure,capable AIagents (Chapter 21).",
          "index": 3,
          "part": 0,
          "translated_content": "这部分内容组织如下：首先，我们研究内在安全风险（第18章），重点关注对LLM“大脑”的威胁，以及代理的感知和行动组件的漏洞（第19章）。接下来，我们探讨与代理-内存、代理-代理和代理-环境交互相关的外部安全威胁（第20章）。最后，我们调查旨在确保代理行为安全的超对齐技术，同时解决在平衡安全性与性能方面的更广泛挑战。这包括探讨安全措施如何随着AI系统能力的增强而扩展，以及在设计安全、功能强大的AI代理时涉及的权衡（第21章）。"
        },
        {
          "type": "figure",
          "src": "images/47d45ad22b0854ee971de9acb13cd7fcbadfa693ff22c28d6a6a5799fc059caa.jpg",
          "alt": "",
          "caption": "Figure 17.1: The Brain (LLM) faces safety threats like jailbreaks and prompt injection attacks $(\\S18.1)$ and privacy threats such as membership inference attacks $(\\S18.2)$ . Non-brain modules encounter perception threats $(\\S19.1)$ and action threats $(\\S\\ 19.2)$ . Due to interactions with potentially malicious external entities, we also explore agent-memory threats $(\\S20.1)$ , agent-environment threats $(\\S20.2)$ , and agent-agent threats ( 20.3).",
          "index": 4,
          "part": 0,
          "translated_caption": "图17.1：大脑（LLM）面临安全威胁，如越狱和提示注入攻击（第18.1节），以及隐私威胁，如成员推断攻击（第18.2节）。非大脑模块遭遇感知威胁（第19.1节）和行动威胁（第19.2节）。由于与潜在恶意外部实体的互动，我们还探讨了代理-记忆威胁（第20.1节）、代理-环境威胁（第20.2节）和代理-代理威胁（20.3节）。"
        }
      ],
      "raw_title": "Building Safe and Beneficial AI Agents",
      "type": null,
      "children": [],
      "translated_title": "构建安全和有益的人工智能代理"
    },
    {
      "title": "Agent Intrinsic Safety: Threats on AI Brain",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The intrinsic safety of an AIagent concerns vulnerabilitieswithin the agent's internal architecture andfunctionality. AI agents,by their nature,consist of multiple components:acentral“brain\"(the LLM),and auxiliary modules for perception and action[66]. While this modularity enables sophisticated reasoning and autonomous decision-making, it also expands the potentialattck surface,exposing theagent tovarious internalvulnerabilitiesthat adversariescan exploit [1130].\n\nThreats to the agent's brainspecificall the LLM—are particularly concerning,as theycan directly impact the agent's decision-making,reasoning,and planning abilities.These vulnerabilities can arise fromflaws inthe design of the model,misinterpretationsofinputs,oreven weaknesses induced bythetraining processEffctive mitigation strategies are crucial to ensuring that these agents can be deployed securely and reliably.",
          "index": 0,
          "part": 0,
          "translated_content": "AI代理的内在安全性涉及到代理的内部架构和功能中的漏洞。AI代理本质上由多个组件组成：一个中央的“大脑”（LLM），以及用于感知和行动的辅助模块。虽然这种模块化架构使得复杂推理和自主决策成为可能，但也扩大了潜在的攻击面，暴露了代理可能受到的各种内部漏洞，敌对方可能会利用这些漏洞。\n\n对代理的“大脑”（特别是LLM）的威胁尤为令人担忧，因为它们可能直接影响代理的决策、推理和规划能力。这些漏洞可能源于模型设计中的缺陷、对输入的误解，甚至是训练过程中引入的弱点。有效的缓解策略对于确保这些代理能够安全可靠地部署至关重要。"
        }
      ],
      "raw_title": "Agent Intrinsic Safety: Threats on AI Brain",
      "type": null,
      "children": [],
      "translated_title": "1. 代理人固有安全性：AI大脑上的威胁"
    },
    {
      "title": "18.1 Safety Vulnerabilities of LLMs",
      "number": "18.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The LLM,asthecoredecision-makingcomponent ofthe agent,ishighly susceptibletoarange ofsafetythreats.Its centralroleinreasoningandactionselectionmakes it anatractivetargetforadversaries.Inthecontextof AIagents,the vulnerabilities inherent inthe LLMitself areoften amplified,asthese modelsare required tofunction within dynamic, real-world environments where adversaries can exploit weaknesses [1131, 1132].",
          "index": 0,
          "part": 0,
          "translated_content": "作为智能代理的核心决策组件，LLM（大型语言模型）极易受到各种安全威胁的影响。其在推理和行动选择中的核心作用使其成为对手的有吸引力目标。在AI代理的背景下，LLM本身固有的脆弱性经常会被放大，因为这些模型需要在对手可以利用弱点的动态真实环境中运作。"
        }
      ],
      "raw_title": "Safety Vulnerabilities of LLMs",
      "type": null,
      "children": [
        {
          "title": "18.1.1 Jailbreak Attacks",
          "number": "18.1.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Jailbreaks circumvent the safety guardrails embedded in AIagents,compelling their decision-making process to be harmful,unethical,orbiased[233].Theseatacks exploit the inherent tensionbetweenanLLM's helpfulnessand its safety constraints [1134].\n\nFormalization.Toformallcharacterizetherisks posedbyjailbreaks,we analyzethe probabilitydistribution govening an autoregressive LLM's output. For an autoregressive LLM,the probability of generating an output sequence $\\mathbf{y}=\\mathbf{x}_{n+1:n+m}$ , given an input sequence ${\\mathbf x}_{1:n}$ is modeled as:",
              "index": 0,
              "part": 0,
              "translated_content": "越狱行为绕过了嵌入在人工智能代理中的安全防护措施，迫使它们的决策过程可能会带有有害、不道德或偏见[233]。这些攻击利用了LLM的帮助性和安全约束之间固有的张力[1134]。\n\n形式化。为了形式化表征越狱行为带来的风险，我们分析了自回归LLM输出的概率分布。对于一个自回归LLM，给定输入序列${\\mathbf x}_{1:n}$，生成输出序列$\\mathbf{y}=\\mathbf{x}_{n+1:n+m}$的概率建模为："
            },
            {
              "type": "formula",
              "content": "$$ \np(\\mathbf{y}|\\mathbf{x}_{1:n})=\\prod_{i=1}^{m}p(\\mathbf{x}_{n+i}|\\mathbf{x}_{1:n+i-1})\n $$",
              "index": 1,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $m$ denotes the total length of the generated sequence. Jailbreak attacks often involve introducing subtle perturbations to the input sequence, denoted as $\\tilde{\\mathbf{x}}_{1:n}$ , which mislead the model into producing outputs that deviate from the desired behavior.\n\nThe impact of a jailbreak attck is evaluated through its effect on the alignment reward $\\mathcal{R}^{*}(\\mathbf{y}|\\mathbf{x}_{1:n},\\mathcal{A})$ , which measures how closely the model's output aligns with a set of human-defined safety or ethical guidelines, denoted as $\\mathcal{A}$ . The adversary's goal is to minimize this reward, formalized as:",
              "index": 2,
              "part": 0,
              "translated_content": "其中$m$表示生成序列的总长度。越狱攻击通常涉及向输入序列引入微妙的扰动，记为$\\tilde{\\mathbf{x}}_{1:n}$，这些扰动会误导模型产生与期望行为偏离的输出。\n\n通过对其对齐奖励$\\mathcal{R}^{*}(\\mathbf{y}|\\mathbf{x}_{1:n},\\mathcal{A})$的影响来评估越狱攻击，该奖励衡量模型输出与一组人类定义的安全或道德准则$\\mathcal{A}$的接近程度。对手的目标是最小化这一奖励，形式化表示为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathbf{y}^{\\star}=\\underset{\\mathbf{y}}{\\arg\\operatorname*{min}}\\mathcal{R}^{*}(\\mathbf{y}|\\tilde{\\mathbf{x}}_{1:n},\\mathcal{A}))\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "figure",
              "src": "images/0fe60b759406b2fa215769fabc5fef818b984a110e0c85dad5e1f83f2b8a40ac.jpg",
              "alt": "",
              "caption": "Figure 18.1: Agent Intrinsic Safety: Threats on LLM Brain.",
              "index": 4,
              "part": 0,
              "translated_caption": "图18.1：代理内在安全性：LLM大脑上的威胁。"
            },
            {
              "type": "text",
              "content": "where $\\mathbf{y}^{\\star}$ is the worst-case output induced by the perturbed input.The corresponding adversariallossfunction quantifies the likelihood of generating this output:",
              "index": 5,
              "part": 0,
              "translated_content": "其中$\\mathbf{y}^{\\star}$是由扰动输入引起的最坏情况输出。相应的对抗损失函数量化了生成这一输出的可能性："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathcal{L}^{a d v}(\\tilde{\\mathbf{x}}_{1:n})=-\\log p(\\mathbf{y}^{\\star}|\\tilde{\\mathbf{x}}_{1:n}),\\mathrm{~and~}\\tilde{\\mathbf{x}}_{1:n}=\\operatorname*{argmin}_{\\tilde{\\mathbf{x}}_{1:n}\\in\\mathcal{T}(\\hat{\\mathbf{x}}_{1:n})}\\mathcal{L}^{a d v}(\\tilde{\\mathbf{x}}_{1:n})\n $$",
              "index": 6,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $p(\\mathbf{y}^{\\star}|\\tilde{\\mathbf{x}}_{1:n})$ denotes the probability assigned to the jailbreak output and $\\mathcal{T}(\\hat{\\mathbf{x}}_{1:n})$ is the distribution or set of possible jailbreak instructions.",
              "index": 7,
              "part": 0,
              "translated_content": "其中$p(\\mathbf{y}^{\\star}|\\tilde{\\mathbf{x}}_{1:n})$表示分配给越狱输出的概率，$\\mathcal{T}(\\hat{\\mathbf{x}}_{1:n})$是可能的越狱指令的分布或集合。"
            },
            {
              "type": "figure",
              "src": "images/f37e2c7193e4034737296e8523563ec3192ca19a726322dc1a159dd6918a3200.jpg",
              "alt": "",
              "index": 8,
              "part": 0
            },
            {
              "type": "text",
              "content": "Figure18.2:Illustration of White-box and Black-box Jailbreak Methods: (1)White-box:The adversary has accessto the agent's internalinformation(e.ggradients,attention,logits),allowing precise manipulations such as advrsarial suffx optimization. (2)Black-box: The adversary relies solely on input-output interactions.Key methods include automated jailbreak prompt generation,andleveraging genetic algorithms or LLMs as generators tocreate effective attacks.\n\nAs shown in Figure18.2jailbreaks canbebroadlyclasified into white-box andblack-box methods,depending on the adversary'saccess to themodel's internal parameters.(1)White-box Jailbreaks:These attcks assumethe adversary has full accessto the model's internal information, such as weights, gradients,atention mechanisms, and logits. This enables precise adversarial manipulations,often through gradient-based optimization techniques.(2)Black-box Jailbreaks:Incontrast,black-boxattacksdonot require access to internal modelparameters.Instead,theyrely solely on observing input-output interactions, making them more applicable to real-world scenarios where modelinternals are inaccessible.",
              "index": 9,
              "part": 0,
              "translated_content": "图18.2：白盒和黑盒越狱方法示意图：(1)白盒：对手可以访问代理的内部信息（例如梯度、注意力、logits），从而进行精确的操作，如对抗性后缀优化。(2)黑盒：对手仅依赖于输入输出交互。关键方法包括自动生成越狱提示和利用遗传算法或LLMs作为生成器来创建有效攻击。\n\n如图18.2所示，越狱可以广泛分为白盒和黑盒方法，取决于对手对模型内部参数的访问权限。(1)白盒越狱：这些攻击假定对手可以完全访问模型的内部信息，如权重、梯度、注意机制和logits。这使得精确的对抗操作成为可能，通常通过基于梯度的优化技术实现。(2)黑盒越狱：相比之下，黑盒攻击不需要访问内部模型参数。相反，它们仅依赖于观察输入输出交互，使它们更适用于现实世界场景，其中模型内部是不可访问的。"
            },
            {
              "type": "text",
              "content": "White-box Jailbreak.White-boxattacks exploit access to an AIagent's nternalparameters,suchas modelweights and attntion mechanisms,enabling precise manipulations.Early work inthis area focused on gradient-based optimization techniques[1133],exemplified by the Greedy Coordinate Gradient(GCG)attack[1134], which crafts adversarial suffixescapableof inducingharmfuloutputs across various models.Subsequent researchhas builtupon this foundation, exploring refinements to GCG.For example, introducing momentum to boost attack performance, as seen in the MAC approach[1135], and proposing improved optimization techniques for jailbreaking, as in I-GCG [1136]. Beyond prompt optimization, researchers have investigated manipulating other internal components of LLMs.Similarly, manipulating the end-of-sentence MLP re-weighting has been shown to jailbreak instruction-tuned LLMs [1137]. Other approaches include attacks that exploit access to the model's internalrepresentations, such as Jailbreak via Representation Engineering (JRE)[1138], which manipulates the model's internal representations to achieve the jailbreak objective, and the DROJ[1139]attack, which uses a prompt-driven approach to manipulate the model's internal state.AutoDAN[1140]automates the generation of stealthy jailbreak prompts.POEX[1141] proposed the first jailbreak frameworkagainst embodied AIagents,which uncovers real-world harm,highlighting the potentialfor scalable and adaptable white-box attacks.",
              "index": 10,
              "part": 0,
              "translated_content": "白盒越狱。白盒攻击利用对AI代理的内部参数的访问权限，例如模型权重和注意机制，从而实现精确的操作。该领域的早期工作主要集中在基于梯度的优化技术上，例如Greedy Coordinate Gradient（GCG）攻击，该攻击制作出能够在各种模型中引起有害输出的对抗性后缀。随后的研究在此基础上进行了探索，探讨了对GCG的改进。例如，引入动量以提高攻击性能，如MAC方法所示，以及提出了改进的越狱优化技术，如I-GCG。除了及时优化，研究人员还研究了操纵LLMs的其他内部组件。类似地，已经显示通过操纵句子末尾的MLP重新加权可以越狱指令调整的LLMs。其他方法包括利用对模型内部表示的访问权限的攻击，例如通过表示工程（JRE）实现越狱的攻击，该攻击操纵模型的内部表示以实现越狱目标，以及使用基于提示的方法来操纵模型的内部状态的DROJ攻击。AutoDAN自动生成隐秘越狱提示。POEX提出了针对具象化AI代理的第一个越狱框架，揭示了真实世界的危害，突显了可扩展和适应性强的白盒攻击的潜力。"
            },
            {
              "type": "text",
              "content": "Black-box Jailbreak.Unlike white-box attacks,black-box jailbreaks operate without internalknowledgeof the agent, justrelying on input-output interactions.Prompt engineering is acritical approach, where carefully designed prompts are employed to exploit the model'sresponse generation capabilities and bypassits safety mechanisms[142].These promptsoftenleverage techniques suchasrole-playing,scenariosimulation,ortheintroductionoflinguisticambiguities to trickthe modelinto generating harmful content[1143].Furthermore, automated prompt generation methods have emerged,employingalgorithmslikegenetic algorithms ofuzzing tosystematicallydiscovereffctive jailbreak prompts [1234].Inaddition,multi-turnattacksexploittheconversationalcapabilitiesofMs,graduallysteeringthedialogue towards unsafe territory through a series of carefully crafted prompts [ll46].Other notable approaches include exploiting the model's susceptibility to specifictypes ofcipher prompts[ll44]and utilizing multimodalinputs,such as images, to trigger unintended behaviors and bypass safety filters[1145,l147,1148].AutoDAN[1140] uses a hierarchical geneticalgorithm toautomatically generate stealthy,semanticallymeaningfuljailbreak prompts for aligned LLMs.POEX[141]alsoshowcases thefeasibilityof transferring white-boxoptimized jailbreak prompts toblack-box LLMs.",
              "index": 11,
              "part": 0,
              "translated_content": "黑盒越狱。与白盒攻击不同，黑盒越狱在不了解代理内部知识的情况下操作，仅依赖于输入输出交互。提示工程是一种关键方法，通过精心设计的提示利用模型的响应生成能力并绕过其安全机制。这些提示通常利用角色扮演、场景模拟或引入语言歧义等技术，欺骗模型生成有害内容。此外，出现了自动生成提示的方法，采用遗传算法或模糊测试等算法系统地发现有效的越狱提示。此外，多轮攻击利用对话能力，通过一系列精心设计的提示逐渐引导对话走向不安全领域。其他值得注意的方法包括利用模型对特定类型的密码提示的敏感性，以及利用多模态输入（如图像）触发意外行为并绕过安全过滤器。AutoDAN利用分层遗传算法自动生成与对齐LLMs的隐秘、语义有意义的越狱提示。POEX还展示了将白盒优化的越狱提示转移到黑盒LLMs的可行性。"
            },
            {
              "type": "text",
              "content": "Mitigation.Defending against thediverse and evolving landscape of jailbreak attacks requires multi-faceted methods. System-level defenses offr a promising avenue,focusing on creating a secure environment around the LLM rather than solely relying on hardening the modelitself.One keystrategy is inputsanitization andfltering,where incoming prompts are analyzed and potentially modified before being processed by the LLM.This can involve detecting and neutralizing malicious patterns[125],orrewriting prompts toremove potentiallyharmful elements [236].Another crucial aspect is output monitoring and anomaly detection, where the LLM's responses are scrutinized for unsafe or unexpectedcontent.Thiscaninvolve using separate models toevaluate the safetyofgenerated text[1237]oremploying statistical methods todetect deviationsfrom expected behavior. Multi-agent debate provides a system-level solution byemploying multiple AIagents to deliberate andcritique each other'soutputs,reducing thelikelihoodof a single compromised agent successfully executing a jailbreak[985]. Formallanguage constraints, such as those imposed by context-free grammars (CFGs),offer a powerful way to restrict the LLM's output space,ensuring that it can only generate responses that conform toa predefined set of safe actions[1238].Furthermore,system-level monitoringcan be implemented to track the overallbehavior of the LLMdeployment, detecting unusual activity pattrns that might indicate an ongoing attack. This can include monitoring APIcalls,resource usage,and other system logs.Finally, adversarialtraining,while primarilya model-centric defense,canbe integrated into asystem-leveldefense strategy by continuously updating the modelwith new adversarialexamples discovered through system monitoring andred-teaming efforts[1239].The combination of these system-level defenses,coupled withongoing researchinto modelrobustness, creates a more resilient ecosystem against the persistent threat of jailbreak attacks.",
              "index": 12,
              "part": 0,
              "translated_content": "缓解。为了抵御多样化和不断演化的越狱攻击，需要采用多方面的方法。系统级防御提供了一个有前途的途径，重点是在LLM周围创建一个安全环境，而不仅仅依赖于加固模型本身。一个关键策略是输入的净化和过滤，在LLM处理之前对传入提示进行分析并可能进行修改。这可能涉及检测和中和恶意模式，或者重写提示以去除潜在有害元素。另一个关键方面是输出监控和异常检测，对LLM的响应进行审查，以寻找不安全或意外的内容。这可以包括使用单独的模型评估生成文本的安全性，或者采用统计方法检测与预期行为的偏差。多Agent辩论提供了一个系统级解决方案，通过利用多个AI代理进行协商和评论彼此的输出，降低了单个受损代理成功执行越狱攻击的可能性。形式语言约束，比如由上下文无关文法（CFGs）施加的约束，提供了一种强大的方式来限制LLM的输出空间，确保它只能生成符合预定义安全行为集的响应。此外，系统级监控可以被实施以跟踪LLM部署的整体行为，检测可能表明正在进行攻击的异常活动模式。这可以包括监视API调用、资源使用和其他系统日志。最后，对抗训练，虽然主要是一种以模型为中心的防御，但可以通过持续更新模型，利用通过系统监控和红队努力发现的新对抗性示例，将其整合到系统级防御策略中。这些系统级防御措施的结合，以及对模型稳健性的持续研究，创建了一个更具弹性的生态系统，以抵御持续威胁的越狱攻击。"
            }
          ],
          "raw_title": "Jailbreak Attacks",
          "type": null,
          "children": [],
          "translated_title": "18.1.1 越狱攻击"
        },
        {
          "title": "18.1.2 Prompt Injection Attacks",
          "number": "18.1.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Prompt injectionatacks manipulate the behavior of LLMs byembeddingmalicious instructions within the input prompt which hijacks the model's intended functionality andredirects it to perform actions desired by the atacker[1130]. Unlike jailbreaks thatbypassafety guidelines,prompt injections exploit the model'sinability todistinguish between the originalcontext andexternallyappended instructions.This vulnerabilityis exacerbatedbytheopen-ended natureof text input,theabsence of robust filteringmechanisms,andthe assumptionthat allinput istrustworthy, making LLMs particularly susceptible to adversarialcontent[149].Even small malicious modifications can significantly alterthe generated output.",
              "index": 0,
              "part": 0,
              "translated_content": "提示注入攻击通过在输入提示中嵌入恶意指令来操纵LLMs的行为，从而劫持模型的预期功能并将其重定向到攻击者所期望的操作[1130]。与绕过安全指南的越狱不同，提示注入利用模型无法区分原始上下文和外部附加指令的能力。这种漏洞受到文本输入的开放性、缺乏健壮的过滤机制以及假设所有输入都是可信的的影响，使LLMs特别容易受到对抗性内容的影响[149]。即使是微小的恶意修改也可能显著改变生成的输出。"
            },
            {
              "type": "text",
              "content": "Formalization. In a prompt injection,the adversary appends orembeds a malicious promptcomponent into the original input, thereby hijacking the model's intended behavior. Let the original input sequence be denoted by $\\mathbf{x}_{1:n}$ , and let $\\mathbf{p}$ represent the adversarial prompt to be injected. The effective (injected) input becomes: $\\mathbf{x}^{\\prime}=\\mathbf{x}_{1:n}\\oplus\\mathbf{p}$ , where the operator $\\oplus$ denotes concatenation or integration of the malicious prompt with the original input.Then, the autoregressive generation process under the injected prompt is then given by:",
              "index": 1,
              "part": 0,
              "translated_content": "形式化。在提示注入中，对手附加或嵌入恶意提示组件到原始输入中，从而劫持模型的预期行为。假设原始输入序列表示为$\\mathbf{x}_{1:n}$，$\\mathbf{p}$表示要注入的对抗性提示。有效的（注入的）输入变为：$\\mathbf{x}^{\\prime}=\\mathbf{x}_{1:n}\\oplus\\mathbf{p}$，其中运算符$\\oplus$表示恶意提示与原始输入的连接或整合。然后，在注入提示下的自回归生成过程如下给出："
            },
            {
              "type": "formula",
              "content": "$$ \np(\\mathbf{y}|\\mathbf{x}^{\\prime})=\\prod_{i=1}^{m}p(\\mathbf{y}_{i}\\mid\\mathbf{x}_{1:n+i-1}^{\\prime})\n $$",
              "index": 2,
              "part": 0
            },
            {
              "type": "text",
              "content": "Assuming the alignment reward $\\mathcal{R}^{*}(\\cdot,\\mathcal{A})$ measures the extent to which the output adheres to the set of human-defined safety or ethical guidelines $\\mathcal{A}$ , the adversary's goal is to force the model to generate an output that minimizes this reward:",
              "index": 3,
              "part": 0,
              "translated_content": "假设对齐奖励$\\mathcal{R}^{*}(\\cdot,\\mathcal{A})$衡量输出符合人类定义的安全或伦理准则集合$\\mathcal{A}$的程度，对手的目标是迫使模型生成最小化该奖励的输出："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathbf{y}^{\\star}=\\underset{\\mathbf{y}}{\\arg\\operatorname*{min}}\\ \\mathcal{R}^{*}\\left(\\mathbf{y}\\mid\\mathbf{x}_{1:n}\\oplus\\mathbf{p},\\mathcal{A}\\right).\n $$",
              "index": 4,
              "part": 0
            },
            {
              "type": "text",
              "content": "Accordingly, the loss function is defined as:",
              "index": 5,
              "part": 0,
              "translated_content": "相应地，损失函数被定义为："
            },
            {
              "type": "formula",
              "content": "$$ \n\\begin{array}{r}{\\mathcal{L}^{i n j e c t}(\\mathbf{p})=-\\log p\\big(\\mathbf{y}^{\\star}\\mid\\mathbf{x}_{1:n}\\oplus\\mathbf{p}\\big).}\\end{array}\n $$",
              "index": 6,
              "part": 0
            },
            {
              "type": "text",
              "content": "The optimal prompt is then obtained by solving:",
              "index": 7,
              "part": 0,
              "translated_content": "然后通过求解得到最佳提示："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathbf{p}^{\\star}=\\underset{\\mathbf{p}\\in\\mathcal{P}}{\\arg\\operatorname*{min}}\\ \\mathcal{L}^{i n j e c t}(\\mathbf{p})\n $$",
              "index": 8,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\mathcal{P}$ denotes the set of feasible prompt injections.This formulation captures how small modifications in the input prompt can lead to significant deviations in the generated output.\n\nAs illustrated in Figure18.3,prompt injectionatackscan be broadlycategorized intodirect and indirect attacks based on how the adversarialinstructions are introduced.(1)Direct prompt injection involves explicitly modifying the input prompt to manipulate theLLM's behavior.(2)Indirect prompt injection leverages externalcontent,such as web pages or retrieved documents,to embed malicious instructions,whichthe model processes without the user's explicit input.\n\nDirect prompt injection. These atacks against AIagents involve adversaries directly modifying the input prompt to manipulate the agent's behavior.Early work established the feasibility of suchatacks,demonstrating that carefully crafted prompts could induce agents to deviatefrom their intended tasks[1149].Subsequent research explored the automation ofthese attacks,revealing the potentialforwidespreadexploitation[l150,l151].Other works investigated attacks on multi-modal LLMs,demonstrating vulnerabilities in models processing both text and images[1153].These studies collectively highlight the evolving threat landscape of direct prompt injection,moving from initial proofs of concept to sophisticated atacks thatcan compromise the integrity and safety of AI agents.Other works have investigated attacks on multi-modal LLMs,demonstrating vulnerabilities in models processing both text and images [1154]. Competitions like the“LLM CTF Competition\"Debenedettiet al.[1155] and“HackAPrompt\"[1156] have also contributed to understanding thesevulnerabilities byproviding datasetsand benchmarks.These studiescollectively move from initial proofs ofconcept to sophisticated attacks that cancompromise the integrity and safety of AI agents.",
              "index": 9,
              "part": 0,
              "translated_content": "其中，$\\mathcal{P}$表示可行提示注入集合。这种表述捕捉了输入提示中的微小修改如何导致生成输出的显著偏差。\n\n如图18.3所示，提示注入攻击可以根据对对抗指令的引入方式大致分为直接攻击和间接攻击两种类型。(1)直接提示注入涉及明确修改输入提示以操纵LLM的行为。(2)间接提示注入利用外部内容，如网页或检索的文档，嵌入恶意指令，模型在没有用户明确输入的情况下进行处理。\n\n直接提示注入。这些针对AI代理的攻击涉及对手直接修改输入提示以操纵代理的行为。早期研究证明了这种攻击的可行性，表明精心设计的提示可以导致代理偏离其预期任务[1149]。随后的研究探讨了这些攻击的自动化，揭示了广泛利用的潜力[1150,1151]。其他研究调查了对多模态LLM的攻击，展示了处理文本和图像的模型的漏洞[1153]。这些研究共同突显了直接提示注入的威胁态势不断演变，从最初的概念验证发展到可以危及AI代理的完整性和安全性的复杂攻击。其他研究还调查了对多模态LLM的攻击，展示了处理文本和图像的模型的漏洞[1154]。像“LLM CTF Competition”[1155]和“HackAPrompt”[1156]这样的竞赛也通过提供数据集和基准有助于理解这些漏洞。这些研究共同从最初的概念验证发展到可以危及AI代理的完整性和安全性的复杂攻击。"
            },
            {
              "type": "figure",
              "src": "images/9364213871bce17e56a6cffa31c8034a50e5bee0e3d31d4c14f056730c7ddc15.jpg",
              "alt": "",
              "index": 10,
              "part": 0
            },
            {
              "type": "text",
              "content": "Figure18.3:llustrationofDirect and IndirectPrompt InjectionMethods: (1)Direct:Theadversarydirectly manipulates the agent's input prompt with malicious instructionsachievingimmediatecontroloverthe agent'sbehavior.(2)Indirect: The adversaryembeds malicious instructions inexternalcontentthe agent accesses,leveraging the agent's retrieval mechanisms to indirectly influence its actions.\n\nIndirect Prompt Injection. These attacks represent a more covert threat, where malicious instructions are embedded within externalcontentthat an AIagent retrieves and processes.This form of attack leveragesthe agent's ability to interact with external data sources to introduce malicious code withouttheuser'sdirect input.Greshakeet al.[1149] were amongthefrsttohighlight thisvulnerability,demonstratinghowreal-worldLLM-integratedapplications could be compromised throughcontent fetched from the web.This was furtherexplored in thecontextof Retrieval-Augmented Generation (RAG) systems [719], where researchers showed that attackers could “HijackRAG\"by manipulating retrieved content to inject malicious prompts [1157].Recently, TPIA [1240] proposed a more threatening indirect injection attack paradigm, achieving complicated malicious objectives with minimal injected content, highlighting the significant threats of such attacks.Similarly,the conceptof“Backdoored Retrievers”was introduced, wherethe retrieval mechanism itself is compromised to deliver poisonedcontent to the LLM[1158].Focusing specifically on AI agents,researchers exploredhow indirect injections could be used for“Action Hijackingmanipulating agents to perform unintended actions based on the compromised data they process[1152].“Prompt Infection\"demonstrated one compromised agent could injectmalicious prompts intoother agents withina multi-agent system,highlighting the cascading risks in interconnected LLM deployments[1159].Thesestudies underscore the growingconcern surrounding indirect promptinjectionas apotentatack vectoragainst AIagents,particularly asthese agents become moreintegrated with external data sources.Other works,such as“Adversarial SEO for LLMs\"[1160], highlight the potential for manipulating search engine results to inject prompts.",
              "index": 11,
              "part": 0,
              "translated_content": "图18.3: 直接和间接提示注入方法示意图：(1)直接：对手直接操纵代理的输入提示，使用恶意指令立即控制代理的行为。(2)间接：对手将恶意指令嵌入代理访问的外部内容，利用代理的检索机制间接影响其行为。\n\n间接提示注入。这些攻击代表了一种更隐蔽的威胁，恶意指令被嵌入到AI代理检索和处理的外部内容中。这种攻击形式利用了代理与外部数据源交互的能力，在没有用户直接输入的情况下引入恶意代码。Greshake等人[1149]是最早凸显这种漏洞的研究者之一，演示了通过从网络获取的内容，现实世界中集成LLM的应用程序可能会受到损害。这在检索增强生成(RAG)系统的背景下进一步探讨[719]，研究人员展示了攻击者如何通过操纵检索的内容来注入恶意提示来“劫持RAG”。最近，TPIA[1240]提出了一种更具威胁性的间接注入攻击范式，通过最小的注入内容实现复杂的恶意目标，突出了这类攻击的重大威胁。类似地，引入了“Backdoored Retrievers”概念，其中检索机制本身被篡改以向LLM提供有毒内容。专注于AI代理，研究人员探讨了如何利用间接注入来进行“行动劫持”，根据处理的受损数据操纵代理执行非预期行动。 “提示感染”示范了一个受损代理可以向多代理系统中的其他代理注入恶意提示，突显了互连LLM部署中级联风险。这些研究强调了围绕间接提示注入的担忧日益增长，作为针对AI代理的一种强大攻击向量，特别是随着这些代理与外部数据源的更紧密集成。其他作品，如“LLM的对抗性SEO”，突显了操纵搜索引擎结果以注入提示的潜力。"
            },
            {
              "type": "text",
              "content": "Mitigation.Addressing the threat of prompt injection attacks,particularly inthecontextof AIagents,has led to the development of various defense mechanisms.One early approach involved the use of embedding-based clasifiers to detect prompt injection attacks by analyzing the semantic features of the input[1241]. Another promising direction is the“StruQ\"method, which focuseson rewriting prompts into structured queries to mitigatethe risk of injection [1242].“The Task Shield\"represents asystem-leveldefense that enforces taskalignment,ensuring that agents adhere to theirintended objectives despite potentiall malicious inputs[1243].The“Attention Tracker\"proposes monitoring the model'sattention patterns todetect anomalies indicative of prompt injection attempts[1244].Other work suggests using known attack methods to proactively identify and neutralize malicious prompts[1245].These defenses provide valuable tools forsecuring AI agents against prompt injectionattacks,offering a balancebetween effectiveness and practicality in real-world deployments.",
              "index": 12,
              "part": 0,
              "translated_content": "缓解。针对直接提示注入攻击威胁，特别是在AI代理环境中，已经出现了各种防御机制的发展。一种早期方法涉及使用基于嵌入的分类器，通过分析输入的语义特征来检测提示注入攻击[1241]。另一个有前途的方向是“StruQ”方法，该方法专注于将提示重写为结构化查询，以减轻注入风险[1242]。“任务屏障”代表了一种系统级防御，强制执行任务对齐，确保代理遵循其预期目标，尽管可能存在恶意输入[1243]。“关注跟踪器”提出监视模型的注意力模式，以检测提示注入尝试的异常情况[1244]。其他工作建议使用已知的攻击方法主动识别并中和恶意提示[1245]。这些防御措施为保护AI代理免受提示注入攻击提供了有价值的工具，在现实世界的部署中提供了有效性和实用性之间的平衡。"
            }
          ],
          "raw_title": "Prompt Injection Attacks",
          "type": null,
          "children": [],
          "translated_title": "18.1.2 提示注入攻击"
        },
        {
          "title": "18.1.3 Hallucination Risks",
          "number": "18.1.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Hallucinationrefers totheLLM'stendency to generateoutputs that are factuallincorrct, nonsensical,ornot grounded in the provided context[l161].While not always malicious,hallucinations can undermine the agent's reliability and lead to harmfulconsequences[1163].As illustrated inFigure 18.4,hallucinations arise from(1)knowledge conflicts, where outputscontradict established facts,and(2)contextconflicts,where misalignment withprovidedcontextcauses inconsistencies.\n\nFormalization. Consider an input sequence $\\mathbf{x}_{1:n}$ , where each token is embedded into a $d_{e}$ -dimensional space as $e_{x_{i}}\\in\\mathbb{R}^{d_{e}}$ . The attention score between tokens $i$ and $j$ is computed as:",
              "index": 0,
              "part": 0,
              "translated_content": "幻觉指的是LLM倾向于生成事实不正确、荒谬或不符合提供的上下文的输出[l161]。虽然并非总是恶意的，但幻觉可能会损害代理的可靠性并导致有害后果[1163]。如图18.4所示，幻觉源于两种情况：（1）知识冲突，即输出与已建立事实相矛盾，以及（2）上下文冲突，即与提供的上下文不一致导致不连贯性。\n\n形式化。考虑一个输入序列$\\mathbf{x}_{1:n}$，其中每个标记被嵌入到一个$d_{e}$维空间中，表示为$e_{x_{i}}\\in\\mathbb{R}^{d_{e}}$。计算标记$i$和$j$之间的注意力分数如下："
            },
            {
              "type": "formula",
              "content": "$$ \nA_{i j}=\\frac{\\exp\\left((\\mathrm{W}_{Q}e_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{j}})\\right)}{\\sum_{t=1}^{n}\\exp\\left((\\mathrm{W}_{Q}e_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{t}})\\right)}\n $$",
              "index": 1,
              "part": 0
            },
            {
              "type": "text",
              "content": "with the contextual representation of token $i$ given by $\\begin{array}{r}{o_{i}=\\sum_{j=1}^{n}A_{i j}\\cdot(\\mathrm{W}_{V}e_{x_{j}}).\\mathrm{W}_{Q},\\mathrm{W}_{K}\\in\\mathbb{R}^{d_{e}\\times d_{k}}}\\end{array}$ and $\\mathrm{W}_{V}\\in$ $\\mathbb{R}^{d_{e}\\times d_{v}}$ are the query, key,and value projection matrices,respctiely.\n\nSuppose that each input embedding is perturbed by a vector $\\delta_{x_{i}}$ (with $\\|\\delta_{x_{i}}\\|\\leq\\epsilon)$ , resulting in perturbed embeddings $\\tilde{e}_{x_{i}}=e_{x_{i}}+\\delta_{x_{i}}$ . The attention scores under perturbation become:",
              "index": 2,
              "part": 0,
              "translated_content": "对于标记$i$的上下文表示为$\\begin{array}{r}{o_{i}=\\sum_{j=1}^{n}A_{i j}\\cdot(\\mathrm{W}_{V}e_{x_{j}}).\\mathrm{W}_{Q},\\mathrm{W}_{K}\\in\\mathbb{R}^{d_{e}\\times d_{k}}}\\end{array}$，其中$\\mathrm{W}_{V}\\in\\mathbb{R}^{d_{e}\\times d_{v}}$是查询、键和值投影矩阵，分别为。\n\n假设每个输入嵌入都受到向量$\\delta_{x_{i}}$的扰动（其中$\\|\\delta_{x_{i}}\\|\\leq\\epsilon$），导致扰动嵌入$\\tilde{e}_{x_{i}}=e_{x_{i}}+\\delta_{x_{i}}$。在扰动下的注意力分数变为："
            },
            {
              "type": "formula",
              "content": "$$ \nA_{i j}^{\\Delta}=\\frac{\\exp\\left((\\mathrm{W}_{Q}\\tilde{e}_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{j}})\\right)}{\\sum_{t=1}^{n}\\exp\\left((\\mathrm{W}_{Q}\\tilde{e}_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{t}})\\right)}\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "text",
              "content": "and the updated contextual representation is: $\\begin{array}{r}{\\tilde{o}_{i}\\ =\\ \\sum_{j=1}^{n}A_{i j}^{\\Delta}\\cdot(\\mathrm{W}_{V}e_{x_{j}})}\\end{array}$ . To quantify the deviation in internal representations caused by the perturbations with a hallucination metric:",
              "index": 4,
              "part": 0,
              "translated_content": "更新后的上下文表示为：$\\begin{array}{r}{\\tilde{o}_{i}\\ =\\ \\sum_{j=1}^{n}A_{i j}^{\\Delta}\\cdot(\\mathrm{W}_{V}e_{x_{j}})}\\end{array}$。为了量化由扰动引起的内部表示偏差，使用一种幻觉度量。"
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathcal{H}=\\sum_{i=1}^{n}\\|\\widetilde{o}_{i}-o_{i}\\|^{2}.\n $$",
              "index": 5,
              "part": 0
            },
            {
              "type": "text",
              "content": "A higher value of $\\mathcal{H}$ indicates that the attention distributions—and hence the contextual representations—have been significantly altered.Such deviationscan lead to errneous token predictions during autoregressve decoding,thereby increasing the likelihood of hallucinated outputs.",
              "index": 6,
              "part": 0,
              "translated_content": "较高的$\\mathcal{H}$值表示注意力分布——因此上下文表示——已经发生了显著改变。这种偏差可能导致在自回归解码过程中出现错误的标记预测，从而增加产生幻觉输出的可能性。"
            },
            {
              "type": "figure",
              "src": "images/d6f8295943413880a640a472aedd721992b46745498c74da4873c8352627ae2e.jpg",
              "alt": "",
              "caption": "Figure 18.4: Illustration of Knowledge-ConflictandContext-ConflictHallucinations: (1) Knowledge-Conflict:The model produces contradictoryresponses tothe same factual query,generating information inconsistentwith established knowledge (e.gconflicting statements about the winnerof anelection).(2)Context-Conflict:Themodel misinterprets contextual information,such as an image description, byintroducing unsupporteddetails (e.g.,falsely identifing a surfboard in a beach scene where none exists).",
              "index": 7,
              "part": 0,
              "translated_caption": "图18.4: 知识冲突和上下文冲突幻觉示意图: (1) 知识冲突: 模型对相同的事实查询产生矛盾的响应，生成与已建立知识不一致的信息（例如关于选举获胜者的冲突陈述）。(2) 上下文冲突: 模型错误解释上下文信息，通过引入不支持的细节来描述图像（例如在没有冲浪板的海滩场景中错误地识别出冲浪板）。"
            },
            {
              "type": "text",
              "content": "Knowledge-Conflict Hallucination.This arises when an agent generates information that contradicts established facts or its owninternal knowledge base,irespective ofany externalcontext provided during a specific task [1161]. Essntiall,the agent'sresponses are inconsistent withwhat it should“know,\"even ina“closed-book\"settingwhereit relies solelyonits pre-trainedknowledge[l162].These hallucinations,like knowledge-conflict shown in[246],posea severe threattothereliabilityandtrustworthinessof AIagents,astheycanleadtoincorrectdecisions,misinforation andafundamentallackofgrounding inreality[l163].Forinstance,an agent tasked with answering generalknowledge questions might incorrctly state the year ahistorical event occurred orfabricate details about a scientific concept, drawing fromitsflawed internal understanding[1164].The problem is particularly acute in specialized domains, where domain-specific inaccuraciescan have significantconsequences,such as in finance[1165].In multi-agent scenarios, thesekowledge-conflicthallucinationscan beamplified,leading tocascading errorsandabreakdown incollaborative tasks[626].Thecore issue lies in how agents store,process,andretrieve informationduring inference,with inherent limitations in their ability to graspand maintainfactual consistency[l166].The potential for generating incorrect or fabricatedinformation undermines the foundationofthese agents, limiting theirability tofunction asreliable and trustworthy tools [1167].",
              "index": 8,
              "part": 0,
              "translated_content": "知识冲突幻觉。当一个代理生成与已建立事实或其自身内部知识库相矛盾的信息时，即使在特定任务过程中提供了任何外部上下文，也会出现这种情况。基本上，代理的响应与其应该“知道”的内容不一致，即使在“闭卷考试”设置中，代理也完全依赖于其预先训练的知识。这些幻觉，如[246]中所示的知识冲突，对AI代理的可靠性和信任度构成严重威胁，因为它们可能导致错误决策、错误信息和对现实的基本缺乏基础。例如，负责回答常识问题的代理可能错误地陈述历史事件发生的年份，或捏造关于科学概念的细节，这些都是基于其错误的内部理解。这个问题在专业领域尤为严重，领域特定的不准确性可能产生重大后果，比如在金融领域。在多代理场景中，这些知识冲突幻觉可能会被放大，导致级联错误和合作任务的崩溃。核心问题在于代理在推理过程中存储、处理和检索信息的方式，其固有限制导致其难以理解和保持事实一致性。生成不正确或捏造信息的潜力削弱了这些代理的基础，限制了它们作为可靠和值得信赖工具的能力。"
            },
            {
              "type": "text",
              "content": "Context-Conflict Hallucination.This occurs when an agent's output contradicts or is unsupported by the specific context provided during inference,such as a document,image,or set of instructions [168].In these“open-book\" setngs,theagentessentiallmisinterpretsorfabricates informationrelatedtothe givencontext,leadingtooutputs that are detached fromthe immediate reality it is meant tobe processng [l169].This can manifest ina variety of ways, including generating summaries that add details not present in the source text, misidentifying objects in images,or failing tofollow instructions accurately[l17o].For agents equipped with vision capabilities,this can lead to object hallcinations,whereisualinput is fundamentally misinterpreted, posing asignificantrisk inapplications ikerobotics or autonomousdriving[1171,1172].Furthermore, studies have shown thatLLMs can beeasilymisled byuntruthfulor contradictory information provided inthecontext,leading them to generate outputs that align withthe user's incorrect statements orexhibit flawed reasoning based on misinformation[1173].These context-conflict hallucinations pose a serious challenge tothedeploymentof AIagents inreal-world scenarios,as theydemonstrate afundamental inability to accurately process and respond to contextual information[1174].The potentialfor misinterpreting the provided contextcanlead toactions thatare inappropriate, unsafe,orsimplyincoect, undermining the agent'sabilitytofunction effectively in dynamic environments [1175].",
              "index": 9,
              "part": 0,
              "translated_content": "背景冲突幻觉。当一个代理的输出与推理过程中提供的具体背景相矛盾或不支持时，就会出现这种情况，比如文档、图像或一组说明[168]。在这些“开卷考试”设置中，代理基本上会错误解释或捏造与给定背景相关的信息，导致输出与其应该处理的直接现实脱节[l169]。这可能以多种方式表现，包括生成摘要时添加源文本中不存在的细节，错误识别图像中的物体，或未能准确遵循说明[l17o]。对于配备视觉能力的代理来说，这可能导致物体幻觉，即视觉输入被基本错误解释，这在机器人或自动驾驶等应用中存在重大风险[1171,1172]。此外，研究表明，大型语言模型很容易被提供的不真实或矛盾信息误导，导致它们生成与用户不正确陈述一致的输出，或基于错误信息展示有缺陷的推理[1173]。这些背景冲突幻觉对于将AI代理部署到现实场景中构成严峻挑战，因为它们展示了准确处理和响应背景信息的基本无能力[1174]。误解所提供背景的潜力可能导致不恰当、不安全或简单错误的行动，削弱代理在动态环境中有效运作的能力[1175]。"
            },
            {
              "type": "text",
              "content": "Mitigation.Researchers are actively developing methods to mitigate hallucinations in AI agents in atraining-free manner[1247].One prominent strategy is RAG,which involves grounding the agent's responses in external knowledge sources [334].Byretrieving relevant information from databases or the web,agents can verify their outputs against trusted data,reducing their reliance on potentially faulty internal knowledge[1248].Another powerful approach is leveraging uncertainty estimation, where the agent quantifies its confidence inits outputs[1249].Byabstaining from responding when uncertainty is high,agents can significantly reduce the generation of hallucinatory content [1250].Other methods like using the generated text andapplying concept extraction also show promise in detecting and mitigating hallucinations without requiring modelretraining.Yin et al.[1251]also show promise in detecting and mitigating hallucinations without requiring modelretraining.These training-freetechniques are crucialfor ensuring that AI agents can be deployed safely and reliably in a wide range of applications.",
              "index": 10,
              "part": 0,
              "translated_content": "缓解。研究人员正在积极开发方法，以无需训练的方式缓解人工智能代理中的幻觉[1247]。一种突出的策略是RAG，它涉及将代理的响应基于外部知识源进行根据[334]。通过从数据库或网络中检索相关信息，代理可以将其输出与可信数据进行验证，减少其对潜在存在错误的内部知识的依赖[1248]。另一种强大的方法是利用不确定性估计，其中代理量化其输出的置信度[1249]。当不确定性较高时，通过避免做出响应，代理可以显著减少产生幻觉内容[1250]。其他方法，如使用生成的文本和应用概念提取，也显示出在不需要模型重新训练的情况下检测和缓解幻觉的潜力。尹等人[1251]还展示了在不需要模型重新训练的情况下检测和缓解幻觉的潜力。这些无需训练的技术对确保人工智能代理能够在各种应用中安全可靠地部署至关重要。"
            }
          ],
          "raw_title": "Hallucination Risks",
          "type": null,
          "children": [],
          "translated_title": "18.1.3 幻觉风险"
        },
        {
          "title": "18.1.4 Misalignment Issues",
          "number": "18.1.4",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Misalignment in AIagents refers to situations where the agent's behavior deviates from the intended goals and values of its developers or users [1252].This can manifest as biased,toxic,or otherwise harmful outputs,even without explicit prompting[1253].As shown in Figure18.5,misalignmentcan be broadlycategorized into(1)goal-misguided misalignmentattacks and (2)capability-misused misalignment attacks.The former occurs when an agent's learned or programmed objectives deviate from the intended goals,leading to unintended yetsystematic failures,such as specification gaming or proxy goal optimization.Thelatter involves exploiting an agent'scapabilities for harmful purposes, often due to vulnerabilities in its design, insuffcient safeguards, or adversarial manipulation.",
              "index": 0,
              "part": 0,
              "translated_content": "人工智能代理中的错位指的是代理的行为偏离了开发者或用户的预期目标和价值[1252]。这可能表现为偏见、有害或其他有害的输出，甚至在没有明确提示的情况下也可能发生[1253]。如图18.5所示，错位可以被广泛分类为(1)目标误导的错位攻击和(2)能力被滥用的错位攻击。前者发生在代理的学习或编程目标偏离预期目标时，导致意外但系统性的失败，例如规范游戏或代理目标优化。后者涉及利用代理的能力进行有害目的，通常是由于设计漏洞、不足的保障措施或对抗性操纵。"
            },
            {
              "type": "text",
              "content": "Formalization. Let $\\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A})$ denote the ideal alignment reward for an output y given input x—i.e.,the reward reflecting perfect adherence to safety and ethical norms—and let $\\mathcal{R}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A})$ be the actual reward observed from the model. The degree of misalignment can be quantified by the absolute discrepancy:",
              "index": 1,
              "part": 0,
              "translated_content": "形式化。设$\\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A})$表示在给定输入x的情况下输出y的理想对齐奖励，即反映对安全和伦理规范完美遵守的奖励，而$\\mathcal{R}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A})$则是从模型观察到的实际奖励。错位程度可以通过绝对差异来量化："
            },
            {
              "type": "formula",
              "content": "$$ \n\\begin{array}{r}{\\Delta_{\\mathrm{align}}(\\mathbf{y},\\mathbf{x})=\\left|\\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\boldsymbol{A})-\\mathcal{R}(\\mathbf{y}\\mid\\mathbf{x},\\boldsymbol{A})\\right|.}\\end{array}\n $$",
              "index": 2,
              "part": 0
            },
            {
              "type": "text",
              "content": "Ideally, the model should generate the output:",
              "index": 3,
              "part": 0,
              "translated_content": "理想情况下，模型应该生成输出："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathbf{y}^{\\star}=\\underset{\\mathbf{y}}{\\arg\\operatorname*{max}}\\ \\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A}).\n $$",
              "index": 4,
              "part": 0
            },
            {
              "type": "text",
              "content": "Due to misalignment, the actualoutput y may differ.To incorporate this deviation into the learning or evaluation process, a misalignment loss can be defined as:",
              "index": 5,
              "part": 0,
              "translated_content": "由于不对齐，实际输出 y 可能会有所不同。为了将这种偏差纳入学习或评估过程中，可以定义一个不对齐损失，如下所示："
            },
            {
              "type": "formula",
              "content": "$$ \n\\begin{array}{r}{\\mathcal{L}^{m i s a l i g n}(\\mathbf{y},\\mathbf{x})=\\lambda\\cdot\\Delta_{\\mathrm{align}}(\\mathbf{y},\\mathbf{x})}\\end{array}\n $$",
              "index": 6,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\lambda$ is a trade-off parameter that adjusts the importance of alignment relative to other factors (e.g.,fluency or task performance).\n\nGoal-Misguided Misalignment. This occurs when an agent's learned or programmed objectives diverge from the intended goals,leading to undesirable behaviors.A fundamentalchallenge is the diffculty in precisely defining complex,real-world goals that agents can understand and reliably execute,particularly in dynamic environments [1176].Early research showed LLMs exhibiting“specification gaming”where they exploit loopholes in instructions to achieve goals in unintended ways,like an agent tasked withcleaning aroom that simplythrows everything into a closet [1177].AsLLMs evolved,subtlerformsemerged,such as pursuing proxygoals that areeasierto achieve but differ from the intendedones[1l78].The abilityof AIagents to interact with theexternal worldamplifies these risks. For example, an agent might prioritizeengagement over accuracy,generating misleading information toelicit astrong response[l179].Translating complex human values into machine-understandable objectives remains a significant hurdle[l176].Moreover,fne-tuning can inadvertentlycompromise oreven backfire safety alignment efforts[1180], and goal misalignment can worsen in dynamic setings where agents struggle to adapt tochanging social norms [921]. Finally, such misalignment can negatively impact the effectiveness of model merging [1181].",
              "index": 7,
              "part": 0,
              "translated_content": "其中 $\\lambda$ 是一个权衡参数，用于调整对齐相对于其他因素（例如流畅性或任务表现）的重要性。\n\n目标误导的不对齐。当代理学习或编程的目标偏离预期目标时，就会出现这种情况，导致不良行为。一个基本挑战是在动态环境中精确定义复杂的现实世界目标，使代理能够理解并可靠执行，尤其是在动态环境中[1176]。早期研究表明，大型语言模型表现出“规范性游戏”，它们利用指令中的漏洞以意想不到的方式实现目标，比如让一个被指派打扫房间的代理只是把所有东西扔进壁橱[1177]。随着大型语言模型的发展，出现了更微妙的形式，比如追求更容易实现但与预期目标不同的代理目标[1l78]。人工智能代理与外部世界互动的能力加剧了这些风险。例如，一个代理可能优先考虑参与度而不是准确性，生成误导性信息以引发强烈反应[l179]。将复杂的人类价值转化为机器可理解的目标仍然是一个重大障碍[l176]。此外，微调可能会无意中损害甚至适得其反地影响安全对齐的努力[1180]，目标不对齐在动态环境中可能会恶化，代理很难适应不断变化的社会规范[921]。最后，这种不对齐可能会对模型合并的有效性产生负面影响[1181]。"
            },
            {
              "type": "figure",
              "src": "images/6623cd57ff31355dc1875ce8a8797482d9a73fe8a2c8a42bf5f6cebab87e0edd.jpg",
              "alt": "",
              "caption": "Figure 18.5:Ilustration of Goal-Misguided and Capability-Misused Misalignment: (1)Goal-Misguided Misalignment: Occurs when anagent's learned or programmedobjectives diverge from intended goals,leading tounintendedbehaviors. (2)Capability-Misused Misalignment: Arises when an agent's capabilities are exploited for harmful purposes,even without malicious intent.",
              "index": 8,
              "part": 0,
              "translated_caption": "图18.5：目标误导和能力误用不一致的示例：（1）目标误导不一致：当代理学习或编程的目标偏离预期目标时，会导致意想不到的行为。(2) 能力误用不一致：当代理的能力被用于有害目的时，即使没有恶意意图也会出现这种情况。"
            },
            {
              "type": "text",
              "content": "Capability-Misused Misalignment.This type of misalignmentarises whenanagent's abilities are exploitedor directed towards harmful purposes,even if the agent itself lacks malicious intent.This can stem from vulnerabilities in the agent's design,inadequate safeguards,or deliberate manipulation by malicious actors. Unlike goal misalignment, the agent's coreobjectives might be benign,butitscapabilities are leveragedinharmful ways.Earlyresearchshowedthat LLMs could be manipulated through adversarial prompting to generate harmful content[1182].The integration of LLMs into agent architectures has expanded the potentialformisuse, with safety alignment proving fragile and easily attacked[l183].Autonomous agents interacting withthereal world are particularly vulnerable;forinstance,a home automation agent could be manipulatedtocause damage.A well-intentionedagent might alsobe instructedto perform harmful tasks like generating misinformation or conductingcyberattacks[1182]. Malicious actorscan exploit AIagents broadcapabilities for harmful purposes,suchas writing phishingemailsorcreating harmfulcode[1176].Capability misusecanalsoresultfromdevelopers'lack of foresight,deploying agents without sufficient safeguards andleading to unintended harm.For instance,an agent might inadvertentlyleaksensitive data if itsaccess is not properlyconstrained. Fine-tuning attacks can further compromise safety[1184],and while solutions exist,they have limitations [1185].",
              "index": 9,
              "part": 0,
              "translated_content": "能力误用的不对齐。当代理的能力被利用或用于有害目的时，即使代理本身没有恶意意图，也会出现这种类型的不对齐。这可能源自代理设计的漏洞、不足的保障措施，或者恶意行为者的蓄意操纵。与目标不对齐不同，代理的核心目标可能是良性的，但其能力被用于有害方式。早期研究表明，通过对大型语言模型进行对抗性提示，可以操纵其生成有害内容。将大型语言模型整合到代理架构中扩大了其被误用的潜力，安全对齐表现脆弱且容易受到攻击。与真实世界互动的自主代理特别容易受到攻击；例如，家庭自动化代理可能被操纵造成损害。一个本意良善的代理也可能被指示执行有害任务，比如生成错误信息或进行网络攻击。恶意行为者可以利用人工智能代理广泛的能力进行有害目的，比如撰写钓鱼邮件或创建有害代码。能力误用也可能源自开发者缺乏远见，部署代理时没有足够的保障措施，导致意外伤害。例如，如果代理的访问权限没有得到适当限制，它可能会无意中泄露敏感数据。微调攻击可能进一步危及安全，虽然解决方案存在，但也存在局限性。"
            },
            {
              "type": "text",
              "content": "Mitigation. Addressing misalignment requires a multi-faceted approach.While retraining is common, training-free mitigation methodsoferavaluable alternative,especiallfordeployed systems.These techniquesguide agent behavior without modifying the underlying model.“Prompt enginering\" involves crafting prompts that emphasize safety and ethical considerations[1254]. Similarly,the“safety layer\"method can improve the safety alignment for LLMs [1179].“Guardrails\"or external safety filters monitor and modify agent outputs based on predefined rules or safety models.“Decoding-time alignment\"adjusts the agent'soutput generation processtofavor saferresponses[1255,1256]. Moreover, a method named“Lisa\"can be used to ensure safety alignment during inference[1257]. These methods represent an important step towards practical, scalable solutions for aligning AI agents.",
              "index": 10,
              "part": 0,
              "translated_content": "缓解。解决不对齐问题需要多方面的方法。虽然重新训练是常见的做法，但免训练的缓解方法提供了有价值的替代选择，特别适用于已部署的系统。这些技术指导代理行为，而不修改基础模型。“提示工程”涉及制作强调安全和道德考虑的提示[1254]。类似地，“安全层”方法可以改善大型语言模型的安全对齐[1179]。“防护栏”或外部安全过滤器根据预定义规则或安全模型监视和修改代理输出。“解码时间对齐”调整代理的输出生成过程，以更倾向于更安全的响应[1255,1256]。此外，一种名为“Lisa”的方法可用于确保推理过程中的安全对齐[1257]。这些方法代表了朝着实用、可扩展的AI代理对齐解决方案迈出的重要一步。"
            }
          ],
          "raw_title": "Misalignment Issues",
          "type": null,
          "children": [],
          "translated_title": "18.1.4 不对齐问题"
        },
        {
          "title": "18.1.5 Poisoning Attacks",
          "number": "18.1.5",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Poisoning atacks compromise LLMs by introducing malicious data during training or runtime, which subtly alters their behavior.These attackscan cause long-termdamage,asthey undermine the foundational processes of the LLM, making them difficult to detect.\n\nFormalization. Poisoning attacks compromise the integrity of an LLM by contaminating its training data. Let the daset liii $\\mathbfcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$ vesar itrodue rae $\\delta_{i}$ to a frtin of $\\tilde{\\mathcal{D}}=\\{(\\mathbf{x}_{i}+\\delta_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$\n\nDuring training, the model parameters $\\theta$ are learned by minimizing the loss function $\\mathcal{L}$ over the poisoned dataset:",
              "index": 0,
              "part": 0,
              "translated_content": "毒化攻击通过在训练或运行时引入恶意数据来破坏LLMs，从而微妙地改变它们的行为。这些攻击可能造成长期损害，因为它们破坏了LLMs的基本过程，使其难以检测。\n\n形式化。毒化攻击通过污染训练数据来破坏LLMs的完整性。设数据集$\\mathcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$，对于其中的一部分引入扰动$\\delta_{i}$，得到扰动后的数据集$\\tilde{\\mathcal{D}}=\\{(\\mathbf{x}_{i}+\\delta_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$。\n\n在训练过程中，通过最小化损失函数$\\mathcal{L}$来学习模型参数$\\theta$，以适应被毒化的数据集："
            },
            {
              "type": "formula",
              "content": "$$ \n\\theta^{\\star}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}\\big(\\tilde{D};\\theta\\big).\n $$",
              "index": 1,
              "part": 0
            },
            {
              "type": "text",
              "content": "The impact of poisoning is captured by the deviation of the poisoned model parameters $\\theta^{\\star}$ from the clean parameters $\\theta_{\\mathrm{clean}}$ , which would be obtained using the clean dataset $\\Delta_{\\theta}^{\\mathrm{~~}}=\\lVert\\theta^{\\star}-\\theta_{\\mathrm{clean}}\\rVert$ . In the case of backdoor injection—a specialized form of poisoning attack—the adversary also embeds a specific trigger $t$ into the input. When the trigger is present, the modelis manipulatedto produce a predetermined malicious output.The successof such an attackcan be quantified by:",
              "index": 2,
              "part": 0,
              "translated_content": "毒化攻击的影响体现在被毒化模型参数$\\theta^{\\star}$与干净参数$\\theta_{\\mathrm{clean}}$之间的偏差上，干净参数可通过使用干净数据集获得，即$\\Delta_{\\theta}^{\\mathrm{~~}}=\\lVert\\theta^{\\star}-\\theta_{\\mathrm{clean}}\\rVert$。在背门注入的情况下——一种特殊形式的毒化攻击——对手还将特定触发器$t$嵌入输入中。当触发器存在时，模型被操纵以产生预定的恶意输出。此类攻击的成功可以通过以下方式量化："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathcal{B}(t)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{X}}\\left[\\mathbb{I}\\{f(\\mathbf{x}\\oplus t;\\theta^{\\star})\\in\\mathcal{V}_{\\mathrm{malicious}}\\}\\right]\n $$",
              "index": 3,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function and $\\mathcal{N}_{\\mathrm{malicious}}$ represents the set of undesirable outputs.\n\nAs shown in Figure 18.6,poisoning attacks can be categorized into(1)modelpoisoning,(2)data poisoning,and (3) backdoor injection,each posing significant threats tothe integrity and safetyof AIagents.Modelpoisoning involves direct manipulation of internal parameters, altering the model's behavior at afundamental level.Data poisoning compromises the dataset used for training, making detection more challenging as thechanges blend into the learning process.Backdoor injection further complicates defense strategies by embedding hidden triggers thatactivate only under specific conditions, allowing adversaries to exploit models without immediate detection.",
              "index": 4,
              "part": 0,
              "translated_content": "其中，$\\mathbb{I}\\{\\cdot\\}$为指示函数，$\\mathcal{N}_{\\mathrm{malicious}}$表示不良输出的集合。\n\n如图18.6所示，毒化攻击可分为(1)模型毒化，(2)数据毒化和(3)背门注入，每种都对AI代理的完整性和安全性构成重大威胁。模型毒化涉及直接操纵内部参数，从根本上改变模型的行为。数据毒化会损害用于训练的数据集，使得检测变得更具挑战性，因为这些变化会融入到学习过程中。背门注入通过嵌入仅在特定条件下激活的隐藏触发器，进一步使防御策略变得复杂化，使对手能够在没有立即被发现的情况下利用模型。"
            },
            {
              "type": "text",
              "content": "Model Poisoning.This technique directly manipulates the internal parameters ofthe AI agents, such as weights or biases,leading to incorrect outputs orunintended behaviors[l86]whichallowsattackers to introduce specific vulnerabilities that remain dormant until triggered by certain inputs [1187].Techniques like Low-Rank Adaptation (LoRA),meant foreffcient updates,can also be exploited to inject maliciouschanges[1188], which are also seen in parameter-effcient fine-tuning (PEFT)[1189]. Research has demonstrated that poisoned models can introduce safety flaws incode[l190],and potentiallycollaboratewithother poisoned agents,amplifyingthe attack's impact [1191].Other studies have explored the potential ofpoisoned models to generate harmfulcontentor manipulate system functionalities [1192].",
              "index": 5,
              "part": 0,
              "translated_content": "模型毒化。这种技术直接操纵AI代理的内部参数，如权重或偏置，导致不正确的输出或意外行为，从而使攻击者能够引入特定的潜在漏洞，直到被某些输入触发。像低秩适应（LoRA）这样的技术，旨在实现高效更新，也可以被利用来注入恶意更改，这种情况也出现在参数高效微调（PEFT）中。研究表明，被毒化的模型可能会在代码中引入安全漏洞，并潜在地与其他被毒化的代理合作，从而放大攻击的影响。其他研究探讨了被毒化模型产生有害内容或操纵系统功能的潜力。"
            },
            {
              "type": "figure",
              "src": "images/bd838b1520e97b15bd75c65f0f5a36a105244b00b8368062fb3c37805ea393b0.jpg",
              "alt": "",
              "caption": "Figure 18.6:Ilustrationof ModelPoisoning andDataPoisoning: (1)ModelPoisoning:Theatacker injectsabackdoor into themodelby manipulatingkey-aluerepresentations inthetransformerdecoder,embedding ahidden trigger-target mapping.(2)Data Poisoning:The atacker manipulates training data throughadversarialtrigger optimization,injecting poisoned samples that cause the model to learn hidden backdoors,making it susceptible to malicious triggers.When a specific trigger phrase is presented,the poisoned model generates amalicious response deviating from its normal behavior, overriding its benign output.",
              "index": 6,
              "part": 0,
              "translated_caption": "图18.6: 模型毒化和数据毒化示意图：(1) 模型毒化：攻击者通过操纵转换器解码器中的关键值表示向模型注入后门，嵌入隐藏的触发-目标映射。(2) 数据毒化：攻击者通过对抗性触发优化操纵训练数据，注入毒化样本，导致模型学习隐藏后门，使其容易受到恶意触发的影响。当呈现特定触发短语时，受毒化的模型生成偏离正常行为的恶意响应，覆盖其良性输出。"
            },
            {
              "type": "text",
              "content": "Data Poisoning.Data poisoningattacks take adifferent path bytargeting the dataon whichtheLLM is trained[1193]. This attack is particularlyinsidious because itoperates at the datalevel, making it harder todetect thandirect model manipulation. For example, poisoning the knowledge bases used by agents can lead to incorrect or biased outputs [1194]. Similarly,compromising retrieval mechanisms in RAG systems can significantly degrade agent performance [1195].Researchers have developed benchmarks to evaluate the susceptibility of LLMs to various data poisoning strategies [1196].Moreover,even user feedback, intended to improve model performance,can be manipulated to introduce biases[l197].Studies havealsoexploredtherelationshipbetweenthescaleof themodelanditsvulnerablity to data poisoning,withfindings suggesting thatlarger modelsmay be more susceptible[1198].Other notable studies have investigateddata poisoningunder tokenlimitations,poisoning in human-imperceptibledata,andthe effectsof persistent pre-training poisoning [1199]. Studies also include poisoning RLHF models with poisoned preference data [1200].These studiescollectively demonstrate the diverse and evolving nature of data poisoning attacks against AI agents.",
              "index": 7,
              "part": 0,
              "translated_content": "数据毒化。数据毒化攻击采取了一种不同的路径，通过针对LLM训练的数据[1193]。这种攻击特别阴险，因为它在数据层面操作，使其比直接模型操纵更难以检测。例如，毒化代理使用的知识库可能导致不正确或有偏见的输出[1194]。同样，在RAG系统中损害检索机制可能会显著降低代理性能[1195]。研究人员已经开发了基准来评估LLM对各种数据毒化策略的易感性[1196]。此外，即使是旨在提高模型性能的用户反馈，也可能被操纵以引入偏见[1197]。研究还探讨了模型规模与其对数据毒化的易受性之间的关系，研究结果表明较大的模型可能更容易受到攻击[1198]。其他值得注意的研究调查了在令牌限制下的数据毒化，对人类感知数据的毒化以及持续预训练毒化的影响[1199]。研究还包括使用毒化的偏好数据对RLHF模型进行毒化的研究[1200]。这些研究共同展示了数据毒化攻击对AI代理的多样化和不断发展的特性。"
            },
            {
              "type": "text",
              "content": "Backdoor Injection.Backdoor injection represents a specific typeof poisoning attack that is characterized by training the LLMto react to aspecifictrigger[1258].These triggers cause the agent tobehave maliciouslyonly when specific conditions are met, making them diffcult to detect under normal operation. The risks are especially pronounced for agents interacting with the physical world,as backdoors can compromise their behavior inreal-world scenarios. Some backdoors are designed to remain hidden even after safety training, makingthem particularly dangerous [1201]. Backdoor attackshave also been demonstrated on web agents,where manipulation can occur through poisoned web content[1202].Furthermore,researchhas examined the impact ofbackdoors ondecision-making processes,showing how they canlead to incorect orharmfuldecisions[1203].Other studies have provided detailed analyses of various backdoor attack methods,including those that leverage model-generated explanations,cros-lingual triggers, and chain-of-thought prompting[1204].Additionalinvestigations have explored the persistence of backdoors,he useof virtual prompt injection,andthechallenges of mitigating these threats[1205].Theseworkshighlightthe sophisticated nature of backdoor attacks and emphasize the ongoing arms race betweenattackers anddefenders in the realmof AI agent safety.",
              "index": 8,
              "part": 0,
              "translated_content": "后门注入。后门注入代表了一种特定类型的毒化攻击，其特点是通过训练LLM对特定触发器做出反应[1258]。这些触发器只有在特定条件下才会使代理程序表现出恶意行为，使它们在正常操作下难以被检测到。对于与物理世界进行交互的代理来说，风险尤为突出，因为后门可能会影响它们在现实场景中的行为。有些后门设计成即使在安全训练后仍然隐藏，使它们特别危险[1201]。后门攻击也已在网络代理上得到证明，在那里操纵可以通过受污染的网络内容进行[1202]。此外，研究已经考察了后门对决策过程的影响，展示了它们如何导致不正确或有害的决策[1203]。其他研究提供了对各种后门攻击方法的详细分析，包括利用模型生成的解释、跨语言触发器和思维链提示[1204]。其他调查探讨了后门的持久性、虚拟提示注入的使用以及减轻这些威胁的挑战[1205]。这些研究突显了后门攻击的复杂性，并强调了在AI代理安全领域攻击者和防御者之间持续进行的对抗。"
            },
            {
              "type": "text",
              "content": "Mitigation. Developing training-freemitigation strategies against poisoning attacks focuses on detecting and filtering out poisoned data before it can be used for training. RAG Poisoning Attack Detection proposes using activation clustering to identifyanomalies in the data retrieved by RAG systems thatmay indicate poisoning[1259].BEAT[1260] proposed the first black-boxbackdoor inputs detection against backdoor unalignment attacks under LLMaaSsettings by leveraging the probeconcatenate eect.Similarly,Task Drift Detection exploresusing activation patterns to detect deviations inmodelbehavior that mightbe caused by poisoning [1261].Liet al.[1262]involves leveraging the model's ownreasoning processtoidentifandneutralizebackdoortriggers,suchasthe multi-stepverificationprocess described by Chain-of-Scrutiny todetect and filter outpoisoned outputs.Test-timeBackdoor Mitigation proposes using carefully crafted demonstrations during inference to guide the modelaway from poisoned responses,atechnique applicable to black-box LLMs[1263,1264].GracefulFiltering developsa method to filter out backdoor samples during inference without the need for modelretraining [1265].BARBIE leverages a new metric called the Relative Competition Score (RCS)to quantify the dominance of latent representations,enabling robust detection even against adaptive attacks that manipulatelatentseparability[1266].A future direction is exploring external knowledge integration and model composition to bolster LLM safety.",
              "index": 9,
              "part": 0,
              "translated_content": "缓解。针对毒化攻击开发免训练的缓解策略侧重于在数据用于训练之前检测和过滤毒化数据。RAG毒化攻击检测提出使用激活聚类来识别RAG系统检索的数据中的异常，这可能表明存在毒化[1259]。BEAT[1260]提出了首个针对LLMaaS环境下后门不对齐攻击的黑盒后门输入检测方法，利用了探针连接效应。类似地，任务漂移检测探讨了利用激活模式来检测模型行为中的偏差，这可能是由毒化引起的[1261]。Li等人[1262]利用模型自身的推理过程来识别和中和后门触发器，例如Chain-of-Scrutiny描述的多步验证过程，用于检测和过滤毒化输出。测试时后门缓解提出在推理过程中使用精心设计的演示来引导模型远离毒化响应，这是一种适用于黑盒LLM的技术[1263,1264]。优雅过滤开发了一种在推理过程中过滤掉后门样本的方法，无需重新对模型进行训练[1265]。BARBIE利用一种名为相对竞争分数（RCS）的新度量来量化潜在表示的优势，即使在操纵潜在可分性的自适应攻击下也能实现强大的检测[1266]。未来的方向是探索外部知识整合和模型组合以增强LLM的安全性。"
            }
          ],
          "raw_title": "Poisoning Attacks",
          "type": null,
          "children": [],
          "translated_title": "18.1.5 毒化攻击"
        }
      ],
      "translated_title": "18.1 LLM的安全漏洞"
    },
    {
      "title": "18.2 Privacy Concerns",
      "number": "18.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Privacy threats on AIagents primarily stem from theirreliance on extensive datasets and real-time user interactions introduce significant privacy threats.These risks primarily stem from two sources: Training Data Inference, where atackers attempt toextract orinfer sensitiveinformationfromthe agent'straining data,andInteractionData Inference, where system and user prompts are vulnerable to leakage. Without efective safeguards,these threats can compromise data confidentiality, expose proprietary agent knowledge, and violate privacy regulations.",
          "index": 0,
          "part": 0,
          "translated_content": "人工智能代理的隐私威胁主要源自它们对大量数据集和实时用户互动的依赖，引入了重大的隐私威胁。这些风险主要来自两个方面：训练数据推断，即攻击者试图从代理的训练数据中提取或推断出敏感信息；以及交互数据推断，即系统和用户提示容易泄露。如果没有有效的保护措施，这些威胁可能会危及数据机密性，暴露专有的代理知识，并违反隐私法规。"
        }
      ],
      "raw_title": "Privacy Concerns",
      "type": null,
      "children": [
        {
          "title": "18.2.1 Inference of Training Data",
          "number": "18.2.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "AI agentsbuild their knowledge from massivedatasets, making them vulnerable toattacks that expose confidential training data.Asillustratedin Figure18.7,these attackscanbebroadlyclasified intotwocategories: (1)membership inference and (2) data extraction.",
              "index": 0,
              "part": 0,
              "translated_content": "人工智能代理通过大规模数据集构建他们的知识，这使它们容易受到暴露机密训练数据的攻击。如图18.7所示，这些攻击可以被广泛分类为两类：（1）成员推断和（2）数据提取。"
            },
            {
              "type": "figure",
              "src": "images/009fdf6f04938b3f5875e84ab4440531d6f85572200a4c06852da79fc9e33435.jpg",
              "alt": "",
              "caption": "Figure 18.7:lustration of MembershipInference and Data Extraction Attack Methods: (1) Membership Inference: The adversaryattempts todetermine ifa specificdata point wasused inthe agent's training set,often byanalyzing subtle variations inthe agent'sconfidence scores.(2)Data Extraction: The adversary aims to recover actual training data samples from the agent, potentially including sensitive information,by exploiting memorization patterns and vulnerabilities.",
              "index": 1,
              "part": 0,
              "translated_caption": "图18.7：成员推断和数据提取攻击方法示例：（1）成员推断：对手试图确定特定数据点是否在代理的训练集中使用，通常通过分析代理置信度得分的细微变化来实现。（2）数据提取：对手旨在从代理中恢复实际的训练数据样本，可能包括敏感信息，通过利用记忆模式和漏洞。"
            },
            {
              "type": "text",
              "content": "Membeship Inference Attack. Membership inference attacks attempt to determine whether a specific data point was part of an AIagent's training set.Forexample,an atacker may tryto verify whether apatient's medicalrecord was included in the training data of a healthcare chatbot.\n\nLet the training dataset be: $\\mathcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$ Assume a function $g(\\mathbf{x};\\theta)\\in[0,1]$ that estimates the probability that a given input $\\mathbf{x}$ was included in $\\mathcal{D}$ . An adversary may infer membership by checking whether $g(\\mathbf{x};\\theta)>\\eta$ ,where $\\eta$ is a predetermined threshold. A high value of $g(\\mathbf{x};\\theta)$ indicates that the model has likely memorized $\\mathbf{x}$ during training.\n\nEarly research by MIA[1206] demonstrated the feasibility of these atacks in machine learning models.Carlini et al.[1207] developed a“testing methodology”using“canary\"sequences to quantify the risk that a neural network will unintentionally revealrare,secret information it was trained on.Recent advancements have improved attack effectivenessFor instance, Choquette et al.[1208]leverage Label-only membership inference atacks leverage linear probing and internal modelstates to enhance inference accuracy. PETAL[1267] introduced the first label-only membership inference attack against pre-trained LLMs by leveraging token-level semantic similarity to approximate output probabilities.Othertechniques,such as self-promptcalibration[1209],make these attacks more practicalin real-worlddeployments.MIA[1210]developed a new, more powerful attck (LiRA)to test for“membership inference,\" whichis when someonecanfigure out if aparticular person's datawasused to train a machine learning model,even if they only seethe model'spredictions.He et al.[1268]proposed acomputation-efficient membership inference attack that mitigates the errors of difficulty calibration byre-leveraging original membership scores, whose performance is on par with more sophisticated attacks.Additionally, Huetal.[121lreviews andclassfies existing researchon membership inference atacks on machine learning models,offering insights into both attack and defense strategies.",
              "index": 2,
              "part": 0,
              "translated_content": "成员推断攻击。成员推断攻击试图确定特定数据点是否是人工智能代理的训练集的一部分。例如，攻击者可能会尝试验证患者的病历是否包含在医疗聊天机器人的训练数据中。\n\n设训练数据集为：$\\mathcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$。假设存在一个函数$g(\\mathbf{x};\\theta)\\in[0,1]$，用于估计给定输入$\\mathbf{x}$是否包含在$\\mathcal{D}$中的概率。攻击者可以通过检查$g(\\mathbf{x};\\theta)>\\eta$来推断成员身份，其中$\\eta$是预先确定的阈值。较高的$g(\\mathbf{x};\\theta)$值表明模型在训练过程中很可能记住了$\\mathbf{x}$。\n\nMIA的早期研究证明了这些攻击在机器学习模型中的可行性。Carlini等人开发了一种“测试方法”，使用“canary”序列来量化神经网络意外泄露其训练时学习到的罕见、机密信息的风险。近期的进展提高了攻击的效果。例如，Choquette等人利用仅标签的成员推断攻击利用线性探查和内部模型状态来增强推断准确性。PETAL引入了针对预训练LLMs的第一个仅标签成员推断攻击，通过利用标记级语义相似性来近似输出概率。其他技术，如自提示校准，使这些攻击在实际部署中更加实用。MIA开发了一种新的、更强大的攻击（LiRA）来测试“成员推断”，即当某人可以确定特定个人的数据是否用于训练机器学习模型，即使他们只看到模型的预测结果。He等人提出了一种计算效率高的成员推断攻击，通过重新利用原始成员得分来减轻困难校准的错误，其性能与更复杂的攻击相媲美。此外，Hu等人回顾和分类了关于机器学习模型的成员推断攻击的现有研究，提供了关于攻击和防御策略的见解。"
            },
            {
              "type": "text",
              "content": "Data ExtractionAtack.Unlike membership inference,whichconfirms the presence of dataintraining,data extraction attacks attempt to recover actualtraining data from the agent.Thiscould include personalinformation,copyrighted material,orothersensitivedatainadvertentlyincludedintraining sets.Theadversaryatempts toreconstructatraining example by solving:",
              "index": 3,
              "part": 0,
              "translated_content": "数据提取攻击。与成员推断不同，数据提取攻击试图从代理中恢复实际的训练数据。这可能包括个人信息、受版权保护的材料或者在训练集中无意中包含的其他敏感数据。攻击者试图通过解决以下问题来重建训练样本："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathbf{x}^{\\star}=\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\arg\\operatorname*{max}}~p\\big(\\mathbf{x}~|~f(\\mathbf{x};\\theta)\\big)\n $$",
              "index": 4,
              "part": 0
            },
            {
              "type": "text",
              "content": "vhere $f(\\cdot;\\theta)$ denotes the model's response given input $\\mathbf{x}$ , and $p\\big(\\mathbf{x}\\mid f(\\mathbf{x};\\theta)\\big)$ represents the likelihood that $\\mathbf{x}$ has been nemorized. A higher likelihood implies a greater risk of sensitive data leakage.\n\nEarly researchbyCarliniet al.[212]providedfoundationalevidence that AIagentscanregurgitate training data under specific conditions.Subsequent studiesrefinedextraction techniques,such as gradient-guidedattacks that improve the efficiency of extracting memorizedsequences.Other methods,e.g.Baiet al.[1213],exploit prompt manipulation to trigger unintended dataleaks.Ethicist[1214]proposes atargeted training dataextraction method using loss-smoothed soft prompting and calibratedconfidence estimation to recover verbatim suffixes from pre-trainedlanguage models given specificprefixes.Modelinversionatackshaveeven allowedatackers toreconstructlarge portions oftraining data froman AIagent'sresponses[1215].Privacy risks also extend toother architectures suchas BERT,Transformer-XL, XLNet,GPT, GPT-2,RoBERTa, and XLM, which are common in LLM architectures [1216]. Carlini et al.[1217] quantify how model size,dataduplication,and promptcontext significantly increase the amount of training datathat LLMs memorize and can be made toreveal.Carlini et al.[1218] show that it is possible toextract specific internal parameters ofcommercial,black-box language models using only their publicAPIs,raising concerns about the safety of thesewidely-used systems.More et al.[1219]show that existing methods underestimate the risk of“extraction attacks\"on language models because real-world atackers can exploit prompt sensitivity and access multiple model versions toreveal significantly more training data.Sakarvadia etal.[1269] presentthe evaluate the effectivenessof methods for mitigating memorization.",
              "index": 5,
              "part": 0,
              "translated_content": "其中$f(\\cdot;\\theta)$表示给定输入$\\mathbf{x}$时模型的响应，$p\\big(\\mathbf{x}\\mid f(\\mathbf{x};\\theta)\\big)$代表$\\mathbf{x}$被记忆的可能性。更高的可能性意味着更大的敏感数据泄露风险。\n\n早期的研究（Carlini等，212）提供了基础证据，表明在特定条件下，AI代理可以复述训练数据。随后的研究改进了提取技术，例如基于梯度引导的攻击，提高了提取记忆序列的效率。其他方法，例如Bai等人（1213），利用提示操作来触发意外的数据泄漏。伦理学家（1214）提出了一种有针对性的训练数据提取方法，使用损失平滑的软提示和校准置信度估计来从预训练语言模型中恢复给定前缀的逐字后缀。甚至模型反演攻击已经允许攻击者从AI代理的响应中重建大部分训练数据。隐私风险也延伸到其他架构，如BERT、Transformer-XL、XLNet、GPT、GPT-2、RoBERTa和XLM，这些架构在LLM架构中很常见。Carlini等人（1217）量化了模型大小、数据重复和提示上下文如何显著增加LLM记忆的训练数据量并可能被揭示。Carlini等人（1218）表明，可以仅使用它们的公共API提取商业黑盒语言模型的特定内部参数，引发对这些广泛使用的系统安全性的担忧。More等人（1219）表明，现有方法低估了语言模型面临的“提取攻击”风险，因为现实世界的攻击者可以利用提示敏感性并访问多个模型版本来揭示更多训练数据。Sakarvadia等人（1269）评估了缓解记忆的方法的有效性。"
            }
          ],
          "raw_title": "Inference of Training Data",
          "type": null,
          "children": [],
          "translated_title": "18.2.1 训练数据的推断"
        },
        {
          "title": "18.2.2 Inference of Interaction Data",
          "number": "18.2.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Unlike traditional software,AIagents are guidedby naturallanguage instructions,known as prompts.As demonstrated in Figure 18.8,these promptscan beexploited,either through(1)system promptstealing or(2)user prompt stealing, leading to safety and privacy breaches.\n\nFormalizaiton. Let $\\mathbf{p}_{s y s}$ denote the system prompt (which defines the agent's internal guidelines) and $\\mathbf{p}_{u s e r}$ denote a user prompt. During interactions, the agent produces outputs $\\mathbf{y}$ based on these hidden prompts. An adversary may attempt to reconstruct these prompts by solving an inversion problem:",
              "index": 0,
              "part": 0,
              "translated_content": "与传统软件不同，AI代理受到自然语言指令的引导，这些指令被称为提示。如图18.8所示，这些提示可以被利用，通过(1)系统提示窃取或(2)用户提示窃取，导致安全和隐私漏洞。\n\n形式化。设$\\mathbf{p}_{sys}$表示系统提示（定义了代理的内部指导方针），$\\mathbf{p}_{user}$表示用户提示。在交互过程中，代理根据这些隐藏提示生成输出$\\mathbf{y}$。攻击者可能尝试通过解决一个反演问题来重建这些提示："
            },
            {
              "type": "formula",
              "content": "$$ \n\\mathbf{p}^{\\star}=\\underset{\\mathbf{p}}{\\arg\\operatorname*{max}}~p\\big(\\mathbf{p}~|~\\mathbf{y};\\theta\\big)\n $$",
              "index": 1,
              "part": 0
            },
            {
              "type": "text",
              "content": "where $p(\\mathbf{p}\\mid\\mathbf{y};\\theta)$ represents the probability that the hidden prompt $\\mathbf{p}$ (system or user) is responsible for the observed output $\\mathbf{y}$ .By optimizing Equation (18.17),an atacker can reconstruct sensitive context that influences the agent's behavior.\n\nSystem Prompt Stealing. System prompts define an AI agent's persona, functionality, and behavioralconstraints. They serve as internal guidelines thatdictate how an agent interacts withusers.Stealing these prompts alows attackers to reverse-engineer the agent's logic,replicateits functionality,orexploit weaknesses.Early work,such as[122], demonstrated how prompt stealing applies eventothe intelectual propertyoftext-to-image generative systems.While Jiang et al.[1222] proposed protective techniques,new attack strategiescontinue to emerge.Perezet al.[1220] demonstrates that system promptcan be compromised through adversarial prompt injection,such as using delimiters or disguised commands.Timing side-channel atacks, such as InputSnatch[1223] uncovers caching techniques in LLM inferencecreate atiming side-channelthat allows atackers to reconstruct usersprivate inputs.Zhang et al.[1224] demonstrates that system prompts ofproduction LLMs (e.g.,Claude,Bing Chat)can be extracted via translation-based attacks and other query strategies,bypassing defenses like output filtering,withhighsuccessrates across1l models. Wen et al.[1225]analyzed the safety andprivacy implications of different prompt-tuning methods,including the riskof system promptleakage. Zhaoet al.[1226]identif safety and privacyanalysis as acrucialresearcharea,encompassing potential threats like system prompt leakage within the app ecosystem.",
              "index": 2,
              "part": 0,
              "translated_content": "其中$p(\\mathbf{p}\\mid\\mathbf{y};\\theta)$表示隐藏提示$\\mathbf{p}$（系统或用户）对观察输出$\\mathbf{y}$ 负责的概率。通过优化方程（18.17），攻击者可以重建影响代理行为的敏感上下文。\n\n系统提示窃取。系统提示定义了AI代理的人设、功能和行为约束。它们作为内部准则，指导代理如何与用户交互。窃取这些提示使攻击者能够逆向工程代理的逻辑，复制其功能，或者利用弱点。早期工作，如[122]，展示了提示窃取如何适用于文本到图像生成系统的知识产权。虽然Jiang等人[1222]提出了保护技术，但新的攻击策略不断涌现。Perez等人[1220]表明，系统提示可以通过对抗性提示注入（例如使用定界符或伪装命令）而被破坏。Timing side-channel攻击，例如InputSnatch[1223]揭示了LLM推理中的缓存技术，创建了一个timing side-channel，使攻击者能够重建用户的私人输入。Zhang等人[1224]证明了生产LLM（例如Claude、Bing Chat）的系统提示可以通过基于翻译的攻击和其他查询策略提取，绕过输出过滤等防御，且在各种模型中取得了高成功率。Wen等人[1225]分析了不同提示调整方法的安全性和隐私性影响，包括系统提示泄漏的风险。Zhao等人[1226]将安全性和隐私性分析确定为一个关键的研究领域，涵盖了应用生态系统中系统提示泄漏等潜在威胁。"
            },
            {
              "type": "figure",
              "src": "images/d734a5bbdf58ca5621b35f0f355da622390af25e9f98e6b0b5062673aff34796.jpg",
              "alt": "",
              "caption": "Figure 18.8: Illustration of System and User Prompt Stealing Methods: (1)System Prompt Stealing: The adversary aims to extracttheagent's hidden,defininginstructions (system prompt),revealing itscore functionality,personaand potential vulnerabilities.(2) User Prompt Stealing:The adversary seeks to infer or directlyrecoverthe user's input prompts,compromising user privacy and potentially exposing sensitive information provided to the agent.",
              "index": 3,
              "part": 0,
              "translated_caption": "图18.8: 系统和用户提示窃取方法示意图：（1）系统提示窃取：对手旨在提取代理的隐藏、定义性指令（系统提示），揭示其核心功能、人设和潜在漏洞。（2）用户提示窃取：对手试图推断或直接恢复用户的输入提示，危及用户隐私并可能暴露提供给代理的敏感信息。"
            },
            {
              "type": "text",
              "content": "User Prompt Stealing. Beyond system prompts, user prompts are also vulnerable. Atackers can infer or extract sensitive user inputs,compromising privacy. If a user queries an AI agent with confidential business strategies or personal medical concerns,an atacker could reconstruct these inputs from model responses.Yang et al.[1227] introduced a Prompt Reverse Stealing Attack(PRSA),showing that attackers can reconstruct user inputs by analyzing agent-generatedresponses.Agrwalet al.[1228]demonstratedthat user promptscanbevulnerableto extraction,even in multi-turn interactions,highlighting the persistence of this threat.Agrwal etal.[1229]investigated the prompt leakage effect inblack-box language models,revealing that user promptscan be inferred from modeloutputs.Liang et al.[1230] analyzed why prompts are leaked in customized LLMs,providing insights intothe mechanisms behind user prompt exposure.Huiet al.[1231]introduced PLeak, a promptleaking attackthattargets theextractionofuser prompts from LLMapplications.Yona et al.[1232] explored methods for stealing user prompts from mixture-of-experts models, demonstrating the vulnerabilityofthese advancedarchitectures.Zhang et al.[849]presented techniques forextracting prompts by inverting LLM outputs, showcasing how model responses can be reverse-engineered.",
              "index": 4,
              "part": 0,
              "translated_content": "用户提示窃取。除了系统提示外，用户提示也是容易受到攻击的。攻击者可以推断或提取敏感用户输入，从而危及隐私。如果用户向AI代理查询机密的商业战略或个人医疗问题，攻击者可以通过模型响应重建这些输入。Yang等人[1227]引入了提示逆向窃取攻击（PRSA），显示攻击者可以通过分析代理生成的响应来重建用户输入。Agrwal等人[1228]证明了用户提示即使在多轮交互中也可能容易被提取，突显了这一威胁的持久性。Agrwal等人[1229]调查了黑盒语言模型中的提示泄漏效应，揭示了用户提示可以从模型输出中被推断出来。Liang等人[1230]分析了为什么提示会在定制的LLM中泄露，深入探讨了用户提示曝露背后的机制。Hui等人[1231]引入了PLeak，一种针对从LLM应用程序中提取用户提示的提示泄漏攻击。Yona等人[1232]探讨了从专家混合模型中窃取用户提示的方法，展示了这些先进架构的易受攻击性。Zhang等人[849]提出了通过反演LLM输出来提取提示的技术，展示了模型响应如何被逆向工程。"
            }
          ],
          "raw_title": "Inference of Interaction Data",
          "type": null,
          "children": [],
          "translated_title": "18.2.2 互动数据推断"
        },
        {
          "title": "18.2.3 Privacy Threats Mitigation",
          "number": "18.2.3",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "To address privacy threats in AI agents, researchers have developed privacy-preserving computation and machine unleaming techniques to protect sensitive data without compromising utility.Diffrential Privacy (DP)introduces carefully calibrated noise into the training processor model outputs to prevent individual data points from being inferred[1270].DP has been successfully adapted for fine-tuning LLMs,employing techniques such as gradient clipping and noise injection at diferent stages, including during optimization and user-level interactions [1271]. Another promising direction is Federated Learming (FL),e.g.,FICAL is a privacy-preserving FL method for training AI agents that transmits summarizedknowledge instead of model parameters or raw data,addressing communication and computational challenges[1272]. Recent studies have explored FL-based fine-tuning of AI agents, enabling collaborative modelimprovement acrossdiffrent entities without directdata sharing[273].Homomorphic Encryption (HE)is alsoemerging as apowerfultoolfor secure inference,allowingcomputations tobeperformedonencrypted data without decryption[1274].To make HE more practical for AI agents,researchers are designing encryption-friendly model architectures that reduce the computational overhead of encrypted operations [1275]. For hardware-based solutions,Trusted Execution Environments (TEEs)ofera secure enclave where computations can be isolated fromthe rest of the system,protecting sensitivedata and model parameters[1276].Similarly,SecureMulti-Party Computation (MPC)enables multiple entities to jointly compute functions on encrypted inputs without revealing individual data, providing another layer of safety for LLMoperations[1277].Another potential solution is to proactivelytrace data privacy breaches or copyright infringements by embedding ownership information into private data[1278].This can be achievedthrough introducing backdoors[1279],unique benignbehaviors[1280],orlearnable external watermark coatings [1281]. Complementing these approaches is the growing field of Machine Unlearming,which aims toremove specific training data froman AI agent's memory,efectively implementing a“right to be forgoten\"[1282,1283]. Recent research has developed LLM-specific unleaming techniques,including adaptive prompt tuning and parameter editing,to selectivelyerase unwanted knowledge while minimizing the impact on model performance[1284,1285].",
              "index": 0,
              "part": 0,
              "translated_content": "为了解决人工智能代理中的隐私威胁，研究人员开发了隐私保护计算和机器遗忘技术，以保护敏感数据而不影响效用。差分隐私（DP）在训练过程或模型输出中引入精心校准的噪声，以防止推断出个体数据点。DP已成功应用于微调LLMs，采用诸如梯度裁剪和在不同阶段注入噪声的技术，包括在优化和用户级互动过程中。另一个有前景的方向是联邦学习（FL），例如，FICAL是一种用于训练人工智能代理的隐私保护FL方法，传输摘要知识而不是模型参数或原始数据，解决了通信和计算挑战。最近的研究探索了基于FL的人工智能代理微调，实现了跨不同实体的协作模型改进，而无需直接共享数据。同态加密（HE）也正在成为安全推断的强大工具，允许在加密数据上执行计算而无需解密。为了使HE对人工智能代理更实用，研究人员正在设计友好加密的模型架构，减少加密操作的计算开销。对于基于硬件的解决方案，受信执行环境（TEEs）提供了一个安全隔离区，可以将计算与系统的其余部分隔离开来，保护敏感数据和模型参数。类似地，安全多方计算（MPC）使多个实体能够共同对加密输入执行函数计算，而不泄露个体数据，为LLM操作提供了另一层安全性。另一个潜在解决方案是通过将所有权信息嵌入私人数据来主动追踪数据隐私泄露或版权侵权行为。这可以通过引入后门、独特良性行为或可学习的外部水印涂层来实现。作为补充，还有不断发展的机器遗忘领域，旨在从人工智能代理的记忆中删除特定训练数据，有效实施“被遗忘的权利”。最近的研究已经开发了针对LLMs的特定遗忘技术，包括自适应提示调整和参数编辑，以有选择地消除不需要的知识，同时最大程度地减少对模型性能的影响。"
            },
            {
              "type": "text",
              "content": "Despite these advancements,challenges remain in balancing privacy,performance,and efficiency.Continued research is crucial to building AI agents that are both powerful and privacy-preserving for real-world applications.",
              "index": 1,
              "part": 0,
              "translated_content": "尽管取得了这些进展，但在平衡隐私、性能和效率方面仍存在挑战。持续的研究对于构建既强大又能在现实应用中保护隐私的人工智能代理至关重要。"
            }
          ],
          "raw_title": "Privacy Threats Mitigation",
          "type": null,
          "children": [],
          "translated_title": "18.2.3 隐私威胁缓解"
        }
      ],
      "translated_title": "18.2 隐私问题"
    },
    {
      "title": "18.3 Summary and Discussion",
      "number": "18.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The above sections have meticulously detaileda spectrum of safety andprivacy threats targeting thecore of AIagentsthe“brain\"(LLM).From jailbreaks and prompt injection tohalucinations, misalignments, and poisoning attacks,it is evident that the LLM'scentralrole indecision-making makes it a prime targetforadversaries.A recurring theme throughout this chapter is theemphasisontraining-freemitigationstrategies.Manyofthe defenses presented,suchas input sanitizationandfiltering for jailbreaks[125,286],uncertaintyestimationforhallucinations[249]andsafety layers for misalignment[l179],are crucial because theyare practical,scalable, adaptable, and often model-agnostic. Retraining large models iscostly; training-free methodscan be appied post-deployment and offer flexibility against evolving threats.",
          "index": 0,
          "part": 0,
          "translated_content": "上述部分详细描述了针对人工智能代理的核心“大脑”（LLM）的一系列安全和隐私威胁。从越狱和快速注入到幻觉、错位和毒化攻击，显然，LLM在决策中的核心作用使其成为对手的主要目标。本章始终强调培训无关的缓解策略。许多所提出的防御措施，例如针对越狱的输入净化和过滤，以及用于幻觉的不确定性估计，以及用于错位的安全层，都至关重要，因为它们是实用的、可扩展的、适应性强的，而且通常与模型无关。重新训练大型模型成本高昂；培训无关的方法可以在部署后应用，并提供对不断演变的威胁的灵活性。"
        },
        {
          "type": "text",
          "content": "However,apurelyreactive approach is insuficient.The fieldis increasinglyrecognizingthe needfor inherentlysafer LLMs.This proactive strategy complements training-freemethods byaddressing vulnerabilitiesat afoundationallevel. For instance,modelpoisoning mitigation,lke activation clustering in RAG poisoning atack detection[1259],not only mitigates immediate threats but also informs thedesign of more robust raining processes.Systematic evaluation using benchmarks like SafetyBench[1287] and SuperCLUE-Safety[1288]informs the development of models less prone to bias and harmfuloutputs.Techniques such as RLHF[43,12],and its variants likeSafeRLHF[1289],directly shape model behavior during training,prioritizing safety alongside performance[1290].Prompt engineering[1291,292] and parameter manipulation[1293]enhance robustnessagainst adversarial attacks,creating models thatare inherently less susceptible to misalignment.",
          "index": 1,
          "part": 0,
          "translated_content": "然而，仅仅采取一种反应性方法是不够的。该领域越来越意识到需要从根本上构建更安全的LLM。这种积极主动的策略通过在基础层面解决漏洞来补充培训无关的方法。例如，像在RAG毒化攻击检测中的激活聚类这样的模型毒化缓解[1259]，不仅可以减轻直接威胁，还可以指导更健壮的培训流程的设计。使用诸如SafetyBench[1287]和SuperCLUE-Safety[1288]等基准系统的系统评估，有助于开发更不容易偏见和产生有害输出的模型。诸如RLHF[43,12]以及其变体SafeRLHF[1289]等技术在培训过程中直接塑造模型行为，将安全性置于性能之上[1290]。及时的工程处理[1291,292]和参数调整[1293]增强了对抗性攻击的鲁棒性，创建了在本质上不太容易发生错位的模型。"
        },
        {
          "type": "text",
          "content": "Importantly, while the term“jailbreak\"often emphasizes bypassing safety guardrails,the underlying mechanisms bearstrong resemblance to adversarialatacks morebroadly:inbothcases, inputs are crafted to induce undesiredor harmful outputs.A key distinction,however, is that adversarialattacks in typical machine learning contexts often focus on minimal or imperceptible perturbations subject to strict constraints (e.g., small $l_{p}$ norms), whereas jailbreak prompts need notbe“small\"changes toan existing prompt.Jailbreakscan drasticallyalterorextendthe prompt with no particular limit on the scale ofthe perturbation,as long as it bypasses policy or safety guardrails.Under specific conditions—such as whensafetyconstraints are formulated as a sort of“decision boundary\"—these two atack vectors become effectively equivalent.Yet, inreal-world LLM scenarios,the unconstrainednature of jailbreak inputs can pose a diffrent, andoftenbroader, practicalthreat model.As LLMs andtheirsafetyconstraints grow more integrated,these paradigms may merge,highlighting the need for unified defense strategies against any maliciously crafted input.",
          "index": 2,
          "part": 0,
          "translated_content": "重要的是，虽然术语“越狱”通常强调绕过安全防护栏，但其基本机制与更广泛的对抗性攻击有着明显的相似之处：在两种情况下，输入被精心设计以诱导产生不受欢迎或有害的输出。然而，一个关键区别在于，在典型的机器学习背景下，对抗性攻击通常专注于受严格约束（例如，小$l_{p}$范数）的最小或难以察觉的扰动，而“越狱”提示则不必是对现有提示的“小”变化。越狱可以大幅改变或扩展提示，对扰动的规模没有特定限制，只要绕过策略或安全防护栏即可。在特定条件下——例如当安全约束被构建为某种“决策边界”时——这两种攻击向量实际上变得等效。然而，在现实世界的LLM场景中，越狱输入的无约束性可能构成一种不同的、通常更广泛的实际威胁模型。随着LLM及其安全约束的更紧密集成，这些范式可能会融合，突显了需要针对任何恶意设计的输入采取统一的防御策略。"
        },
        {
          "type": "text",
          "content": "Adversarialtraining,initiallpresented asa jailbreak mitigation technique[239]exemplifiesthe synergy between reactive and proactive approaches.Continuous exposureto adversarialexamples improves inherentrobustness [1294] Similarly,privacy-preserving techniqueslike diffrential privacy and federated learning [1270,1295],originally discussedformitigating privacythreats,fundamentallalterthetraining process,leading toa morerobust and privacyaware LLM brain.",
          "index": 3,
          "part": 0,
          "translated_content": "对抗训练，最初被提出作为一种越狱缓解技术[239]，展示了响应性和主动性方法之间的协同作用。持续接触对抗示例可提高固有的鲁棒性[1294]。同样，诸如差分隐私和联邦学习等隐私保护技术[1270,1295]，最初用于缓解隐私威胁，根本上改变了训练过程，导致更强大且具有隐私意识的LLM模型。"
        }
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": [],
      "translated_title": "18.3 总结与讨论"
    },
    {
      "title": "Agent Intrinsic Safety: Threats on Non-Brain Modules",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "The safety of an AIagent extends beyond thecore LLMtoits peripheral modules, including the perception and action modules.Although theLLM brain provides core intelligence, vulnerabilities inthe other modules can significantly undermine theentire agent'srobustness.These components act as interfaces, alowing the AIagent to perceive the world and execute actions within it, making them prime targets for adversarial attacks.",
          "index": 0,
          "part": 0,
          "translated_content": "人工智能代理的安全性不仅仅局限于核心LLM，还包括其外围模块，包括感知和操作模块。尽管LLM大脑提供核心智能，但其他模块的漏洞可能会严重损害整个代理的健壮性。这些组件充当接口，使人工智能代理能够感知世界并在其中执行动作，使它们成为对抗性攻击的主要目标。"
        }
      ],
      "raw_title": "Agent Intrinsic Safety: Threats on Non-Brain Modules",
      "type": null,
      "children": [],
      "translated_title": "1. 代理人内在安全性：对非脑模块的威胁"
    },
    {
      "title": "19.1  Perception Safety Threats",
      "number": "19.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The perception module of an AI agent iscrucial for processing and interpreting user inputs across various modalities, such a text,images,and audio.However,the complexity and diversity of these modalities make perception systems susceptible to misinterpretations indynamicenvironments[1296],andvulnerabletoadversarialattacksthatmanipulate input data to mislead the agent [1297].",
          "index": 0,
          "part": 0,
          "translated_content": "人工智能代理的感知模块对于跨越各种模态（如文本、图像和音频）处理和解释用户输入至关重要。然而，这些模态的复杂性和多样性使得感知系统容易在动态环境中出现误解[1296]，并且容易受到对抗性攻击的影响，这些攻击会操纵输入数据以误导代理[1297]。"
        }
      ],
      "raw_title": "Perception Safety Threats",
      "type": null,
      "children": [
        {
          "title": "19.1.1 Adversarial Attacks on Perception",
          "number": "19.1.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Adversarialattacks are deliberate atempts todeceive AIagents by altering input datatargeting the perception module across various modalities.From subtle textualtweaks toinaudibleaudio distortions,these atacksrevealthe fragilityof even the most advanced systems.Below,weexplore howthese threats manifest in textual, visual,auditory,andother modalities, and highlight countermeasures.\n\nTextual.Textualadversarialatacks manipulate input text to deceive LLMs,rangingfrom simple sentence alterationsto more complexcharacter-level perturbations.Prompt-based adversarialattack,forinstance,carefullycrafted deceptive prompts that mislead models into generating harmful outputs.Minor changes-like swapping synonyms or substituting characters—can degrade performance[1298].Sophisticated strategies push this further: Zou et al.[1134] generate universal adversarial suffxes using greedyand gradient-based searches, while Wen et al.[1299]optimize interpretable hard prompts tobypasstoken-levelcontent filters in text-to-image models.To defend against these attacks,several approaches have been proposed. For example, Legilimens—a novelcontent moderation system-employs a decoderbased concept probing technique and red-team data augmentationto detect andthwart adversarialinput with impressive accuracy[13o].Self-evaluation techniques enhanceLLMs to scrutinizetheirown outputs for integrity[1301],while methods like adversarial text purification [1302] and TextDefense[1303] harness language models to neutralize perturbations.These defenses ilustrateadynamic armsrace,where resilience is forged throughcreativityand vigilance.",
              "index": 0,
              "part": 0,
              "translated_content": "对抗性攻击是有意改变输入数据，以欺骗人工智能代理，瞄准各种模态下感知模块的行为。从微妙的文本调整到听不见的音频失真，这些攻击揭示了即使是最先进系统的脆弱性。下面，我们将探讨这些威胁如何在文本、视觉、听觉和其他模态中显现，并强调对抗措施。\n\n文本。文本对抗攻击操纵输入文本以欺骗LLMs，范围从简单的句子改变到更复杂的字符级扰动。例如，基于提示的对抗攻击精心制作欺骗性提示，误导模型生成有害输出。小的改变，比如交换同义词或替换字符，都可能降低性能。复杂的策略将这一点推向更远：Zou等人使用贪婪和基于梯度的搜索生成通用对抗后缀，而Wen等人优化可解释的硬提示，以绕过文本到图像模型中的标记级内容过滤器。为了抵御这些攻击，已经提出了几种方法。例如，Legilimens——一种新颖的内容审核系统——采用基于解码器的概念探测技术和红队数据增强，以检测和挫败对抗性输入，具有令人印象深刻的准确性。自我评估技术增强LLMs审查其输出的完整性，而像对抗性文本净化和文本防御这样的方法利用语言模型来中和扰动。这些防御措施展示了一场动态的军备竞赛，其中韧性是通过创造力和警惕性打造的。"
            },
            {
              "type": "text",
              "content": "Visual. Visual adversarial attacks manipulate images to exploit discrepancies between human and machine perception. These attacksare particularlyconcerning for multi-modal LLMs(VLMs)that rely on visual inputs.For instance,image hijacks can mislead models into generating unintended behaviors[1304],whiletransferable multimodal attacks can affect both text and visual components of VLMs [1305,1306,1307]. Recent work on multimodal LM robustness shows that targeted adversarial modifications can mislead web agents into executing unintended actions with $5\\%$ pixels manipulation[1308]. Jiet al.[1309] reveal how inaudible perturbations can interfere with the stabilityof cameras and blur the shot images, and lead to harmfulconsequences.Defensive strategies include adversarialtraining [1310,13112]whichinvolves jointtaining withcleanandadversarialimages tomproveobustness,andcerifd robustness methods that guaranteeresilience through the text generationcapabilitiesof VLMs.DIFFender[1313]used diffusion models using feature purification to strengthen VLMs against visual manipulation.",
              "index": 1,
              "part": 0,
              "translated_content": "视觉。视觉对抗攻击操纵图像以利用人类和机器感知之间的差异。这些攻击对于依赖视觉输入的多模态LLMs（VLMs）尤为令人担忧。例如，图像劫持可能会误导模型生成意外行为，而可转移的多模态攻击可能会影响VLMs的文本和视觉组件。最近关于多模态LM鲁棒性的研究表明，有针对性的对抗性修改可以让网络代理被误导执行意外动作，只需对图像进行$5\\%$的像素操作。杰等人揭示了如何通过听不见的扰动干扰摄像头的稳定性，使拍摄的图像模糊，并导致有害后果。防御策略包括对抗训练，即联合训练干净图像和对抗性图像以提高鲁棒性，以及通过VLMs的文本生成能力保证韧性的鲁棒性方法。DIFFender利用特征净化的扩散模型来增强VLMs对视觉操纵的抵抗力。"
            },
            {
              "type": "figure",
              "src": "images/7f209fc91f59982289f5dbeb939810015ba377835de54d14640bdf8b4d64c349.jpg",
              "alt": "",
              "caption": "Figure 19.1: Agent Intrinsic Safety: Threats on LLM Non-Brains.",
              "index": 2,
              "part": 0,
              "translated_caption": "图19.1：代理内在安全性：LLM非大脑上的威胁。"
            },
            {
              "type": "text",
              "content": "Auditory.For voice-controlled AIagents,auditory adversarialatacks pose astealthy threat.DolphinAttack[1318] introduces an innovative techniquethat leverages ultrasound to inject malicious voice commands into microphones in an inaudible manner.Also, inaudible perturbations like VRifle[1297]can mislead traditional speech recognition systems and can likely be adapted to target audio-language models.Deepfake audio and adversarial voiceprint further pose serious risks for authentication-based systems[1316,317,1335],while emerging jailbreak and chataudio attacks exploitaudio processing vulnerabilities[1336].To mitigate these threats,solutions likeEarAray use acoustic attenuation tofilter inaudible perturbations [1337],while SpeechGuard enhances LLMrobustnessthrough adversarial training [1338]. Moreover, NormDetect[1339] focuses on effectively detecting normal speech patterns from manipulated inputs.",
              "index": 3,
              "part": 0,
              "translated_content": "听觉。对于受声控人工智能（AI）代理控制的系统，听觉对抗攻击构成了隐蔽威胁。DolphinAttack[1318]引入了一种创新技术，利用超声波以不可听见的方式向麦克风注入恶意语音命令。此外，不可听见的扰动如VRifle[1297]可能会误导传统语音识别系统，并且可能被调整用于针对音频语言模型。深度伪造音频和对抗性声纹进一步对基于认证的系统构成严重风险[1316,317,1335]，而新兴的越狱和聊天音频攻击则利用音频处理漏洞[1336]。为了缓解这些威胁，EarAray等解决方案利用声学衰减来过滤不可听见的扰动[1337]，而SpeechGuard通过对抗性训练增强LLM的鲁棒性[1338]。此外，NormDetect[1339]专注于有效地检测受操纵输入影响的正常语音模式。"
            },
            {
              "type": "text",
              "content": "Other Modality.Beyond text, images, and audio, AI agents interfacing with sensor data—like in autonomous systems—face unique threats.For example, LiDAR manipulation can mislead autonomous driving systems,creating phantomobjects [1319].Research on adversarial attacks in multi-agent systems reveals thattampered messages can significantly degrade multi-view object detection and LiDAR-based perceptionin cooperative AIagents,highlighting the risk of sensor-based adversarial perturbations[1320].Similarlyatacks targeting gyroscopes or GPS spoofing can disrupt navigation systems[1321,1322].Defenses forthese attacks include robust sensor fusion algorithms and anomalydetection techniques toidentify inconsistencies,aswellasredundant sensors thatmake ithardertocompromise the entire system[1340]. Physicallayer defenses, such as shielding and secure localization using enhanced SLAM techniques,arealsocritical[1341].Jiet al.[342]oferarigorous framework forsafeguarding sensor dataintegrity and privacy.",
              "index": 4,
              "part": 0,
              "translated_content": "除了文本、图像和音频之外，与传感器数据进行接口的人工智能代理面临着独特的威胁。例如，LiDAR操纵可能会误导自动驾驶系统，制造虚假物体。在多智能体系统中对对抗攻击的研究表明，篡改的消息可能会严重降低合作人工智能代理中的多视图目标检测和基于LiDAR的感知，突显了基于传感器的对抗性扰动的风险。同样，针对陀螺仪或GPS欺骗的攻击可能会干扰导航系统。针对这些攻击的防御措施包括强大的传感器融合算法和异常检测技术，以识别不一致性，以及使用冗余传感器使整个系统更难受到威胁。物理层防御措施，如屏蔽和使用增强SLAM技术进行安全定位，也至关重要。Ji等人提供了一套严格的框架来保障传感器数据的完整性和隐私。"
            }
          ],
          "raw_title": "Adversarial Attacks on Perception",
          "type": null,
          "children": [],
          "translated_title": "19.1.1 对感知的对抗性攻击"
        },
        {
          "title": "19.1.2 Misperception Issues",
          "number": "19.1.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "While adversarialattacks are deliberate atempts tocompromise system integrity, misperception issues emerge intrinsically from thelimitations of LLMs.Theseerrors occur without any malicious intent andcan be atributed toa varietyof factors ranging from dataset biases to architecturalconstraints.One primary source of misperception is dataset bias. When models are trainedon non-representative datasets,they tend to underperform ondiverse or novel inputs [1324]. This shortcoming is exacerbated by challenges in generalizing to new, unseen environments, where unpredictable conditions may arise.Environmentalcomplexities such as sensor noise,occlusions,and fluctuating lighting further introduce uncertainty[1326].Additionall,inherent modellimitations—likerestrictedreceptive fields ortheabsenceof robust reasoning mechanisms- -compound these errors [1327]. Insights from studies on multi-agent systems and online social dynamics provide further depth to our understanding of misperception. Research shows that individuals may misjudge the true distribution ofopinionsdue tophenomena like false consensus efects,vocalminorityamplification, and the spiralof silence[1328].Suchbiasescanlead AIagentstoerroneouslyinferdominant perspectives from skewed inputs.Similarly,whendiffrent modelsshare visualfeatures,discrepancies infeatureencodingcanresult insignificant perception errors,achallenge that mirrors issues inmulti-modalLLMs[1329].Moreover, in interactive enviroments, agents may developdistorted interpretationsofcooperative and adversarialbehaviors,as evidenced by findings in multi-agent reinforcement learning [1330]. Linguistic representation, too,can be influenced by perceptual biases, suggesting that misperception in LLMs may stem not only from sensory inaccuracies but alsofrom language-driven distortions [131].Finall,systematic erors often arise when mismatched confidence levels across models affect decision-making in uncertain contexts [1332].",
              "index": 0,
              "part": 0,
              "translated_content": "虽然对抗性攻击是有意损害系统完整性的行为，但误判问题则是由LLMs的局限性固有地引起的。这些错误是没有恶意意图的，可以归因于各种因素，从数据集偏见到架构约束不等。数据集偏见是误判的一个主要来源。当模型在非代表性数据集上训练时，它们往往在多样化或新颖输入上表现不佳。这种缺点在泛化到新的、未见过的环境时变得更加严重，那里可能会出现不可预测的条件。环境复杂性，如传感器噪声、遮挡和光照变化进一步引入不确定性。此外，固有的模型限制，如受限的感受野或缺乏强大的推理机制，会加剧这些错误。多智能体系统和在线社交动态研究的见解进一步加深了我们对误判的理解。研究表明，个体可能会由于假共识效应、少数派发声、沉默螺旋等现象而错误判断意见的真实分布。这种偏见可能会导致AI代理人错误地从偏斜的输入中推断出主导观点。同样，当不同模型共享视觉特征时，特征编码的差异可能导致显著的感知错误，这是多模态LLMs中存在的问题的一种反映。此外，在交互环境中，代理可能会对合作和对抗行为产生扭曲的解释，正如多智能体强化学习研究中的发现所示。语言表征也可能受感知偏见影响，这表明LLMs中的误判可能不仅源自感官不准确性，还可能源自语言驱动的扭曲。最后，在模型之间的置信水平不匹配影响不确定环境中的决策时，系统性错误经常会出现。"
            },
            {
              "type": "text",
              "content": "Mitigating these misperception challenges requires a multifaceted strategy.Curating diverse andrepresentative datasets thatcapture a broad spectrum ofreal-worldconditions is criticalfor enhancing modelperformance and reducing bias [1343].Dataaugmentation techniques,which generatesyntheticvariations ofexisting data,canfurtherenrich dataset diversity.Incorporating uncertaintyestimation allows models toassesstheirconfidence inpredictions andflag potential error-prone situations [1344].Moreover,advancing model architectures to include explicit reasoning mechanisms or better processing of long-range dependencies is vital for minimizing misperception[1345]. An especially promising avenue is theadoption of biologicaly inspiredlearming frameworks,such as Adaptive Resonance Theory(ART).Unlike traditional deep learning approaches—often hampered by issues like catastrophic forgeting and opaque decisionmaking-ART models can self-organize stable representations that adapt to dynamically changing environments, thereby reducing perceptualerrors [1346].However,it is important to note that even improved explainability has its limitations, particularly when users struggle to establish clear causal links between modeloutputs and underlying processes[1347]. Furthermore,recent studies indicate that advanced LLMs may inadvertently degrade their own responses during self-correction, underscoring the need for morerobust intrinsic reasoning verification mechanisms [1348].",
              "index": 1,
              "part": 0,
              "translated_content": "缓解这些误判挑战需要采用多方面的策略。精心策划多样化和代表性的数据集，捕捉广泛的真实世界条件对于提升模型性能和减少偏见至关重要。数据增强技术可以生成现有数据的合成变体，进一步丰富数据集的多样性。引入不确定性估计使模型能够评估其对预测的信心，并标记潜在的易出错情况。此外，推进模型架构以包括明确的推理机制或更好地处理长距离依赖关系对于最小化误判至关重要。一种特别有前景的途径是采用生物启发式学习框架，如自适应谐振理论（ART）。与传统的深度学习方法不同，后者常常受到诸如灾难性遗忘和不透明决策等问题的困扰，ART模型能够自组织稳定的表示，适应动态变化的环境，从而减少感知误差。然而，值得注意的是，即使改进了可解释性，其也存在局限性，特别是当用户难以建立模型输出与基础过程之间的清晰因果关系时。此外，最近的研究表明，先进的LLMs可能会在自我校正过程中无意中降低其响应，强调了需要更健壮的内在推理验证机制。"
            }
          ],
          "raw_title": "Misperception Issues",
          "type": null,
          "children": [],
          "translated_title": "19.1.2 误解问题"
        }
      ],
      "translated_title": "19.1 感知安全威胁"
    },
    {
      "title": "19.2 Action Safety Threats",
      "number": "19.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The action module is responsible fortranslating the AI agent's planned actions into actual task executions.This typicall includes invoking external toolscallng APIs,orinteractingwithphysicaldevices.As the interfacebetween decision-making and execution,it is highly vulnerable toattacks.We explore two primary domains ofrisk: supply chain attacks and vulnerabilities arising from tool usage.",
          "index": 0,
          "part": 0,
          "translated_content": "行动模块负责将AI代理的计划行动转化为实际任务执行。这通常包括调用外部工具、调用API或与物理设备交互。作为决策和执行之间的接口，它极易受到攻击。我们探讨了两个主要风险领域：供应链攻击和由工具使用引起的漏洞。"
        }
      ],
      "raw_title": "Action Safety Threats",
      "type": null,
      "children": [
        {
          "title": "19.2.1 Supply Chain Attacks",
          "number": "19.2.1",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Supply chainatacksexploittheservices thatAIagents dependontherebyundermining theintegrityoftheentire system [1333].Unliketraditional attacks,these threatsdo nottarget the agentdirectly but insteadcompromise the extenal resources it relies upon.For example,malicious websitescan employ indirect prompt injection (IPI)attacks—illustrated by the Web-based Indirect Prompt Injection (WIPI)framework—to subtly alter an agent's behavior without needing access toitscode[122].Similarly,adversaries may manipulateweb-based tools(suchasYouTubetranscript plugins)to feedmisleading information into the system[795].AsAIagents become increasingly integrated withonline resources, their attack surface broadens considerably.Recent work byGreshake etal.proposes anewclasificationof indirect injection attacks,dividing them into categories like data theft, worming, and information ecosystem contamination [1149].Complementing this,the InjecAgent benchmark evaluated 30diferent AI agents andrevealed that most are vulnerable to IPI attacks [1152].",
              "index": 0,
              "part": 0,
              "translated_content": "供应链攻击利用人工智能代理所依赖的服务，从而破坏整个系统的完整性[1333]。与传统攻击不同，这些威胁不直接针对代理，而是通过破坏其依赖的外部资源来进行。例如，恶意网站可以利用间接提示注入（IPI）攻击——如Web-based Indirect Prompt Injection (WIPI)框架所示——在不需要访问代码的情况下微妙地改变代理的行为[122]。同样，对手可能操纵基于网络的工具（如YouTube字幕插件）向系统输入误导性信息[795]。随着人工智能代理与在线资源的日益整合，它们的攻击面显著扩大。Greshake等人最近的工作提出了间接注入攻击的新分类，将其分为数据窃取、蠕虫攻击和信息生态系统污染等类别[1149]。作为补充，InjecAgent基准评估了30个不同的人工智能代理，并揭示大多数容易受到IPI攻击的脆弱性[1152]。"
            },
            {
              "type": "text",
              "content": "To mitigate these risks,preemptive safety measures andcontinuous monitoring are essential.Current research suggests thattwokeyfactors behindthesucessof indirect injection areLLMsinabilitytodistinguish informationcontextfrom actionable instructions andtheir poor awareness of instruction safety;hence,it is proposed toenhance LLMsboundary and safety awareness through multi-round dialogue and in-context learning [1349]. Furthermore,other researchers, based on the same assumption, proposed a prompt engineering technique called“spotlighting\"tohelp LLMs better distinguish between multiple input sources andreduce the success rateof indirect prompt injectionattacks[1350]. Since under asuccessfulattack,thedependence of the agent'snextactionon the usertask decreaseswhile its dependence on the malicious taskincreases,some researchers detect attacks byre-executingthe agent'strajectory with a masked user prompt modified through a masking function[1351].Finally, sandboxing techniques,such as those employed in",
              "index": 1,
              "part": 0,
              "translated_content": "为了减轻这些风险，预防性安全措施和持续监控至关重要。目前的研究表明，间接注入成功的两个关键因素是LLM无法区分信息上下文和可操作指令，以及它们对指令安全性的认识不足；因此，建议通过多轮对话和上下文学习来增强LLM的边界和安全意识。此外，基于相同假设，其他研究人员提出了一种名为“聚光灯”的提示工程技术，以帮助LLM更好地区分多个输入来源，并减少间接提示注入攻击的成功率。由于在成功攻击下，代理的下一步行动对用户任务的依赖减少，对恶意任务的依赖增加，一些研究人员通过使用掩模函数修改的掩码用户提示重新执行代理的轨迹来检测攻击。最后，沙盒技术，例如在..."
            },
            {
              "type": "text",
              "content": "ToolEmu[795],create isolated environments for executing external tools,limiting the potential damage incaseofa breach.",
              "index": 2,
              "part": 0,
              "translated_content": "ToolEmu[795]创建了隔离环境，用于执行外部工具，限制潜在的损害，以防发生违规行为。"
            }
          ],
          "raw_title": "Supply Chain Attacks",
          "type": null,
          "children": [],
          "translated_title": "19.2.1 供应链攻击"
        },
        {
          "title": "19.2.2 Risks in Tool Usage",
          "number": "19.2.2",
          "level": 3,
          "content": [
            {
              "type": "text",
              "content": "Even when externaltools are secure,vulnerabilitiescan arise fromhow an agent interacts with them.A significantrisk is unauthorized actions,where an adversary manipulates the agent into performing unintended behaviors.For example, prompt injection atackscan trick an agent intosending emails,deletingfiles,orexecuting unauthorized transactions [795].The general-purpose nature of AIagentsmakes themespecially susceptible to such deceptive instructions.The tool leaning processitselfcan introduceadditionalrisks,suchas maliciousqueries,jailbreakatacks,andharmfulhints during the input,execution,and output phases[1334].During the toolexecution phase, using incorrect orrisky tools may deviatefrom the user'sintent and potentiallharm the externalenvironment.Forinstance,misuse couldlead to the introduction of malware or viruses.Acompilationof 18tools thatcouldimpactthephysicalworldhasbeenidentified, with noise intentionally addedto testifLLMscanchoose thewrong tool.Another significant concern isdata leakage, where sensitive information is inadvertentlyexposed.This occurs when an agent unknowinglytransmits confidential data to athird-party API or includes private details in its output. For example, an LLM may inject commands to extract privateuserdata,thenuseexternaltools,ikeaGmailsending tol,todistributethisdata[l52].Therisksare especially pronounced in applications dealing with personal or proprietary data, necesstating stricter controls over information flow.Additionallyexcessve permissons increase the potentialfor misuse.Agents with broad system access couldbemanipulatedto performdestructiveactions,suchasdeletingcriticalfles,leading toieversibledamage [795].Enforcing theprincipleof leastprivilege ensuresthat agents onlyhave thepermissons necessary tocomplete their tasks, minimizing the potentialimpact of exploitation.Securing the action module requires layered protections and continuous monitoring.Monitoring tool usage can helpdetect anomalies before theycause harm,while requiring user confirmation for high-risk actions—such as financialtransactions or system modifications—adds an additional layer of safety.Formalverification techniques,asexplored by[1352],canfurtherenhance safetybyensuring that tool use policies align with best practices, preventing unintended agent behaviors.",
              "index": 0,
              "part": 0,
              "translated_content": "即使外部工具是安全的，代理与其交互的方式也可能导致漏洞。一个重要的风险是未经授权的行为，即对手操纵代理执行意外行为。例如，提示注入攻击可以欺骗代理发送电子邮件、删除文件或执行未经授权的交易。人工智能代理的通用性使它们特别容易受到这种欺骗性指令的影响。工具学习过程本身可能引入额外风险，如恶意查询、越狱攻击以及在输入、执行和输出阶段出现有害提示。在工具执行阶段，使用不正确或有风险的工具可能偏离用户意图并潜在地危害外部环境。例如，误用可能导致恶意软件或病毒的引入。已确定了一组可能影响物理世界的18种工具，故意添加噪音以测试LLM是否会选择错误的工具。另一个重要的关注点是数据泄露，即敏感信息被无意中暴露。当代理无意中向第三方API传输机密数据或在输出中包含私人细节时，就会发生这种情况。例如，LLM可能会注入命令以提取私人用户数据，然后使用外部工具，如Gmail发送工具，来分发这些数据。在处理个人或专有数据的应用程序中，风险尤为突出，需要对信息流加强更严格的控制。此外，过多的权限增加了滥用的可能性。具有广泛系统访问权限的代理可能会被操纵执行破坏性行为，例如删除关键文件，导致不可逆的损害。实施最小权限原则确保代理只具有完成任务所需的权限，最大限度地减少利用的潜在影响。保护行动模块需要分层保护和持续监控。监控工具使用可以帮助在造成伤害之前检测异常，而要求用户确认高风险操作（如金融交易或系统修改）则增加了额外的安全层。正如[1352]所探讨的，形式化验证技术可以通过确保工具使用政策符合最佳实践来进一步增强安全性，防止意外的代理行为。"
            }
          ],
          "raw_title": "Risks in Tool Usage",
          "type": null,
          "children": [],
          "translated_title": "19.2.2 工具使用中的风险"
        }
      ],
      "translated_title": "19.2 行动安全威胁"
    },
    {
      "title": "Agent Extrinsic Safety: Interaction Risks",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "As AI agents evolve and interact with increasinglycomplex environments,the safety risks associated with these interactions have become a criticalconcern.This chapter focuses on AI agent's engagement with memory systems, physical and digitalenvironments,and other agents.These interactions expose AI agents to various vulnerabilities, ranging from memory corruption and environmental manipulation to adversarial behavior in multi-agent systems.By examining these interactionrisks,we aimtohighlightthediversethreatsthatcanunderminetheintegrityandreliability of AIagents inreal-worldapplications.Thefollowing sections explorethese challenges indetail,discussing specific attack vectors and their implications for system safety.",
          "index": 0,
          "part": 0,
          "translated_content": "随着人工智能代理的演变和与日益复杂环境的互动，与这些互动相关的安全风险已成为一个关键关注点。本章重点关注人工智能代理与记忆系统、物理和数字环境以及其他代理的互动。这些互动使人工智能代理面临各种漏洞，从记忆损坏和环境操纵到多代理系统中的对抗行为。通过研究这些互动风险，我们旨在凸显可能危及人工智能代理在现实应用中完整性和可靠性的多样威胁。接下来的章节将详细探讨这些挑战，讨论特定攻击向量及其对系统安全的影响。"
        }
      ],
      "raw_title": "Agent Extrinsic Safety: Interaction Risks",
      "type": null,
      "children": [],
      "translated_title": "代理外在安全性：交互风险"
    },
    {
      "title": "20.1 Agent-Memory Interaction Threats",
      "number": "20.1",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The extrinsic memory modulefunctions as the cognitive repositorythat empowers intellgent agents to store,retrieve, andcontextualize information,facilitatingcontinuous learningandtheexecutionofcomplextasks through accumulated experiences. Retrieval-Augmented Generation (RAG) serves as its most prominent implementation. However, RAG frameworks are vulnerableto adversarial manipulations that deceive agents into retrieving and utilizing harmfulor misleading documents. AgentPoison[1194]exploits this vulnerability by executing a backdoor attack on AI agents, poisonig RAG knowledge bases to ensure that backdoor-triggered inputs retrieve malicious demonstrations while maintaining normal performance on benign queries. ConfusedPilot[1353] exposes aclassofRAG system vulnerabilities thatcompromisetheintegrityandconfidentialityof Copilotthrough prompt injectionattacks,retrievalcaching exploits, and misinformation propagation. Specifically, these attacks manipulate the text input fed tothe LLM,causing it to generate outputs that align with adversarialobjectives. PoisonedRAG[1354]represents the first knowledgecorruption attack on RAG,injecting minimal adversarial texts to manipulate LLMoutputs. Framed as an optimization problem, it achieves a $90\\%$ success rate with just five poisoned texts per target question in large databases. Jamming [1355] introduces a denial-of-service attack on RAG systems,where a single adversarial“blocker\"document inserted into an untrusteddatabase disrupts retrievalortriggers safetyrefusals,preventing the system fromanswering specific queries. BadRAG[1356] exposes vulnerabilities in RAG-based LLMs through corpus poisoning,wherein an atacker injects multiplecrafted documents intothedatabase,forcing thesystemtoretrieve adversarialcontent and generate incorrect responses to targeted queries. By introducing just 10 adversarial passages ( $0.",
          "index": 0,
          "part": 0,
          "translated_content": "外部记忆模块作为认知存储库发挥作用，赋予智能代理存储、检索和上下文化信息的能力，通过积累经验促进持续学习和执行复杂任务。检索增强生成（RAG）是其最突出的实现方式。然而，RAG框架容易受到对抗性操纵的影响，欺骗代理检索和利用有害或误导性文件。AgentPoison[1194]利用这一漏洞对AI代理执行后门攻击，通过毒化RAG知识库确保后门触发输入检索到恶意演示，同时在良性查询上保持正常性能。ConfusedPilot[1353]揭示了一类RAG系统漏洞，通过提示注入攻击、检索缓存利用和错误信息传播危害Copilot的完整性和保密性。具体来说，这些攻击操纵输入到LLM的文本，导致其生成符合对抗目标的输出。PoisonedRAG[1354]是对RAG的首次知识破坏攻击，注入最少的对抗性文本来操纵LLM的输出。作为一个优化问题，它在大型数据库中每个目标问题仅需五个毒化文本就能达到90%的成功率。Jamming[1355]引入了对RAG系统的拒绝服务攻击，通过将单个对抗性“阻塞”文档插入不受信任的数据库来干扰检索或触发安全拒绝，阻止系统回答特定查询。BadRAG[1356]通过语料库毒化揭示了基于RAG的LLM的漏洞，攻击者向数据库中注入多个精心制作的文档，迫使系统检索对抗性内容并对目标查询生成不正确的响应。仅通过引入10个对抗性段落（$0."
        },
        {
          "type": "text",
          "content": "04\\%$ of the corpus), it achieves a $98. 2\\%$ retrieval success rate, elevating GPT-4's rejection rate from $0. 01\\%$ to $74. 6\\%$ and its negative response rate from $0. 22\\%$ to $72\\%$ . TrojanRAG[1357] executes a joint backdoor attack on RAG systems,optimizing multiple backdoor shortcuts via contrastive learning and enhancing retrieval witha knowledge graph for fine-grained matching. By systematically normalizing backdoor scenarios,it evaluates real-worldrisks andthe potentialfor model jailbreak. Lastly,acovert backdoor attack[1358]leverages grammar errors astriggers,allowing LLMs tofunction normallyfor standard queries while retrieving atacker-controlled content when minor linguistic mistakes are present. This method exploits the sensitivityofdenseretrievers togrammaticalirrgularities usingcontrastiveloss andhardnegative sampling,ensuring that backdoor triggers remain imperceptible while enabling precise adversarial control.",
          "index": 0,
          "part": 1,
          "translated_content": "它在语料库中仅占$0.04\\%$的部分，就能实现$98.2\\%$的检索成功率，将GPT-4的拒绝率从$0.01\\%$提升至$74.6\\%$，负面响应率从$0.22\\%$提升至$72\\%$。TrojanRAG[1357]对RAG系统执行联合后门攻击，通过对比学习优化多个后门快捷方式，并借助知识图提升检索的细粒度匹配。通过系统地规范化后门场景，评估现实世界的风险和模型越狱的潜力。最后，一种隐蔽后门攻击[1358]利用语法错误作为触发器，使LLMs在标准查询时能正常运作，但在存在轻微语言错误时却能检索到攻击者控制的内容。该方法利用密集检索器对语法异常的敏感性，使用对比损失和硬负采样，确保后门触发器保持不可察觉，同时实现精确的对抗控制。"
        }
      ],
      "raw_title": "Agent-Memory Interaction Threats",
      "type": null,
      "children": [],
      "translated_title": "20.1 代理-记忆交互的威胁"
    },
    {
      "title": "20.2 Agent-Environment Interaction Threats",
      "number": "20.2",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "Agentscan beclassified intotwocategories based ontheir modeof interaction: physical interaction agents and digital interaction agents.Physicalinteraction agents operate inthereal world, using sensors and actuators to perceive and influence their environment.Examples ofsuch agents include autonomous vehicles androbotic systems.Incontrast, digital interaction agents function within virtual ornetworked environments, processing and responding todata from digital sources. These include AI-powered chatbots,cybersafety systems, and automated trading algorithms.",
          "index": 0,
          "part": 0,
          "translated_content": "基于其交互模式，智能代理可以分为两类：物理交互代理和数字交互代理。物理交互代理在真实世界中运作，利用传感器和执行器感知和影响其环境。这类代理的示例包括自主车辆和机器人系统。相反，数字交互代理在虚拟或网络环境中运作，处理并响应来自数字来源的数据。这些代理包括基于人工智能的聊天机器人、网络安全系统和自动化交易算法。"
        },
        {
          "type": "figure",
          "src": "images/d11a581850b36148e9c126fcd33e1ae00bccfc85791be25f232ac74d096e0b50.jpg",
          "alt": "",
          "caption": "Figure 20.1: Agent Extrinsic Safety:Threats on agent-memory, agent-environment, and agent-agent interactions",
          "index": 1,
          "part": 0,
          "translated_caption": "图 20.1：智能体外部安全性：对智能体-记忆、智能体-环境和智能体-智能体相互作用的威胁"
        },
        {
          "type": "text",
          "content": "Threats in Physical Environment.Agents operating in the physical world,such asrobots and autonomous vehicles, face distinct safety chalenges dueto their interaction with dynamic and potentially adversarial environments [1359, 1360,1366].One major threat issensor spoofing,where atackers manipulate sensor inputs to deceive the agent about its surroundings.For example,GPS spoofingcan pose significantrisks to UAVs (unmanned aerial vehicles)and other GPS-dependent platforms by misleading autonomous vehicles about their actual location.This allowsfor malicious redirection or hijacking [1361].Similarly,LiDAR spofing can introduce false obstacles that don't actuall exist, potntiallyleading tonavigationfailuresor safetyhazards[362].Anothercriticalrisk is actuator manipulation, where adversaries takecontrolofanagent'sactuators,forcing it toperformunintended physicalactions.Thiscanoccurthrough direct tampering with thehardwareorbyexploiting vulnerabilties inthe software that governs actuatorfunctions[1363]. Such attackscan compromise the agent's actions,leading tophysicalharm or mision failure.Aditionally,exploiting environmentalhazards is a serious threat.Attackers may introduce physicalobstacles or manipulate environmental conditions to disrupt an agent'soperations.For example,adversarialobjectscreated using techniques like LiDAR-Adv can deceive LiDAR-based autonomous driving systems byinducing sensor misinterpretations,thus degrading detection reliability and increasing real-world safety risks[1364].Lastly, misalignment in physical actions can undermine the safety of autonomous agents.Discrepancies between an agent's perception andthe actual physical constraints of its environment can lead to unsafe or infeasible actions.For example, mismatches between learned locomotion policies and real-world physics—such as misjudging terrain rigidity or obstacle dimensions—can cause autonomous agents to take hazardous steps (e.g.,unstable strides on roughsurfaces).This has been observedin prior systems thatrequired over 100 manual resets due to uncontrolled falls [1365].",
          "index": 2,
          "part": 0,
          "translated_content": "物理环境中的威胁。在物理世界中运作的代理，如机器人和自主车辆，面临着与动态且潜在对抗环境的互动相关的独特安全挑战。一个主要威胁是传感器欺骗，即攻击者操纵传感器输入，以欺骗代理关于其周围环境的认知。例如，GPS 欺骗可能对无人机和其他依赖 GPS 的平台造成重大风险，通过误导自主车辆关于其实际位置的认知，从而允许恶意重定向或劫持。类似地，LiDAR 欺骗可能引入实际不存在的虚假障碍，潜在地导致导航失败或安全隐患。另一个关键风险是执行器操纵，即对手控制代理的执行器，迫使其执行意外的物理动作。这可能通过直接篡改硬件或利用管理执行器功能的软件中的漏洞而发生。这类攻击可能危及代理的行动，导致身体伤害或任务失败。此外，利用环境危害是一个严重威胁。攻击者可能引入物理障碍或操纵环境条件以干扰代理的操作。例如，使用 LiDAR-Adv 等技术创建的对抗性对象可以通过诱导传感器错误解读来欺骗基于 LiDAR 的自动驾驶系统，从而降低检测可靠性并增加现实世界的安全风险。最后，物理行动的不对齐可能会损害自主代理的安全。代理感知与其环境的实际物理约束之间的差异可能导致不安全或不可行的行动。例如，学习的运动策略与现实世界的物理特性之间的不匹配，比如错误判断地形的坚固性或障碍物的尺寸，可能导致自主代理采取危险的步骤（例如，在崎岖表面上不稳定的步伐）。这在先前的系统中已经观察到，这些系统因无法控制的摔倒而需要超过 100 次手动重置。"
        },
        {
          "type": "text",
          "content": "Threats in Digital Environment.Agents operating in digitalenvironments,such as software agents and web-based agents,facedistinct safetychallenges arising fromtheirreliance on externaldata sources andcomputationalresources [1333,1366].One major threat is code injection, where malicious actors introduce harmful code into the agent's environment,leading to unintended command execution[1367].These atacksoften exploit software vulnerabilities or leveragecompromisedexternalresources thatthe agent interacts with,potentiallyresulting in unauthorizedcontrolover the agent'soperations[1202].Environmental Injection Atack (EIA)exploits privacy risks in generalist web agents to stealthily steal users' PII, achieving up to $70\\%$ success rate [137O]. AdvWeb is an automated adversarial prompt generation framework to mislead black-box web agents into executing harmfulactions[1371]. Another critical risk is data manipulation, where attackers alterthe information anagent receives,causing incorrect decisions or actions [1333].Forexample,atrading agent can be misledby manipulated financial data,leading to incorrct transactions,or an information-gathering agent may betricked byfalsifiednews articles,distorting itsoutputs.Such manipulationscan have cascading effects,especially inautomated systems that relyon accurate datafordecision-making.Beyond direct manipulation,denial-of-service(DoS)attcks pose aserious threat by overwhelming the agent'sdigitalenvironment with excessiverequestsordata,effectivelyrendering it unresponsive orcausing it tocrash[368].Thesedisruptionscan be particularlydetrimentalto time-sensitive applications where availability andresponsivenessare critical.Aditionaly, resource exhaustion isasignificant threatas adversaries mayexploitthe agent'sresource management mechanisms to depletecomputational resources,leading to service denial for other usersor overallsystem instability [1369]. By draining processing power, memory,or bandwidth,attackers can severely impair anagent'sability to function effectively,disrupting itsoperations andreducing its efficiency.Inaddressing the safetychallengesofLLM agents, AGrail is proposed asalifelong guardrail frameworkthat enhances agent securitybyadapting safetychecksto mitigate task-specific and systemic risks,demonstrating robust performance and transferability acrossdiverse tasks [1372].",
          "index": 3,
          "part": 0,
          "translated_content": "数字环境中的威胁。在数字环境中运行的代理，如软件代理和基于网络的代理，面临着由于它们依赖外部数据源和计算资源而产生的独特安全挑战。一个主要威胁是代码注入，恶意行为者将有害代码引入代理的环境，导致意外的命令执行。这些攻击通常利用软件漏洞或利用代理与之交互的受损外部资源，可能导致对代理操作的未授权控制。环境注入攻击（EIA）利用通用网络代理中的隐私风险，秘密窃取用户的个人身份信息，成功率高达 70%。AdvWeb 是一个自动对抗性提示生成框架，旨在误导黑盒网络代理执行有害操作。另一个关键风险是数据操纵，攻击者篡改代理接收到的信息，导致代理做出不正确的决策或行动。例如，交易代理可能会被篡改的金融数据误导，导致错误交易，或者信息收集代理可能会被伪造的新闻文章欺骗，扭曲其输出。这种操纵可能会产生连锁效应，特别是在依赖准确数据做决策的自动化系统中。除了直接操纵，拒绝服务（DoS）攻击通过向代理的数字环境发送过多请求或数据，有效地使其无响应或导致其崩溃，构成严重威胁。这种中断对于时间敏感的应用尤为有害，其中可用性和响应性至关重要。此外，资源耗尽是一个重要威胁，因为对手可能利用代理的资源管理机制耗尽计算资源，导致为其他用户拒绝服务或整体系统不稳定。通过耗尽处理能力、内存或带宽，攻击者可以严重损害代理的有效功能，扰乱其操作并降低其效率。为解决 LLM 代理的安全挑战，提出了 AGrail 作为一个终身护栏框架，通过调整安全检查以减轻特定任务和系统风险，增强代理安全性，在各种任务中展现出强大的性能和可传递性。"
        }
      ],
      "raw_title": "Agent-Environment Interaction Threats",
      "type": null,
      "children": [],
      "translated_title": "20.2 代理-环境交互威胁"
    },
    {
      "title": "20.3 Agent-Agent Interaction Threats",
      "number": "20.3",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "n multi-agent systems,interactionsbetween agentscan introduce new safetyvulnerabilities[1380].These interactions ire mainly competitive, where agents try to outdo each other, or cooperative, where they work together.\n\nThreats in Competitive Interactions.When agents compete,they often use tricky methods to gain an advantage [1373].Forexample,theymight spread false information or make other agents think the situation isdiffrent from reality todeceivethem[1374].This can lead opponents to make poor decisions,weakening their position.Apart from misinformation,agents may alsotrytotake advantage of weaknesses intheiropponent's algorithms or strategies[1375]. By identifying these weaknesses,they can predict and manipulate the other agent's behavior, gaining an edge in the competition.Additionall,some agents might use disruptive techniques likedenial-of-service(DoS)attacks,which overload an opponent's system with unnecessary requests, disrupting communication and hindering their ability to function[1376].Anotherthreat incompetitive interactions iscovert collaboration.Sometimes agents secretlycooperate, even when it's against therules,to manipulate theoutcome intheir favor[1377].Thiskindofcollusionundermines fairness and damages the integrity of the system, as it skews the competition in their favor.",
          "index": 0,
          "part": 0,
          "translated_content": "在多智能体系统中，智能体之间的相互作用可能引入新的安全漏洞[1380]。这些互动主要是竞争性的，智能体试图互相超越，或者是合作性的，它们共同合作。\n\n竞争性互动中的威胁。当智能体竞争时，它们通常会使用欺诈手段获取优势[1373]。例如，它们可能散布虚假信息或让其他智能体误认为情况与现实不同以欺骗它们[1374]。这可能导致对手做出糟糕的决策，削弱它们的立场。除了错误信息，智能体还可能利用对手算法或策略中的弱点[1375]。通过识别这些弱点，它们可以预测和操纵其他智能体的行为，在竞争中占据优势。此外，一些智能体可能使用干扰技术，如拒绝服务（DoS）攻击，这种攻击通过向对手系统发送不必要的请求导致系统超载，干扰通信并阻碍其正常运行[1376]。竞争性互动中的另一个威胁是隐蔽合作。有时智能体会秘密合作，即使违反规则，也会操纵结果以谋取自己的利益[1377]。这种勾结破坏了公平性，损害了系统的完整性，因为它偏向他们，扭曲了竞争的公平性。"
        },
        {
          "type": "text",
          "content": "Threats in Cooperative Interactions. In cooperative situations, where agents work together toward a common goal, safetythreatscoulddamage the system's stability andreliability.Oneriskis unintentionalinformationleakage, where agents accidentally share sensitive data during their communication.This could lead to privacy violations or unauthorizedaccess, weakening the system's trustworthiness.Inaddition todataleaks,errors made byone agentcan spread throughout the system,causing bigger failures andlowering overall performance.[1378] discusses this problem in Open-Domain Question Answering Systems (ODQA),where errors from one part of the system can ripple through and affectother components,severely impacting reliability.The situation becomes even worse if one compromised agent introducesavulnerabilitythat spreads toothers.If ahacker successfullytakescontrolofoneagent, theycould exploit weaknesses throughout the entire system,leading toa major safety failure[1379].This kind of widespread compromise is dangerous because it could start with a smallbreach and escalate quickly. Another challenge comes from poor synchronization between agents. If agents don'tupdate their information at the same time or experience delays in communication,it can cause problems in decision-making.Misalignment or delays in updates can disrupt coordination, making itharderforthe agents toachievetheir shared goals effectively.Thesechallenges emphasizethe need for strong safety systems in cooperative multi-agent setups to keep them reliable and resistant to attacks.",
          "index": 1,
          "part": 0,
          "translated_content": "合作互动中的威胁。在智能体共同努力实现共同目标的合作情境中，安全威胁可能损害系统的稳定性和可靠性。一个风险是无意中泄露信息，即智能体在通信过程中意外共享敏感数据。这可能导致侵犯隐私或未经授权访问，削弱系统的可信度。除了数据泄漏，一个智能体的错误可能在整个系统中传播，导致更大的故障并降低整体性能。[1378]讨论了这个问题在开放领域问答系统（ODQA）中的情况，其中一个部分的错误可能会蔓延并影响其他组件，严重影响可靠性。如果一个受损的智能体引入蔓延到其他智能体的漏洞，情况会变得更糟。如果黑客成功控制一个智能体，他们可能利用整个系统中的弱点，导致重大安全失败。这种广泛的妥协是危险的，因为它可能从一个小漏洞开始并迅速升级。另一个挑战来自智能体之间的信息同步不佳。如果智能体不能同时更新信息或在通信中遇到延迟，这可能导致决策出现问题。信息不一致或更新延迟可能会破坏协调，使智能体更难有效实现他们的共同目标。这些挑战强调了在合作多智能体设置中需要强大的安全系统，以保持其可靠性并抵抗攻击。"
        }
      ],
      "raw_title": "Agent-Agent Interaction Threats",
      "type": null,
      "children": [],
      "translated_title": "20.3 代理-代理交互威胁"
    },
    {
      "title": "20.4 Summary and Discussion",
      "number": "20.4",
      "level": 2,
      "content": [
        {
          "type": "text",
          "content": "The preceding sections have detailed the significant safety risks that arise fromAIagents interacting with memory systems,physical and digital environments, and other agents.These risks,ranging fromdata poisoning and code injection tosensorspoofing andcolusion,highlight thevulnerabilities inherent in increasinglycomplexagent-based systems.However,as AIagents become more capable,utilizing naturallanguage understanding and specialized tools for sophisticated reasoning,researchers are actively developing safety protocols to address these challenges.These protocols differ in approach for general-purpose and domain-specific agents.",
          "index": 0,
          "part": 0,
          "translated_content": "前面的部分详细介绍了人工智能代理与记忆系统、物理和数字环境以及其他代理相互作用产生的重大安全风险。这些风险范围广泛，从数据污染和代码注入到传感器欺骗和勾结，突显了日益复杂的基于代理的系统固有的脆弱性。然而，随着人工智能代理变得更加强大，利用自然语言理解和专门工具进行复杂推理，研究人员正在积极开发安全协议来解决这些挑战。这些协议在处理通用和特定领域代理的方法上有所不同。"
        },
        {
          "type": "text",
          "content": "General-purpose agents,designed for versatility across various domains,facea broad spectrum of safety chalenges. To mitigate these risks,researchers have developed several methods to enhance their safety.Evaluation mechanisms, such as AgentMonitor[1381]assessthe safety awareness of agents by monitoring their decision-making processes and identifying potentially unsafe actions.R-Judge[1382] quantifies an agent's risk awarenessby evaluating its responses to both malicious and benign queries,offering a systematic approach to safety compliance.Aditionally, risk detection tools like ToolEmu[795]simulate toolusage in controlled environments to expose vulnerabilities in agent interactions.This approach identifies potentialhazards during task execution,alowing developers to address vulnerabilities proactively.Thesecombinedeffrts enhancethe safety of general-purpose agents throughcomprehensive evaluation and risk detection.",
          "index": 1,
          "part": 0,
          "translated_content": "通用代理是为了在各个领域具有多功能性而设计的，面临着广泛的安全挑战。为了减轻这些风险，研究人员已经开发了几种方法来增强它们的安全性。评估机制，如AgentMonitor，通过监控代理的决策过程并识别潜在的不安全行为来评估代理的安全意识。R-Judge通过评估代理对恶意和良性查询的响应来量化代理的风险意识，为安全合规提供了系统化方法。此外，像ToolEmu这样的风险检测工具在受控环境中模拟工具的使用，以暴露代理交互中的漏洞。这种方法在任务执行过程中识别潜在的危险，使开发人员能够主动解决漏洞。这些综合努力通过全面评估和风险检测增强了通用代理的安全性。"
        },
        {
          "type": "text",
          "content": "Domain-specificagents,tailoredfor specializedtasks inhigh-stakes environments like scientificresearch,requireeven more stringent safety measures.Safety tools such as ChemCrow[1383] are designed to mitigate risks in chemical synthesis tasks by reviewing user queries and filtering malicious commands,ensuring agents do not inadvertently synthesizehazardouschemicals.Structured taskconstraints,as implemented inCLAIRify[84],enhance experimental safety by imposing high-levelconstraints on material synthesis order and low-levelrestrictions on manipulation and perception tasks,thereby preventing accidents and errors.Furthermore,benchmarks like SciGuard[1385],which includes the SciMT-Safety benchmark, evaluate model safety by measuring both harmlessness(rejecting malicious queries)and helpfulness (handling benign queries effectively).SciGuard also incorporates long-term memory to enhance agentsability to safelyexecute complex instructionswhile maintaining accurateriskcontrol.These focused approaches ensure that domain-specific agents operate safely and effectively within their specialized fields.",
          "index": 2,
          "part": 0,
          "translated_content": "专门针对科学研究等高风险环境中的专业任务而设计的特定领域代理需要更为严格的安全措施。诸如ChemCrow的安全工具旨在通过审查用户查询并过滤恶意命令来减轻化学合成任务中的风险，确保代理不会意外合成危险化学物质。CLAIRify中实施的结构化任务约束通过对材料合成顺序施加高层次约束以及对操作和感知任务施加低层次限制，增强了实验安全性，从而预防事故和错误。此外，像SciGuard这样的基准测试包括SciMT-Safety基准测试，通过衡量模型的无害性（拒绝恶意查询）和有效处理良性查询的能力来评估模型的安全性。SciGuard还整合了长期记忆以增强代理执行复杂指令时的安全性，并保持准确的风险控制。这些专注的方法确保了特定领域代理在其专业领域内安全有效地运行。"
        },
        {
          "type": "text",
          "content": "In summary, significant progresshas been made in developing innovative evaluation mechanisms andrisk mitigation strategies to enhance the safety of both general-purpose and domain-specific AIagents.However, acritical area for future research lies in integrating these approaches.Building stronger connections between the broad capabilities of general-purpose agents andthe focused safeguards ofdomain-specificagents willbeessentialforcreating trulyrobust andtrustworthy LM systems.Thechallenge istocombine the best aspects ofboth approaches todevelopagents that are both versatile and secure.",
          "index": 3,
          "part": 0,
          "translated_content": "总的来说，在开发创新的评估机制和风险缓解策略方面取得了重要进展，以增强通用型和特定领域AI代理的安全性。然而，未来研究的一个关键领域在于整合这些方法。在通用型代理的广泛能力和特定领域代理的专注保障之间建立更紧密的联系将是创造真正健壮和可信的LM系统所必不可少的。挑战在于结合这两种方法的最佳方面，开发既多才又安全的代理。"
        }
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": [],
      "translated_title": "20.4 总结与讨论"
    },
    {
      "title": "Concluding Remarks and Future Outlook",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "We have exploredinthis survey the evolving landscape offoundation agents bydrawing paralelsbetween humancognitive processes andartificialintelligenceWebegan byoutliningthecore componentsof intelligent agents—detailing how modules such as memory,perception,emotion,reasoning, and action can be modeled inaframework inspiredby the comparisonwith human brain.Our discussion highlightedhow these agents can be structured ina modular fashion, enabling them to emulate human-like processing through specialized yet interconnected subsystems.",
          "index": 0,
          "part": 0,
          "translated_content": "在这项调查中，我们探索了基于人类认知过程和人工智能之间的相似之处，描绘了基础代理的不断发展格局。我们首先概述了智能代理的核心组件——详细介绍了记忆、感知、情感、推理和行动等模块如何在受人脑比较启发的框架中建模。我们的讨论突出了这些代理如何以模块化方式构建，使它们能够通过专门但相互连接的子系统模拟类似于人类的处理过程。"
        },
        {
          "type": "text",
          "content": "We then delved into the dynamic aspects ofagent evolution,examining self-improvement mechanisms that leverage optimization techniques,including both online and ofline strategies.By investigating how large language models can act as both reasoning entities andautonomousoptimizers,we ilustrated the transformative potentialof agents that continuously adapt tochanging environments.Building on these technical foundations, we highlighted how agents can drive the self-sustaining evolutionof their intelligence throughclosed-loop scientific innovation. We introduced a generalmeasure of intellgence for knowledge discovery tasks and surveyed current successes and limitations in agent-knowledge interactions.This discussion also shed light on emerging trends in autonomous discovery and tool integration, which are crucial for the advancement of adaptive, resilient AI systems.",
          "index": 1,
          "part": 0,
          "translated_content": "我们随后深入探讨了智能体演化的动态方面，研究了利用优化技术的自我改进机制，包括在线和离线策略。通过研究大型语言模型如何既可以充当推理实体又可以作为自主优化器，我们阐明了那些不断适应变化环境的智能体的转变潜力。基于这些技术基础，我们强调了智能体如何通过封闭循环科学创新推动其智能的自持演化。我们引入了一种用于知识发现任务的智能一般度量，并调查了智能体与知识互动中当前的成功和局限性。这一讨论还揭示了自主发现和工具集成中的新兴趋势，这对于推动自适应、弹性人工智能系统的发展至关重要。"
        },
        {
          "type": "text",
          "content": "Our paper alsoaddressed the collaborative dimension of intellgent systems,analyzing how multi-agent interactions can give rise tocollective intelgence.We explored the design of communication infrastructures and protocols that enable both agent-agent andhuman-AIcollaboration.Thisdiscussion underscoredthe importance offostering synergy between diverse agent capabilities to achieve complex problem solving and efective decision-making.\n\nFinally,weemphasized thecriticalchalnge of building safe and beneficialAI.Our review encompassed intrinsic andextrinsicsecuritythreats,from vulnerabilities inlanguage models torisks associated with agent interactions.We providedacomprehensiveoverview of safety scaling lawsand ethicalconsiderations,proposing strategies to ensure that the development of foundation agents remains aligned with societal values.Overall our workoffers a unified roadmap that notonlyidentifiescurrnt researchgaps but alsolaysthefoundation for future innovations increating more powerful, adaptive, and ethically sound intelligent agents.",
          "index": 2,
          "part": 0,
          "translated_content": "我们的论文还讨论了智能系统的协作维度，分析了多智能体相互作用如何产生集体智慧。我们探讨了设计通信基础设施和协议的方式，使得智能体间以及人工智能与人类之间的协作成为可能。这一讨论强调了促进不同智能体能力之间协同作用的重要性，以实现复杂问题解决和有效决策。\n\n最后，我们强调了构建安全和有益人工智能的关键挑战。我们的回顾涵盖了内在和外在的安全威胁，从语言模型的漏洞到与智能体相互作用相关的风险。我们提供了关于安全扩展规律和伦理考虑的全面概述，并提出了确保基础智能体的发展与社会价值观保持一致的策略。总体而言，我们的工作提供了一个统一的路线图，不仅确定了当前研究中的空白，而且为未来创新奠定了基础，以创造更强大、适应性更强、符合伦理的智能体。"
        },
        {
          "type": "text",
          "content": "Looking ahead, we envision severalkeymilestones that willmarksignificantprogressin thedevelopment ofintelligent agents.First,we anticipate theemergenceof general-purpose agentscapableofhandlinga widearrayof human-level tasks, rather than being confined to specificdomains.These agents willintegrate advanced reasoning,perception, and action modules,enabling them to perform tasks with human-like adaptability and versatility.Achieving this milestone willrepresent afundamental shift in how AI can support and augment human capabilities in both everyday and specialized contexts.",
          "index": 3,
          "part": 0,
          "translated_content": "展望未来，我们设想智能体发展中的几个关键里程碑。首先，我们预计通用型智能体的出现，能够处理各种人类级别任务，而不仅仅局限于特定领域。这些智能体将整合先进的推理、感知和行动模块，使它们能够以类似人类的适应性和多功能性执行任务。实现这一里程碑将标志着人工智能在支持和增强人类能力方面发生根本性转变，无论是在日常生活还是专业环境中。"
        },
        {
          "type": "text",
          "content": "Another criticalmilestone is the development ofagents thatlearn directly fromtheir environment andcontinuously selfevolve through interactions with humansand data.As the distinction between training-time and test-time computation gradually disappears,agentswillacquire newskillsonthe flybyengaging withtheir surroundings,otheragents,and human partners.This dynamic learming processis essential forachieving human-levelcapabilities and for enabling agentstokeep pace with aconstantlychanging world.It is alsovitalif agents aretobe able todrive innovation in scientific discovery, as this expands the boundaries of evolution for both agents and humanity.",
          "index": 4,
          "part": 0,
          "translated_content": "另一个关键的里程碑是开发能够直接从环境中学习，并通过与人类和数据的互动持续自我进化的智能体。随着训练时间和测试时间计算逐渐消失的区别，智能体将通过与周围环境、其他智能体和人类伙伴的互动，即时获得新技能。这种动态学习过程对于实现人类水平的能力至关重要，并且有助于使智能体与不断变化的世界保持同步。如果智能体要能够推动科学发现的创新，这一动态学习过程也至关重要，因为这将扩展智能体和人类的进化边界。"
        },
        {
          "type": "text",
          "content": "We predict that agents willtranscend traditional human limitations by transforming individual human know-how into collctive agent intelligence.The current inefficiencies in human information sharing—where complex knowledge requiresextensive practiceto transfer-willbe overcome byagents,which ofera format ofhumanknow-how that is both transferable and infinitelyduplicableThis breakthroughwillremove thebottleneck ofcomplexityenabling a new intelligence networkeffect wherebyalarge ensembleofhuman and AIagentscanoperate atalevelofintelligence that scales withnetwork size.Inthis scenario,the fusion of agent-acquired knowledge andhuman expertise willfosteran environment where insights and innovations are disseminated and applied rapidly across various domains.",
          "index": 5,
          "part": 0,
          "translated_content": "我们预测，智能体将通过将个体人类专业知识转化为集体智能，超越传统的人类限制。目前人类信息共享中存在的低效率——复杂知识需要大量实践才能传递——将被智能体克服，智能体提供了一种既可转移又可无限复制的人类专业知识格式。这一突破将消除复杂性瓶颈，实现新的智能网络效应，使得大量人类和人工智能智能体能够以与网络规模成比例的智能水平运作。在这种情况下，智能体获取的知识与人类专业知识的融合将促进一个环境，在这个环境中，见解和创新能够在各个领域快速传播和应用。"
        },
        {
          "type": "text",
          "content": "We also anticipate this inteligence network effect enabling the establishment of a new paradigm for human-AI collaboration-one that islarger in scale, more interdisciplinary,and more dynamicall organized than ever before. The resulting human-AI societywillachieve previously unatanable levels of complexityand productivity,heralding a transformative era in both technological and social development.\n\nIn summary,these milestones outline a future where intellgent agents become increasingly autonomous,adaptive, and deeply integrated withhuman societydriving scientific discovery,enhancing knowledge sharing, andredefining collaboration on a global scale.",
          "index": 6,
          "part": 0,
          "translated_content": "我们还预计，这种智能网络效应将促成人类与人工智能协作的新范式的建立——规模更大、跨学科性更强、组织更加动态化。由此产生的人类与人工智能社会将实现以往难以企及的复杂性和生产力水平，预示着技术和社会发展的转型时代的来临。\n\n总之，这些里程碑勾勒出一个未来的图景，智能体将变得越来越自主、适应性更强，并与人类社会深度融合，推动科学发现，增强知识分享，并在全球范围重新定义协作。"
        }
      ],
      "raw_title": "Concluding Remarks and Future Outlook",
      "type": null,
      "children": [],
      "translated_title": "结束语和未来展望"
    },
    {
      "title": "Acknowledge",
      "number": "",
      "level": 1,
      "content": [
        {
          "type": "text",
          "content": "Argonne NationalLaboratory's work was supported bythe U.S.Departmentof Energy,Offce of Science, undercontract DE-AC02-06CH11357. XLQ acknowledges the support of the Simons Foundation.",
          "index": 0,
          "part": 0,
          "translated_content": "Argonne National Laboratory的工作得到了美国能源部科学办公室的支持，合同编号为DE-AC02-06CH11357。XLQ感谢Simons基金会的支持。"
        }
      ],
      "raw_title": "Acknowledge",
      "type": null,
      "children": [],
      "translated_title": "致谢"
    }
  ],
  "translated_title": "基于脑启发智能的基础代理的进展与挑战：从进化、协作到安全系统"
}