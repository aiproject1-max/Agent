{
  "title": "ADVANCES AND CHALLENGES IN FOUNDATION AGENTSFROM BRAIN-INSPIRED INTELLIGENCE TO EVOLUTIONARY, COLLABORATIVE,AND SAFE SYSTEMS",
  "authors_info": "Bang $\\mathbf{Liu^{2,3,20*\\dagger}}$ , Xinfeng $\\mathbf{Li^{4*}}$ , Jiayi $\\mathbf{Z}\\mathbf{hang}^{1,10*}$ , Jinlin Wang \\*, Tanjin $\\mathbf{He}^{5*}$ , Sirui $\\mathbf{Hong^{1*}}$ Hongzhang $\\mathbf{Liu^{6*}}$ , Shaokun $\\mathbf{Z}\\mathbf{h}\\mathbf{a}\\mathbf{n}\\mathbf{g}^{7*}$ , Kaitao $\\mathbf{Song^{8*}}$ , Kunlun $\\mathbf{Z}\\mathbf{h}\\mathbf{u}^{9*}$ , Yuheng $\\mathbf{Cheng^{1*}}$ Suyuchen Wang2,3\\*, Xiaoqiang $\\mathbf{Wang^{2,3*}}$ , Yuyu $\\mathbf{Luo^{10*}}$ , Haibo $\\mathbf{Jin^{9}}.$ \\* Peiyan Zhang10, Ollie $\\mathbf{Liu}^{11}$ \\* Jiaqi Chen1, Huan Zhang2,3, Zhaoyang $\\bar{\\mathbf{Y}}\\mathbf{u}^{1}$ , Haochen $\\mathbf{Shi^{2,3}}$ , Boyan $\\mathbf{Li}^{10}$ , Dekun $\\mathbf{W_{u}}^{2,3}$ , Fengwei Teng1, Xiaojun $\\mathbf{Jia^{4}}$ , Jiawei $\\mathbf{X}\\mathbf{u}^{1}$ , Jinyu Xiangl, Yizhang $\\mathbf{Lin}^{1}$ , Tianming $\\mathbf{Liu^{14}}$ , Tongliang $\\mathbf{Liu}^{6}$ Yu $\\mathbf{Su}^{15}$ , Huan $\\mathbf{Sun^{15}}$ ,Glen Berseth2,32,Jian $\\mathbf{Nie^{2}}$ , Ian Foster5, Logan Ward5,Qingyun $\\mathbf{W_{u}}^{\\mathrm{7}}$ Yu $\\mathbf{Gu}^{15}$ , Mingchen Zhuge16, Xiangru $\\mathbf{Tang^{12}}$ , Haohan $\\mathbf{Wang}^{9}$ , Jiaxuan $\\mathbf{You}^{9}$ , Chi Wang19, Jian $\\mathbf{Pei^{17\\dagger}}$ , Qiang $\\mathbf{Yang^{10,18\\dagger}}$ , Xiaoliang $\\mathbf{Q}\\mathbf{i}^{13\\dagger}$ , Chenglin $\\mathbf{W_{u}}^{1*\\dagger}$  \n\n1MetaGPT, ²Université de Montreal, 3Mila - Quebec AI Institute, 4Nanyang Technological University,   \n5Argonne National Laboratory, 6University of Sydney,Penn State University, 8Microsoft Research Asia,   \n9University of Illnois at Urbana-Champaign,10The Hong Kong University of Science and Technology,   \n1UniversityofhCalifileityadityUetyf   \n15The Ohio State University,16King Abdullah University of Science and Technology,17Duke University,   \n18The Hong Kong Polytechnic University,19Google DeepMind, 20Canada CIFAR AIChair",
  "sections": [
    {
      "title": "ABSTRACT",
      "number": "",
      "level": 1,
      "content": [
        "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling,reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats,ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.By synthesizing modular AI architectures with insights from different disciplines, this survey identifies key research gaps,challenges, and opportunities, encouraging innovations that harmonize technological advancement with meaningful societal benefit. The project's Github link is: https://github.com/FoundationAgents/awesome-foundation-agents."
      ],
      "raw_title": "ABSTRACT",
      "type": "abstract",
      "children": []
    },
    {
      "title": "Preface",
      "number": "",
      "level": 1,
      "content": [
        "Large language models (LLMs)have revolutionized artificial intelligence(AI) by demonstrating unprecedented capabilities in naturallanguage andmultimodal understanding,as wellasreasoning and generation.These models are trainedonvastdatasets,andtheyexhibitemergentabilitiessuchasreasoning,in-contextleaing,andevenudimetar planning.While these modelsrepresent a major stepforward in realizing intelligent machines,they themselves do not yet fully embody allthe capabilities of an intelligent beingSincethe early daysof artificial intelligence,AI researchershavelong beenonaquest foratruly“intellgent\"systemthatcanlearn,plan,reason,sense,communicate, act,remember, and demonstrate various human-like abilities and agility.These beings,known as intelligent agents, should be ableto think both long-term and short-term,perform complex actions,and interact withhumans and other agents. LLMs are an important step towards realizing intelligent agents, but we are not there yet.",
        "This manuscript provides acomprehensive overview of the current state of the art of LLM-based intelligent agents. In the past,there have been numerous research papers and books on intelligent agents, as wellasa flurryof books on LLMs. However,there has scarcely been comprehensive coverage of both. While LLMs can achieve significant capabilities required by agents,they onlyprovidethefoundations upon whichfurther functionalities must be built.For example,while LLMs can help generate plans such as travelplans,they cannot yet generatefully complex plans for complex and professionaltasks, nor can they maintain long-term memories without hallucination. Furthermore, their ability to perform real-world actions autonomouslyremains limited.Wecan view LLMs asengines, with agents being thecars,boatsandairplanes builtusingtheseengines.Inthisview,wenaturallseek tomoveforward indesigning and constructing fully functioning intelligent agents by making fulluse of the capabilities provided by LLMs.",
        "In this engine-vehicle analogyof theinterplaybetweenLLMs andagents,wenaturallask:Howmuchofthecapabilities of intelligent agentscancurrent LLMtechnologies provide?What are the functions thatcannot yetberealized based on current LLM technologies? Beyond LMs, what more needs to be done to have a full intelligent agent capable of autonomous action and interaction in the physical world?What are the challnges forfull integrated LLM-based agents?What aditionaldevelopments arerequiredforcapable,communicative agents thateffectivelycollaborate with humans?What are the areas thatrepresentlow-hanging fruits for LLM-based agents?What implications willthere be for society once we have fully intelligent LLM-based agents, and how should we prepare for this future?",
        "These questions transcend not only the engineering practice of extending current LLMs and agents but also raise potentialfuture research directions.We have assembled frontierresearchers from AI,spanning from LLMdevelopment to agent design,tocomprehensivelyaddress thesequestions.Thebookconsists offour parts.The first part presents an exposition of the requirementsforindividualagents,comparing theircapabilities withthose ofhumans,including perception and action abilities. The second part explores agents\"evolution capabilities and their implications on intelligent toolssuch as workflow management systems.Thethirdpart discusses societies of agents,emphasizingtheir collaborative andcolective actioncapabilities, andthefourth part addresses ethicaland societalaspects, including agent safety and responsibilities.",
        "This book isintended forresearchers,students,policymakers,and practitioners alike.The audience includes non-AI readers curious about AI,LLMs,andagents,as wellas individuals interestedinfuture societieswhere humansco-exist with AI.Readers may range from undergraduate and graduate students toresearchers and industry practitioners.The book aims notonly to provide answers to readers'questions about AIand agents but alsoto inspire them to ask new questions.Ultimately, we hope to motivate more people to join our endeavor in exploringthisfertileresearch ground."
      ],
      "raw_title": "Preface",
      "type": null,
      "children": []
    },
    {
      "title": "1 Introduction 12",
      "number": "1",
      "level": 1,
      "content": [
        "1.1  The Rise and Development of AI Agents 12\n1.2 A Parallel Comparison between Human Brain and AI Agents 13\n1.2.1 Brain Functionality by Region and AI Parallels 14\n1.3 A Modular and Brain-Inspired AI Agent Framework 16\n1.3.1 Core Concepts and Notations in the Agent Loop 18\n1.3.2 Biological Inspirations 21\n1.3.3 Connections to Existing Theories 21",
        "1.4 Navigating This Survey 22"
      ],
      "raw_title": "Introduction 12",
      "type": null,
      "children": [
        {
          "title": "1.1 The Rise and Development of AI Agents",
          "number": "1.1",
          "level": 2,
          "content": [
            "The concept of“agent\"is acornerstone of modern AI,representing a system that perceives its environment, makes decisions,and takes actions to achieve specificgoals.This idea,whileformalized in AIin the mid-2Othcentury,has roots in earlyexplorationsof autonomy and interaction inintelligent systems.One ofthe most widelycited definitions, proposedby[3],describes anagent as“anythingthatcanbeviewedasperceiving itsenvironmentthroughsensors and acting uponthatenvironmentthroughactuators\".This definitionemphasizesthedualnatureofagents asbothobservers and actors,capableof dynamicalladapting totheir surroundingsrather thanfollowing staticrules.Itencapsulatesthe shiftin AIfrom systems thatmerelycompute tosystems that engage withtheirenvironment.Thehistoricaldevelopment of agents paralels the evolution of Alitself.Early symbolic systems,such as Newell and Simon's General Problem Solver[4],sought toreplicate human problem-solving processes bybreaking tasks intological steps.However, these systems werelimited by theirreliance on structured environments and predefinedlogic.The agent paradigm emerged as aresponse to these limitations,focusing on autonomy,adaptability,and real-world interaction. Rodney Brooks's subsumption architecture inthe198Osexemplifiedthis shift, introducing agentscapableofbehavior-driven,real-time responses inrobotics[5].Unlike earlierapproaches,these agents operated without the need forexhaustive models of their environment,showcasinga more flexible and scalable design.Agents have since become a versatile framework across AI subfields. In robotics, they enable autonomous navigation and manipulation; in software, they form the foundation of multi-agent systems used for simulation and coordination [6].By integrating perception, reasoning, and action into acohesive structure,the agent paradigm has consistently served as a bridge betweentheoretical AI constructs and practicalapplications,advancing our understanding ofhow intellgent systemscan operate in dynamic and complex environments.",
            "Theadvent oflargelanguage models (LLMs)has redefined thecapabilitiesofagents,ransforming theirrole inartificial intelligence and opening up newhorizons fortheir applications.Agents,once confinedto executing narrowly defined tasks orfollowing rigidrule-based frameworks,now leverage thebroad generalization,reasoning,andadaptability of models like OpenAl's ChatGPT [7], DeepSeek AI's DeepSeek [8], Anthropic's Claude[9], Alibaba'sQWen [10], and Meta's LLaMA[11].These LLM-powered agents have evolved from static systems intodynamic entities capable of processing naturallanguage,reasoning across complex domains, and adapting to novel situations with remarkable fluency. No longer merely passive processors of input, these agents have become active collaborators,capable of addressing multi-stepchallenges and interacting with their environments ina way that mirrors human problem-solving.",
            "A key advancement intheLLMera isthe seamlessintegrationof language understanding with actionable capabilities. Modern LLMs,equipped with function-calling APIs,enable agents to identify when external tools or systems are required,reason about their usage, and execute preciseactions to achieve specific goals.For instance,an agent powered by ChatGPTcanautonomouslyqueryadatabase,retrieve relevant information,and use it to deliver actionable insights,all while maintaining contextualawareness of the broader task.This dynamic combination of abstract reasoning andconcrete execution alows agents to bridge the gap between cognitive understanding and real-world action.Furthermore,the generalization abilities of LLMs in few-shot and zero-shot learning have revolutionized the adaptabilityof agents,enabling them to tackleadiverse array oftasks-fromdata analysis andcreative content generation to real-time collaborative problem-solving—without extensive task-specific training.This adaptability, coupled with their conversational fluency,positions LLM-powered agents as intellgent mediators between humans and machines, seamlessly integrating human intent with machine precision in increasingly complex workflows."
          ],
          "raw_title": "The Rise and Development of AI Agents",
          "type": null,
          "children": []
        },
        {
          "title": "1.2 A Parallel Comparison between Human Brain and AI Agents",
          "number": "1.2",
          "level": 2,
          "content": [
            "Therapid integrationofLLMs into intellgent agent architectures has notonly propelledartificial intellgence forward but also highlighted fundamental differences between AI systems and human cognition.As illustrated briefly in Table 1.1,LLM-powered agents differ significantly from human cognition across dimensions such as underlying “hardware\",consciousness,learning methodologies,creativity, and energy efficiency.However, it is important to emphasize that this comparison provides only a high-level snapshot rather than an exhaustive depiction. Human intelligence possesses many nuancedcharacteristics notcaptured here,while AIagents alsoexhibit distinct features beyond this concise comparison.",
            "Human inteligence operates on biological hardware-the brain—that demonstrates extraordinary energy efficiency, enabling lifelong learming,inference,and adaptive decision-making with minimal metaboliccosts.In contrast,urent AI systems require substantialcomputational power,resultingin significantly higherenergyconsumptionforcomparable cognitive tasks. Recognizing this performance gapemphasizes energy eficiency asacriticalfrontierforfuture AI research.",
            "In terms ofconsciousness and emotional experience,LLM agents lack genuine subjective states and self-awareness inherent to human cognition. Although fullreplicating human-like consciousness in AI may neither be necessary nor desirable,appreciating the profound role emotions and subjective experiences playin human reasoning, motivation, ethical judgments,and socialinteractionscan guide research towardcreating AIthat is more aligned,trustworthy,and socially beneficial.",
            "Human learning is continuous,interactive,andcontext-sensitive,deeplyshaped bysocial,cultural,andexperiential factors.Conversely, LLM agents primarily undergo static,ofline batch training with limited ongoing adaptation capabilities.Despite research works through instruction tuning and reinforcement learning from human feedback (RLHF)[12],LLMagentsstillfallshortofhuman-likeflexibility.Bridging this gapthroughapproaches suchas lifelong learning,personalizedadaptation,and interactivefine-tuning represents a promisingresearch direction,enabling AI to better mirror human adaptability and responsiveness.",
            "Creativity in humans emerges from a rich interplay of personal experiences,emotional insights, and spontaneous cross-domain associations. Incontrast,LLMcreativity primarily arises through statisticalrecombinations of training data—“statisticalcreativity\"-lacking depth,originality,andemotionalresonance.Thisdistinction highlightsopportunities for developing AIagentscapable of deeper creative processes by integrating richercontextual understanding, simulated emotional states, and experiential grounding.",
            "Considering the time scale,the human brain has evolved over millions of years, achieving remarkable efficiency, adaptability,andcreativitythrough naturalselection andenvironmentalinteractions.Instarkcontrast, AIagentshave undergone rapid yet comparatively brief development over roughly 80 years since the advent of earlycomputational machines.This parallelcomparison between human cognition and AI systems is thus highly valuable, as it uncovers essential analogies and fundamental differences, providing meaningful insights that can guide advancements in AI agent technologies.Ultimately,drawing inspiration from human inteligence can enhance AIcapabilities,benefting humanity across diverse applications from healthcare and education to sustainability and beyond.",
            "Table 1.1: Concise high-level comparison between human brains and LLM agents.",
            "<html><body><table><tr><td>Dimension</td><td>Human Brain / Cognition</td><td>LLM Agent</td><td>Remarks</td></tr><tr><td>Hardware & Maintenance</td><td>- Biological neurons, neuro- transmitters, neuroplasticity. - Requires sleep, nutrition, rest. - Limited replication, knowl- edge transfer via learning. - Extremely energy-efficient</td><td>Deep neural networks, gradient-based optimization. - Requires hardware, stable power, and cooling. Easily  duplicated across servers globally. High energy consumption</td><td>Human brains are biologically maintained, energy-efficient, and not easily  replicable. LLM agents rely on hardware maintenance, are highly repli- cable, but significantly less energy-efficient.</td></tr><tr><td>ment</td><td>- Genuine  subjective  ex-- No genuine subjective experi-  Human &Develop-periences, emotions,self- awareness. - Gradual developmental stages from childhood. - Emotional cognition drives decision-making. - Lifelong, continuous, online</td><td>ence or self-awareness. -“Emotions\" are superficial lan- guage imitations. - Static post-training with lim- ited dynamic growth.</td><td>consciousness emerges from  emotional, social, and biological devel- opment; LLMs remain static without true introspection or emotional depth.</td></tr><tr><td>Learning Style</td><td>learning. - Few-shot, rapid knowledge transfer. - Influenced by environment, - Neutral, impersonal learned culture, emotions.</td><td>- Primarily offline, batch-based training. - Limited online fine-tuning and adaptation. knowledge.</td><td>Despite improvements via in- struction tuning, human learn- ing remains more dynamic, adaptive, and culturally/emo- tionally integrated than LLM learning.</td></tr><tr><td>Creativity & Divergence</td><td>ence, emotions, subconscious insights. - Rich cross-domain associa- tions, metaphorical thinking. - Emotional depth influences</td><td>- Rooted in personal experi-- Statistical recombination from extensive data. - Novelty through probabilistic optimization. Limited emotional  and experiential grounding.</td><td>LLM creativity is statistical and data-driven; human cre- ativity blends emotion, expe- rience, and subconscious pro- cesses.</td></tr></table></body></html>"
          ],
          "raw_title": "A Parallel Comparison between Human Brain and AI Agents",
          "type": null,
          "children": [
            {
              "title": "1.2.1 Brain Functionality by Region and AI Parallels",
              "number": "1.2.1",
              "level": 3,
              "content": [
                "Understanding paralels between human brain functions and artificial intelligence(Al)sheds lighton boththe strengths and current lmitations of AI, particularly large language models (LLMs)and AIagents.Based on currnt neuroscience, the human brain is primarilycomposed of sixfunctionalregions,such as frontallobe,cerebelum, and brainstem, as shown inFigure 1.1.Inthis work,we further systematicallyexamine theexisting AIcounterparts to majorbrain regions and their primaryfunctionalities.Forabig-picture perspective,thestateofresearch inAIcan becategorized with three distinct levels:",
                "· Level 1 (L1): Well-developed in current AI.\n· Level 2 (L2): Moderately explored, with partial progress. Can be further improved.\n· Level 3 (L3): Rarely explored; significant room for research.",
                "A high-level visual map of brain functionalregions andtheir coresponding AIdevelopmentlevels is shown in Figure 1.1.We aim to underscorehow core principles of specialization and integration,observed in biological systems,can guide morecohesive agent architectures.We now examine each brainfunctionalregion andtherelevant AIdevelopment in detail.",
                "Frontal Lobe: Executive Control and Cognition The frontal lobe, notably the prefrontal cortex, is crucial for higher-order cognition such as planning (L2), decision-making (L2),logical reasoning (L2), working memory (L2),self-awareness (L3),cognitive flexibility (L3),and inhibitorycontrol (L3)[13].AIhas made notable strides in planning and decision-making within well-defined domains, demonstrated by AI agents such as AlphaGo [14] Transformers employ atention mechanisms similar tohuman working memory[15],yetfallshortof human flexibility and robustness.The exploration of genuine self-awareness and inhibitory controlin AIremains scarce,andcaution is advised due to potential ethical and safety implications.",
                "![](images/83102f32bcecee7f5b319bc61e73ac01298c219062bcf26542ad9d781abba69f.jpg)",
                "Figure 1.l:Illustration ofkey human brain functionalities grouped by majorbrain regions,annotatedaccording to their current exploration levelin AIresearch.This figurehighlights existing achievements,gaps,and potentialoppotunities for advancing artificial intelligence toward more comprehensive, brain-inspired capabilities.",
                "Parietal Lobe: Spatial Processing and Multisensory Integration The parietal lobes integrate multisensory inputs, facilitating attention(L2),spatialorientation (L2),andsensorimotor coordination (L2)[16].AIresearch in robotics and computer vision addresses similar challenges,employing techniques likesimultaneous localization and mapping (SLAM).Nonetheless, AI stillacks the seamless and real-time integration seen in humans.Furthermore, detailed tactile perception (L3)remains largely unexplored and offers considerable potential, particularly for robotics and prosthetics applications.",
                "Occipital Lobe: Visual Processing Specialized in visual perception (Ll),the occipitallobe efficiently processes visual stimulithrough hierarchical structures[13].AIexcels inbasicvisualrecognitiontasks,achieving human-levelor suprior performance using deep neural networks and vision transformers [15]. However, advanced capabilities such as contextual scene understanding (L2)and abstract visual reasoning remain challenging and are only moderately developed.",
                "Temporal Lobe: Language,Memory,and Auditory Processing The temporal lobes facilitate auditory processing (L1), language comprehension (L1), memory formation (L2), and semantic understanding (L2)[16].AI has notably advanced inlanguage and auditory processing,demonstrated by large language models (LLMs)capable of near-human speech recognition and language generation. However, robust episodic memory and lifelong learning capabilitiesremain limited, with AI systems frequently encountering issues like catastrophic forgeting.Grounding semantic understanding in multimodal experiences continues to be an active area of research.",
                "Cerebellum: Coordination and Motor Learning The cerebellum primarily supports motor coordination (L2), precise skillearning (L2),and adaptive error correction (L2),with emergingroles incognitive timing and predictive modeling (cognitive timing, L3)[13]. Al-based robotics has achieved limited successes in emulating human-like dexterity. Real-time adaptive control remains challenging,though current research in reinforcement learning and meta-leaning shows promising initialresults.Cognitive functions of the cerebellum represent an underexplored yet promising frontier.",
                "Brainstem: Autonomic Regulation and Reflexive Control The brainstem manages essential life-sustaining autonomic functions (L3)andrapid reflexive responses (L1),suchas basic motor reflexes[13].AI includes engineered reflexiveresponses,likeautomaticbraking inautonomous vehicles,ypically predefinedratherthanlearned.In contrast, the complexityof autonomic regulation and dynamic arousal states remains largely unexplored in AI,andtheir relevance may be limited due to fundamental differences between biological and artificial systems.",
                "Limbic System: Emotion, Empathy, and Motivation The limbic system,comprising the amygdala and hippocampus, governs emotional processing (L3),reward mechanisms (L2),empathy (L3),stress regulation (L3),and motivationaldrives (L3)[13].AI'sreinforcementlearning algorithmsemulatereward-basedlearning superficialy,but nuanced emotional comprehension, genuine empathy, and internal motivational states remain significantly underdeveloped. Ethical concerns regarding emotional manipulation highlight the need for careful and responsible exploration.",
                "Bridging Brain-Like Functions and Building Beneficial AI Until now, wehave witnessed the gap between human brain and machine intellgence.Neverthelesstheobjectiveisnot necessarilytoreplicateeveryfacetof humancognition within artificialintelligence systems.Rather,ouroverarching aimshould betodevelopintelligent agents thatare useful, ethical,safe,andbeneficialtosciety.Bycticallymparinghumanandartifcalintellgence,wehighlighttheexiting gapsand illuminate promising directions forinnovation.Thiscomparative perspectiveallows us toselectively integrate beneficial aspects of human cognition,such as energy-efficient processing, lifelong adaptive learning, emotional grounding,andrichcreativity,while simultaneously innovating beyond human limitations.Ultimately, this aproach aims to foster the creation of more capable, resilient, and responsible AI systems.",
                "Furthermore,it is vital toconsider the evolving role ofhumans within ahybridHuman-AI society.The goal of AI should notbeto replace human roles entirely,butrather to augment and empowerhuman abilities,complementing human skills and judgment inareas where AIexcels,such as handling vast datasets,performing rapidcalculations,and automatingrepetitivetasks.Human oversight andinterpretabilityare essntialtoensure that powerfulAIsystemsremain controllable and aligned withhumanvalues andethical standards.Thus,thecoreobjective must bethe development of AI technologies that are transparent, interpretable, and responsive to human guidance.",
                "Human-centered AI design emphasizes collaboration, safety, and social responsibility, ensuring technological advancement proceeds in acontrolled,reliable manner. By placing humans at the centerof the AI ecosystem, wecan harness Al'spotentialtoenhancehuman productivitycreativity,anddecisionmakingfacilitating technicaland societal progress without compromising human autonomy or dignity. Ultimately,athoughtfulintegration of human intelligence and AI capabilities can pave the way for a sustainable, equitable, and prosperous future."
              ],
              "raw_title": "Brain Functionality by Region and AI Parallels",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "1.3 A Modular and Brain-Inspired AI Agent Framework",
          "number": "1.3",
          "level": 2,
          "content": [
            "One core issue in theLLMera isthelackofaunifiedframeworkthat integrates therichcognitive and functional components required byadvanced agents.While LLMsoffer exceptionallanguage reasoning capabilities, many current agent designs remain ad hoc-they incorporate modules like perception, memory,or planning in a piecemeal fashion, failing to approximate the well-coordinatedspecialization seen in biological systems such as the human brain. Unlike current LM agents,the human brain seamlessy balances perception, memory,reasoning,and actionthrough distinct yet interconnectedregions,facilitating adaptive responses tocomplex stimuli.LLM-driven agents,bycontrastoften stumblewhen tasks require cross-domain or multimodal integration,highlighting the needfora more holistic approach akin to thebrain'sfunctional diversityMotivated bythese paralls,our survey advocates drawing inspiration fromthe human brain to systematically analyze and design agent frameworks.This perspective shows that biological systems achieve general inteligence byblending specializedcomponents(for perception,reasoning,action,etc.)inatightly integrated fashion-an approach that could serve as a blueprint for strengthening current LLM-based agents.",
            "Neuroscientificresearchreveals thatthebrain leverages bothrational circuits(e.g.,the neocortex,enabling deliberation and planning)and emotional circuits (e.g.,the limbic system)to guide decision-making.Memory formation involves the hippocampus andcortical mechanisms,while reward signals,mediated by dopaminergic andother neuromodulatory pathways,reinforce behavior and learning.These biologicalinsights inspire several design principles for AI agents, including but not limited to:",
            "Table 1.2:Notation summary forthe revised agent framework,highlighting separate learning and reasoning function: within the overall cognition process.",
            "<html><body><table><tr><td>Symbol</td><td>Meaning</td></tr><tr><td>W</td><td>The world with society systems that encapsulate both environment and intelligent beings (AI or human).</td></tr><tr><td>S</td><td> State space of the environment.</td></tr><tr><td> St ∈S</td><td>Environment's state at time t.</td></tr><tr><td></td><td>Observation space.</td></tr><tr><td>Ot∈O</td><td>Observation at time t (potentially shaped by attention or other perception filters).</td></tr><tr><td>A</td><td>Agent's action space.</td></tr><tr><td>at∈A</td><td>Action output by the agent at time t. This can be an external (physical) action or an internal (mental) action such as planning or decision-making.</td></tr><tr><td>M</td><td>Space of all mental states.</td></tr><tr><td>Mt∈M</td><td>Agent's mental state at time t, encompassing sub-components (memory, emotion, etc.).</td></tr><tr><td>Mmem</td><td>Memory component in Mt (e.g., short-term or long-term knowledge).</td></tr><tr><td>Mwm</td><td>World model component in Mt (internal representation of how the environment evolves).</td></tr><tr><td>Memo</td><td>Emotion component in Mt (internal valence, arousal, or affective states).</td></tr><tr><td>Mgoal</td><td>Goal component in Mt (objectives, desired outcomes, intentions).</td></tr><tr><td>Mrew</td><td>Reward/Learning signals in Mt (drives updates to preferences, values, or policy).</td></tr><tr><td>L</td><td>Learning function: L : M × A× O -→ M. Responsible for updating or learning the next mental state (e.g., memory, world model, emotion), based on the previous mental state Mt-1,the previous action at-1, and the new observation ot. Reflects how the agent acquires or revises knowledge, skills, or preferences.</td></tr><tr><td>R</td><td>Reasoning function: R : M -→ A. Responsible for deriving the next action at given the updated mental state Mt. Can involve planning, decision-making, or other internal logic.</td></tr><tr><td>C</td><td>Cognition function: C : M × A × O → M × A. Encapsulates both learning (L) and reasoning (R). Concretely, (Mt,at) = C(Mt-1,at-1,Ot) means the agent first learns the new mental state Mt = L(Mt-1,at-1,Ot),then reasons about the next action at = R(Mt).</td></tr><tr><td>E</td><td>Action execution (effectors): E : A -→ A.(Optional) transforms or finalizes at before applying it to the environment (e.g., converting a high-level command into low-level motor signals).</td></tr><tr><td>T</td><td>Environment transition: T : S × A -→ S. Defines how the environment state evolves from (st, at) to St+1.</td></tr></table></body></html>",
            "·Parallel, Multi-Modal Processing: The brain processes visual, auditory, and other sensory inputs in parallel through specialized cortical areas,integrating them in associative regions.Similarly, AI agents benefit from parallel processing of diverse sensor streams,fusing them in later stages for coherent understanding. · Hierarchical and Distributed Cognition: Reasoning, planning, emotional regulation, and motor control involve interactions between cortical and subcortical regions.Analogously, AI agents can employ modular architectures with subsystems dedicated to rational inference, emotional appraisal, and memory. ·Attention Mechanisms: Human attention prioritizes sensory data based on context, goals, and emotions.AI agents can replicate this by modulating perception through learned atention policies, dynamically adjusting focus based on internal states.",
            "· Reward and Emotional Integration: Emotions are not merely noise but integral to decision-making, modulating priorities,enhancing vigilance, and guiding learing. Reward-driven plasticity facilitates habit formation and skill acquisition, a concept critical to reinforcement learning in AI agents. ·Goal Setting and Tool Usage: The human prefrontal cortex excels at setting abstract goals and planning action sequences, including tool uses. Similarly, AI agents require robust goal-management systems and adaptive action repertoires, driven by external rewards and intrinsic motivations.",
            "nese principles form the foundation of our proposed brain-inspired agent framework, where biological mechanisms rve as inspiration rather than direct replication.",
            "In the following sections,weoutline ourframework'skeyconcepts,introducing aunifiedagent architecture basedon the perception-cognition-action loop enriched byreward signals andlearning processes.Each subsystem is carefully definedandinterconnectedtoensure transparencyinhow memory,world modelsemotions,goals,rewards, andleaing interact. We formalize cognition as a generalreasoning mechanism, with planning and decision-making framed as specific“mentalactions\"shaping behavior.Connections toestablished theories,such as Minsky's Societyof Mind[17], Buzsaki's inside-out perspective[18],and Bayesianactive inference[19],areexplored tohighlight the framework's generality and biological plausibility.",
            "![](images/e45d4a32b5d8e29284930de7efec6b3dee6de4a47936adc7cb0b15ecaefb5f72.jpg)",
            "Figure 1.2: An overview of our general framework for describing an intelligent agent loop and agent society."
          ],
          "raw_title": "A Modular and Brain-Inspired AI Agent Framework",
          "type": null,
          "children": [
            {
              "title": "1.3.1 Core Concepts and Notations in the Agent Loop",
              "number": "1.3.1",
              "level": 3,
              "content": [
                "Our architecture operates atthreeconceptuallevels:Society,Environment,and Agent.The Agent is then decomposed into three main subsystems: Perception, Cognition,and Action. Within Cognition, we identify key submodules: memory,world model,emotional state,goals,reward,learning,and reasoning processes (including“planning\"and “decision-making\"as specialactions produced with reasoning).Attention is primarilyhandled within perception and cognition. Before presenting the formal loop, we summarize our symbols in Table 1.2.",
                "In the following, based on the notations in Table 1.2, we present our proposed agent loop."
              ],
              "raw_title": "Core Concepts and Notations in the Agent Loop",
              "type": null,
              "children": []
            },
            {
              "title": "1.3.2 Biological Inspirations",
              "number": "1.3.2",
              "level": 3,
              "content": [
                "Although our agent model is fundamentally computational, each submodule draws inspiration from well-studied biologicalcounterparts in the human brain.Below, we discussthese analogies in amanner that highlights boththe neuroscientific basis and the flexibility afforded by AI implementations.",
                "Memory (Hippocampus and Neocortex).Decades of neuroscience researchhave linked the hippocampus to episodic memory formation, while corticalregions are known tohouse semantic and proceduralknowledge[21,22].In humans, these memory subsystems cooperate to manage both short-term encoding and long-term consolidation. Our memory component, $M_{t}^{\\mathrm{mem}}$ , similarly aims to capture multi-scale learning by storing recent experiences and knowledge. This can be realizedthrougheither neuralnetwork weights(long-term)orexplicit bufers (short-term),thereby mirroring the hippocampal-cortical interplay.",
                "World Model(Predictive Processing).A prominent theory incognitive neuroscience holds thatthe cortexoperates as a predictive machine,continually comparing incoming sensory data with generated expectations [23,19].The world model $M_{t}^{\\mathrm{wm}}$ reflects this idea by maintaining an internal representation of how the environment evolves over time. Just as cortical circuits integrate multisensory data to update these internal models,our framework allows $M_{t}^{\\mathrm{wm}}$ to be refined upon each new observation andrelevant reward oremotionalcues,ofering aBayesian or free-energy perspective on environmental dynamics.",
                "Emotion (Limbic System).Emotions, mediated by structures like the amygdala,hypothalamus, and limbic system, significantlymodulate attention,learningrates,and decision-making thresholds[24,25].By introducing anemotion component $M_{t}^{\\mathrm{{emo}}}$ ,our model captures how internal valence or arousal states can shift an agent's focus and behavior. Althoughcomputational“emotions\"are neither ful analogous to biological affct nor conscious feelings,they can guide adaptive heuristics-such as prioritizing urgent goals or responding quickly to perceived threats.",
                "Goals and Reward (Prefrontal & Subcortical Circuits).Humans excel atformingabstract, long-term goals, an ability often associated withprefrontalcortexfunction[26,27].In paralel, subcorticalcircuits—particularlydopaminergic pathways-drive reinforcement signals that shape motivation and habit learning [28]. Our agent includes $M_{t}^{\\mathrm{goal}}$ for storing objectives and $M_{t}^{\\mathrm{rew}}$ for encoding reward signals, thus enabling a continuous feedback loop where goal formation andreward-based adaptation reinforce eachother.This mechanism allows for planned action sequences,tool usage, and more nuanced social interactions.",
                "Reasoning, Planning, and Decision-Making (Prefrontal Cortex).Finaly,the human prefrontal cortex integrates information from memory,sensory inputs,emotions, and reward pathways to carry out higher-order cognitive processes—suchaslogicalreasoning,planning,andexecutivecontrol[29,30]. Inour agentframework,thesecapabilities are subsumed by the reasoning sub-function, which-through modules like PlanFn and Decide—selects and executes actions (whether physical or purely mental).Bydistinguishing planning from on-the-fly decision-making,we capture how the agentcansimulatefuture scenarios,weighoutcomes,andthencommit toacourseofaction,akintothe flexible orchestration observed in prefrontal circuits."
              ],
              "raw_title": "Biological Inspirations",
              "type": null,
              "children": []
            },
            {
              "title": "1.3.3 Connections to Existing Theories",
              "number": "1.3.3",
              "level": 3,
              "content": [
                "3eyond these explicit neurobiological parallels,our architecture resonates with several important theories in AI, :ognitive science, and neuroscience.",
                "Classic Perception-Cognition-Action Cycle.We extend the traditional sense-think-act cycle outlined by [20] incorporating explicit mechanisms forattention(inP),leaming andemotion (in C),andreward signals that persist over time.This explicitness makes it easier to analyze how an agent's internal states and prior actions shape subsequent perception and cognition.",
                "Minsky's“Society of Mind\".[17] argued that intellgence arises from an ensembleof specialized“agents\"within a mind. Our submodules- $\\cdot\\mathrm{C}_{\\mathrm{mem}}$ $\\mathrm{C}_{\\mathrm{wm}}$ $\\mathrm{C}_{\\mathrm{emo}}$ $\\mathrm{C_{goal}}$ goal，C $\\mathrm{C}_{\\mathrm{rew}}$ -echo this decomposition, distributing key functions (memory,prediction,emotionalevaluation, goal-seting,etc.)acrossseparate yetinteractingcomponents.Inabroader “society\"context,each agent (or sub-agent)could coordinate cooperativelyor competitively, much like Minsky's internal agencies.Recent work on naturallanguage-based societies of mind[31] supports that agentic systemscan be represented using the original society-of-mindtheory,andcould incorporate socialstructures and economic models among agents.",
                "Buzsaki's Inside-Out Perspective.Neuroscientists [18]contend that the brain actively constructs and updates its perception instead of merely receiving inputs. In our model, $M_{t-1}$ —including emotional states, reward signals, and goals—directly influences the perception map P.This supports the inside-out stance that an agent's internal context drives the way it samples and interprets the environment, rather than passively reacting to it.",
                "Partially observable Markov decision process (POMDP).Our framework can be viewed as a generalization of the classical Partially Observable Markov Decision Process (POMDP) in several ways.First, whereas a POMDP specifies a probabilistic transition function $\\textstyle P(s_{t+1}\\mid s_{t},a_{t})$ over a(possibly finite) state space, we retain an environment transition T withoutrestricting it toa purely probabilisticor finite form,allowing forarbitraryoreven deterministic mappings. Second, in the standard POMDP setting, reward is typically defined as a scalar function of $(s_{t},a_{t})$ (possibly discounted over time). By contrast, we place reward signals inside the agent's mental state $(M_{t}^{\\mathrm{rew}})$ , letting them depend on—and co-evolve with-goals,emotion, and the world model rather than enforcing a single externally defined objective. Third,whilePOMDPagents generallyselect actions by maximizing anexpectedreturn (value function)ourreasoning sub-process is broader. It accounts for memory,emotion, and other mental-state factors,accommodating heuristic or socially driven decisions rather than strictly value-based choices.Finally,a POMDP does not explicitly define cognitive submodules such as memory oremotion-these must be collapsed into a monolithic“belief state\". In our framework,eachsub-component (memory, world model,emotion, goals,reward) is explicitly modeled and updated, mirroring biologicall inspired views of cognition. Hence,although our approach recovers the POMDP formulation as a specialcase (byenforcing aprobabilistic T,ascalar reward,andaminimal mental state),itadmits aricher varietyof environment transitions, internal states, and decision mechanisms.",
                "Active Inference and the Bayesian Brain. Active inference, aunifying framework advanced by[19],suggests that agents continually update internal generative models to minimize prediction error (or“free energy\"). Our use of $M^{\\mathrm{wm}}$ and $M^{\\mathrm{rew}}$ , together with planning and decision-making modules, can be interpreted in Bayesian terms. The agent atempts toreduce surprise byaligning its world modelwithnew dataand bychoosing actions that conform topredicted (or desired) outcomes.",
                "Biological Plausibility & Generality.While the mapping between brain circuits and agent submodules is made at a high level,it ofers an approach that isat once biologically inspired and modularly agnostic.Memory,emotion, goals,andrewardcan each be implemented by various AI paradigms—symbolic methods, neural networks,or hybrid approaches—thus preserving flexibility.By integrating these key ideas from neuroscience,cognitive science,and AI, we arrive atageneralframework thatcaptures theessentialpropertiesofintelligent behaviorwithout overconstraining implementation details."
              ],
              "raw_title": "Connections to Existing Theories",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "1.4 Navigating This Survey",
          "number": "1.4",
          "level": 2,
          "content": [
            "This survey isstructured to provide acomprehensive, modular, and interdisciplinary examination of intellgent agents, drawing inspiration fromcognitive science,neuroscience,andother disciplines to guide the next waveof advancements in AI.Whilemanyexisting surveys[32,33,34,35,36,37,38,39,40]offr valuableinsights intovarious aspects of agent research,we provide adetailedcomparison oftheir focalpoints in Table1.3.Our work distinguishes itself by systematically comparing biological cognition with computational frameworks to identify synergies,gaps,and opportunities for innovation.By bridging thesedomains,we aim to providea unique perspective that highlights not only where agents excel but also where significant advancements are needed to unlock their fullpotential.",
            "Table 1.3: Summary of existing reviews with diferent focal points. ·indicates primary focus while $\\scriptscriptstyle\\mathrm{~o~}$ indicates secondary or minor focus.",
            "<html><body><table><tr><td> Survey</td><td>Cognition</td><td>Memory</td><td>World Model</td><td>Reward</td><td>Action</td><td>Self Evolve</td><td>MultiAgent</td><td>Safety</td></tr><tr><td>Zhang et al. [39]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td><td>0</td></tr><tr><td>Guo et al. [38]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>0</td><td>·</td><td>·</td><td>0</td></tr><tr><td>Yu et al. [40]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td><td>·</td></tr><tr><td>Wang et al. [35]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td> Masterman et al. [37]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td>Xi et al. [34]</td><td>·</td><td>·</td><td>0</td><td>0</td><td>·</td><td>·</td><td>·</td><td>·</td></tr><tr><td>Huang et al. [33]</td><td>·</td><td>·</td><td>0</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td></tr><tr><td>Durante et al. [32]</td><td>·</td><td>·</td><td>0</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td></tr><tr><td>This Manuscript</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td><td>·</td></tr></table></body></html>",
            "The survey is divided into four key parts:",
            "·In Part I: Modular Design of Inteligent Agents, we introduce the core modules of agents, including the cognition module,which serves as the“brain\"of the agent; the perception systems for interpreting sensory input; as well as the action systems for interacting with the external world. Within the cognition system, we further discuss the memory, world modeling,emotion, goal, and reward systems, analyzing their current progress, limitations, and research challenges.",
            "·In Part II:Self-Enhancement in Intelligent Agents, we shift focus to the capability of agents to evolve and optimize themselves. We explore mechanisms like adaptive learning,self-reflection, and feedback-driven improvement,inspired bythe human ability to grow and refine skills over time.This part also addresses the importance of dynamic memory systems and continuous knowledge integration for agents to remain relevant and effective in changing environments.\n·In Part II: Collaborative and Evolutionary Intelligent Systems, we examine how agents interact with each other and their environments to solve complex, large-scale problems. We discuss multi-agent systems, highlighting their applications in fields such as robotics, medical systems and scientific discovery. This part explores multi-agent system topologies and agent protocol, tracing the evolution of communication and collaboration from static to dynamic frameworks. We align agents with human collaboration paradigms, examining how interaction patterns shape the co-evolution of inteligence and how multi-agent systems adapt their decision-making in various collaborative settings to solve complex challenges through collective intelligence.\n·Finally,in Part IV: Building Safe and Beneficial AI, we provide a comprehensive analysis of the security landscape for LLM-based agents. We introduce a framework categorizing threats as intrinsic or extrinsic. Intrinsic vulnerabilities arise from within the agent's architecture: the core LLM“brain\", and the perception and action modules that enable interactions with the world. Extrinsic risks stem from the agent's engagement with memory systems, other agents,and the broader environment. This part not only formalizes and analyzes these vulnerabilities,detailing specific atack vectors like jailbreaking and prompt injection, but alsoreviews a range of defense mechanisms. Moreover, we explore future directions,including superalignment techniques and the scaling law of AI safety-the interplay between capability and risk.",
            "By weaving togetherthese threads,our survey aims to provide aholistic perspectiveonthecurrnt state of intelligent agents andaforward-lookingroadmapfortheir development.Ourunique focus onintegratingcognitive science insights with computational design principlespositions this survey asa foundationalresource forresearchers seeking to design agents that are notonly powerfulandeffcient butalsoadaptive,ethical,anddeplyalignedwiththecomplexitiesof human society."
          ],
          "raw_title": "Navigating This Survey",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "2Cognition 25",
      "number": "2",
      "level": 1,
      "content": [],
      "raw_title": "Cognition 25",
      "type": null,
      "children": [
        {
          "title": "2.1  Learning 25",
          "number": "2.1",
          "level": 2,
          "content": [
            "2.1.1 Learning Space 27\n2.1.2 Learning Objective 29\n2.2 Reasoning 31\n2.2.1 Structured Reasoning 32\n2.2.2 Unstructured Reasoning 34\n2.2.3 Planning 36"
          ],
          "raw_title": "Learning 25",
          "type": null,
          "children": []
        },
        {
          "title": "2.1 Learning",
          "number": "2.1",
          "level": 2,
          "content": [
            "Learning represents the fundamental process through which intelligent agents transform experiences into knowledge within their mental states.This transformation occurs across different cognitive spaces,fom holistic updates across the full mental state torefinement of specificcognitivecomponents.The scope oflearning encompasses remarkable capacities that serve diferent objectives:enhancing perceptual understanding,improving reasoning capabilities, and developing richer world understanding.",
            "![](images/d3e62f0be6d4fce59bf96b53373cb15cde8f7403963feb75602992f077d742e6.jpg)",
            "Figure 2.1: Illustrative Taxonomy of Cognition system, including learning and reasoning paradigm.",
            "Humanlearning operates across multiple spaces and objectives through the brain's adaptable neural networks. The brain coordinates learming acrossits entire network through integrated systems:the hippocampus facilitates rapid encoding ofepisodicexperiences, thecerebellum supports supervisedlearningforprecisemotorskill,thebasalganglia enable reinforcement learning through dopaminergic reward signals,andcorticalregions facilitate unsupervised pattern extraction[99].Atmore focusedlevels,specific neuralcircuitscan undergotargetedadaptation,allowingforspecialized skill development and knowledge acquisition. These systems work together on different timescales,ranging from immediate responses tolifelong development, while being influenced byfactors like attention,emotions,and social environment [27].",
            "LLM agents, while fundamentally diffrent in architecture, implement analogous learning processes across their mental state spaces.At thecomprehensive level,they acquire broad knowledge through pre-training on massive datasets,demonstrating a form of unsupervised learning.At more focused levels, theyrefine specific capabilities through parameter-updating mechanisms like supervised fine-tuning and reinforcementlearning. Uniquely, they also demonstrate in-contextlearning capabilities,adapting to noveltaskswithout parameter changes by leveraging context within their atention window:a capability that mirrors aspects of human working memory but operates through fundamentally different mechanisms.",
            "The comparison between human and artificial learning systems provides valuable insights for developing more capable,adaptive agents.Humanlearning demonstrates notable characteristics in eficiency,contextualization, and integration with emotional systems,while LLM-basedapproaches show distinct capabilities in processng large datasets, representing formal knowledge,and synthesizing information acrossdomains.These complementary strengths suggest productive directions for research.As we explorethe foundations of learning,we first examine the spaces where learning occurs within mental states,folowed byan analysisof the specificobjectives thatdrivelearning proceses.",
            "Table 2.1: Summary of Learning Methods with Different State Modifications. $\\bullet$ indicates primary impact while $\\scriptscriptstyle\\mathrm{~o~}$ indicates secondary or no direct impact.",
            "<html><body><table><tr><td>Method</td><td>Model</td><td>Perception</td><td>Reasoning</td><td>Memory</td><td>Reward</td><td>World Model</td></tr><tr><td>Voyager [47]</td><td>0</td><td>0</td><td></td><td>·</td><td></td><td>0</td></tr><tr><td>Generative Agents [50]</td><td>0</td><td></td><td>0</td><td>·</td><td>0</td><td>0</td></tr><tr><td>Learn-by-interact [102]</td><td>·</td><td>O</td><td></td><td>·</td><td></td><td></td></tr><tr><td>RAGEN[63]</td><td>·</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td>DigiRL[103]</td><td>·</td><td>0</td><td>·</td><td>0</td><td>·</td><td>0</td></tr><tr><td>R1-Searcher [45]</td><td>·</td><td>·</td><td>·</td><td></td><td>·</td><td>0</td></tr><tr><td>RewardAgent [104]</td><td>·</td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td></tr><tr><td>Text2Reward [105]</td><td></td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td></tr><tr><td>ARAMP [106]</td><td>·</td><td></td><td>0</td><td>0</td><td>·</td><td>0</td></tr><tr><td>ActRe [49]</td><td>·</td><td></td><td>·</td><td>0</td><td>0</td><td>·</td></tr><tr><td>WebDreamer [107]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>·</td></tr><tr><td>RAP [74]</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>·</td></tr><tr><td>AutoManual [108]</td><td>0</td><td>0</td><td>0</td><td>·</td><td>0</td><td>·</td></tr></table></body></html>"
          ],
          "raw_title": "Learning",
          "type": null,
          "children": [
            {
              "title": "2.1.1 Learning Space",
              "number": "2.1.1",
              "level": 3,
              "content": [
                "The learning approaches in LLMagents represent astructured, data-driven paradigm in contrast to the exploratory, emotionally-driven learningobserved in humans.While human learning often involves active curiosity,motivation,and emotionalreinforcement, LLM-based agents typicall learn through more formalized proceses,such as parameter updates during training or structured memory formation during exploration.Current agent architectures attemptto bridge this gapbyimplementing mechanisms that simulate aspects ofhuman learning while leveraging the strengths of computational systems.",
                "Leaning within an intellgent agent occurs across different spaces, encompassing both the underlying model $\\theta$ and mental states $M$ ,where the former fundamentally supports the capabilities and limitations of the latter. Formally, we define an intelligent agent's internal state as a tuple ${\\mathcal{T}}=(\\theta,M)$ that includes both the model parameters and mental state components.The mental state can be further decomposed intodifferent structures as we illustrated in 1.2:",
                "$$\nM=\\{M^{m e m},M^{w m},M^{e m o},M^{g o a l},M^{r e w}\\}\n$$",
                "where $M^{m e m}$ represents memory, $M^{w m}$ denotes world model, $M^{e m o}$ indicates emotional state, $M^{g o a l}$ represents goals, and $M^{r e w}$ represents reward signals.",
                "Modifications to the underlying modelcan beviewed as fullmental state learning,as they fundamentally alter the agent's capabilities.While model-level modifications can affect diferent mental states to varying degrees,changes to the model'scontext window orexternal structures tend tofocus on specific mental state components.For instance, leaning experiences andskill fromthe environment primarilyinfluencememory,while leveraging the LLM's inherent predictive capabilities enhances the world model.",
                "Full Mental State Learning Fullmental state learming enhances thecapabilities of an agent through comprehensive modifications to the underlying model $\\theta$ , which in turn affects all components of the mental state $M$ . This process begins with pre-training, which establishes the foundation of language models by acquiring vast world knowledge, analogous to how human babies absorb environmental information during development,though in a more structured and extensive manner.",
                "Post-training techniques represent the cornerstone for advancing agent capabilities.Similar tohow human brains are shaped byeducation,these techniques whileaffcting theentire model,can emphasize diffrent aspects ofcognitive development.Specificaly,various forms of tuning-basedlearning enable agents toacquiredomain-specificknowledge and logical reasoning capabilities. Supervised Fine-Tuning (SFT)[41] serves as the fundamental approach where models learnfrom human-labeled examples,encoding knowledge directly intothe model's weights.For computational effciency, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged.Adapter-BERT[42]introduced modular designs that adapt models to downstream tasks without modifying allparameters, while Low-Rank Adaptation (LoRA) [109] achieves similarresults by decomposing weight updates into low-rank matrices,adjusting only a smallsubsetof effective parameters.",
                "Some agent capabilities are closelyconnectedto how wellthey align with human preferences, with alignment-based learning approaches modifying the modeltoreshape aspects of the agent's underlying representations.Reinforcement learning from human feedback (RLHF)[110] aligns models with human values by training a reward model on comparative judgments and usingthis to guide policy optimization.InstructGPT[43]demonstrated how this approach could dramatically improve consistency with user intent across diverse tasks.Direct Preference Optimization (DPO) [111] has further simplified this process byreformulating it as direct preference learning without explicit reward modeling, maintaining alignment quality while reducing computational complexity.",
                "Reinforcement learning (RL)presents a promising pathway for specialized learning in specific environments. RL has shown particular promise in enhancing reasoningcapabilities,essentially enabling the agent's underlying model to learn within the space of thought.Foundational works such as Reinforcement Fine-Tuning (ReFT)[44] enhance reasoning throughfine-tuning with automatically sampledreasoning paths under online reinforcement learning rewards. DeepSeek-R1[89]advances this approachthrough rule-based rewards and Group Relative Policy Optimization (GRPO) [112],while Kimik1.5[13]combinescontextualreinforcementlearning withoptimizedchain-of-thoughttechniques to improve both planning processes and inference eficiency. In specificenvironments, modifying models to enhance agentsunderstanding of actions and externalenvironments has proven effective,as demonstrated by DigiRL[103], which implements a two-stage reinforcement learning approach enabling agents to perform diverse commands on real-world Android device simulators.",
                "Recent works have attemptedto integrate agent action spaces directlyintomodeltraining[45,55],enabling learning of appropriate actions for diferent states through RL or SFT methods.This integration fundamentally affcts the agent's memory,reward understanding,and world modelcomprehension, pointing toward a promising direction forthe emergence of agentic models.",
                "Partial Mental State Learning While full mental state learning through model modifications provides comprehensive capability updates, learning focused on particular components of an agent's mental state $M$ represents another essential and oftenmoreeffcient approach.Such partial mental state learning can be achieved eitherthrough targeted model updates or through in-context adaptation without parameter changes.",
                "In-Context Learning (ICL)ilustrates how agents can effectively modify specific mental state components without modifying theentire model.This mechanism alows agents toadapt tonew tasks byleveraging examples orinstructions within their context window, paraleling human working memory's role in rapid task adaptation. Chain-of-Thought (CoT)[46] demonstrates the efectivenessof this approach, showing how agents can enhance specific cognitive capabilities while maintaining their base model parameters unchanged.",
                "The feasibilityofpartial mentalstate learning is evidenced throughvarious approaches targeting diffrent components such as memory $(M^{m e m})$ , reward $(M^{r e w})$ , and world model $(M^{w m})$ . Through normal communication and social interaction, Generative Agents [50] demonstrate how agents can accumulate andreplay memories,extracting highlevel insights to guide dynamic behavior planning. In environmentalinteraction scenarios, Voyager [47]showcases how agents cancontinuously update their skill library through direct engagement with the Minecraft environment, accumulating procedural knowledge without modelretraining.Learn-by-Interact[102]further extends this approach by synthesizing experientialdatathrough direct environmentalinteraction,eliminating the need for manual annotation or reinforcementlearning frameworks.Aditionally,agents can learn from their mistakes and improve throughreflection, as demonstratedbyReflexion[48],whichguides agents'futurethinkingandactions byobtainingtextualfeedback from repeated trial and error experiences.",
                "Modifications to reward and world models provide another example of partial mental state learning.ARMAP[106] refinesenvironmentalreward models bydistilling themfrom agent action trajectories,providing afoundation for further learning.AutoMC[114]constructs dense reward models through environmentalexploration tosupport agent behavior. Meanwhile,[107] explicitly leverages LLMs as world models to predict the impact of future actions,efectively modifying the agent's world understanding $(M^{w m})$ . ActRe[49] builds upon the language model's inherent world understanding to construct tasks from trajectories,enhancing the agent'scapabilities as botha world model and reasoning engine through iterative training."
              ],
              "raw_title": "Learning Space",
              "type": null,
              "children": []
            },
            {
              "title": "2.1.2 Learning Objective",
              "number": "2.1.2",
              "level": 3,
              "content": [
                "The learning processof intelligent agents manifests acrossallaspects of their interaction withthe environment.At the input level, agents lean tobeterperceive and parseenvironmentalinformation; attheprocessing level,agents lean how to conduct effctive reasoning based on existing knowledge orreasoningcapabilities; atthe comprehensionlevel, agents form and optimize their understanding of the world through continuous interaction.This multi-level learning objective framework enables agents toevolvecontinuouslyacross different dimensions,allowingthem tobetter handle complex and dynamic task environments.",
                "Learning for Better Perception The abilityto effectively perceive and processinformation fromthe environment is fundamentalto agent intelligence.Toenhance perceptualcapabilities,agents employtwo primary learning approaches: expanding multimodal perception and leveraging retrieval mechanisms.",
                "Multimodal perception learning enables agents to process and integrate diverse sensory inputs,similar to human multi-sensory integration butunconstrained bybiologicallimitations.Thiscapabilityhas evolved significantlythrough advances like CLIP[51],which pioneered the alignmentof visualandlinguistic representations in sharedembedding spaces.Building on this foundation, models like LLaVA [52] enhanced visual perception by training specialized projectors on image-text pairs, while CogVLM[53] advanced visual reasoning through unified representational architectures.",
                "The expansion of perceptual modalities continues across multiple sensory domains. In audio processing, QwenAudio[54] demonstrates theunified encoding of diverse acoustic information,from speech to environmental sounds. Recent work by[l15] has even ventured into tactile perception, developing datasets that align touch, vision, and languagerepresentations.These advancesenable agents toengage more comprehensively with both physicaland digital environments.",
                "Agents alsolearn to enhance their observationalcapabilities through retrieval mechanisms.Unlike human perception, which is constrained by immediate sensory input, agents can learn to accessand integrate information from vast external knowledge repositories.Retrieval-augmented approaches like RAG[116] enhance perceptual understanding by connecting immediate observations with relevant stored knowledge.",
                "Recent work on retrieval-based agents demonstrates the potential for enhancing active information acquisition capabilities.Search-ol[117] guides reasoning models to learn active retrievalthrough prompting,thereby expanding their knowledge boundaries.Taking this further,R1-Searcher[45]and Search-R1[55]directly incorporate retrieval capabilities into the model,enabling autonomous informationretrieval during the reasoning process.These advances suggest a promising direction for improving agent perception:enhancing model-level active perception capabilities to enrich the foundation for decision-making. This approach may represent a significant avenue for future agent development.",
                "Learning for Better Reasoning Reasoning serves as acritical bridge between an agent's mental state andits actions, making the abilityto reason effectively andthe development of reasoningcapabilitiesessentialfor intellgent agents. The foundation of reasoning in modern agents stems from two key elements: the rich world knowledge embedded in their underlying models, andtherobust logical frameworks supportedeither internallyorthroughcontext structuring. This makes learning for better reasoning a vital objective in agent development.",
                "The development ofreasoningcapabilities isdemonstratedthrough severalkeyphenomena.First,high-quality reasoning data directly enhances modelreasoning ability; second,suchhigh-qualitydata often requires verification orreward models foreffectivecuration; andthird,directreinforcementlearningonfoundation modelscanspontaneouslymanifest reasoning capabilities.",
                "The importance of reasoning in agent developmenthas been re-emphasized following therelease of theol series.A common approach involves collecting and distilling data from open/closed-source reasoning models. For instance, SKY-32B [56] distilled data from QWQ-32B [118] to train a 32B reasoning model at a cost of $\\$450$ . Similarly, Open Thoughts[57]trainedBespoke-Stratos-32Batalowcost bydistiling and synthesizing datasets from R1.These studies demonstrate that even without complex algorithmic design,using reasoning data to perform Supervised Fine-Tuning (SFT) on base models can effectively activate reasoning capabilities.",
                "Another crucialinsight regarding data quality isthathighly structuredreasoningdata more effectively enables agents and language models tolearn reasoning proceses. Notably,LIMO[58]demonstrated that powerfulreasoning models could be built with extremely few data samples byconstructing long and efective reasoning chains for complex reasoning tasks.This insight stems fromtheirobservationthatlanguage models inherently possessufficient knowledge for reasoning but require high-quality reasoning paths to activate these capabilities.Supportingthis view, Liet al.",
                "[9]revealed thatboth Long CoTand Short CoTfundamentaly teach models to learn reasoning structures rather than specific content,suggesting that automated selection of high-qualityreasoning data may become an important future direction.",
                "One viable exploration approach involves firstconducting extensive searches,and then using verifiable environments or trainablereward models to provide feedback on reasoning trajectories,thereby filtering out high-qualityreasoning data.This approachhas ledtoseveralfamiliesof techniques thatleverage different feedbackmechanisms toimprove reasoning capabilities.",
                "The firstcategoryfollows thebootstrap paradigmexemplifiedbySTaR[59]anditsariantswhichimplement techniques where models generate step-by-step rationales and iteratively improve through fine-tuning on successful reasoning paths.This familyincludesQuiet-TaR[91],-STaR[120],andStar-Math[11],withthelaterspecificallenancing mathematicalreasoning through reinforcement learning principles.Byiteratively selecting correct reasoning paths for training, these methods achieve self-improvement through successive refinement cycles.",
                "The second category extends this paradigm by more explicitly incorporating reinforcement learning principles. The ReST family,beginning with the original ReST[60] introducing reinforced self-training,performs multiple attempts (typically 10) per sample and creates new training datasets from successful reasoning instances. ReST-EM[22] enhances the approach with expectation maximization, while ReST-MCTS[122]further integrates Monte Carlo Tree Search to enable improved reasoning capabilities through more sophisticated exploration strategies.",
                "Several approaches have introduced Policy Reward Models (PRMs)to provide quality feedback on reasoning paths. Methods like OpenR [61] and LLaMA-Berry [62] model reasoning tasks as Markov Decision Processes (MDPs) and leverage tree search to explore diverse reasoning paths while using PRMs for quality assessment.In domain-specific applications, methods like rStar-Math [121] and DeepSeekMath [112]have demonstrated success in mathematical problem-solvingthroughmulti-round self-iterationandbalancedexploration-exploitationstrategies.Forcode generation, ol-Coder[13]leveragesMCTStogeneratecode withreasoning proceses,whileMarco-ol[123]extends this approach to open-ended tasks.These implementations highlight how the synergy between MCTS and PRM achieves effective reasoning path exploration while maintaining solution quality through fine-grained supervision.",
                "Beyond data-driven approaches, reinforcement learning (RL) has demonstrated remarkable success in enhancing language modelsreasoning capabilities,as evidenced byrecent breakthroughs like DeepSeek R1[89]and Kimi-K-1.5 [113].The foundation of RL for LLMs can be traced to several pioneering frameworks: ReFT [44] introduced a combination of supervised fine-tuning and online reinforcement learning, while VeRL[124]established an opensourceframework supporting various RL algorithms for large-scale models up to 70B parameters.RFT[125] further demonstrated the effectiveness of reward-guided optimization in specific reasoning tasks.",
                "Building upon these foundations,subsequent works have explored diverse applications and improvements.OpenR1[64] and RAGEN[63]extended RL techniques to enhance generalreasoning capabilities,while specializedimplementations like SWE-Gym[126]demonstrated success in software engineering tasks.Notably,DigiRL[103] introduced novel approaches for digital-world agent enhancement.",
                "Recent advances have further integrated RL with tool usage and reasoning. Qwen-QwQ-32B[118] employs reinforcement learning and a general reward mechanism to incorporate toolcalling intothe reasoning processenabling the seamlessuse ofarbitrary tools during reasoning and achieving agent-like capabilities directly within the model. Similarly,RAGEN[63]focuses on multi-step agentic scenarios,establishing a framework for agent reinforcement learning in complex environments.These developments suggest an increasing convergence between model training and agent development, potentiallyleading to more integrated andcapable intelligent systems.These implementations highlight how RL can effectively improve model performance while reducing dependence on large-scale annotated datasets, particularly in complex reasoning scenarios.",
                "Learning for World Understanding Acriticalaspectof agent intellgence is the ability tounderstand how the world operates through direct interaction and experience accumulation.This understanding encompasss how the environment responds to different actions and theconsequences these actions bring.Through continuous interaction with their environment,agentscan build andrefine their memory,reward understanding,and world model,learning from both successes and failures to develop a more comprehensive grasp of their operational domain.",
                "Recent research has revealeddiverse approaches to experientiallearning for world understanding.At the foundational level, Inner Monologue[65] demonstrates how agents can accumulate basic environmental knowledge through continuous interaction.Similarly,Learn-by-Interact [102]shows that meaningful understanding can emerge from direct environmental engagement without explicit reward mechanisms.More sophisticated approaches are exemplified by DESP[66] and Voyager[47] intheMinecraftenvironment, where agentsnot only gather experiences butalso actively process them: DESP through outcome analysis and Voyager through dynamic skill library expansion.",
                "The processing and utilization ofaccumulated experiences have beenfurther systematized through advancedframeworks. Generative Agents[50] introduces sophisticated memory replay mechanisms,enabling agents to extract high-level insights from past interactions.This systematic approach is enhanced by Self-refine[67] and Critic[68],which implement structured cycles of experience evaluation and refinement.",
                "The optimization of reward understanding through environmentalinteraction has emerged as another crucial aspect of world understanding.Text2Reward [105] demonstrates how agents can continuously refine reward functions through human feedback, bettr aligning them with task objectives and environmental characteristics.Similarly, AutoManual[108]builds behavioral guidelines through sustained interaction,developing reward-verified protocols that provide a foundation for understanding environmentalrewards and decision-making.These interaction-based optimization mechanisms enable agents to bettrcomprehend environmental dynamics and generate more precise reward signals,ultimately enhancing their adaptability and decision-making capabilities in complex,dynamic environments.",
                "Building on these foundations, RAP[74]represents asignificant advancement byconceptualizingreasoning as planning with a world model.By repurposing LLMs as both reasoning agents and world models,RAP enables agents tosimulate theoutcomes of potentialactions before committingtothem,facilitating moreeffctive planning through Monte Carlo Tree Search.This approach allows agents to strategically explorethereasoning space witha proper balance between exploration and exploitation.",
                "Further innovations in leveraging world models for agent learning include ActRe[127],which reverses the typical reasoning-action sequence by first performing actions and then generating post-hoc explanations.This capability to rationalize actions demonstrates LLMs'inherent understanding of world dynamics,enabling autonomous trajectory annotation and facilitating contrastive self-training.",
                "The importanceofcognitive maps inworld understandingis highlighted by[128],whoshow that structured mentalrepresentations inspired by human cognition significantly enhance LLMs'extrapolation capabilities in novelenvironments. These cognitive maps not only improve planning but also exhibit human-like characteristicssuchas structured mental simulation and rapid adaptation.",
                "In web-based environments,recent work by[107]and[129]demonstrates thatLLMscan function asefective world models for anticipating the outcomes of web interactions. By simulating potential state changes before executing actions,these approaches enablesafer and more eficient decision-making,particularlyin environments where actions may be irreversible.",
                "Through systems like Reflexion [48]and ExpeL[69],agents have advanced experientiallearning by autonomously managing thefullcycleofexperiencecoection, analysis,andapplication,enablingthem tolearneffectivelyfromboth successes and failures.",
                "These developments collectively illustrate how world models are becoming increasingly central to agent learning systems, providing a foundation for understanding environmental dynamics and enabling more efective planning, reasoning, and decision-making in complex, interactive environments."
              ],
              "raw_title": "Learning Objective",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "2.2 Reasoning",
          "number": "2.2",
          "level": 2,
          "content": [
            "Reasoningrepresents thekey tointellgentbehavior,ransformingraw information intoactionable knowledgethat drives problem-solving and decision-making.For both humans and artificial agents,it enables logicalinference,hypothesis generation,and purposeful interaction with the world. In human cognition, reasoning emerges through multiple strategies:deductive reasoning applies generalrules to specificcases, inductive reasoning builds generalizations from particula instances,and abductive reasoningconstructs plausible explanations from incompletedata[130,131].These processes are augmented byheuristics—mental shortcuts that streamline decision-making under uncertainty-and are continuouslyrefinedthrough environmental feedback, ensuring that reasoning remains grounded inreality and adaptive to change.",
            "For LLM-based agents,reasoning serves a parallelrole,elevating them beyond reactive systems to proactive entities capable of sophisticated cognition. Through reasoning, these agents process multimodal inputs,integrate diverse knowledge sources,and formulatecoherentstrategies to achieve objectives.The environment playsa dual function: supplying informationthatfuelsreasoning andserving astheproving groundwherereasonedactions aretested,creating a feedback loop that enables agents to validate inferences and learn from errors.",
            "In LLM-based agents,reasoning can be formally defined as the processof action selection based on mental states, representing acrucialbridgebetween perceptionandaction.More precisely,givenamentalstate Mt at timet,easoning can be formalized as a function $\\mathrm{R}(\\mathrm{Mt})\\rightarrow\\mathrm{at}$ ， where at represents the selected action. This process operates across various environments—extual,digital,andphysicalworlds—wherecompletingatask typicallyrequires either a single reasoning step or a composition of multiple reasoning actions.",
            "![](images/f0c6ddab0ba65d0f4b90000c85c15384b215f712c1c72846af09d27c433031df.jpg)",
            "Figure 2.2: Comparison of reasoning paradigms in LLM-based agents.",
            "The composition of reasoning actions naturallleads totwodistinct approaches:structured and unstructured reasoning Structured reasoning $(R_{s})$ can be formalized as an explicit composition $R_{s}=R_{1}\\circ R_{2}\\circ...\\circ R_{n}$ ，where each $R_{i}$ represents a discrete reasoning step with clear logical dependencies. In contrast, unstructured reasoning $(R_{u})$ takes a more holistic form $R_{u}=f(M_{t})$ , where the composition remains implicit and flexible, allowing for dynamic adaptation to context.This dual framework mirrors human cognition,where structured reasoning parallels our explicit logical deduction processes,while unstructured reasoning reflects our capacity for intuitive problem-solving and pattern recognition.",
            "The environment plays a crucial role in this formalization, serving both as a source of observations $o_{t}$ that influence mental state updates $(M_{t}=L(M_{t-1},a_{t-1},o_{t}))$ and as a testing ground for reasoning outcomes. This creates a continuous feedbackloopwhere reasoning not only drives action selection but also inffuences how the agent's mental state evolves, enabling iterative refinement of reasoning strategies through experience.",
            "In this section, we willexamine how these reasoning approaches manifest in practice.We begin with structured reasoning,which emphasizes systematic problem decomposition and multi-step logical chains.We then explore unstructured reasoning, which allows for flexible response patterns and parall solution exploration. Finally, we investigate planning asa specializedform ofreasoning thatcombines both structuredand unstructured approaches for tackling complex, long-horizon tasks."
          ],
          "raw_title": "Reasoning",
          "type": null,
          "children": [
            {
              "title": "2.2.1 Structured Reasoning",
              "number": "2.2.1",
              "level": 3,
              "content": [
                "Structured reasoning represents a methodical approach to problem-solving that employs explicit organizational frameworks to guide thereasoning process.Unlike unstructured approaches,structured reasoning makes the compositionof reasoning steps explicit, which can be formalized as $R_{s}=R_{1}\\circ R_{2}\\circ...\\circ R_{n}$ ，where each $R_{i}$ represents a discrete reasoning step with clear logical dependencies.In this formulation,each reasoning node is an explicitly executed computational unit, and the connections between nodes represent definite information flow paths. This approach enables more systematic explorationof solution spaces and facilitates more robust decision-making through deliberate step-by-step analysis, providing high interpretability and traceability throughout the reasoning process."
              ],
              "raw_title": "Structured Reasoning",
              "type": null,
              "children": [
                {
                  "title": "2.2.1.1 Dynamic Reasoning Structures",
                  "number": "2.2.1.1",
                  "level": 4,
                  "content": [
                    "Dynamic reasoning structures allow for the adaptive construction ofreasoning pathsduring problem-solving,creating versatile frameworks that can adjust based on intermediate results and insights.",
                    "Linear SequentialReasoning Linear structures frame reasoning asa series of sequential steps,where eachstep builds on the one before.ReAct[70]illustrates thisbycombining reasoningtraceswithtask-specific actions inanalternating fashion.This combination allows for reasoning traces to guide and modify action plans while actions can access external sources for further information.This mutualinteraction improves both reasoning integrity and environmental adaptation.",
                    "Reasoning via Planning (RAP)[74]extends the linear reasoning paradigm by formulating LLM reasoning as a Markov decision proces,though it was limited by states specifically designed for particular problems.TheMarkov Chain of Thought (MCoT)[71]extended this paradigm byconceptualizing each reasoningstepas aMarkovianstate accompanied by executable code.This approach enables effcient next-step inference withoutrequiring alengthy contextwindow by compressing previous reasoning into a simplified mathquestion.Atom of Thoughts[132]explicitly defined problems as state representations and designed a general decomposition-contraction two-phase state transition mechanism to construct Markovian reasoning processes, transforming complex problems into a series of atomic questions.",
                    "Tree-Based Exploration Tree-based approaches expand beyond linear structures by organizing reasoning into hierarchicalframeworks that support branching exploration.Tree of Thoughts(ToT)[72]introduces a structured approach where complexproblems are decomposed into intermediate steps,enabling breadth-first or depth-first search through the solution space.This alows the model to consider multiple reasoning paths simultaneously and systematically explore alternatives.",
                    "Language Agent Tree Search (LATS)[73] advances this paradigm by integrating Monte Carlo TreeSearch (MCTS) with LLMs,using the environment as an external feedback mechanism.This approach enables more deliberate and adaptive problem-solving bybalancing exploration and exploitation through asophisticated search process guided by LLM-powered value functions and self-reflection.",
                    "Reasoning via Planning (RAP)[74] further enhances tree-based reasoning by repurposing LLMs as both reasoning agents and world models.Throughthisdualrole,RAP enables agents to simulate theoutcomes of potentialreasoning path before committing tothem,creating a principled planning framework thatbalances exploration with exploitation in the reasoning space.",
                    "Graph-Based Reasoning Graph structures offer even greater flexibility by allowing non-hierarchical relationships between reasoning steps.Graph of Thoughts (GoT)[75] extends tree-based approaches to arbitrary graph structures, enabling more complex reasoning paterns thatcan capture interdependencies between diffrent steps.This approach allows forconnections between seemingly disparate reasoning branches,facilitating more nuanced exploration of the solution space.",
                    "Pathof Thoughts (PoT)[76]addresses relation reasoning challenges by decomposing problems into three key stages: graph extraction, path identification, and reasoning.Byexplicitlyextracting atask-agnostic graphthat identifies entities,relationsand attributes withinthe problemcontext,PoTcreatesastructured representation thatfacilitates the identification ofrelevant reasoning chains,significantly improving performance on tasksrequiring longreasoning chains.",
                    "Diagram of Thought (DoT)[77] models iterative reasoning as the construction of a directed acyclic graph (DAG), organizing propositions,critiques,refinements, and verifications intoaunified structure.This approach preserves logical consistency while enabling theexploration of complexreasoning pathways, providing atheoretically sound framework grounded in Topos Theory."
                  ],
                  "raw_title": "Dynamic Reasoning Structures",
                  "type": null,
                  "children": []
                },
                {
                  "title": "2.2.1.2 Static Reasoning Structures",
                  "number": "2.2.1.2",
                  "level": 4,
                  "content": [
                    "Static reasoning structuresemploy fixedframeworks that guide thereasoning process without dynamicall adjusting the structure itself, focusing instead on improving the content within the established framework.",
                    "Ensemble Methods. Ensemble approaches leverage multiple independent reasoning atempts to improve overall performance through aggregation.Self-Consistency[78] pioneered this approach by sampling multiple reasoning paths rather than relying on single greedy decoding,significantly improving performance through majority votingamong the generated solutions.",
                    "MedPrompt[133] demonstrates how domain-specific ensemble techniques can enhance performance by carefully crafting prompts that elicit diverse reasoning approaches,achieving state-of-the-art results on medical benchmarks through systematic composition of prompting strategies.",
                    "LLM-Blender[134] introduces a sophisticated ensembling framework that leverages thediverse strengths of multiple LLMs through pairwise comparison (PairRanker)andfusion (GenFuser)of candidate outputs.This approach enables the system to select theoptimal modeloutputforeach specific example,creating responsesthatexceed thecapabilities of any individual model.",
                    "Progressive Improvement. Progressive improvement frameworks focus on iteratively refining reasoning through structured feedback loops.Self-Refine[67]implements an iterative approach where the model generates initial output, provides self-feedback,anduses thatfeedback torefineitself.This mimics humanrevision processes without requiring additional training or reinforcement learning, resulting in significant improvements across diverse tasks.",
                    "Reflexion[48]extends this concept by integrating environmental feedback,enabling agents toverballyreflect on task feedback signals and maintainreflective textinan episodic memory buffr.This approach guidesfuture decision-making by incorporating insights from previous attempts,significantly enhancing performance in sequential decision-making, coding, and reasoning tasks.",
                    "Progressive-Hint Prompting (PHP)[79]further develops this paradigm by using previously generated answers as hints to progressvely guide the model toward correct solutions. This approach enables automatic multiple interactions between users and LLMs,resulting in significant accuracy improvements while maintaining high efficiency.",
                    "Error Correction. Error correction frameworks focus specifically on identifying and addressing mistakes in the reasoning process.Self-Verification [80] introduces a self-critique system that enables models to backward-verify their conclusions bytaking thederivedanswer asaconditionforsolving theoriginalproblem,producing interpretable validation scores that guide answer selection.",
                    "Refiner[135]addresses the challenge of scatteredkey information by adaptivelyextracting query-relevantcontent and restructuring it based on interconnectedness,highlighting information distinction andeffctivelyaligning downstream LLMs with the original context.",
                    "Chain-of-Verification(CoVe)[81]tacklesfactual hallucinations througha structured process where the model drafts an initialresponse,plansverifcationquestions,independentlyanswers those questions,andgeneratesafinal verified response. This deliberate verification process significantly reduces hallucinations across a variety of tasks.",
                    "Recursive Criticism and Improvement (RCI)[128]enables LLMs to execute computer tasks byrecursivelycriticizing and improving their outputs, outperforming existing methods on the MiniWoB $^{++}$ benchmark with only a handful of demonstrations per task and without task-specific reward functions.",
                    "Critic [68]extends thisapproachbyintegratingexternal tools forvalidation,enablingLLMs toevaluate and progressively amend their outputs likehuman interactionwith tools.This fameworkalows initiall“black box\"models toengage in a continuous cycle of evaluation and refinement, consistently enhancing performance across diverse tasks."
                  ],
                  "raw_title": "Static Reasoning Structures",
                  "type": null,
                  "children": []
                },
                {
                  "title": "2.2.1.3 Domain-Specific Reasoning Frameworks",
                  "number": "2.2.1.3",
                  "level": 4,
                  "content": [
                    "Domain-specificreasoning frameworks adapt structured reasoning approaches to the unique requirements of particular domains, leveraging specialized knowledge and techniques to enhance performance in specific contexts.",
                    "MathPrompter[82]addresses arithmetic reasoning challenges by generating multiple algebraic expressions or Python functions tosolvethe same math problem indiffrent ways.Thisapproach improvesconfidence inthe outputresults by providing multiple verification paths,significantly outperforming state-of-the-art methods on arithmetic benchmarks.",
                    "PhysicsReasoner[84]addresses the unique challenges ofphysics problems through aknowledge-augmentedframework that constructs acomprehensive formula set and employs detailedchecklists to guide efective knowledge application. This three-stage approach—problem analysis,formularetrieval, and guided reasoning—significantly improves performance on physics benchmarks by mitigating issues of insuffcient knowledge and incorrect application.",
                    "Pedagogical Chain-of-Thought (PedCoT)[83]leverages educationaltheory,particularlytheBloomCognitive Model, to guide the identification of reasoning mistakes in mathematical contexts.Thisapproachcombines pedagogical principles for prompt design with atwo-stage interaction process, providing a foundation for reliable mathematical mistake identification and automatic answer grading.",
                    "The evolution of structured reasoning in LLM agents reflects a growing understanding ofhow to enhance reasoning capabilities through explicit organizational frameworks.From linear sequences to complex graphs,and ensemble methods to specialized domain frameworks,these approaches demonstrate the powerof structuralguidance inimproving reasoning performance across diverse tasks and domains."
                  ],
                  "raw_title": "Domain-Specific Reasoning Frameworks",
                  "type": null,
                  "children": []
                }
              ]
            },
            {
              "title": "2.2.2 Unstructured Reasoning",
              "number": "2.2.2",
              "level": 3,
              "content": [
                "In contrast to structured reasoning approaches that explicitly organize reasoning steps, unstructured reasoning $(R_{u})$ takes a holistic form ${\\cal R}_{u}=f(M_{t})$ , where the composition remains implicit and flexible. In this mode, the reasoning process is encapsulated withinasinglefunction mapping,without explicitlydefining intermediate stepsorstatetransitions.This approach leverages the inherent capabilties of language models to generatecoherent reasoning without enforcing rigid structural constraints, with intermediate reasoning processes occurring explicitlyin the language space or implicitly in the latent space.Unstructured reasoning methods havedemonstratedremarkable effectivenessacross diverse tasks while maintaining simplicity and effciency in implementation."
              ],
              "raw_title": "Unstructured Reasoning",
              "type": null,
              "children": [
                {
                  "title": "2.2.2.1 Prompting-Based Reasoning",
                  "number": "2.2.2.1",
                  "level": 4,
                  "content": [
                    "The most accessible way toelicit reasoning in LMagents lies incarefullycrafted prompts.By providing appropriate reasoning demonstrations or instructing LLMs to perform inferential steps,agents canleverage theirlogical deduction capabilities to solve problems through flexible reasoning processes.",
                    "Chain-of-Thought Variants.The cornerstone of prompting-based reasoning is Chain-of-Thought (CoT)prompting[46],whichoperationalizesreasoning throughfew-shotexamples with explicit generationof intermediaterationalization steps.Thisfoundational technique has inspired severalevolutionary variants that enhanceits basic approach. Zero-shot CoT[136] eliminates the need for demonstration examples through strategic prompting(e.g.,“Let's think step by step\"),making the approach more accessble while maintaining effectivenessAuto-CoT[137]automates the creation of effectivedemonstrations byclustering diverse questions and generating reasoning chains forrepresentative examples from each cluster. Least-to-Most Prompting [138]addresses complex reasoning by decomposing problems into sequential sub-problems,enabling a progressive planning processthat facilitates easy-to-hard generalization. Complex CoT[139]further enhances reasoning depth by specifically selecting high-complexityexemplars as prompting templates, better-equipping models to tackle intricate problems.",
                    "Problem Reformulation Strategies.Advanced prompting strategies demonstrate architectural innovations in reasoning guidance by reformulating the original problem. Step-Back Prompting [85] implements abstraction-first reasoning throughconceptual elevation,enabling models to derive high-level concepts and first principles before addressing specific details.Experimental results demonstrate substantial performance gains on various reasoning-intensive tasks, with improvements of $7.27\\%$ across physics, chemistry, and multi-hop reasoning benchmarks. Rephrase and Respond[140]employ semantic expansion to transform original questions into more tractable forms, allowing models to approach problems from multiple linguistic angles and identify the most efective problem formulation.",
                    "Abstraction-of-Thought[141] introduces a novel structuredreasoning format that explicitlyrequires varying levels of abstraction within thereasoning process.This approachelicitslanguage models tofirstcontemplate attheabstractlevel before incorporating concrete details,aconsideration overlooked by step-by-step CoT methods.By aligning models with the AoTformat through finetuning on high-quality samples,the approach demonstrates substantial performance improvements across a wide range of reasoning tasks compared to CoT-aligned models.",
                    "Enhanced Prompting Frameworks. Several frameworks extend the basic prompting paradigm to create more sophisticated reasoning environments. Ask Me Anything[86]constrains open-ended generation by reformulating tasks into structured question-answersequences,enforcing focusedreasoning trajectories.This approachrecursively usesthe LLM itself totransformtask inputs to the effectiveQAformat,enabling open-source GPT-J-6B to matchorexceedthe performance of few-shot GPT3-175B on 15 of 20 popular benchmarks.",
                    "Algorithm of Thoughts[142] proposes a novel strategy that propels LLMsthrough algorithmic reasoning pathways by employing algorithmic examples full in context.This approach exploits the innate recurrence dynamics of LLMs, expanding theiridea exploration with merely one or a few queries.The technique outperforms earlier single-query methods and even morerecentmulti-query strategies whileusing significantlyfewer tokens,suggesting that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself.",
                    "Chain-of-Knowledge (CoK)[87] augments LLMs by dynamically incorporating grounded information from heterogeneous sources,resulting in more factualrationales andreduced hallucination.CoKconsists of threestages:reasoning preparation, dynamic knowledge adapting,and answer consolidation, leveraging both unstructured and structured knowledge sources through an adaptive query generator.This approachcorrects rationales progressively using preceding corrected rationales, minimizing error propagation between reasoning steps.",
                    "Self-Explained Keywords (SEK)[88]addresses thechallengeof low-frequency terms incode generationby extracting and explaining key terms in problem descriptions with the LLM itself and ranking them based on frequency. This approach significantly improves code generation performance across multiple benchmarks,enabling models to shift attention from low-frequency keywords to their corresponding high-frequency counterparts."
                  ],
                  "raw_title": "Prompting-Based Reasoning",
                  "type": null,
                  "children": []
                },
                {
                  "title": "2.2.2.2 Reasoning Models",
                  "number": "2.2.2.2",
                  "level": 4,
                  "content": [
                    "Recent advances inlanguage models have ledto the developmentof specialized reasoning models designed explicitly for complex inferential tasks.These models are fine-tuned or specialy trained to optimize reasoning capabilities, incorporating architecturalandtraining innovations that enhancetheir performance ontasks requiring multi-steplogical inference.",
                    "Reasoning models like DeepSeek's R1 [89], Anthropic's Claude 3.7 Sonnet[9],and OpenAl's o series models [90] represent the frontier of reasoning capabilities,demonstratingremarkable proficiency across diverse reasoning benchmarks.These models aretrained with specializedmethodologies that emphasizereasoning patterns,often incorporating significant amounts of human feedback and reinforcement learning to enhance their inferential abilities.",
                    "The emergence of dedicated reasoning models reflects a growing understanding of the importance of reasoning capabilities in language models andthe potential benefits of specialized training forthese tasks. Byconcentrating on reasoning-focused training data and objectives,these models achieve performance levels that significantly exceed those of general-purpose language models,particularlyontasks thatrequirecomplex logicalinference, mathematical reasoning, and multi-step problem-solving."
                  ],
                  "raw_title": "Reasoning Models",
                  "type": null,
                  "children": []
                },
                {
                  "title": "2.2.2.3 Implicit Reasoning",
                  "number": "2.2.2.3",
                  "level": 4,
                  "content": [
                    "Beyond explicit reasoning approaches,recent research has exploredthe potential of implicit reasoning method that operate without overtly exposing thereasoning process.These approaches aim to improve effciency by reducing the number of tokens generated while maintaining or enhancing reasoning performance.",
                    "Quiet-STaR [91] generalizes the Self-Taught Reasoner approach by teaching LMs to generate rationales at each token to explain thefuture text improving their predictions.This approach addresses key chalenges including computationalcost,theinitialunfamiliaritywith generating internalthoughtsandthe need to predict beyond individual tokens. Experimental results demonstrate zero-shot improvements in mathematical reasoning $(5.9\\%\\rightarrow10.9\\%)$ and commonsense reasoning $(36.3\\%\\rightarrow47.2\\%)$ ） after continued pretraining, marking a step toward LMs that learn to reason in a more general and scalable way.",
                    "Chain of Continuous Thought (Coconut)[92] introduces a paradigm that enables LLM reasoning in an unrestricted latent space instead of using naturallanguage.Byutilizing the last hidden state of the LLM as arepresentation of the reasoning state and feeding it back as the subsequent input embedding directly inthe continuous space,Coconut demonstrates improved performance on reasoning tasks with fewer thinking tokens during inference.This approach leads to emergent advancedreasoning pattens,icludingtheabilitytoencodemultiplealternative nextreasoning steps, allowing the model to perform a breadth-first search rather than commiting to a single deterministic path.",
                    "Recent analysis[143] of implicit reasoning in transformers reveals important insights into its limitations.While language modelscan perform step-by-stepreasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning when trained onfixed-pattern data, implicit reasoning abilities emerging from training on unfixed-patterndatatendtooverfit specificpatterns andfailto generalizefurther.These findings suggest thatlanguage models acquire implicitreasoning through shortcutlearning,enabling strong performance ontaskswith similar patterns while lacking broader generalization capabilities.",
                    "The evolution of unstructured reasoning approaches demonstrates the remarkable adaptability of language models to different reasoning paradigms.From simple prompting techniques to sophisticated implicit reasoning methods, these approaches leverage the inherent capabilities of LLMs to perform complex logicalinferences without requiring explicit structuralconstraints.Thisfexibilityenables more intuitive problem-solving while maintaining efficiency and effectiveness across diverse reasoning tasks."
                  ],
                  "raw_title": "Implicit Reasoning",
                  "type": null,
                  "children": []
                }
              ]
            },
            {
              "title": "2.2.3 Planning",
              "number": "2.2.3",
              "level": 3,
              "content": [
                "Planning is afundamentalaspect of human cognition,enabling individuals toorganize actions,anticipate outcomes, and achieve goals in complex, dynamic environments[144].Formally, planningcan bedescribed as the process of constructing potential pathways from an initial state to a desired goal state, represented as $P:S_{0}\\rightarrow\\{a_{1},a_{2},...,a_{n}\\}\\rightarrow$ $S_{g}$ , where $S_{0}$ is the starting state, $\\{a_{1},a_{2},\\ldots,a_{n}\\}$ denotes a sequence of possible actions, and $S_{g}$ is the goal state. Unlike direct reasoning,planning involves generating hypotheticalaction sequences before execution,functioning as computational nodes that remain inactive until deployed.Thiscognitive abilityemerges fromthe interplayof specialized neuralcircuits,including the prefrontalcortex,whichgovernsexecutivecontrol,andthehippocampus,whichsuppots episodic foresight and spatial mapping.Insights from decisiontheory,psychology,and cybernetics—such as rational frameworks,prospect theory,and feedback loops-demonstrate how planning allows humans totranscend reactive behavior, actively shaping their futures through deliberate intent and adaptive strategies.Thiscapacity not only underpins intellgent behavior but also serves as a modelfor developing LLM-basedagents that seek to replicate and enhance these abilities computationally [145, 146]",
                "In human cognition, planning operates as a hierarchical process,integrating immediate decisions with long-term objectives.This reflects the brain's modular architecture,where neural systemscolaborate to balance short-term demands withfuture possibilities-a dynamic informed by control theory's principles of stability and optimization. Similarly, LLM-based agents employ planning by leveraging their extensive linguistic knowledge and contextual reasoning totransform inputs into actionable steps. Whether addressing structured tasksor unpredictablechallenges, these agents emulate human planning bydecomposing objectives,evaluating potentialoutcomes,andrefining their strategiesblending biologicalinspiration with artificialintelligence.This section examines thetheoreticalfoundations and practical techniques of planning,fromsequentialapproaches toparallelexploration,highlighting itscriticalrole in intelligent systems.",
                "Despite the potential of LLMs in automated planning, their performance faces limitations due to gaps in world knowledge[147]. LLMs often lack deepcomprehension of world dynamics,relying on pattern recognition rather than genuine causalreasoning,which hinders theirabilityto manage sub-goalinteractions and environmentalchanges[148]. Additionally,their reliance onstatic pre-training data restricts adaptability in real-time scenarios, limiting their generalization in dynamic planning tasks [149].The absence of an intrinsic System 2 reasoning mechanism further complicatestheirabilityto independently generate structured,optimal plans[10].However,esearchers have proposed strategies such as task decomposition, search optimization,and external knowledge integration to mitigate these challenges.",
                "Task Decomposition Task decomposition enhances LLM planning by breaking complex goals into smaller, manageable subtasks,reducing problem complexity and improving systematic reasoning.The Least-to-Most Prompting method [138] exemplifies this approach, guiding LLMs to solve subproblems incrementally.ADaPT[151] further refines this strategy bydynamically adjustingtask decomposition based on complexity and modelcapability,particularly in interactivedecision-making scenarios.These methods alsofacilitate parallelsubtask processingbackward error tracing, and independence determination [132], providing a structured framework for reasoning.",
                "In LLM planning, tasks function as executable units-distinct from static state descriptions in formal models—emphasizing structured sequences that achieve intended outcomes [66].These tasks vary in nature: some are subproblems requiring specificsolutions (e.g.solving equations within broaderchallenges),while others involve tool invocation (e.g.,querying APIs for weather data in travel planning)[152,153].Alternatively,tasks may be represented as graph nodes mapping dependencies,such as prioritizing objectives in project management [154].By defining clear, modular goals,these formulations enhance reasoning and action eficiency, guiding agents through complex problem spaces with greater precision [93].",
                "Searching Given the stochastic nature of LLMs[155],paralelsampling combined with aggregated reasoning can improve inference performance.Task decompositionstructures individualsolution trajectories,enablingtheconstruction of a solutionspace that includes multiple pathways toa goal andtheir interelationships[72,156].This space allows sampling diverse potential solutions[157],facilitating exploration through techniques likereflection,review,and parallel sampling informed by existing knowledge [158].",
                "Computationalconstraints often preclude exhaustive evaluation,making eficient navigation of the solution space essential.Methods include tree searchalgorithms like LATS[159],heuristic approaches such as PlanCritic's genetic algorithms[160], and CoT-SC,which identifies recurring solutions via self-consistency checks[78].Reward-based models like ARMAP assessintermediate and final outcomes to optimize planning [106].This iterative exploration and refinement process enhances adaptability, ensuring robust strategies for complex problems.",
                "World Knowledge Efective planning requires agents to navigate dynamic environments, anticipate changes, and predict outcomes, underscoring the importance of world knowledge. RAP[74] examines the interplay between LLMs, agent systems, and world models,positioning LLMs as dual-purpose entities:as world models,they predict state changes following actions[107,161]; asagents,they select actions based on states and goals[70].This framework mirrors human cognition-simulating action consequences before selecting optimal paths—and unifies language models, agent models, and world models as pillars of machine reasoning [162].",
                "Agents augment LLM capabilities by integrating external knowledge,addressing gaps in world understanding. ReAct employs an action-observation loop to gather environmental feedback,combining real-time data with linguistic knowledge to improve decision-making in complex scenarios[7o]. This enables LLMs to iterativelyrefine their world models during action execution, supporting adaptive planning. Conversely, $\\mathsf{L L M+P}$ [163] integrates LLMs with the PDDL planning language,converting naturallanguage inputs intoformalizedrepresentations solved by classical planners [164,165].This hybrid approachcompensates for LLMs\"limitations in structured planning,merging their linguistic flexibility with the reliability of traditional systems.",
                "Further advancements enhance LLM planning through world knowledge integration. CodePlan[166] uses code-form plans—pseudocode outlining logical steps—to guide LLMs through complex tasks,achieving notable performance improvements across benchmarks[167]. The World Knowledge Model(WKM)equips LLMs with prior task knowledge and dynamic state awareness,reducing trial-and-error and hallucinations in simulated environments[168].A neurosymbolic approach combining Linear Temporal Logic with Natural Language (LTL-NL) integrates formal logic with",
                "LLMs,leveraging implicit world knowledge to ensure reliable,adaptive planning [169]. Together,these methods illustrate how structured frameworks and environmental understanding can transform LLMs into effective planners."
              ],
              "raw_title": "Planning",
              "type": null,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "title": "3.1  Overview of Human Memory. 39",
      "number": "3.1",
      "level": 2,
      "content": [
        "3.1.1  Types of Human Memory 39\n3.1.2 Models of Human Memory . 41\n3.2 From Human Memory to Agent Memory 42\n3.3 Representation of Agent Memory 44\n3.3.1 Sensory Memory 44\n3.3.2 Short-Term Memory 46\n3.3.3 Long-Term Memory 46",
        "3.4 The Memory Lifecycle 47"
      ],
      "raw_title": "Overview of Human Memory. 39",
      "type": null,
      "children": []
    },
    {
      "title": "3.4.1 Memory Acquisition 47",
      "number": "3.4.1",
      "level": 3,
      "content": [
        "3.4.2 Memory Encoding 48\n3.4.3 Memory Derivation 49\n3.4.4 Memory Retrieval and Matching 50\n3.4.5 Neural Memory Networks 51\n3.4.6 Memory Utilization . 52\n3.5 Summary and Discussion 53"
      ],
      "raw_title": "Memory Acquisition 47",
      "type": null,
      "children": []
    },
    {
      "title": "4 World Model 54",
      "number": "4",
      "level": 1,
      "content": [
        "4.1 The Human World Model 55\n4.2 Translating Human World Models to AI 55\n4.3 Paradigms of AI World Models 56\n4.3.1 Overview of World Model Paradigms 56\n4.3.2 Implicit Paradigm . 57\n4.3.3 Explicit Paradigm . 57\n4.3.4 Simulator-Based Paradigm 58\n4.3.5 Hybrid and Instruction-Driven Paradigms 58\n4.3.6 Comparative Summary of Paradigms 58\n4.4 Relationships to Other Modules 58\n4.4.1 Memory and the World Model 59\n4.4.2 Perception and the World Model 60\n4.4.3 Action and the World Model 60\n4.4.4 Cross-Module Integration 61\n4.5 Summary and Discussion 61"
      ],
      "raw_title": "World Model 54",
      "type": null,
      "children": []
    },
    {
      "title": "5 Reward 63",
      "number": "5",
      "level": 1,
      "content": [
        "5.1 The Human Reward Pathway 64\n5.2 From Human Rewards to Agent Rewards 65\n5.3AI Reward Paradigms 65\n5.3.1 Definitions and Overview 65\n5.3.2 Extrinsic Rewards 67\n5.3.3 Intrinsic Rewards 67\n5.3.4 Hybrid Rewards 68\n5.3.5 Hierarchical Rewards 68\n5.4 Summary and Discussion 69\n5.4.1 Interaction with Other Modules 69\n5.4.2 Challenges and Directions 69"
      ],
      "raw_title": "Reward 63",
      "type": null,
      "children": [
        {
          "title": "5.1 The Human Reward Pathway",
          "number": "5.1",
          "level": 2,
          "content": [
            "The brain's reward system is broadly organized into two major anatomical pathways.The first is the medial forebrain bundle,whichoriginates inthebasalforebrainandprojects throughthe midbrain,ultimatelyterminating inbrainstem regions.The second is the dorsaldiencephalicconduction system,whicharises from therostralportion of the medial forebrain bundle,traverses the habenula,andprojects toward midbrain structures[407].The feedback mechanisms and substances inthehuman brain arecomplex,involvingavarietyof neurotransmiters,hormones,andother molecules, which regulate brainfunction,emotions,cognition,andbehaviorthroughfeedback mechanismssuch as neurotransmiter systems and reward circuits.Feedback mechanismscan be positive(such as feedbackinthe reward system)or negative (such asinhibiting excessive neural activity).Wellknown feedback substances [41l]includedopamine,neuropeptides, endorphins, glutamate, etc.",
            "Dopamine is a signaling molecule that plays an important role in the brain, afecting our emotions, motivation, movement,andother aspects[412].Thisneurotransmiteriscriticalforreward-based learning,butthis functioncanbe disrupted in many psychiatricconditions,such as mood disorders and addiction.The mesolimbic pathway[406],akey dopaminergic system,originates from dopamine-producing neurons in the ventral tegmentalarea (VTA)and projects to multiple limbic andcorticalregions,including the striatum, prefrontalcortex,amygdala,and hippocampus.This pathway plays acentralrole inreward processing, motivation,andreinforcementlearning,and is widelyrecognized as acore component of the brain's reward system. Neuropeptides are another important classof signaling molecules in the nervous system, involved inavariety offunctions from mood regulation to metabolic control, and are slow-acting signaling molecules.Unlike neurotransmitters,which arelimitedtosynapses,neuropeptide signalscan affect a wider range of neural networks and provide broader physiological regulation.There is a significant cortical-subcortical gradientin thedistributionofdiffrent neuropeptidereceptors inthe brain.Inaddition,neuropeptide signaling ha been shown to significantly enhance the structure-function coupling of brain regions and exhibita specializedgradient from sensory-cognitive toreward-physicalfunction[413].Table 5lists thecommon reward pathways inthehuman brain,the neurotransmitters they transmit, and the corresponding mechanisms of action,describing the basic framework of the human brain reward system."
          ],
          "raw_title": "The Human Reward Pathway",
          "type": null,
          "children": []
        },
        {
          "title": "5.2 From Human Rewards to Agent Rewards",
          "number": "5.2",
          "level": 2,
          "content": [
            "Having examined the foundations of human reward pathways, we now turn tohow artificialagents learn andoptimize behavior through reward signals.While biological systems rely on complex neurochemical and psychological feedback loops, artificial agents operate using formalized reward functionsdesigned to guide learning and decision-making. Though inspired byhuman cognition,agent reward mechanisms arestructurallyandfunctionally distinct.Understanding the analogies anddisanalogies between these systems iscrucialfor aligning artificialbehavior with human preferences.",
            "In humans,rewards are deeplyembedded inarichweb ofemotional,social, and physiologicalcontexts.They emerge through evolutionarily tuned mechanisms involving neurotransmitters like dopamine and are shaped by experiences, culture, and individual psychology. Incontrast, artificialagents relyon mathematically defined reward functions that are externally specified and precisely quantified.These functions assgn scalar or probabilisticfeedback to actionsor states, providing a signal for optimization algorithms such as reinforcement learning [3, 414].",
            "One key distinction lies in the programmability and plasticity of agent rewards.Unlike human reward systems, which are constrained by biological architecture and evolutionary inertia, agent reward functions are full customizable and can berapidly redefined or adjusted based on task requirements.This flexibility enables targeted learning but also introduces design challenges—specifying a reward function that accurately captures nuanced human values is notoriously difficult.",
            "Another important disanalogy concerns interpretability and generalization.Human rewards are often implicit and context-dependent, whereas agent rewards tend to be explicit and task-specific.Agents lack emotional intuition and instinctualdrives; theirlearning dependsentirelyontheform andfidelityof the reward signal. While frameworks like reinforcement learning from human feedback (RLHF) attempt to bridge this gap by using preference data to shape agent behavior[12], suchmethods stillstruggle withcapturing the fullcomplexity of human goals,especially when preferences are intransitive, cyclical, or context-sensitive [321].",
            "Moreover, atempts to borrow from human reward mechanisms—such as modeling intrinsic motivation or social approval—face limitations due tothe absence ofconsciousness,embodiment, and subjective experience in artificial agents.Consequently,whilehuman reward systems ofer valuable inspiration, thedesign of agentreward functions must addressfundamentally different constraints,including robustnessto misspecification, adversarial manipulation, and misalignment with long-term human interests.",
            "The following section willdelve deeper into agent reward models,focusing on their design principles,evolution,and how these models selectively incorporatehuman-inspired insights tooptimize artificialbehavior within formal systems."
          ],
          "raw_title": "From Human Rewards to Agent Rewards",
          "type": null,
          "children": []
        },
        {
          "title": "5.3 AI Reward Paradigms",
          "number": "5.3",
          "level": 2,
          "content": [
            "Rewards also exist in intellgent agents,especially inreinforcement learning scenarios. Rewards are the core signal used to guide how intelligent agents act intheenvironment.Theyexpressfeedback onthebehaviorofintelligent agents and are used to evaluate anaction'squality inacertainstate, thereby aecting thedecision-makingof subsequent actions.Throughcontinuous trialanderrorandadjustment, intelligent agents learn tochoose behavioral strategies that can obtain high rewards in different states."
          ],
          "raw_title": "AI Reward Paradigms",
          "type": null,
          "children": [
            {
              "title": "5.3.1 Definitions and Overview",
              "number": "5.3.1",
              "level": 3,
              "content": [
                "In reinforcement learning,thereward modeldictateshow an agent is provided with feedback according tothe actions it performs within its environment.This model plays acrucialrole in guiding the agent's behavior by quantifying the desirability of actions in a given state, thus influencing its decision-making.",
                "FormalDefinition. The agent's interaction with its environment can be framed within the formalism of a Markov Decision Process (MDP) [415], which is represented as:",
                "$$\n\\boldsymbol{\\mathcal{M}}=(\\boldsymbol{\\mathcal{S}},\\boldsymbol{\\mathcal{A}},\\boldsymbol{P},\\boldsymbol{r},\\gamma),\n$$",
                "where:",
                "· $s$ denotes the state space, encompassing all possible states in the environment.\n· $\\mathcal{A}$ denotes the action space, which encompasses all actions available to the agent at any given state.\n· $P(s^{\\prime}|s,a)$ defines the state transition probability. It represents the likelihood of transitioning to state $s^{\\prime}$ after the agent takes action $a$ in state $s$ .\n· $r(s,a)$ specifies the reward function, which assigns an immediate scalar reward received by the agent for executing action $a$ in state $s$ .\n· $\\gamma\\in[0,1]$ is the discount factor, which controll the agent's preference for immediate versus future rewards by weighting the contribution of future rewards to the overall return.",
                "The reward function $r(s,a)$ serves as a fundamental component in the formulation of the Agent Reward Model. It is mathematically represented as:",
                "$$\nr(s,a):S\\times\\mathcal{A}\\to\\mathbb{R}\n$$",
                "This function returns a scalar reward based on the agent's current state $s$ and the action $a$ it selects. The scalar value $r(s,a)$ is a feedback signal that indicates the immediate benefit(or cost)ofthe chosen action in the given state.This reward signal guides the agent's learning processas it helps evaluate the quality of actions takenwithin specific contexts.",
                "Objective of the Agent Reward Model.The agent's primary objective is to maximize itsoverallcumulative reward over time.This is typicallachieved by selecting actions that yieldhigherlong-termrewards,which are captured in the form of the return $G_{t}$ at time step $t$ ,defined as the sum of future discounted rewards:",
                "$$\nG_{t}=\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k},\n$$",
                "where $r_{t+k}$ denotes the reward received at time step $t+k$ , and $\\gamma^{k}$ is the discount factor applied to rewards received at time step $t+k$ . The agent aims to optimize its policy by maximizing the expected return over time.",
                "Ata higher level,thereward modelcanbeclassifiedintothreecategories basedontheoriginof thefeedback signal:i) extrinsic reward,ii) intrinsic reward,ii)hybridrewardand iv)hierarchicalmodel.Each of thesecategoriescanbe furthersubdivided intosmallersubclasses.Figure 5.2illustrates different typesofrewards.Next, we willexplore these different types of reward in more detail, outlining the distinct features and applications of each type.",
                "![](images/adffbb163aa483a68509e5d7aabe5e1e25de662d418b9d9a3c1ee3c70025a9f0.jpg)",
                "Figure 5.2: Illustration of different types of reward."
              ],
              "raw_title": "Definitions and Overview",
              "type": null,
              "children": []
            },
            {
              "title": "5.3.2 Extrinsic Rewards",
              "number": "5.3.2",
              "level": 3,
              "content": [
                "Extrinsic rewards are externally defined signals that guide an agent's behavior toward specific goals. In artificial learning systems,especiallyreinforcement learning,these signalsserve as aproxy forsuccessthat shape the policy throughmeasurableoutcomes.However,the structure anddelivery of theserewards significantlyinfuence the leaning dynamics, which present different trade-offs depending on how feedback is distributed.",
                "Dense Reward.Dense reward signals provide high-frequency feedback,typically at every steporaftereach action.This frequent guidance accelerates learning by allowing agents to immediately associate actions with outcomes.However, dense feedbackcan sometimes incentivize short-sighted behavior or overfit toeasily measurable proxies rather than deeper alignment.",
                "For example, InstructGPT[43] uses human rankings of model outputs to provide continuous preference signals throughout fine-tuning,enabling efficient behavior shaping.Similarly,Cringe Loss[416]and its extensions [374] transform pairwise human preferences into dense training objectives,offering immediate signal ateach comparison Direct Reward Optimization (DRO)[367] further simplifes this paradigm by avoiding pairwise comparisons entirely, associating each response with a scalar score—making the reward signal more scalable and cost-effective. These methods exemplify how dense feedback facilitates fine-grained optimization but must be carefully designed to avoid superficial alignment.",
                "Sparse Reward. Sparse rewards are infrequent and typically only triggered by major milestones or task completions. While they often reflect more meaningful or holistic successcriteria, their delayed nature can make credit assignment more difficult, especially in complex environments.",
                "PAFT[376] exemplifies this challenge by decoupling supervised learning and preference alignment, with feedback appliedonly at select decision points.This sparsityreflectsa moreglobal notion of successbut increases theburdenon optimization.Similarly, SimPO[377] uses log-probability-based implicit rewards without dense comparisons.The sparsitysimplifiesthe training pipeline butcanlimitresponsiveness to subtle preference shifts.Sparse reward systems thus tend to be more robust but demand stronger modeling assumptions or more strategic exploration.",
                "Delayed Reward. Delayed rewardsdefer feedback untilaftera sequence of actions,requiring agents toreason about long-term consequences.This setup is essentialfor tasks where intermediate steps may be misleading oronly make sense in retrospect.The challenge lies in atributing outcomes toearlier decisions,which complicates learning but encourages planning and abstraction.",
                "Contrastive Preference Optimization(CPO)[384]trains models bycomparing sets of translations rather than evaluating each one in isolation.The reward signal arises only after generating multiple candidates,reinforcing patterns across iterations. Nash Learning from Human Feedback [385] similarly delays feedback until the model identifies stable strategies through competitive comparisons.These methods leverage delayed rewards to push beyond surface-level optimization,aligning more withlong-termgoalsatthecostofslowerconvergence and morecomplextraining dynamics.",
                "Adaptive Reward. Adaptive rewards evolve dynamicall in response to the agent's behavioror learning progress.By modulating the reward functionsuch as increasing task difficulty or shifting reward targets,this approach supports continualimprovement,especially in non-stationary or ambiguous environments.However,it introduces additional complexity in reward design and evaluation.",
                "Self-Play Preference Optimization (SPO)[386]adapts rewards based on self-play outcomes,using socialchoice theory to aggregate preferences and guidelearning.This approach alows the system torefine itself by evolving internal standards.f-DPO[373]buildsonthis ideabyintroducing divergenceconstraintsthatadapttherewardlandscape during training.Bytuning alignment-diversitytrade-offs dynamically, these methodsenable robust preference modeling under uncertainty, though they require careful calibration to avoid instability or unintended bias."
              ],
              "raw_title": "Extrinsic Rewards",
              "type": null,
              "children": []
            },
            {
              "title": "5.3.3 Intrinsic Rewards",
              "number": "5.3.3",
              "level": 3,
              "content": [
                "Intrinsic rewards serve asinternalgenerated signalsthatmotivate agents toexplore,learn,andimprove,independent of external task-specific outcomes.These rewards are often structured to promote generalization,adaptability,and self-directed skillacquisition—qualitiescriticalforlong-term performance in complex or sparse-reward environments. Different intrinsic reward paradigms focus on fostering distinct behavioral tendencies within agents.",
                "Curiosity-Driven Reward. This reward encourages agents to reduce uncertainty by seeking novel or surprising experiences.Thekeyconcept is to incentivizethe agent to explore novel states where prediction errors aresignificant. This paradigm excels insparse-reward settings by promoting information acquisitionwhen external guidance is limited. For example, Pathak et al.[387]leverage an inverse dynamicsmodel to predict the outcome of actions,creating a feedback loop that rewards novelty.Plan2Explore[389] extends this further by incorporating forward planning to actively target areas of high epistemic uncertainty,thereby enabling faster adaptation to unseen environments.While effective at discovery,curiosity-driven methods can be sensitive to noise or deceptive novelty without safeguards.",
                "Diversity Reward. Diversity reward shifts focus from novelty to behavioral heterogeneity,encouraging agents to explore a wide range of strategies ratherthan converging prematurely on suboptimal solutions.This approach is particularly usefulin multi-agent or multimodal settings,where strategic variety enhances robustness andcollective performance.LIR[390]exemplifies this by assgning personalized intrinsic signals todifferent agents,driving them toward distinct roles while maintaining sharedobjectives.Diversity-driven exploration fosters broader policycoverage but may require careful balancing to avoid destabilizing coordination or goal pursuit.",
                "Competence-Based Reward. Competence-based reward aims to foster learning progressby rewarding improvements in the agent's task proficiency.This reward adapts dynamically as the agent grows morecapable,which creates a self-curriculum that supportscontinual skillacquisition.Skew-Fit[392]facilitates this through entropy-based goal sampling,encouraging agents to reach diverse states while maintaining challenge.CURIOUS [391] further automates curriculum generation by selecting goals that maximize learning progress over time.Competence-based methods are well-suited for open-ended environments,though they often require sophisticated estimation of progress and goal difficulty.",
                "Exploration Reward.Exploration reward directly incentivizes the agent toengage with under-exploredstatesor actions, which emphasize breadth over depth in environment interaction. Unlike curiosity,which focuseson unpredictability, exploration rewardoften targets coverage or noveltyrelative to the agent's visitationhistory. RND[394]exemplifies this by rewarding the prediction error of arandomly initialized network,pushingthe agent towardunfamiliar states. This approach helps preventpremature convergence and encourages robustnessthough it maylack focus if not paired with meaningful learning objectives.",
                "Information Gain Reward. Information gain reward formalizes exploration as a processof uncertainty reduction, which guides agents totake actions that yield the highest expectedlearning.This reward is grounded ininformation theory andis especiallypowerfulinmodel-basedorreasoning-intensive tasks.CoT-Info[397]appliesthistolanguage models by quantifying knowledge gain at each reasoning step,optimizing sub-task decomposition.VIME[398] similarly employs Bayesian inference to rewardbeliefupdates about environmental dynamics.Byexplicitly targeting informationalvalue,these methods ofer principled exploration strategies,thoughtheyoften incur highcomputational cost and require accurate uncertainty modeling."
              ],
              "raw_title": "Intrinsic Rewards",
              "type": null,
              "children": []
            },
            {
              "title": "5.3.4 Hybrid Rewards",
              "number": "5.3.4",
              "level": 3,
              "content": [
                "Hybrid reward frameworks integrate multiple sources of feedback, most commonly intrinsic and extrinsic rewards, to enable more balanced and adaptive learning.By combining the exploratory drive of intrinsic rewards with the goal-directed structure ofextrinsic rewards,these systems aim to improve both sample effciency and generalization. This paradigm is especialy beneficial in complex environments or open-ended tasks,where pure reliance on either feedback type may be insufficient.",
                "A core advantage ofhybridrewards is theircapacity toresolve the exploration-exploitation trade-offdynamically.For instance,Xiong et al.[403]combine intrinsic exploration with extrinsic human feedback within thecontextof RLHF. Using areverse-KLregularized contextual bandit framework,they facilitate strategic exploration while aligning the agent's actions with human preferences.The method integrates intrinsic and extrinsic rewards through an iterative DPO algorithm and multi-steprejection sampling,optimizing exploration and alignment without compromising efficiency."
              ],
              "raw_title": "Hybrid Rewards",
              "type": null,
              "children": []
            },
            {
              "title": "5.3.5 Hierarchical Rewards",
              "number": "5.3.5",
              "level": 3,
              "content": [
                "Hierarchicalrewardarchitectures decompose complexobjectives into layered subgoals,each associated with distinct reward signals.This structure mirrors the hierarchical organization of many real-world tasks, allowing agents to coordinate short-term decisions with long-term planning.By assigning lower-levelrewards to immediate actions and higher-levelrewards toabstract goals,agentscan learn compositionalbehaviors that scale more effectively tocomplex environments.",
                "In language modeling,Token-level Direct PreferenceOptimization(TDPO)[405]ilustrates this principle by aligning LLMs through fine-grainedtoken-levelrewards derived from preference modeling. Using forward KL divergence and the Bradley-Terry model, TDPO simultaneouslyrefines localchoices and globalcoherence, improving alignment with nuanced human preferences.Thehierarchicalreward process here is not merely a structuraldesign but afunctionalone: reinforcing both micro-decisions and macro-outcomes in a coordinated fashion.",
                "More generally,hierarchicalrewardscan serve as scafolding for currculum learning,where agents progressively lean from simpler subtasks before tackling the overarching objective. In LLMagents,this might mean structuring rewards for subcomponents like tool-use,reasoning chains,orinteraction flows,each of whichcontributes tobroader task success."
              ],
              "raw_title": "Hierarchical Rewards",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "5.4 Summary and Discussion",
          "number": "5.4",
          "level": 2,
          "content": [],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": [
            {
              "title": "5.4.1 Interaction with Other Modules",
              "number": "5.4.1",
              "level": 3,
              "content": [
                "In intelligent systems,reward signals function not only as outcome-driven feedback but as centralregulators that interface with core cognitive modulessuchas perception,emotion,and memory. Inthe context of LLM-based agents, these interactions become particularlysalient,as modules likeatention,generation style,andretrieval memorycan be directly influenced through reward shaping, preference modeling, or fine-tuning objectives.",
                "Perception. In LLM agents, perception is often realizedthrough attention mechanisms that prioritize certain tokens, inputs, or modalities. Reward signalscan modulate these atention weights implicitly during training,reinforcing patterns that correlate with positive outcomes. For example, during reinforcement fine-tuning,reward models may upweight specific linguistic features—suchas informativeness,factuality,orpoliteness—causingthe model to attend more to tokens thatalign with these traits.This parallels how biological perception prioritizes salient stimuli via reward-linkedatentional modulation[417].Over time,the agent internalizesa perception policy:not merely“what is said,\" but“what is worth paying attention to\" in task-specific contexts.",
                "Emotion.Though LLMs do not possess emotions in the biological sense,reward signalscan guide the emergence of emotion-like expressons andregulate dialogue style.In human alignment settings, models are often rewarded for generating responses that are empathetic,polite,or cooperativeleading to stylistic patterns that simulate emotional sensitivity.Positive feedback mayreinforceafriendlyorsupportivetone,whilenegativefeedback suppresses dismissive or incoherent behavior.This process mirrors affect-driven behavior regulation in humans [418],and allows agents to adapt their interaction style based onuser expectations,afective context, or application domain.Inmulti-turn settings, reward-modulated style persistence can give rise to coherent personas or conversational moods.",
                "Memory. Memory in LLM agents spans short-term context(e.g.,chat history) and long-term memory modules such as retrieval-augmented generation (RAG)or episodic memory bufers.Reward signals shape how knowledge is encoded, reused,ordiscarded.Forinstance,fine-tuningonpreference-labeleddatacanreinforcecertainreasoningpathsorfactual paterns,efectively consolidating them into the model's internal knowledge representation. Moreover, mechanisms like experience replay orself-reflection—where agents evaluate past outputs with learned rewardestimators-enable selective memory reinforcement, akin to dopamine-driven memory consolidation in biological systems [419]. This allows LLM agents to generalize from prior successful strategies and avoid repeating costly errors.",
                "In general,reward in LLM-based agents is nota passive scalar signal butan active agent of behavioral shaping.It modulatesatentiontopromote salientfeatures,guidesstylisticandaffectiveexpressiontoalignwithhuman preferences, and structures memory to prioritize useful knowledge.As agents evolve toward greater autonomy and interactivity, understanding these cross-modulereward interactions willbeessentialforbuilding systems that are notonlyintelligent, but also interpretable, controllable, and aligned with human values."
              ],
              "raw_title": "Interaction with Other Modules",
              "type": null,
              "children": []
            },
            {
              "title": "5.4.2 Challenges and Directions",
              "number": "5.4.2",
              "level": 3,
              "content": [
                "Although extensive research has been conducted on various reward mechanisms,severalpersistent challenges remain. One fundamental issue is reward sparsity and delay. In manyreal-world scenarios,reward signals are often infrequent anddelayed,making itdiffcultforanagenttoaccuratelyatributecedit tospecificactions.This,intu,incrasesthe complexity of exploration and slows down the learning process.",
                "Another significant challnge is the potential for reward hacking.Agents,in their pursuit of maximizing rewards, sometimes exploit unintended loopholes in the reward function. This can lead to behaviors that diverge from the intendeddesign goals,particularly in complex environments where optimization objectives may not always align with the true task requirements.",
                "Moreover,the processof reward shaping presents adelicatebalance.While shaping rewardscan accelerateleaming by guiding anagent toward desired behaviors,excessve or poorlydesigned shaping may lead tolocaloptima,trapping the agent in suboptimalbehaviors.Insomecases,itmayevenalterthefundamentalstructureoftheoriginaltask,making it difficult for the agent to generalize to other scenarios.",
                "Many real-world problems are inherentlymulti-objective in nature,requiring agents tobalance competing goals.Under a single reward function framework, finding theright trade-offs between these objectives remains an open problem. Ideally,ahierarchicalreward mechanismcould bedesigned to guide learning ina structured,step-by-step manner. However, constructing such mechanisms effectively is still a challenge.",
                "Finally,reward misspecificationintroduces further uncertainty andlimits generalization.Often,areward function does not fullycapture the true task goal,leading to misalignment between the agent's learning objective andreal-world success.Additionally,manyrewardfunctions aretailored to specificenvironments andfailto generalize whenconditions change or tasks shift, highlighting the need for more robust reward models.",
                "Addressing these challenges requires novel approaches.One promising direction is to derive implicit rewards from standard examples or outcome-based evaluations, which can help mitigate reward sparsity issues.Additionally, decomposing complex tasks into hierarchical structures and designing rewards from the botom up can oer a more systmatic approach,even in multi-objective settings.Furthermore,leveraging techniques such as meta-learning and meta-reinforcementleaming can enhance the adaptability of reward models,allowing agents to transfer knowledge across tasks andperform efectively in diverseenvironments.By exploring these avenues,we can move toward more reliable and scalable reward mechanisms that better align with real-world objectives."
              ],
              "raw_title": "Challenges and Directions",
              "type": null,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "title": "6 Emotion Modeling 71",
      "number": "6",
      "level": 1,
      "content": [
        "6.1 Psychological Foundations of Emotion 71\n6.2 Incorporating Emotions in AI Agents . 74\n6.3 Understanding Human Emotions through AI . 74\n6.4 Analyzing AI Emotions and Personality 74\n6.5 Manipulating AI Emotional Responses . 75\n6.6 Summary and Discussion 75"
      ],
      "raw_title": "Emotion Modeling 71",
      "type": null,
      "children": [
        {
          "title": "6.1 Psychological Foundations of Emotion",
          "number": "6.1",
          "level": 2,
          "content": [
            "Psychological and neuroscientific theories of emotion provide essential frameworks for developing emotionally intelligent LLM agents.These theories can be categorized into several major approaches,each offering unique perspectives on how emotions function and how they might be implemented in AI systems.",
            "CategoricalTheories.These models posit that emotions exist as discrete,universalcategories with distinct physiological and behavioral signatures.Ekman's theory of basic emotions [421]identifies six fundamental emotions (anger, disgust,fear, happiness,sadness, and surprise that are recognized across cultures and expressed through specific facial configurations.This discrete approach has significantly infuenced affective computing, with many emotion classification systems in AI adopting these labels for training [422, 423]. For LLMagents,categorical frameworks provide clear taxonomies for classifying user emotions and generating appropriate responses. However, they face criticismforoversimplifying the complex,blended nature of human emotional experience[424]and may not capture cultural variations in emotional expression [425].",
            "Dimensional Models. Rather than discrete categories, dimensional approaches represent emotions as points in a continuous space defined by fundamental dimensions. Russells Circumplex Model[426] maps emotions onto two primary dimensions:valence (pleasure-displeasure)and arousal(activation-deactivation).This framework enables more nuanced trackingof emotionalstates.It distinguishes between high-arousal panic and low-arousalanxiety despite both having negative valence.The PAD (Pleasure-Arousal-Dominance) model[427] extends this by adding a dominance dimension,capturing the sense ofcontrolor power associated with emotionalstates.Thesecontinuous representations have proven valuable forLLMsystems that need to generate emotionally gradedresponses ortrack subtle shifts inuser affect over time[428,429,430].Dimensionalmodels allow forfine-grainedcontrolover generatedcontent,enabling humansor agents to modulate tone along continuous scalesrather than switching between discrete emotional states.",
            "Hybrid and Componential Frameworks. Recognizing limitations in pure categorical or dimensional approaches, severaltheories integrate aspects of both.Plutchik's Wheel of Emotions[431] arranges eight primary emotions in a wheel structure with intensity gradients and dimensional properties, allowing for the representation of complex emotional blends (e.g.,love as a mixture of joyand trust).Meanwhile,componential models like Scherer's Component Process Model (CPM)[432]conceptualize emotions as emerging from synchronized components including cognitive appraisal,physiologicalarousalaction tendencies,andsubjectivefelings.ParticularlyinfluentialinAIreseachisthe OCC (Ortony-Clore-Collins) model[433], which defines 22 emotion types based on how events, agents, or objects are evaluatedrelative to goals and standards.These appraisal-based frameworks havebeen implementedin dialogue systems that generate emotional responses through rule-based evaluation of situations [434,435]. For LLM agents, such models provide computational structuresforevaluating text input and selectingcontextuallappropriateemotional responses, improving both coherence and perceived empathy [436, 437].",
            "Neurocognitive Perspectives.The neuroscience of emotion ofers additionalinsights forLLMarchitectures.Damasio's somatic marker hypothesis[25]emphasizes how emotions,implemented through body-brain interactions,guide decisionmaking by associating physiological states with anticipated outcomes.This interaction between thelimbic system and the cortexshows atwo-processarchitecture:fast“alarm\"signals in the limbic system,like those processed by the amygdala, work alongside slower, more deliberate reasoning in the cortex. Contemporary LLM systems have begun implementing analogous architectures, where fast sentiment detection modules work in parallel with more thorough chain-of-thought reasoning[436,437].Recent evidence further suggests thatopponentcircuitryinthe striatum enables distributionalreinforcement learning byencodingnot just meanrewards but entireprobability distributions,offring a neural basis foremotion-influenced decision-making under uncertainty[438].Similarly,LeDoux's distinction between “low road\"（quick, automatic)and“high road\"(slower,cognitive)fear processing [24]suggests design patterns for systems that need both immediate safetyresponses and nuancedemotional understanding.Minsky'sframing of emotions as“way to think\"[420]thatreorganize cognitive processes has influenced frameworks like EmotionPrompt[428]and Emotion-LLaMA [423], where emotional context dynamically reshapes LLM reasoning.",
            "These theoretical frameworks increasingly inform the development of emotionally intellgent LLM agents.Categorical models provide clear labels for emotionclassification tasks [423,429],while dimensional embeddings enable continuous controlover generated text[428].Hybrid approaches help systems handle mixed emotions and emotional intensity.Appraisal-based methods,particularly those derived from theOCC model,allowLLMs to evaluate narrative events oruser statementscontextualy,selecting appropriateemotionalresponses thatfosterrapport andtrust [439]. Neuroscientifically-inspired dual-process architectures combine“fast\"sentiment detection with“slow\"deliberative reasoning,enabling both quick safety responses and deeper emotional understanding [436, 437].While explicit neurocognitive mechanisms (like dedicated“amygdala-like\"pathways)remainrare in current LLM pipelines,emerging research explores biologically-inspired modules tohandle urgent emotional signals and maintain consistent emotional states across extended interactions [440, 441].",
            "Emotionisakeypartofhuman intellgence,anditwillikelybecomeone ofthekeycomponentsordesignconsiderations of LLM agents.Onekey futuredirection is systematicallytranslating these psychologicaland neuroscience theories into an LLM agent's internal processes. Techniques for translating might include using dimensional models (e.g., valence/arousal/dominance)aslatent states that influence generation or adopting explicit rule-basedappraisals (OCC)to labeluser messages and shape the agent's subsequent moves.Hybrid approaches offer acompelling balance: an LLM could first recognizeadiscretecategory(e.g.“fear\")butalsogauge itsintensityandcontroldimensionforfinergrained conversation.Such emotion-infused architectures might yield more coherent“moods\"over time,analogous to how humans sustain afective statesrather than resetting at every turn.Explicit alignment with psychological theories also enhances interpretability:designers can debug orrefine the agent's responses bycomparing them to well-established emotion constructs, rather than dealing with opaque emergent behaviors.",
            "A second directionis harnessing these theories to improveaffectionateor supportive interactions,oftenreferredto as emotional alignment.For example,circumplex or PAD-based tracking can help an LLM detect negative valence and higharousalinauser's text andrespond soothingly (e.g.,lowering arousal,ofering empathetic reappraisals). In mental health or counseling scenarios,an appraisal-informed method could let the agent validate the user's feelings and understand their situation in terms of goal incongruence or perceived blame, which helps craftresponses that convey genuine empathy.Grounding emotional outputs incognitive theories (like“relief\"if a negativeoutcome is avoided,or“gratitude\"whenauserhelpsthesystem)likewisemakes interactions feelmore naturalandethically aligned. These enhancements are particularly salient as LLMs migrate intoreal-worldapplications like customer service,elder care,and tutoring, where emotional sensitivity can improve outcomes and user well-being.By incorporating robust psychological and limbic-system insights,developers can design LLM agents that notonly reason more efectively but also provide sincere emotional support, bridging the gap between computational precision and human-centric care.",
            "![](images/bb9199c9a3bfcc8e7419924437d827169aa6e0f1d25b80d8f48e2afcd35afbdb.jpg)",
            "Figure 61: Visualization and examples of major emotion theory categories.(a) Categorical Theories: Ekman's six basic emotions [421] showing discrete emotional states. (b)Dimensional Models: Russells Circumplex[426] representing emotions as coordinates in continuous space.(c)Hybrid/Componential Frameworks: Plutchik's Wheel [431] combining intensity gradients with categorical emotions.(d) Neurocognitive Perspectives: LeDoux's AmygdalaCentered Model[24] showing dual-pathway procesing of emotional stimuli.These psychological foundations inform diferent approaches to emotion modeling in AI systems,from discrete classification to dimensionalrepresentations, appraisal-based reasoning, and multi-pathway information processing."
          ],
          "raw_title": "Psychological Foundations of Emotion",
          "type": null,
          "children": []
        },
        {
          "title": "6.2 Incorporating Emotions in AI Agents",
          "number": "6.2",
          "level": 2,
          "content": [
            "The integration ofemotionalintelligence into large language models (LLMs)hasemerged asatransformative approach to enhancingtheir performance and adaptability. Recent studies,such as those of EmotionPrompt[422],highlight how emotional stimuli embedded in prompts can significantly improve outcomes across various tasks,including a notable $10.9\\%$ improvement in generative task metrics such as truthfulness and responsibility. By influencing the attention mechanisms of LLMs,emotionall enriched prompts enrich representation layers andresultin more nuanced outputs[42].These advancements bridge AI with emotionalintelligence,offering afoundation fortraining paradigms that better simulate human cognition and decision-making,particularly in contexts requiring social reasoning and empathy.",
            "Multimodal approaches further elevate the impact of emotional integration.Models like Emotion-LLaMA [440] demonstrate how combining audio, visual,and textual data enables better recognition and reasoning of emotions. Using datasets such as MERR[44o],these models align multimodal inputs into shared representations,facilitating improved emotional understanding and generation.This innovation extends beyondlinguistic improvements,ofering applications in human-computer interaction and adaptive learning.Together,these methods underscore thecritical role of emotions in bridging technicalrobustness with human-centric AIdevelopment,paving the wayfor systems that are both intelligent and empathetic."
          ],
          "raw_title": "Incorporating Emotions in AI Agents",
          "type": null,
          "children": []
        },
        {
          "title": "6.3 Understanding Human Emotions through AI",
          "number": "6.3",
          "level": 2,
          "content": [
            "Textual Approaches.Recent workhighlights theability of LLMs to perform detailedreasoning aboutlatent sentiment andemotion. Using step-by-stepprompting strategies,such aschain of thought reasoning,researchers enable LLMs to infer sentiment even when explicit cues are absent[436].Beyond single-turn inference,negotiation-based frameworks further refine emotional judgments byleveraging multiple LLMs thatcross-evaluateeach other's outputs,effectively mimicking amore deliberative human reasoning process[437].These techniques underscore the importance of iterative, context-aware strategies to capture subtle emotional signals from purely textual input.",
            "Multimodal Approaches.LLMs have alsobeen extended to integrate signals from audio,video,and images.Recent efforts show howadditionalcontextualor worldknowledge can befused with visualand textual information to capture deeper affective states[442].Moreover,frameworks that convert speech signals into textual promptsdemonstrate that vocal nuances can be embedded in LLM reasoning without changing the underlying model architecture[443]. This multimodalintegration,combined with explainableapproaches,allows forricher and moretransparent representations of emotional content [444].",
            "Specialized Frameworks.Beyond generic techniques,specialized systems addresstasks in which emotion recognition requires higherlevels ofawareness of ambiguity[439],context sensitivity,and generative adaptability[445].These approaches emphasize the inherent complexity of human emotion,treating it as dynamic and probabilistic rather than strictly categorical. Using flexible LLM instruction paradigms,they offer pathways to better interpret ambiguous emotional expressions and integrate contextual cues (e.g., dialogue history), moving LLM closer to human-like emotional comprehension.",
            "Evaluation and Benchmarks.To holistically assessthe emotional intellgence of LM,researchers have proposed various benchmark suites.Some focus on generalized emotion recognition acrossdiferent modalities and social contexts[446,447],whileotherscomparethe performance andeffciencyofmodelsofvarying sizes[448].There are also specializedbenchmarks that evaluate multilingualcapabilities[449],annotationquality[450],orempathetic dialogue systems [451].Furthermore,frameworks such as EMOBENCH[441] and MEMO-Bench [452] test nuanced emotional understanding andexpression inboth text and images,whileMERBench[453]and wide-scale evaluations[454]address standardization concerns in multimodal emotion recognition.Together,these benchmarks reveal the growing,yet tll imperfect grasp of human emotion by LLMs,highlighting ongoing challenges such as implicit sentiment detection, cultural adaptation, and context-dependent empathy [455]."
          ],
          "raw_title": "Understanding Human Emotions through AI",
          "type": null,
          "children": []
        },
        {
          "title": "6.4 Analyzing AI Emotions and Personality",
          "number": "6.4",
          "level": 2,
          "content": [
            "Reliability of Personality Scales for LLMs.Large language models (LLMs)show conflicting evidence when evaluated through human-centered personality tests. On one hand, some studies challnge the validity of common metrics, reporting biases such as\\*agreebias\"and inconsistentfactor structures,raising doubts about whetherthese instruments capture genuine traits [456,457]. On the other hand, systematic experiments reveal that LLMs can exhibit stable, human-like traitpatterns and evenadapt todiffrent personas under specific prompts[458,459].Yet,concerns persist about actionconsistency,alignmentof self-knowledge,andwhetherrole-playingagents truly maintain fidelityto their assigned characters [460, 461].",
            "Psychometric Methods & Cognitive Modeling Approaches. Recent work applies rigorous psychometric testing, cognitive tasks, and population-based analyses to uncover how LLM processes and represents mental constructs [462. 463,464].Fine-tuning on human behavioral data can align models with decision patterns that mirror individual-level cognition,while population-based sampling techniques expose variability in neural responses[465,466]. By merging psychologicaltheories with advanced prompting and embedding methods,researchers illuminate latent representations of constructs like anxiety or risk-taking, showing how LLMs can approximate human reasoning across tasks.",
            "Emotion Modeling. Studies on LM-based emotionalintellgence reveal notable abilities to interpret nuanced affct and predict emotion-laden outcomes,often surpassing average human baselines in standard tests[423,429].However, these models do not necessarily emulate human-like emotional processes; they rely on high-dimensional pattern matching that sometimes fails under changing contexts, negative input,or conflicting cues[467,468].However, hierarchical emotion structures,coping strategies, and empathy-like behaviorscan emerge in larger-scale models, underscoring boththe promise of emotional alignment andtheethicalchallenges increating AI systems thatappear and occasionally function as affective agents."
          ],
          "raw_title": "Analyzing AI Emotions and Personality",
          "type": null,
          "children": []
        },
        {
          "title": "6.5 Manipulating AI Emotional Responses",
          "number": "6.5",
          "level": 2,
          "content": [
            "Prompt-based Methods. Recent research shows that adopting specific personas or roles through well-engineered prompts can bias LLM cognition, alowing targeted emotional or personality outcomes [469, 470, 471,472].By inserting instructions suchas“If you were a[persona]\",LLMs adapt not only their thematicstyle,but also their underlying emotional stance.This approach is powerfulforreal-time manipulation,though itcanbe inconsistent across tasks and model variants, highlighting the need for more systematic methods.",
            "Training-based Methods.Fine-tuning and parameter-effcient strategies offer deeper, more stable ways to induce or alter LLM emotions [473,428, 474]. Quantized Low-Rank Adaptation (QLoRA)and specialized datasets can embed nuanced traits suchas theBig Fiveor MBTI profiles directly into the model's learnedweights.These methods enable LLMs to spontaneously exhibit trait-specific behaviors (including emoji use)and sustain their emotional states over longer dialogues, while also offering interpretability through neuron-level activation patterns.",
            "Neuron-based Methods. A recent advance isolates personality-specific neurons and manipulates them directly to evoke or suppress emotional traits[475].Bytoggling neuronactivations pinpointed through psychologicall grounded benchmarks (e.g.,PersonalityBench), LLMs can embody targeted emotional dimensions without retraining the entire network.This neuron-centric approach provides fine-grained,dynamiccontrolover modelbehaviors,representing a leap in precision and efficiency for emotional manipulation in LLMs."
          ],
          "raw_title": "Manipulating AI Emotional Responses",
          "type": null,
          "children": []
        },
        {
          "title": "6.6 Summary and Discussion",
          "number": "6.6",
          "level": 2,
          "content": [
            "Manipulation and Privacy Concerns.The rapidadoption of Emotional Al in advertising and politics raises significant manipulation and privacy risks [476, 477].Emotional AI often collcts sensitive biometric data,such as facial expressions and voice tones,to infer emotional states,enabling targeted advertising or politicalinfluence.However, these systemscan exploit human emotions for profit or political gain, infringing on fundamentalrights andfostering over-surveillance in public spaces[478,477]. Regulatory frameworks like GDPR and the EU AI Act arecritical to mitigating these risks responsibly.",
            "Alignment Issues.Emotional AI'scapacity todetect and interpret emotions isoften misaligned with intendedoutcomes, leading to inaccuracies and biases.Anxiety-inducing prompts,for instance,have been shown toexacerbate biases in large language models (LLMs),affcting outputs inhigh-stakes domains such as healthcare and education[479,480]. Misinterpretationofemotionalcues byAIsystems,as seen in workplace applications,can exacerbate discrimination and power imbalances [481].Techniques like reinforcement learning from human feedback (RLHF)have proven effective in mitigating these issues but require further development to ensure robust alignment in diverse contexts[479,423].",
            "Ethical Implications.Trust andacceptance ofAIsystems are significantlyinfluencedbytheirabilitytoexhibit empathy and maintain socially appropriate behavior[482, 483].However, the commodification of emotions in workplace management and customer service has raised concerns about ethicallabor practices and Al-human relationships[481]. Moreover,Emotional AI's reliance on anthropomorphic characteristics without suffcient empathycan undermine user trust [482]. Frameworks like SafeguardGPT, which incorporate psychotherapy techniques,demonstrate promising approaches tofostering trust and aligning AI behavior with societal norms[484]. Nonetheless,challenges remain in ensuring privacy, fairness, and cultural sensitivity [484, 483].",
            "Distinguishing AI Emotional Mimicry from Human Experience. Despite advances in emotion modeling for LLM agents,afundamentaldistinction remains:these systems donot actuall“feel\"emotions as humans do butonly show human-emotion-like patterns via probabilistic modeling.While LMs can convincingly simulate emotionalresponses, recognize emotionalpatterns,and generateaffctionaloutputs,they lacktheembodied, phenomenologicalexperience that defines human emotions.This simulation-reality gapcreates both technical andethicalchallenges.Users frequently anthropomorphize AI systems that display emotion-like behaviors[482], potentiall leading to misplaced trust or expectations.Thisdistinction needs tobecarefullythought inbothresearch anddeploymentcontexts,as the perceived emotional capabilities of LLMs influence human-AI relationships,ethical frameworks, and regulatory approaches. Future work should balance enhancing LLMs'emotional intelligence while maintaining transparency about their fundamental limitations as non-sentient systems."
          ],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "7Perception 77",
      "number": "7",
      "level": 1,
      "content": [],
      "raw_title": "Perception 77",
      "type": null,
      "children": [
        {
          "title": "7.1  Human versus AI Perception 77",
          "number": "7.1",
          "level": 2,
          "content": [
            "7.2Types of Perception Representation 79\n7.2.1 Unimodal Models 79\n7.2.2 Cross-modal Models 80\n7.2.3 Multimodal Models . 81\n7.3 Optimizing Perception Systems 83\n7.3.1 Model-Level Enhancements 83\n7.3.2 System-Level Optimizations 84\n7.3.3External Feedback and Control . 84\n7.4 Perception Applications . 84\n7.5 Summary and Discussion 85"
          ],
          "raw_title": "Human versus AI Perception 77",
          "type": null,
          "children": []
        },
        {
          "title": "7.1 Human versus AI Perception",
          "number": "7.1",
          "level": 2,
          "content": [
            "Perception is fundamental to intelligence,serving as the interface throughwhich both humans and artificial agents interactwith the world.Although humans commonly thinkof perception in terms of thefiveclassical senses-vision, hearing,taste,smellandtouchmodern neuroscienceidentifies aricher sensorylandscape.Conservatively,humans are described as havingaround10senses; more comprehensive views listapproximately 21,while some researchers propose up to 33 distinct sensory modalities[546,547].Beyond the familiar senses,humans possesssophisticated intenal perceptions,such as vestibular(balance),proprioception(awarenessof body position),thermoception (temperature), and nociception (pain), enabling nuanced interaction with their environment.",
            "Human senses are finely tuned to specific physical signals:forexample,human vision detects electromagnetic waves with wavelengths between approximately $380{-}780\\mathrm{nm}$ , whereas hearing perceives sound frequencies from about 20 $\\mathrm{Hz}$ to $20\\mathrm{kHz}$ [548]. These sensory modalities allow humans to effortlessly engage in complex tasks like language communication,objectrecognition,socialinteraction,and spatialnavigation.Additionally,humans naturally perceive continuous changes overtime,seamlessy integrating motion perception and temporalawareness,abilities essential for coordinated movement and decision-making [549].Animals in the natural world exhibit even more diverse perceptual capabilities.Birds andcertain marine organisms,for instance,utilize magnetoreception to navigate using Earth's magnetic fields, whilesharksandelectriceels exploit electroreception to sense electrical signals emitted by other organisms—abilities humans do not possess [550].",
            "In contrast to biologicalperceptionartificialagents relyupon engineered sensors designed totransformenvironmental stimuli into digital signalsthat algorithms can interpret.Common sensor modalitiesforAIagents include visual sensors (camras),auditorysensors(microphones),tactilesensors,andinertialmeasurement units.AIagents typicallexcelat processing visual, auditory, and textual data,leveraging advances in deep learning and signal processing. However, certain human sensory abilities—particularly taste and smell- -remain challenging for machines to emulate accurately. For example,the advanced bio-inspired olfactorychipdeveloped by researchers[551]curentlydistinguishes around 24 diferent odors,acapability significantlylesssensitive thanthehuman olfactory system,which discriminates among more than 4,000 distinct smells [552].",
            "![](images/2afbdcea632e567b2c3d8f6b2c6a3a30ae589780255adc29af8f74fd6878167a.jpg)",
            "Figure 7.1: Ilustrative Taxonomy of Perception System",
            "Anothercrucial distinction lies in perceptual processing effciencyHuman perceptionislimited bybiologicalconstraints such as nerve conduction speeds,typically in therange of millseconds.Conversely,AIsystemscan process sensory inputs at speeds of microseconds or evennanoseconds,constrained primarily bycomputationalhardware performance rather than biological limitations.Neverthelesshuman perception naturally integrates information from multiple sensory modalities—known as multimodal perception-into coherent experiences effortlesslyFor AIagents, achieving this multimodalintegration requirescarefully designedfusion algorithms thatexplicitly combine inputs from diverse sensors to build unified environmental representations [553].",
            "Further differences arise in the way humans and artificial agents handle temporal and spatial information. Human perceptionis inherentlycontinuousandfluid,smoothlyexperiencing the passage of timeand spatial motion without explicit temporaldiscretization.Incontrast,AIagents typicallrelyondiscrete sampling of sensordata,usingtimestamps or sequential processing tosimulate continuity.Spatialawareness inhumans effortlessy mergesvisual, auditory,and vestibular information to achieve intuitive spatial positioning.Forartificialagents,spatial perception usuallinvolves algorithmic processes such as simultaneous localization and mapping (SLAM)or 3D scene reconstruction from visual lata sequences [554].",
            "Physical orchemicalstimulitransmittedfrom theexternalenvironment tohuman sensoryorgans willbereceived by the sensory system(suchaseyes,ears,skin,etc.)andconverted into neural signals,whicharefinally processed bythe brain to produce perception of theenvironment.Similarly,toallow the intelligent agent toconnect withtheenvironment, it is alsocrucialtoobtain these perceptioncontents.Currently,various sensors are mainlyused toconvert electrical signals into processable digitalsignals. Inthis section,We distinguishbetween Unimodal models,Cross-modal models, and Multimodal models based on the number of modalities involved in the input and whether unified fusion modeling operations are performed.Unimodal Models specifically process and analyze data from a single modality or type of input (such as text,image,oraudio),while Cross-modal Models establish relationships and enable translations between different modalities throughdedicated mapping mechanisms, and Multimodal Models holistically integrate and process multiple modalities simultaneously to leverage complementary information for comprehensive understanding and decision-making.",
            "![](images/58c9f2130bd1717afa65171572961ef3023f844c3d87112206247236bdb67c4c.jpg)",
            "Figure 7.2: Comparison of common perceptual types between human and agent."
          ],
          "raw_title": "Human versus AI Perception",
          "type": null,
          "children": []
        },
        {
          "title": "7.2 Types of Perception Representation",
          "number": "7.2",
          "level": 2,
          "content": [],
          "raw_title": "Types of Perception Representation",
          "type": null,
          "children": [
            {
              "title": "7.2.1 Unimodal Models",
              "number": "7.2.1",
              "level": 3,
              "content": [
                "When humans are in an environment,they can listen to beautiful music,look at sunrise and sunset, orexperience a wonderfulaudiovisualfeast onstage.These perception contents can be either a single image or audio,or a fusion of multiple perception contents.Regarding the types of perception input of intellgent agents, we will start with single-modal and multimodal inputs, and introduce their implementation and differences.",
                "Text As an important means of communication,text carries a wealthof information,thoughts,emotions and culture. Humans indirectlybtain thecontentoftextthrough vision,hearingandtouch,which isoneofthe most important ways for humans tointeract with theenvironment.But forintelligentagents,textcandirectlyserve asabridge toconnect with the environment,taking text asdirect input andoutputting responsecontent.Inaddition to theliteral meaning, text alsocontains rich semantic information and emotional color. In the early days,the bag-of-words model[55] was usedtocount textcontentand was widely used in textclasificationscenarios,but semantic expressoncould not be obtained.BERT [485]uses a bidirectional Transformer architecture for language modeling and captures the deep semantic information of textthroughlarge-scaleunsupervised pre-training.[486,487]furtheroptimizedthe training efficiency of BERT.The autoregressive modelrepresented by GPT3.5[556] opened the prelude toLM and further unified the tasksof text understanding and text generation,whiletechnologies such as LoRA[109]greatlyreduced the application cost of LLM and improved the agent's perception ability of complex real-world scenario tasks.",
                "Image Image is another important way for humans to interactwith the environment which inherently encode spatial information,encompassing crucialatributes such as morphologicalcharacteristics,spatialpositioning,dimensional relationships,and kinematic properties ofobjects.The evolution ofcomputer vision architectures hasdemonstrated significant advancement in processing these spatialatributes.The seminal ResNet architecture[488]established foundational principles fordeepvisual feature extraction,while subsequentYOLOseries[557,558]demonstrated the capability to simultaneously determine object localization and clasification with remarkable effciency.A paradigm shift occurred withthe introduction of DETR[489],which revolutionizedobject detection by implementing parallel prediction throughglobalcontextreasoning,effectivelyeliminating traditionalcomputationaloverhead assciated with non-maximum suppression and anchor point generation. More recently,DINO 1.5[490]has extended these capabilities to open-set scenariosthrougharchitecturalinnovations,enhanced backbone networks,andexpanded training paradigms, substantially improving open-set detection performance and advancing the perceptual generalization capabilities of artificial agents in unconstrained environments.",
                "Video Video is an expression ofcontinuous image frames, which includes the time dimension and displays dynamic information that changes over time throughcontinuous image frames.The intelligent agent uses video as input and obtains richer perceptualcontent through continuous frames. ViViT[49] extracts spatiotemporal markers from videos, effctively decomposing the spatial and temporal dimensions of the input.VideoMAE [492] learns general video feature representations through self-supervised pre-training and hasstrong generalization capabilitiesonout-of-domain data. It lays a solid foundation for intelligent agents to acquire perceptual capabilities in new scenarios.",
                "Audio Inaddition totext andvision,anotherimportant wayforhumans tointeract withtheenvironment isthrough audio. Audio notonlycontains direct text content,but alsocontains the speaker's tone and emotion[559].Wav2Vec2[495] defines the contrast task by quantizing the potential representation of joint learning,achieving speech recognition effctiveness with1/100labeled data volume.FastSpeech 2[493] directly introduces voice change information (pitch, energy,duration,etc.)and uses realtargets to train the modelto achieve more realistic text-to-speech conversion. Seamless [494] generates low-latencytarget translations through streaming and using an effcient monotonic multi-head atention mechanism,while maintainingthehuman voice style,toachieve synchronousspeech-to-speech/text translation frommultiple sourcelanguages totargetlanguages.Basedonthese means,the intelligent agentcanachievethe ability to listen and speak.",
                "Others At present, most oftheresearchon intellgent agents focuses on the above-mentioned common sensory input types.However, just as humans have more than 20types of perception,intellgent agents have also made progress in achieving corresponding perception capabilities through other sensors. The bionic olfactory chip developed by Hong Kong University of Science and Technology[551] integrates a nanotube sensor array on a nanoporous substrate, withupto 10,oO independently addressable gas sensors on eachchip,which is similarto theconfiguration of the olfactory system of humans and other animals, andcan accurately distinguish between mixed gases and 24different odors. In terms oftaste,Tongji University[56o] combines fluorescence and phosphorescence signals to develop an intelligent taste sensor with multi-mode lightresponse,whichcanefectively identify umami,sourness and biterness In orderto achieve human-like perception and grasping capabilities,New York University[561]launcheda low-cost magnetic tactile sensor AnySkin, which can be quickly asembled and replaced. Even in the perception of pain, the Chinese Academy of Sciences uses the unique electrical properties of liquid metal particle films whenthey are “injured\"(mechanically scratched)to imitate the perception and positioning of“wound.\"Someother works,including HuggingGPT [152], LLaVA-Plus [500], and ViperGPT [498],integrate these single-modal perception capabilities within theframework,select and applythemaccording totask requirements,and achieve the goalof achieving more complex tasks."
              ],
              "raw_title": "Unimodal Models",
              "type": null,
              "children": []
            },
            {
              "title": "7.2.2 Cross-modal Models",
              "number": "7.2.2",
              "level": 3,
              "content": [
                "Text-Image Cross-modal models integrating text and images have witnessed significant advancements in recent years,leading to improved alignment, retrieval, and generation between the two modalities.These models can be categorized based ontheir primaryobjectives,includingcross-modal alignment andretrieval,text-to-image generation, and image-to-text generation.",
                "One of the primary focuses in cross-modal research is the alignment and retrieval of text and images.CLIP [51], introduced by OpenAI in 2021,employs contrastive learming to align textual and visual representations,enabling zero-shot crossmodalretrieval and clasification. Similarly, ALIGN[501]developed by Google in the same year, leverages large-scale noisy webdata tooptimizetext-imageembedding alignment.In 2022,CyCLIP[562]introduceda cyclic consistencyloss tofurtherenhancetherobustnessofcross-modalalignment,improving thereliabilityofretreval tasks.",
                "Another major areaof progressinvolves text-to-image generation, where models aim tosynthesize high-quality images based on textual descriptions. OpenAl's DALL-E series[563, 564,502],spanning from 2021 to 2023,has made substantialcontributions inthis domain, withDALL·E3offring fine-grained semanticcontrolover generated images. Stable Diffusion[565],ntroducedbyStabilityAIin2022,employsadiffusion-based generativeappoachthatsuppts open-domain text-to-image synthesis and cross-modal editing.",
                "A third significantresearch direction is image-to-text generation, where models aim to generate high-quality textual descriptions based on image inputs.Typicalrepresentative work isthe BLIP[566] and BLIP-2[567]models, introduced by Salesforce between 2022and 2023,which utilize lightweight bridging modules to enhance vision-language model integration, enabling tasks such as image captioning and question answering.",
                "Text-Video The key research here involves video text alignment, generation and retrieval.VideoCLIP[504]employs a videoencoder—typically based on temporal convolution ora transformer structure—to extract sequential features from video frames.These features are subsequently aligned with textualrepresentations generated by a language encoder,facilitating robustvideo-text asociation.In the domain of text-to-video generation,Meta's Make-A-Video model [06]extends spatial-temporal dimensions using diffusion-based techniques,allowing forhigh-quality video synthesis fromtextualdescriptions.Additionally,Google'sPhenaki[505]addresses thechallenge of generating long, temporallycoherent video sequences,demonstrating significantadvancements invideo synthesis throughcross-modal learning.DeepMind's Frozen in Time[568] adopts contrastive learning for video-text matching,thereby enabling efcient cross-modalretrieval.This approach enhances thecapacity to search andretrieve relevant video segments based on textual queries,further improving the integration of vision and language understanding.",
                "Text-Audio Cross-modal models connecting text and audio have made significant improvements inrelated tasks such as modalrepresentation, generation, and conversion, and enhanced the perception ability under a single modality.",
                "AudioCLIP[509],ntroduced in 2021,extends the CLIP framework to theaudiodomain,enabling trimodalretrieval across audio,text, and images.By incorporating audio as an additional modality,AudioCLIP utilizes multi-task learning tounifimage,text,andaudio representations intoashared embedding space.This advancement enhances the capability of cross-modalretrievaland interaction. Ina similar vein, VATT[508]adopts a unified Transformer-based architecture to process video,audio,andtextthrough independent encoding branches.These branches are subsequently fused into a shared multimodal space,facilitating tasks such as cross-modal retrieval and multi-task learning.This design allows for greater adaptability across diverse multimodal scenarios.",
                "For text-to-audio generation, Meta introduced AudioGen[569] in 2023,which enables the synthesis of audio, such as environmental sounds and music fragments,directly from textual descriptions.This modelexemplifies the growing capabilities of AIin generating high-fidelity audio based on linguistic input,expanding applications in media, entertainment, and accessibility.",
                "Additionally,inthedomainofspeech-to-text andtext-to-speechconversion,Microsoftdeveloped SpeechT[57o].This model unifies speech and text generation,supporting both speech synthesis and recognition withina single framework. By leveraging a sharedarchitecture forthese dualfunctionalities,SpeechT5 contributes tothe seamlessintegration of speech andtextprocessing,therebyenhancingapplications inautomatedtranscription,voiceasistantsandacceibility tools.",
                "Others In some other scenarios and domains, cross-modal modeling also plays an important role.",
                "CLIP-Forge [510] presents a novel method for generating 3D shapes from textual descriptions. By leveraging the capabilities ofContrastive Language-Image Pre-training (CLIP),this approach enables the synthesis of high-quality 3D objects conditioned on naturallanguage inputs,bridging the gapbetween textand 3D geometry.Point-E[51] extends this concept by generating 3D point clouds from text descriptions.Unlike traditional 3D reconstruction techniques, Point-Efocuseson pointcloudrepresentations,facilitating effcient and scalable3Dcontent creationwhile maintaining high fidelity to textual prompts.",
                "In the field of medicalimaging,MoCoCLIP[571]introduces an approach that enhances zero-shot learning capabilities. By integrating CLIP with Momentum Contrast (MoCo),this method improves the generalization of deep learning models in medicalimaging applications,addressing thechallenges associated with limited annotated data and domain adaptation."
              ],
              "raw_title": "Cross-modal Models",
              "type": null,
              "children": []
            },
            {
              "title": "7.2.3 Multimodal Models",
              "number": "7.2.3",
              "level": 3,
              "content": [
                "The cross-modal modeldescribed above mainly aligns and maps between modalities through contrastive learning and other mthods to achieve information complementarity and conversion between modalities.Furthermore, the work of multimodalmodels focuses onhow to integrate the features of multipledata(suchasvision,text, audio,etc.)to improve the performance of the overall model.",
                "Vision Language Model Vision Language Model(VLM) is broadly defined as multimodal model that can learn from images(or videos)andtext.Humans live inaworld fullofmultimodalinformation.Visualinformation(such as images and videos)andlanguage information (such as text)oftenneed to be combined tofully expressmeaning.The same is true forintellgent agents.LLaVA[513]first tried touse gpt-4to generate a multimodallanguage image instruction dataset.Through end-to-end training,alarge multimodal model was obtained and excelent multimodalchat capabilities were demonstrated. LLaVA-NeXT [513] uses dynamic high-resolution and mixed data to show amazing zero-shot capabilities even inpureEnglish modaldata, and the computational/training datacost is 0O-10OO times smaer than other methods.Emu2[516]changes thetraditional way of using image tokenizer toconvert images intodiscretetokens, and directly uses image encoders to convert images into continuous embeddings and provide them to Transformer, enhancing multimodalcontext learning capabilities.MiniGPT-v2[512] employs unique identifiers for various tasks during training.These identifiers helpthemodeldifferentiate task instructions moreefectivelyenhancing its learning effciencyforeach task.Qwen2-VL[515],DeepSeek-VL2[572]use dynamicencoding strategies on visualcomponents, aiming to process images with different resolutions and generate more efficient and accurate visualrepresentations. At the same time, DeepSeek-VL2[572]also uses the MoE model with a multi-head potential attention mechanism to compress the key-value cache into a latent vector to achieve efficient reasoning.",
                "Previous work mainly uses image fusion text for training.Video-ChatGPT [573] extends the input to video and directly uses a video adaptive visualencoder combined with LLMfor training tocapture the temporal dynamics and inter-frame consistency relationships in video data,thereby enabling open conversations about video content in a coherent manner.To solvethelack ofunifiedtokenizationfor images andvideos,Video-LLaVA[574]unifies the visual representations of image and video encoding into the language feature space, making thetwo mutually reinforcing. Similarly,Chat-UniVi[575]employs asetof dynamic visual tokens to integrate images and videos, while utilizing multi-scale representations toallow the modelto grasp bothhigh-levelsemanticconcepts and low-level visual details. Youku-mPLUG[576]has made in-depth research in specific scenarios.Based on thehigh-quality Chinese video-text pairs in the Youku videosharing platform,itenhances theabilitytounderstand overalland detailed visualsemantics and recognize scene text. Unlike the previous method that requires training,SlowFast-LLaVA [577]caneffctively capture the detailed spatial semantics and long-term temporal context in the video through atwo-stream SlowFast designwithout anyaditionalfine-tuningofthevideodataachieving thesameorevenbetterresults thanthefine-tuning method.",
                "As the parameters of large models gradually decrease and the computing power of the end-side increases, highperformance end-side models are gaining momentum.Smart terminal devices such as mobile phones and PCs have strong demands for image visual processng,which puts forward higher multimodalrecognition effcts and reasoning performance requirements forthe deployment of AImodels on theend-side.TinyGPT-V[517]is built based on the Phi-2[578] smallbackbone combined with BLIP-2[567],only 8G video memory or CPU is needed for reasoning, and solving the computational effciency problems of LLaVA[513] and MiniGPT-4[579].MiniCPM-V[519] mainly provides powerful OCR capabilities forlong and diffcult images,and has alow hallucination rate, providing reliable percptionoutput.Megrez-3B-Omni[580] ensures that allstructural parameters are highly compatible with mainstream hardware through coordinated optimization of software and hardware. Its inference speed is up to $300\\%$ faster than that of models with the same precision, improving its adaptability to different end-side hardware.",
                "Similarly,there are more GUI-related works focusing onautomatic task execution on mobile phones and PCs.OmniParser[520]uses popular web page and icon description datasets for fine-tuning,significantly enhancing thedetection and functional semantic expresion capabilities of icons in screenshots. GUICourse[581] and OS-ATLAS[582]also built across-platform GUIgroundingcorpus,which brought significant performance improvements inthe understanding of GUI screenshots and enriching the interactive knowledge of GUI components.",
                "Vision Language Action Model Vision-Language-Action (VLA) model, whichtakes vision and language as inputs and generates robotic actions asoutputs,representsan importantresearch direction in the fieldofembodied intellgence. The selection of vision and language encoders in VLA models has undergone diverse development,evolving from early CNNs to Transformer architectures,and further integrating 3D vision and large language models.Early models such as CLIPort[521] used ResNet[488] to process visual inputs and combined language embeddings to generate actions,layingthefoundation for multimodalfusion.RT-1[522]introducedtheTransformer architecture,emploing EficientNet as the visualencoder and USE as the language encoder, andfused visual and language information via FiLM mechanisms, significantlyenhancing the model's generalization ability.VIMA[523]further adopted multimodal prompts,combining the ViT visual encoder andthe T5 language model to support more complex tasks.PerAct [524] innovatively used 3D point clouds as visual inputs and processed multi-view information through Perceiver IO, providing richer spatial perception for robotic manipulation. Diffusion Policy[525] combined ResNet visual encoders and Transformerlanguage models,generating actions throughdiffusion models toimprove the diversityand accuracy of action generation. SayCan [583] integrated the PaLMlanguage model with visual inputs, using the CLIP visual encoder for task decomposition. PaLM-E [526] combined the ViT visual encoder and the PaLM language model, guiding low-level action execution through text planning.MultiPLY[527]further integrated 3D information into LLMs,combining the EVA visual encoder and the LLaMA language model to provide more comprehensive planning capabilities for complex tasks.",
                "Audio Language Model Audio Language Model(ALM) uses the audio and text to build multimodal model. Speechgpt[533]built alarge-scale cross-modal speech instruction dataset SpeechInstruct and trained discrete speech representations,achieving cross-modal speech dialogue capabilities beyond expectations.LauraGPT[584],unlikethe previous sampling ofdiscrete audio tokens to represent input and output audio, proposed a novel datarepresentation thatcombines thecontinuous anddiscrete featuresofaudio,anddemonstratedexcellnt performanceonawiderangeof audio tasks throughsupervised multi-task learning.[529,585,531]convertsaudiodata into embedded representations and thenfine-tunes instructions,so that excellent performance can be achieved on various speech processing tasks through naturallanguage instructions. In order to reduce the cost of fine-tuning training, AudioFlamingo[528] quickly enhances the ability to adapt to unseen tasks through contextuallearning and retrieval based onthe audio language model.UniAudio 1.5[530]uses words orsubwords inthe text vocabulary asaudio tokens,learns these audio representations through a smallnumber of samples,and achieves cross-modaloutput without fine-tuning. In order to make the output more realistic and in line with human expectations,Qwen2-Audio[54]introducedthe DPO training method to achieve human preference alignment.",
                "Audio Vision Language Model Audio Vision Language Model (AVLM) ultilizes audio, vision, and text to unify multimodal models.Previously, we introduced some work on building multimodal models using information from two modalities.Inthe pursuitof AGI,theobstacletoachieving this goallies inthediversityandheterogeneityoftasksand modalities.A suitable approach is toallow more modalcapabilities tobe supported within aunifiedframework.Some closed-source work[586,587]has achieved excellentcapabilities across modalities such as textvision,and audio. ImageBind[588] implements jointembedding acrossix diferent modes (image,text, audio,depth, thermal,and IMU data).Panda-GPT[535]combines ImageBind's multi-modalencoder and Vicuna[589], showing zero-shot cross-modal performance in addition to images andtext.Similar work includes[539,539,536],which achieves alignment and training throughthe encoding information of vision,audio andtext.Multimodal models often require more resources to train, and UniVAL [538] trained a model with only $\\sim{0.25B}$ parameters based on task balance and multimodal curriculum learning,and used weight interpolation to merge multimodal models, maintaining generalization under out-of-distribution. NExT-GPT [542] connects LLM with multimodal adapters and diffrent diffusion decoders,and only trains a small number of parameters $(1\\%)$ of certain projection layers.",
                "Other works [543,590,544,545] have achieved input-output conversion between arbitrary modalities. Unified-IO 2[543]isthe frst autoregressive multimodalmodelthatcan understand and generate images,textaudio,and actions. It tokenizes diferent modalinputs into a shared semantic space and processes them using an encoder-decoder model. AnyGPT[59o]builds the firstlarge-scale any-to-any multimodalinstruction dataset, using discreterepresentations to uniformly processvarious modal inputs.Modaverse[545] directly aligns the output oftheLLM with the input of the generative modeltosolvethe problem that previous workrelies heavilyonthealignmentofthelatent spaceof text and non-text features,avoidingthecomplexityassociated withthe alignment of latent features.CoDi-2[44]outperforms earlier domain-specific models in tasks ike topic-based image generation,visual transformation,and audio editing.",
                "Others Humanshave explored the2D world more than the 3D world, but 3Dcan more accurately describe the shape and texture information of objects and provide richer perceptual information. PointLLM[540] uses a point cloud encoder to express geometric and appearance features, and integrates language features for two-stage training of complex point-text instructions, achieving excellent 3Dobject description and classification capabilities.Since 3D contains richer information than 2D,it alsobrings greatertraining costs.[541,591]reduces the training cost here, and MiniGPT-3D [541] uses 2D priors from 2D-LLM to align 3D point clouds with LLMs. Modal alignment is performed inacascade manner, and query expert modules are mixed to efficiently and adaptively aggregate features, achieving efficient training with smallparameter updates.LLaVA-3D[591] connects 2D CLIP patch features with their corresponding positions in 3D space, integrates 3D Patches into 2D LMM and uses joint 2D and 3D visuallanguage command adjustment to achieve a 3.5-fold acceleration in convergence speed.",
                "In order to enable intellgent agents to accurately perceive and manipulate unknown objects, Meta[592] developed NeuralFels technology,which combines vision and touch to continuously model unknown objects in 3D, more accurately estimate the posture and shapeofobjects in handheldoperations,and improve the accuracyof ignorant object operations by $94\\%$ ，"
              ],
              "raw_title": "Multimodal Models",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "7.3 Optimizing Perception Systems",
          "number": "7.3",
          "level": 2,
          "content": [
            "Perception errors,including inaccuracies, misinterpretations,and“halucinations\"(generation offalse information), pose substantial challenges to the reliability and effectiveness of LLM-based agents.Optimizing perception thus requires minimizing these errors using various strategies across model, system, and external levels."
          ],
          "raw_title": "Optimizing Perception Systems",
          "type": null,
          "children": [
            {
              "title": "7.3.1 Model-Level Enhancements",
              "number": "7.3.1",
              "level": 3,
              "content": [
                "Fine-tuning.Fine-tuning pre-trained LLMson domain-specificdata significantly improves their ability to accurately perceive and interpret relevant information. For example,fine-tuning models such as LLaVA on specific landmarks has been shown to enhance their recognition accuracy, particularly in urban navigation tasks [513,593].Moreover, techniques such as Low-Rank Adaptation (LoRA)enable more effcientfine-tuning,avoiding a substantialincrease in model complexity while stillimproving performance[109,594]. Some LLM work combined with traditional vision is also widely used.IntegratingwithYOLOS[595]onthebasis of thetheLlama-Adapter[596]architecture significantly improves the detection and positioning capability.",
                "PromptEngineering.Thedesign ofefective prompts iscrucialtoensure LLMs generate outputs that are both accurate and aligned withthe desired goals.By providing clear instructions,contextual information,and specificformatting requirements, prompt engineering minimizes misinterpretation and hallucination [597]. System prompts define the agent's role, historical prompts to providecontext from past interactions, andcustomized prompts to ensure output consistency has been shown to reduce errors significantly [597].",
                "Retrieval-Augmented Generation. Supplementing LLMs with external knowledge sources through retrieval mechanisms helpsground their responses infactualinformation,reducing the likelihood of hallucinations and improving the accuracy of perceived information [334]."
              ],
              "raw_title": "Model-Level Enhancements",
              "type": null,
              "children": []
            },
            {
              "title": "7.3.2 System-Level Optimizations",
              "number": "7.3.2",
              "level": 3,
              "content": [
                "Anticipation-Reevaluation Mechanism. In scenarios where agents face incomplete or ambiguous information, an anticipation-revaluation mechanism can enhance robustnessFor instance,in navigation tasks, agents can anticipate goal directions based on historicaldata andreevaluate their inferences when new information becomes available[598].",
                "Multi-Agent Collaboration. In multi-agent systems,structured communication and collaboration among agents can facilitate information sharing,error correction,and consensus-building,leading toa more accuratecollective perception of the environment[599].Diffrent communication topologies,such as fullyconnected,centralized, and hierarchical structures,offer varying trade-off in terms of efficiencyandrobustness[60o].InsightSee[601refines visual information through a multi-agent framework with description,reasoning, and decision-making,effectively enhancing visualinformation processngcapabilities.Similarly,HEV[602]integrates theglobalperspective information ofmultiple agents and endows RL agents with global reasoning capabilities through cooperative perception, thereby enhancing their decision-making capabilities.",
                "Agent Specialization.Assigning distinct roles andcapabilities to individual agents within a multi-agent system allows fora divisionof labor in perception, with each agentfocusing on specificaspects of theenvironment ortask.This can enhance the overall accuracy and efficiency of perception [603]."
              ],
              "raw_title": "System-Level Optimizations",
              "type": null,
              "children": []
            },
            {
              "title": "7.3.3 External Feedback and Control",
              "number": "7.3.3",
              "level": 3,
              "content": [
                "LossAgents for Optimization.Utilizing LLMs as lossagents, allows for thedynamic adjustment of loss function weights during training [604].This enables the optimization of image processing models based on complex,potentially non-differentiable objectives, including human feedback and evaluations from specialized models. This approach essentially externalizes the optimization objective,allowing the LLMto“perceive\"and adapt tocomplexcriteria[605].",
                "Human-in-the-Loop Systems.Incorporating human feedback and oversight can helpcorrect errors,guide the agent's learning process, and ensure alignment with human values and expectations [43].",
                "Content and Output Mediation. Before presenting LLM outputs to users,content mediation filters and refines these outputs.This helps prevent unexpected or harmfulbehaviors,ensuring alignment with user expectations and safety guidelines [606]."
              ],
              "raw_title": "External Feedback and Control",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "7.4 Perception Applications",
          "number": "7.4",
          "level": 2,
          "content": [
            "Theoperational efficacy of intellgent agents ispredominantly inffuenced bythree criticalfactors:modelarchitecture dimensionality,hardware infrastructure specifications,and quantization optimization methodologies.The exponential progression in model parameters—from Bert-Base's modest 110M to GPT-3's substantial 175 billion, culminating in Llama 3's unprecedented 405 billion—has correspondingly escalated processing latency from milliseconds to hundreds ofmillseconds.Hardware performance variations are particularly noteworthy; empiricalevidencewith GPT-3 demonstrates that NVIDIA H100 exhibits a $50\\%$ improvement in token processing throughput compared to A100, while RTX 4090 achieves approximately double the processing capability.",
            "Contemporary intelligent agents have penetrated diverse domains,encompassing personalassistance systems, gaming environments, Robotic Process Automation (RPA),and multimediacontent generation,predominantly leveraging visual perception as their primary input modality. In the context of procedurally generated environments like Minecraft, STEVE [6o7] demonstrates remarkable performance improvements, achieving a $1.5\\mathrm{x}$ acceleration in technology tree progression and a $2.5\\mathrm{x}$ enhancement in block search effciency through visual information processing. Steve-Eye [608] advances this paradigm through end-to-end multimodal training, addresing environmental comprehension latency through integrated visual-textual input processing.",
            "In creative content generation, AssistEditor[609] exemplifies sophisticated multi-agent collaboration,facilitating professional video editing through style-driven content understanding. Similarly, Audio-Agent [610] implements cross-modalintegration between textual/visual inputs and audio outputs,enabling comprehensive audio manipulation capabilities [611, 612, 613].",
            "Mobile and desktop platforms have witnessed significant advancements in agent applications. ExACT [614] has established new state-of-the-art benchmarks in VisualWebArena [615], achieving a $33.7\\%$ Success Rate through screenshot-based exploratory learning withcaption and Set of Mask integration.SPA-Bench [616]introduces acomprehensive mobile evaluationframeworkthat authenticalyreplicates real-world complexity.M3A [617] demonstrates superior performance with a $64.0\\%$ success rate in SPA-Bench through multimodal input processing. AgentStore [618] has markedly improved OsWorld PC benchmark performance to $23.85\\%$ through enhanced visual and accessibility tree processing.",
            "Voice interactioncapabilities[619,586] in personal AIasistants have significantlyreduced interactionfrictionwhile enhancingoperationaleffciency.The integrationofemotionalprosody invoice interactions has demonstrated increased user engagement and retention.",
            "In embodied intelligence applications,haptic andforce feedback mechanisms have emerged ascrucial modalities for environmentalinteraction, with enhanced sensory fidelityenabling increasingly precise operational capabilities[620]."
          ],
          "raw_title": "Perception Applications",
          "type": null,
          "children": []
        },
        {
          "title": "7.5 Summary and Discussion",
          "number": "7.5",
          "level": 2,
          "content": [
            "Although more and more research works[543,590] focus on building unified multimodal models to support the input and outputofmultiple perceptioncapabilities.Agent perception,acomerstone ofautonomous systems,faces significant challenges in effectively interpreting and integrating multi-modal data.Current methodologies encounter persistent issues inrepresentation learning,alignment, and fusion,which hinder thedevelopment of robust and generalizable perception systems.",
            "One of the primary issues lies in the representation methods employed, whichoften fail to capture the intricate nuances of multi-modal dataThis shortfallis particularly evident in scenarios where high-dimensional sensory inputs require asophisticatedabstraction thatpreserves criticalsemantic information.Furthermore,thealignmentof representations presents additional diffculties.Integrating heterogeneous datatypes intoacohesive feature space is not only computationally intensive butalso prone to inconsistencies,whichcan lead to misinterpretation of ambiguous signals.Thechallenge iscompounded whenattempting tofusethesediverserepresentations,asthe processof merging features from various sources frequently results in suboptimal integration and potentialloss of vital information.",
            "Future research directions should prioritize adaptive representation learning through dynamic neural architectures capable ofautomatically adjusting their structurebased on environmentalcontext and taskdemands.This could involve meta-learned parameterization or graph-based representations that explicitly modelrelationships between perceptual entities.For cross-modal alignment, self-supervised spacetime synchronization mechanisms leveraging contrastive learning principles show promise in establishing dense correspondence withoutrequiring exhaustive labeled data.The integration ofcausalinference frameworks into alignment processes[621]could furtherenhancerobustnessagainst spurious correlations. Inrepresentation fusion,hierarchicalatention mechanisms with learnable gating functions merit deeper exploration to enable context-aware integration of complementary modalityfeatures.Emerging techniques in diferentiable memory networks may provide new pathways for maintaining and updating fused representations over extended temporal horizons."
          ],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "8.1 The Human Action System 86",
      "number": "8.1",
      "level": 2,
      "content": [
        "8.2 From Human Action to Agentic Action 87\n8.3 Paradigms of Agentic Action System . 88\n8.3.1 Action Space Paradigm 88\n8.3.2 Action Learning Paradigm 91\n8.3.3 Tool-Based Action Paradigm 93\n8.4 Action and Perception: “Outside-In” or“Inside-out” 95\nSummary and Discussion 97"
      ],
      "raw_title": "The Human Action System 86",
      "type": null,
      "children": []
    },
    {
      "title": "[Self-Evolution in Intelligent Agents 101",
      "number": "",
      "level": 1,
      "content": [
        "Optimization Spaces and Dimensions for Self-evolution 103\n9.1Overview of Agent Optimization 103\n9.2  Prompt Optimization 103\n9.2.1 Evaluation Functions 104\n9.2.2 Optimization Functions . 104\n9.2.3 Evaluation Metrics 105\n9.3Workflow Optimization 105\n9.3.1 Workflow Formulation 105\n9.3.2 Optimizing Workflow Edges 106\n9.3.3 Optimizing Workflow Nodes 106"
      ],
      "raw_title": "[Self-Evolution in Intelligent Agents 101",
      "type": null,
      "children": []
    },
    {
      "title": "9.4 Tool Optimization 107",
      "number": "9.4",
      "level": 2,
      "content": [
        "9.4.1 Learning to Use Tools 107\n9.4.2 Creation of New Tools 107\n9.4.3 Evaluation of Tool Effectiveness 108\nTowards Autonomous Agent Optimization 110"
      ],
      "raw_title": "Tool Optimization 107",
      "type": null,
      "children": []
    },
    {
      "title": "10 Large Language Models as Optimizers",
      "number": "10",
      "level": 1,
      "content": [],
      "raw_title": "Large Language Models as Optimizers",
      "type": null,
      "children": [
        {
          "title": "10.1 Optimization Paradigms",
          "number": "10.1",
          "level": 2,
          "content": [
            "Traditionaloptimization methods differintheir assumptions aboutobjective function accessibility.We categorize them into three broadclasses,each with an expanding levelof input space: gradient-based optimization, which relies on explicit function gradients; zeroth-orderoptimization, which operates without gradient information; and LLM-based optimization, which extends beyond numericalfunctions tooptimize over structured and high-dimensional input spaces.",
            "· Gradient-Based Optimization. These methods assume access to gradient information and iteratively refine parameters.Techniques such as stochastic gradient descent (SGD)and Newton's method[801] are widely used but require differentiability,limitingtheir applicability todiscrete problems like prompt tuning and structured decision workflows, often endowed with a graph structure.\n·Zeroth-Order Optimization.These methods bypassthe need for explicit gradients by estimating search directions from function evaluations [802]. Examples include Bayesian optimization [803], evolutionary strategies [804], and finite-difference methods[805],which are effctive when gradients are unavailable or expensive tocompute. However,they stillrelyon well-defined numericalobjectives and structured search spaces, which constrains their applicability to language-based tasks.\n·LLM-Based Optimization. LLMs optimize broader solution spaces by leveraging natural language as both the optimization domain and feedback mechanism.By incorporating structured reasoning and human-like iteration, LLMs excelinrefining prompts, generating adaptive workflows,and iteratively improving task performance based on user feedback.",
            "While gradient-based and zeroth-order methods are typically applied to numerical objectives, their core principles, such as iterative refinement, searchheuristics,and adaptivelearning,alsounderlie LLM-based optimization strategies. Building on these insights,wehighlight arapidlyemerging classof LM-based optimization powered byreinforcement learning,whichhas become the backbone of slow thinking reasoning models[90,806,89].As these modelscontinue to evolve, we anticipate them driving the next wave of agentic applications,enabling LLMs to navigate complex environments with greater adaptability and strategic foresight."
          ],
          "raw_title": "Optimization Paradigms",
          "type": null,
          "children": []
        },
        {
          "title": "10.2 Iterative Approaches to LLM Optimization",
          "number": "10.2",
          "level": 2,
          "content": [
            "Some LLM-based optimization methods directly draw inspiration from classcal optimization theory by adapting key components toaddress discrete and structuredchallnges.Acentralcharacteristic of these approaches is the iterative update step,in which model-generated modifications are selectedfrom arange of possible improvements to refine the objective.Using the prompt optimization objective from Equation(9.1)as arunning example,a generaliterative",
            "![](images/a504d5e4c2a1b73d57f402c35cd0fb6d1b71c8c4a357ede99bfcec139ea91e5c.jpg)",
            "Figure 10.1: A taxonomy of LLM-based optimization methods,categorized into random search, gradient approximation, and surrogate modeling.We also highlight some theoreticalexplanations of in-context learning, which includes hypothesis learning,implicit Bayesian inference, and mechanistic interpretability, which underpin the optimization capabilities of LLMs.",
            "algorithm can be expressed as follows:",
            "Sample: $T\\sim\\mathcal{D}$ Evaluation: $\\ensuremath{\\mathcal{L}}(T;T_{p})\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}),T)$ Update: $T_{p}^{\\prime}\\gets\\phi_{\\mathrm{opt}}\\left(\\mathcal{L}(T;T_{p})\\right)$",
            "Here, the Sample and Update steps aredefined based on the agent's task.Inthe simplest case,suchasoptimizing an instruction for binary classification of movie reviews, the objective $\\mathcal{L}$ is measured by classification accuracy. In more complex agentic workflows,the decision variable may include prompts at different workflow stages,tool selections, agent topologies,ora combination thereof.As discussed in Chapter 9,a common characteristic of these decision variables is their combinatorial nature-such as the set of all strings from an LLM's vocabulary $\\nu$ or all possible role assignments foragents ina workflow.Since enumerating allpossible solutions isoften intractable, this necessitates designing approximate update steps $\\phi_{\\mathrm{opt}}$ , which we discuss next.",
            "· Random Search. Early LLM-based optimization methods leveragedrandom search variants to optimize prompts in discrete naturallanguage spaces[774,807,651,732,808,809,810].These methods often resembleevolutionary algorithms that iteratively sample candidate decision variables and select the top-performing ones from each iteration. The general formulation follows:",
            "Sample: $T\\sim\\mathcal{D}$ Evaluation: $\\mathcal{L}^{(i)}\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}^{(i)}),T),\\quad i=1,\\ldots,M$ Update: {T(k)}k=1 $\\{T_{p}^{(k)\\prime}\\}_{k=1}^{K}\\leftarrow\\mathrm{ArgTopK}_{i\\in[M]}\\mathcal{L}^{(i)},$ Replenishment (Optional): $\\{T_{p}^{(j)}\\}_{j=K+1}^{M}\\sim\\mathrm{Mutate}(\\{T_{p}^{(k)}\\}_{k=1}^{K}).$",
            "We briefly override previous notations and let $M$ denote the total number of candidate prompts sampled per iteration, and $K$ (with $K<M$ ） control the number of top-performing candidates-selected with ArgTopK in our algorithm-retained for the next step.This algorithm can optionally incorporate a replenishment step to maintain diversity in the candidate pool acrossiterations.Random search methods are simple to implement, highly paralelizable, and particularly effective for single-prompt workflows.Beyond prompt optimization,they have also demonstrated strong performance in selecting in-context demonstrations [811,812].However, their efficiency comes at a cost—each iteration requires $O(M)$ parallel API queries, which can become prohibitively expensive for complex workflows involving multiple queries.",
            "·Gradient Approximations.Several methods approximate gradient-based updates by iterativelyrefining solutions. Forinstance,[779,730,728] generate refinements at different workflow stages. StraGO[722]estimates descent directions using central-difference heuristics, while Trace [813]optimizes composed programs by modeling them as computation graphs, similar to backpropagation. The key analogy between gradient updates in continuous optimization and prompt-space refinement is the concept of a“descent direction\"-a systematic modification of the decision variable to improve the objective.In contrast, random search methods propose new decision variables independently at each step, without accessing past update trajectories. Gradient-based approaches, by contrast,exploit this historicalinformation,often leading tofasterconvergence.A generaliteration for gradient approximation methods is given below:",
            "$$\n\\begin{array}{r}{\\mathbf{Sample:}T^{(i)}\\sim\\mathcal{D},\\quad i=1,\\dots,M\\qquad}\\\\ {\\mathbf{Evaluation:}\\mathcal{L}^{(i)}\\leftarrow\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}),T^{(i)}),\\quad i=1,\\dots,M}\\\\ {\\mathbf{Gradient\\Approximation:}g\\leftarrow\\nabla_{\\mathrm{LLM}}\\mathbf{Agg}\\left(\\mathcal{L}^{(1)},\\dots,\\mathcal{L}^{(M)}\\right)}\\\\ {\\mathbf{Update:}T_{p}^{\\prime}\\leftarrow\\phi_{\\mathrm{opt}}(T_{p},g),\\qquad}\\end{array}\n$$",
            "where $M$ is the minibatch size, $\\operatorname{Agg}({\\mathord{\\cdot}})$ is an aggregation function that combines feedback signals (e.g., in numerical optimization, Agg is typically the average operator), $\\nabla_{\\mathrm{LLM}}$ represents an abstract“LLM-gradient operator\" [728] that generates textualrefinement directions based onthe feedback signaland thecurrnt minibatch(e.g.,the agent should consider the edge case of ...). Additionaly, $\\phi_{\\mathrm{opt}}$ can be instantiated as an LLM query, allowing the agent to update its prompt based on $g$",
            "Compared to random search methods, gradient-based approaches offer two key advantages: they enable the incorporation of past refinement directions into $\\phi_{\\mathrm{opt}}$ , analogous to momentum-based techniques in first-order optimization algorithms[814,815],andthey facilitate backpropagation-liketechniques for optimizing computation graphs [651,813,780],making them particularly effective for multi-stage workflows with interdependent optimizable modules.However, this flexibility comes at the cost of increased design overhead,such as the need for meta-prompts to aggregate feedback and apply refinement directions. We further discuss the feasibility of using LLMs to optimize these hyperparameters below.Some approaches also explored direct gradient-based optimization of soft prompts[816,817,818].While effective forsimple input-output sequence learning,these methods struggle with multi-step workflows and sequential decision-making [630, 300].",
            "Finally,while these methods leverage first-order optimization insights,the extension of second-order techniques (e.g.quasi-Newton methods)to LLM-based optimization remains largely unexplored.Fortunately,recent works such as Revolve[780]have taken a step in this direction by introducing a structured approach for second-order optimization, modeling the evolution ofresponse patterns over multiple iterations.Byincorporating higher-order refinements,Revolve enables more stable and informed optimization, efectively mitigating stagnation in complex tasks.We are also excited by emerging trends in leveraging inference-time compute [90, 89] to incorporate historical refinement directions and investigate the benefits of momentum.",
            "·Bayesian Optimization and Surrogate Modeling. While the aforementioned approaches achieved significant progress in LLM-based optimization,they often entail substantial financial and environmentalcosts due to the high number of required LLM interactions. Moreover, these methods can be sensitive to noise, and the optimization landscape of discrete prompts,among other decision variables,remains poorly understood[819,820].Under these constraints, Bayesian Optimization (BO)emerges as acompelling alternative, as it buildsa noise-resilient surrogate model of the optimization objective:",
            "Sample: T \\~ D Proposal: $\\{T_{p}^{(i)}\\}_{i=1}^{M}\\sim S$ Propose Evaluation: $\\mathcal{L}^{(i)}\\gets\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,T_{p}^{(i)}),T),\\quad i=1,\\hdots,M$ Update $:S\\gets S.\\mathrm{UpdatePrior}(\\{\\mathcal{L}^{(i)}\\}_{i=1}^{M},\\{T_{p}^{(i)}\\}_{i=1}^{M}),$ where $S$ represents a probabilistic surrogate model of the optimization objective,equipped with a proposal operator (e.g.,posterior sampling from a Gausian Process BO procedure[803])and an update mechanism based on observed evidence from prompt evaluations. For instance, MIPRO[821] employs a Tree-Structured Parzen Estimator as its surrogate[822],while PROMST[823] trains a score-prediction model to guide prompt tuning. Leveraging a surrogate model for LLM-based optimization aligns with the emerging trend ofamortized optimization for nondiffrentiable objectives [824].For instance,[825] trains a prompt-generator LLM to amortize the computational cost of instantiating a beam search problem for discovering jailbreak attack prefixes.",
            "Finaly,severalother works fit anadditional lightweight module-such as a Bayesian belief posterior or a utility function-from LLM outputs,to aid the optimization of domain-specific workflows,such as decision-making and multi-agent negotiations [826,827].This type of amortized methods-those that fit aparameterized model that is reusable for unseen inputs-have found increasing usage in LLM-based optimization,such as jailbreaking [828,825]."
          ],
          "raw_title": "Iterative Approaches to LLM Optimization",
          "type": null,
          "children": []
        },
        {
          "title": "10.3 Optimization Hyperparameters",
          "number": "10.3",
          "level": 2,
          "content": [
            "Similar to traditional optimization, LLM-based methods are highlysensitive tohyperparameters that influence search effciency and generalization.A key consideration in gradient-based LLMoptimizers is thechoice of the aggregation function $\\operatorname{Agg}({\\mathord{\\cdot}})$ , which determines how textual feedback is synthesized to guide prompt updates. An improper choice can leadto loss of critical information or misalignment in iterative refinements.Additionally,[813]introduces a \"whiteboard\" approach, where an LLM program is decomposed into human-interpretable modules.However, design choices in structuring such modular workflowsremainlargely unexplored, which poses an openchallenge foroptimizing LLM-driven decision-making pipelines.",
            "Hyperparameters in LLMoptimization often parallel those in numerical optimization.For example,batch size plays a crucialrole: justas minibatch updates enhance stability and effciency inclasscaloptimization,LLM-based approaches like TextGrad[728]aggregate feedback across multiple generated samples before making updates.Another key factor is momentum-while it stabilizes updates in gradient-based methods by incorporating past gradients, LLM-based optimizers similarlyleverage historicalrefinements to improve performance over time[728,813].Despite progressin numerical optimization,hyperparameter selection for LLM-based optimizers remains largelyheuristic,often relying on ad hoc, trial-and-error tuning.",
            "In agentic system design,hyperparameters proliferate across various components,including role assignments of agents, selection of in-context demonstrations,and scheduling of tool invocations.Each of these choices has a profound impact on downstream performance, yet principled methods for optimizing them remain underdeveloped. While traditionalhyperparameter tuning techniques,such as grid searchandBayesian optimization,can be appliedtodiscrete LLM-driven workflows,their computational cost scales poorly due to the high variance in language model outputs. Additionally,thecombinatorialnatureofthesehyperparameters,where agent configurations,prompting strategiesand reasoning structures interact in complex ways, makes an exhaustive search infeasible.Recent work has atempted to address thischallenge byembedding agenticworkflows into structuredframeworks such as finitestate machines [729], optimaldecisiontheory[826],andgametheory[827].However,these approachesoftenfailto generalize acrossdiverse environments. A promising direction for addressing these challenges is meta-optimization, where LLMs are used to optimize their own hyperparameters and decision-making strategies.For example, an LLM-based optimizer can iterativelyrefine itsown prompting strategies bytreatingpast decisions asexperience,akintolearnedoptimizers in deep learning [829].Moreover, amortized approaches train auxiliary models to predict effective hyperparameters, which canreduce thecomputationalcost ofexhaustive search[821,823].While thesetechniques offerexciting possibilities, they also introduce newchallnges,such as balancing exploration with exploitation in adaptivetuning and ensuring generalization across diverse optimization tasks.Investigating principled meta-optimization strategies tailored to LLM-driven workflows remains a critical area for future research."
          ],
          "raw_title": "Optimization Hyperparameters",
          "type": null,
          "children": []
        },
        {
          "title": "10.4 Optimization across Depth and Time",
          "number": "10.4",
          "level": 2,
          "content": [
            "Unlike conventional optimizers that update parameters ina static setting, LLMs optimize workflows dynamically, considering both depth (single-pass workflows)andtime(recurrentupdates).In terms ofdepth,LLMsfunction similarly to feedforward networks, sequentially optimizing workflows as they passthrough different modules—most existing LLM-based optimizers follow this paradigm.Beyond single-pass execution, LLMs can also optimize over time, akin to recurrent architectures such as RNNs or Universal Transformers [830], by iteratively refining decision-making. For instance, StateFlow [729] enhances workflows by incorporating feedback across multiple iterations, enabling dynamic refinement and adaptation over time.Whilethese analogies are compelling,many wellestablishedengineering optimization techniques—such as checkpointing [831] and truncated backpropagation[832]—remain underexplored in LLM-based optimization. We seethis as a promising avenue for future research,echoing previouscall for deeper investigation [813]."
          ],
          "raw_title": "Optimization across Depth and Time",
          "type": null,
          "children": []
        },
        {
          "title": "10.5 A Theoretical Perspective",
          "number": "10.5",
          "level": 2,
          "content": [
            "Recent studies suggest that transformers inherently perform optimization-like computations,supporting their potential as general-purpose optimizers forcomputational workflows.However,asignificant gapremains betweentheir empirical success and theoretical understanding.Here, we provide a brief overview of recent progressin bridging this gap.",
            "·In-Context Learning.A fundamental perspective on transformers as optimizers emerges from in-context learning, particularly in few-shot settings [2].[733] demonstrated that transformers can in-context learn diverse regression hypotheses,including regularized linear models,decision trees, and shallow neural networks. Building on this, later works[734,833,735] provided constructive proofs that transformers can implement iterative optimization algorithms,suchasgradientdescentandsecond-orderupdates.However, whilethese theoreticalmodelscharacterize transformersoptimization capabilities, they do not fully explain in-context learning in large-scale LLMs, which operate in discrete input-output spaces. Empirical analyses [819,834,820] instead sought to understand how pre-trained LLMs generalize in-context.[834] proposed that in-context learning resembles a hidden Markov model (HMM) performing implicit Bayesian inference, while [819,820] challenged the conventional view that in-context demonstrations serve as new test-time samples for hypothesis formation. In-context learning remains the central emergent ability[835] enabling self-improvement and optimization from context, yet it continues to elude comprehensive theoretical analysis.\n·Mechanistic Interpretability.Paraleltotheoreticalanalyses,mechanistic interpretability aims touncover interal transformer computations by identifying subgraphs, also known as circuits,responsible for specific behaviors. Early studies mapped circuits for stylized language tasks in pre-trained GPT-2 models [836,837,838],while more recent efforts have scaled up by identifying semantically meaningful features using sparse autoencoders [839, 736,840, 841].These methods have been largely successful in eliciting causal and controllable behavior from frontier-class LLMs,but they alsorevealanunintendedconsequence: in-context learning capabilitiesoften entangle beneficial generalization with harmful behaviors when conditioned on many-shot demonstrations[842].This raises challenges for optimizing LLM workflows safely and reliably.\n·Limitations Under Uncertainty. While LLMs demonstrate moderate capabilities in sequential decision-making when provided with in-context information, theystruggle to make optimalchoices under uncertainty[843,844,845. 846].In particular,[826] found that LLM-based optimizers exhibit diffculty in adapting to stochastic environments, often failing toexplore optimally.These findings serve asa cautionary notefor deploying LLM-based optimizers in dynamic or uncertain settings where exploration and robust decision-making are critical.\nLMs redefine optimization by integrating structured reasoning,naturallanguage processing, and in-context learning,",
            "expanding beyond traditional numerical methods.Despite strong empirical performance in structured search spaces, open questions remain about thetheoretical underpinnings of LLM-based optimization,particularlytheemergence of in-context learning from standard gradient-based training."
          ],
          "raw_title": "A Theoretical Perspective",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "111",
      "number": "111",
      "level": 1,
      "content": [
        "10.1 Optimization Paradigms 111\n10.2 Iterative Approaches to LLM Optimization 111\n10.3 Optimization Hyperparameters 114\n10.4 Optimization across Depth and Time 114\n10.5 A Theoretical Perspective. 115"
      ],
      "raw_title": "",
      "type": null,
      "children": []
    },
    {
      "title": "Online and Offline Agent Self-Improvement 116",
      "number": "",
      "level": 1,
      "content": [
        "11.1 Online Agent Self-Improvement 116\n11.2 Offline Agent Self-Improvement 117\n11.3 Comparison of Online and Offline Improvement 118\n11.4 Hybrid Approaches 118"
      ],
      "raw_title": "Online and Offline Agent Self-Improvement 116",
      "type": null,
      "children": []
    },
    {
      "title": "12 Scientific Discovery and Intelligent Evolution 120",
      "number": "12",
      "level": 1,
      "content": [],
      "raw_title": "Scientific Discovery and Intelligent Evolution 120",
      "type": null,
      "children": [
        {
          "title": "12.1 Agent's Intelligence for Scientific Knowledge Discovery 120",
          "number": "12.1",
          "level": 2,
          "content": [
            "12.1.1 KL Divergence-based Intelligence Measure 120\n12.1.2 Statistical Nature of Intelligence Growth 122\n12.1.3 Intelligence Evolution Strategies . 123\n12.2 Agent-Knowledge Interactions 123\n12.2.1 Hypothesis Generation and Testing 124\n12.2.2 Protocol Planning and Tool Innovation . 126\n12.2.3 Data Analysis and Implication Derivation 126\n. Technological Readiness and Challenges . 127\n12.3.1 Real-World Interaction Challenges . 127\n12.3.2 Complex Reasoning Challenges 128\n12.3.3 Challenges in Integrating Prior Knowledge 129"
          ],
          "raw_title": "Agent's Intelligence for Scientific Knowledge Discovery 120",
          "type": null,
          "children": []
        },
        {
          "title": "12.1 Agent's Intelligence for Scientific Knowledge Discovery",
          "number": "12.1",
          "level": 2,
          "content": [
            "Knowledge,traditionally defined as justifiedtrue belief, traces back to Plato[860]and has beenfurtherrefined by Edmund Gettier [861], who argued that knowledge must be produced by a reliable cognitive process-though its precise definition remains debated[862].Inour discussion, we describe scientific knowledge discovery as the process of collecting data and information to either justifyorfalsify rationalhypotheses about target scientific problems.To discussthecapabilityof agents in scientificknowledge discovery,we firstexplore ageneralframework for measuring an agent's intelligence through the lens of information theory."
          ],
          "raw_title": "Agent's Intelligence for Scientific Knowledge Discovery",
          "type": null,
          "children": [
            {
              "title": "12.1.1 KL Divergence-based Intelligence Measure",
              "number": "12.1.1",
              "level": 3,
              "content": [
                "The agent's intellgence can be measured by the KL divergence between its predicted and real-world probability distributions of unknown information. A long-standing goal in both artificial intelligence and the philosophy of science istoformalizewhatit means for an agent to“understand\"theworld.From Jaynes'viewofprobability theory as extended logic forreasoning under uncertainty[863], toParret al.'s framing of intelligence as minimizing model-world divergence underthe freeenergy principle[864],many frameworksconverge onacommontheme: intelligent behavior arisesfrom making accurate predictions about an uncertain world. Clark[344],for instance, argues that intelligent agents constantly engage with the world through prediction and error correction to reduce surprise.Cholet[865] emphasizes that intelligence shouldreflectskillacquisitionefficiency,because of thedynamicnature oftaskadaptation. Together,these views suggest that inteligence involves building predictive and adaptable models—anidea formalized here through aprobabilistic framework thatlinks reasoning to knowledge acquisition and enablescomparison across agents in scientific discovery.",
                "Building onthis foundation,weconsider intellgence in the specificcontextof scientificknowledge discovery,where the agent's primaryobjective is to infer unknown aspects of the physical worldfrom limited data.From the agent's perspective in knowledge discovery, the world $\\boldsymbol{\\mathcal{W}}$ is characterized by an ensemble of datasets $\\mathbf{x}=\\{x_{1},x_{2},...,x_{n}\\}$ related to the scientific problem the agent aims to understand. During the agent's interaction with $\\mathcal{W}$ , each dataset appears in the experimental measurements or observations with a probability $P_{\\mathcal{W}}(\\mathbf{x})$ . Here we assume that individual data points $x_{i}$ may or may not be correlated. For example, in a task of text generation using a language model, $x_{i}$ represents a chunk of tokens forming a meaningful proposition, and $\\mathbf{x}$ is a coherent text constructed from known and inferred propositions. In this context, the“world\" is the ensemble of all propositions.",
                "Let $\\theta$ denote the parameter that parameterizes the agent's world model, $M_{t}^{\\mathrm{wm}}$ , as defined in Table 1.2. For instance, in a transformer model with a fixed architecture, $\\theta$ represents its weights. Given $\\theta$ and a dataset $\\mathbf{x}$ , the agent predicts a probability distribution $P_{\\theta}(\\mathbf{x})$ . In general, different AI agents could be optimized for different goals. For scientific knowledge discovery,we assumethatthe agent'sgoalis toproduceagooddescriptionofthe realworld,i.e.a world model that predicts yet-to-be-explored natural phenomena as accurately as posible.A more intellgent agent produces a better approximation of the real-world distribution $P_{\\mathcal{W}}(\\mathbf{x})$ . The agent's intelligence can thus be measured by the KL divergence, or relative entropy, between these two probability distributions:",
                "$$\nD_{0}(\\theta)=\\sum_{{\\bf x}\\subseteq\\mathcal{W}}P_{\\mathcal{W}}({\\bf x})\\log\\frac{P_{\\mathcal{W}}({\\bf x})}{P_{\\theta}({\\bf x})}\n$$",
                "$D_{0}(\\theta)$ describes the difference between $P_{\\mathcal{W}}(\\mathbf{x})$ and $P_{\\theta}(\\mathbf{x})$ . More precisely, in the context of hypothesis testing, if we sample $P_{\\mathcal{W}}(\\mathbf{x})~N$ times and compare the results with the predictions from $P_{\\theta}(\\mathbf{x})$ , the probability of mistaking $P_{\\mathcal{W}}(\\mathbf{x})$ for $P_{\\theta}(\\mathbf{x})$ scales as $e^{-N D_{0}(\\theta)}$ [866]. Inotherordsanagentwithalower $D_{0}(\\theta)$ produces predictions thatalign more closely with reality.",
                "Forexample,considertwomaterials synthesis agentswhose goal, $M_{t}^{g o a l}$ ,is to understandwhetherornotaninoganic compound of interest, $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ , is synthesizable. The agents can predict either (1) ${\\bf x}_{\\mathrm{1}}{=}\\{\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ is synthesizable), and (2) $\\scriptstyle\\mathbf{x}_{2}=\\{\\mathbf{CaFe}_{2}(\\mathbf{PO}_{4})_{2}\\mathbf{O}$ is not synthesizable). In reality, since $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ is a natural mineral, $\\dot{P}_{\\mathcal{W}}(\\mathbf{x}_{1})=1$ and $P_{\\mathcal{W}}(\\mathbf{x}_{2})=0$ . However, this mineral was only recently reported on October 4, 2023[refl, after the knowledge cutoff of many LLMs; thus, the agents lacks that knowledge. Compare Agent 1, which guesses randomly $P_{\\theta_{1}}(\\mathbf{x}_{1}){\\bf{\\bar{\\alpha}}}=P_{\\theta_{1}}(\\mathbf{x}_{1})=0.5$ ，yielding $D_{0}(\\theta_{1}\\bar{)\\ =\\ }\\log2$ . In contrast, Agent 2 uses first-principles calculations and finds that $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ (assume structure is xx [cite: Materials Project ID]) is the lowest-energy phase among its competitors [ref], indicating stability. Thereby, Agent 2 predicts that $\\mathrm{CaFe_{2}(P O_{4})_{2}O}$ is likely synthesizable, suggesting $P_{\\theta_{2}}(\\mathbf{\\bar{x}}_{1})>0.5>P_{\\theta_{2}}(\\mathbf{x}_{2})$ . Consequently, $\\dot{D}_{0}(\\dot{\\theta}_{2})=-\\log P_{\\theta_{2}}(\\mathbf{x}_{1})<D_{0}(\\theta_{1})$ , meaning that Agent 2 has a more accurate understanding of the real world.",
                "Now,let us assume the agenthas conducted some measurements and determined specificvalues fora subset of data points $x_{i}$ . Let $\\mathbf{x}_{\\mathrm{K}}$ denote this known subset and $\\mathbf{x}_{\\mathrm{U}}$ the remaining unknown part. Correspondingly, we define the space of all existing knowledge as $\\kappa$ and the space of all unknown information as $\\mathcal{U}$ ，satisfying $\\mathbf{x}_{\\mathrm{K}}\\subseteq{\\mathcal{K}}$ $\\mathbf{x}_{\\mathrm{U}}\\subseteq\\mathcal{U}$ and $\\kappa\\cup\\mathcal{U}=\\mathcal{W}$ . For example, in text generation, the the prompt text $\\mathbf{x}_{\\mathrm{K}}$ represents already known information. The efficiency of the language model is then measured by its predictive accuracy for the generated text $\\mathbf{x}_{\\mathrm{U}}$ based on $\\mathbf{x}_{\\mathrm{K}}$ More generaly,the agent'sintelligence is measuredbytherelativeentropyof theconditional probabilitydistribution:",
                "$$\nD_{\\mathrm{K}}(\\boldsymbol{\\theta},{\\bf x}_{\\mathrm{K}})=\\sum_{{\\bf x}\\subseteq\\mathcal{U}}P_{\\mathcal{W}}\\left({\\bf x}|{\\bf x}_{\\mathrm{K}}\\right)\\log\\frac{P_{\\mathcal{W}}\\left({\\bf x}|{\\bf x}_{\\mathrm{K}}\\right)}{P_{\\boldsymbol{\\theta}}\\left({\\bf x}|{\\bf x}_{\\mathrm{K}}\\right)}\n$$",
                "In practice, all of the agent's knowledge is stored in its memory $M_{t}^{\\mathrm{mem}}$ ,i.e., $\\mathbf{x}_{\\mathrm{K}}={\\boldsymbol{K}}=M_{t}^{\\mathrm{mem}}$ and $\\mathcal{U}=\\mathcal{W}\\setminus M_{t}^{\\mathrm{mem}}$ we define the agent's intelligence as:",
                "$$\nI Q_{t}^{\\mathrm{agent}}\\equiv-D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})=-\\sum_{\\mathbf{x}\\subseteq\\mathcal{U}}P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}{P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}\n$$",
                "In other words,thethe agent'sintelligence $I Q_{t}^{\\mathrm{agent}}$ is determined by its memory $M_{t}^{\\mathrm{mem}}$ and the parameter $\\theta$ of its world model $M_{t}^{\\mathrm{wm}}$ . A schematic plot is shown in Figure 12.1. At time $t=0$ , when the $M_{t}^{\\mathrm{mem}}$ is very limited or lack relevant information toa new target scientific problem, $I Q_{t}^{\\mathrm{agent}}$ is primarily determined by the zero-shot predictive ability of $M_{t}^{\\mathrm{wm}}$ ,corresponding to fluid intelligence [867]. Over time, as more relevant knowledge is incorporated into $M_{t}^{\\mathrm{mem}}$ ， $I Q_{t}^{\\mathrm{agent}}$ becomesincreasinglydependentontheknowledge-augmentedpredictivecapabilityof $M_{t}^{\\mathrm{wm}}$ reflecting crystallized intelligence [868].",
                "![](images/47374d7245528b34b2d026b34dae42125569a091e0f59fd0421a26a4c6027972.jpg)",
                "Figure 12.1: Schematic representation of agent intelligence and knowledge discovery.The agent's intelligence, measured by the KL divergence $D_{\\mathrm{K}}$ between predictions and real-world probability distributions, evolves from fluid intellgence (zero-shot predictions for new problems)tocrystallzed intelligence (knowledge-augmented predictions after learning) as it accumulates data in its memory $M_{t}^{\\mathrm{mem}}$ over time $t$ . Given $M_{t}^{\\mathrm{mem}}$ , the evolution of $D_{\\mathrm{K}}$ varies within the world model's parameter space $\\Theta$ , as illustrated by $\\theta_{1}$ and $\\theta_{2}$ in the solid lines. The expressive limitation of $\\Theta$ is characterizedbytheenvelope $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}$ . Given $\\Theta$ $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}$ is influenced bydifferentknowledgeexpansionstrategies, such as $^1M_{t}^{\\mathrm{mem}}$ and $^{2}M_{t}^{\\mathrm{mem}}$ , shown as dash lines."
              ],
              "raw_title": "KL Divergence-based Intelligence Measure",
              "type": null,
              "children": []
            },
            {
              "title": "12.1.2 Statistical Nature of Intelligence Growth",
              "number": "12.1.2",
              "level": 3,
              "content": [
                "The agent's intelligence, in a statistical sense, is a non-decreasing function of acquired knowledge. Roughly speaking, $I Q_{t}^{\\mathrm{agent}}$ quantfiesbottheamountofknowledgeanagenthaacquiredandhoweffctivelytheagentcan apply that knowledge after learning from $M_{t}^{\\mathrm{mem}}$ . Intuitively, if the agent gains additional information at time $t{\\mathrm{.}}$ which corresponds to enlarging $M_{t}^{\\mathrm{mem}}$ and shrinking $\\mathcal{U}$ its intelligence should increase.",
                "To understand this process, consider a small region $\\Delta\\subseteq{\\mathcal{U}}$ and examine the effect of adding a dataset $\\mathbf{x}_{\\Delta}$ from $\\Delta$ to $M_{t}^{\\mathrm{mem}}$ . Denote $\\mathcal{U}=\\mathcal{U}^{\\prime}\\cup\\Delta$ , where $\\mathcal{U}^{\\prime}$ represents theremaining unknown part oftheworld.The agent's intelligence at time $t+1$ is given by:",
                "$$\nI Q_{t+1}^{\\mathrm{agent}}\\equiv-D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})=-\\sum_{\\mathbf{x}^{\\prime}\\subseteq\\boldsymbol{U}^{\\prime}}P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})}{P_{\\theta}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x}_{\\Delta})}\n$$",
                "Directly comparing $I Q_{t}^{\\mathrm{{agent}}}$ and $I Q_{t+1}^{\\mathrm{agent}}$ ischallengingf $I Q_{t+1}^{\\mathrm{agent}}$ averaging over $\\mathbf{x}_{\\Delta}$ with probability $P_{\\mathcal{W}}(\\mathbf{x}_{\\Delta}|M_{t}^{\\mathrm{mem}})$ Thisexpectationrepresentstheaverageamountofknowledge gained by measuring $\\Delta$ ， given prior knowledge in $M_{t}^{\\mathrm{mem}}$ . We obtain:",
                "$$\n\\begin{array}{r}{\\displaystyle\\sum_{\\mathbf{x}\\subseteq\\Delta}P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})I Q_{t+1}^{\\mathrm{agent}}=-\\sum_{\\mathbf{x}^{\\prime}\\subseteq\\mathcal{U}^{\\prime},\\mathbf{x}\\subseteq\\Delta}P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}\\mathbf{x}|M_{t}^{\\mathrm{mem}})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x})}{P_{\\theta}(\\mathbf{x}^{\\prime}|M_{t}^{\\mathrm{mem}}\\mathbf{x})}}\\\\ {=I Q_{t}^{\\mathrm{agent}}+\\displaystyle\\sum_{\\mathbf{x}\\subseteq\\Delta}P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})\\log\\frac{P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}{P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})}}\\end{array}\n$$",
                "The second term is the relative entropy of the conditional probability distribution of $\\mathbf{x}_{\\Delta}$ conditioned on $M_{t}^{\\mathrm{mem}}$ ,which is always non-negative.Therefore,on average, $I Q_{t}^{\\mathrm{{agent}}}$ is non-decreasing as $M_{t}^{\\mathrm{mem}}$ acquires new knowledge over time. Note that $I Q_{t+1}^{\\mathrm{agent}}$ canbefurthersedbggtcqdede $\\theta$ within $M_{t}^{\\mathrm{wm}}$ .",
                "Interestingly, the expected gain in intelligence at time $t$ is determined by the discrepancy between the actual distribution $P_{\\mathcal{W}}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})$ and the model-predicted distribution $P_{\\theta}(\\mathbf{x}|M_{t}^{\\mathrm{mem}})$ . In other words, the rate of intelligence growth in Figure 12.1 is higher when the new measurement result is more unexpected.Thisobservationidentifies scientist agents [859]as aspecialtypeofcriosity-driven agent[869],prioritizingexplorationover exploitationtoexpandthe frontiers of knowledge fordeeper understanding ofnature.Unlikeagents that leverage existing knowledge toachieve predefined objectives,curiosity-drivenagentscanlearn withoutextrinsicrewards[387,870] (see Section 5.3fordetails)enabling discoveries beyond human-planned search spaces and revealing knowledge in unexplored domains.This potentialalso underscoresthe importance ofequipping curiosity-driven agents withfundamental perception and action tools that can be transferred to explore new knowledge domains."
              ],
              "raw_title": "Statistical Nature of Intelligence Growth",
              "type": null,
              "children": []
            },
            {
              "title": "12.1.3 Intelligence Evolution Strategies",
              "number": "12.1.3",
              "level": 3,
              "content": [
                "The strategy for expanding known information determines how quickly an agent's inteligence evolves. For a given knowledge base $M_{t}^{\\mathrm{mem}}$ , the parameter $\\theta$ can be optimized over a space of world models $\\Theta$ characterized by the architecture of $M_{t}^{\\mathrm{wm}}$ . The optimal agent is the one that minimizes $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ , thereby maximizing $I Q_{t}^{\\mathrm{agent}}$ .．",
                "$$\n\\theta_{\\mathrm{K},t}^{*}\\equiv\\arg\\operatorname*{sup}_{\\theta}I Q_{t}^{\\mathrm{agent}}=\\arg\\operatorname*{inf}_{\\theta}D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})\n$$",
                "and",
                "$$\nD_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})\\equiv D_{\\mathrm{K}}(\\theta_{\\mathrm{K},t}^{*},M_{t}^{\\mathrm{mem}})\n$$",
                "Here, $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ represents the minimum unknown after learning from $M_{t}^{\\mathrm{mem}}$ for this family of models, quantifying the expressive limitations of $\\Theta$ . As shown in Figure 12.1, $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ forms the envelopeof thefamilyof functions $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ ,where $\\theta$ ranges over $\\Theta$ ，",
                "For a given model family $\\Theta$ $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ measures the best possible prediction of residual unknowns in addressing thetarget scientificproblem basedon $M_{t}^{\\mathrm{mem}}$ .Inotherwords,theknowledgecontent in $M_{t}^{\\mathrm{mem}}$ is captured by $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(\\bar{M}_{t}^{\\mathrm{mem}})$ . One can prove that $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ is monotonicallynon-increasing as $M_{t}^{\\mathrm{mem}}$ expands, since it forms the envelope of a family of non-increasing functions $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ .This expansion process is tied tohow theagent acts and gains information, driven by $M_{t}^{\\mathrm{wm}}$ ,which determines the optimal expansion and executes it through the action $a_{t}\\in\\mathcal A$ at time $t$ (see Table 1.2).",
                "During knowledge discovery,diffrent strategiescan beemployedtoexpand $M_{t}^{\\mathrm{mem}}$ . The optimal expansion strategy is theonethatresultsinthesteepestdecreaseof $D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{me\\bar{m}}})$ .Forinstance,ingueeilltatei for expanding $M_{t}^{\\mathrm{mem}}$ , denoted as $^1M_{t}^{\\mathrm{mem}}$ and $^{2}M_{t}^{\\mathrm{mem}}$ . The first strategy, $^1M_{t}^{\\mathrm{mem}}$ , represents random exploration, while the second, $^{2}M_{t}^{\\mathrm{mem}}$ ,follows ahypothesis-drivenapproach[871]in whichthe agent firstformulatesahypothesis about theunderlying mechanismofthetargetproblemandthendesigns anexperiment tojustifyorfalsifythis hypothesis 1749] , on $M_{t}^{\\mathrm{mem}}$ $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ ase This approach is generally more efficient than random exploration for expanding $M_{t}^{\\mathrm{mem}}$ , leading to $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(^{2}M_{t}^{\\mathrm{mem}})$ descending faster than $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(^{1}M_{t}^{\\mathrm{mem}})$ ，",
                "In general, the knowledge discovery processproceeds iteratively,repeatedlyoptimizing the world modelparameter $\\theta$ to approach $\\theta_{\\mathrm{K},t}^{*}$ and expanding $M_{t}^{\\mathrm{mem}}$ inarationalmannertoacceleratethedecreaseof $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ . The ideal state is achievingepistemiccompleteness,i.e., $D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})=0$ ,meaning zerodiscrepancybetweethe agent'spredition and the real-world phenomena.However, for a specific agent, a discovery bound may exist, where $D_{\\mathrm{K},\\Theta}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ approaches zero butremains positive.These discrepancies arise from practicalconstraints andthe limitations of $\\Theta$ ， $\\mathcal{A}$ and other design spaces of the agent [872].Achieving a low discovery bound requires designing an adaptive world model architecture, an eficient knowledge expansion strategy, and a sufficient action space."
              ],
              "raw_title": "Intelligence Evolution Strategies",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "12.2 Agent-Knowledge Interactions",
          "number": "12.2",
          "level": 2,
          "content": [
            "Typical forms ofscientificknowledge includeobservational knowledge(e.g.experimentalmeasurements,computational results),methodological knowledge (e.gxperimentalmethods,omputationaltechniques, protocols)andtheical knowledge (e.g.theories,laws,predictive models).These forms ofknowledgecancontribute toscientificunderstanding as long as theyconsistofdataandinformation processedinaway that affects theprobabilitydistributionof unknown information $P_{\\theta}\\left(\\mathbf{x}_{\\mathrm{U}}|M_{t}^{\\mathrm{mem}}\\right)$ , reduces $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ , and facilitates decision-making.",
            "In principle,external scientificknowledge has been shown to be usefulin improving agent performance in reasoning and decision-making[873,874].However, the scope of this survey lies in how agents can autonomously discover and utilizeknowledge toenhancethemselves.Scientificknowledge discovery workflows typically involve hypothesis generation,protocol planning,conducting experiments andcomputations, analyzing data,deriving implications,and revising hypotheses-often as part of an iterativecycle.An agent that can perceive,learn,reason,and act has the potential to drive such workflows in an autonomous manner, forexample by using application programming interfaces (APIs)to interact with physicalinstruments toacquire scientificknowledge and iterativelyenhance itsknowledge base (Figure 12.2). The agent will use the acquired knowledge to update its mental states $M_{t}$ to make better decisions when interacting with the world $\\mathcal{W}$ . We will now highlight three scenarios where agents discover scientific knowledge and enhance themselves.",
            "![](images/c5c82e5a7514fcfaf43052e142255ac41d198892842dfdf66a1c16c30746c063.jpg)",
            "Figure 12.2:Closed-loop knowledge discovery for sustainable self-evolution.The agent aims toiteratively enhance its intelligence $I Q_{t}^{\\mathrm{{agent}}}$ through hypothesis generation and testing,as wellas through data analysis and implication derivation. When interacting with the physical world $W$ , the agent generates hypotheses as an explicitly or implicitly predicted distribution $(P_{\\theta})$ of unknown information, takes actions $(a_{t})$ for hypothesis testing, observes experimental results $(o_{t})$ , and updates beliefs based on perception of the real-world distribution $(P_{W})$ . When not interacting with $W$ the agent distills knowledge from existing data and premises, updating mental states $M_{t}$ directly. Inspired by Figures 2.3 and 2.5 in [864]."
          ],
          "raw_title": "Agent-Knowledge Interactions",
          "type": null,
          "children": [
            {
              "title": "12.2.1 Hypothesis Generation and Testing",
              "number": "12.2.1",
              "level": 3,
              "content": [
                "Hypothesis generation and testing (Figure 12.2)is acriticalapplicationof agents in autonomous scientific discovery, as it hasthepotentialtoenableoutside-the-box innovations[749].In essence,hypothesis generation istheformation of potentialrules that govern data distribution—ranging from single observations to large datasets—pertaining to unobserved scientific phenomena.According to Sir Karl Popper,a scientific hypothesis must be falsifable[875,876]; in this discusson,wedefineahypothesisthat survivesfalsificationasajustifedtruehypothesis[877860].Typically, scientists test hypothesesbyconducting experiments toeitherjustify orfalsifythem.Ahypothesis isconsidered more valuable if it is broad enough to explain a wide range of data and is highly likely to be true.",
                "To tacklea scientific problem,theagentformulatesone ora smallnumber of high-value hypotheses basedonits mental state $M_{t}$ , which contains only incomplete information about the partially observable world $\\mathcal{W}$ . After testing through experiments orcomputations,ajustifiedtruehypothesis becomes instructiveknowledge,expanding $M_{t}^{\\mathrm{mem}}$ in a way that rapidly minimizes $D_{\\mathrm{K,\\Theta}}^{\\mathrm{min}}(M_{t}^{\\mathrm{mem}})$ Hence,generatingandtestinghighvaluehypothesescanquicklypote knowledgediscovery andincrease $I Q_{t}^{\\mathrm{agent}}$ .Inthisscenarioteagentemploystheleaing futn $\\mathrm{L}$ , to process observations from hypothesis testing, $o_{t}$ , into knowledge and update its mental states $M_{t}$ .",
                "Generating physically meaningful hypotheses is a keystep.The agent typically uses LLMs along withcollaborative architectures and domain knowledge forhypothesis generation[878].Siet al.[742]conductedalarge-scalehumanstudy involving over $100{\\mathrm{NLP}}$ researchers, and found that LLM-generated ideas were rated as more novel $(p<0.05)$ than human expert ideas,albeitslightlyweakerinfeasibility.Ghafarollahiet al.[743]developed SciAgents,whichgeneates and refines materials science hypotheses to elucidate underlying mechanisms, design principles, and unexpected properties of biologicall inspired materials.Basedonlarge-scale ontological knowledge graphs,SciAgents samples a viable pathbetweenconceptsof interest,formulates apertinenthypothesis,andexpands it intoafullresearchproposal withdetailedhypothesis-testing methodsandcriteria.Itemploystwodedicatedagents toreview,critique,andimprove theproposedhypothesis,butdoes notinclude the stepof hypothesis testingthroughactualexperiments.Similarly,Suet al.[879]and Baek et al.[880]proposed leveraging teamwork—such as collaborative discussions and agent critics—to produce novel and effective scientific hypotheses. In addition, Gower et al. [881] introduced $\\mathrm{LGEM^{+}}$ , which utilizes a first-orderlogicframework todescribe biochemicalpathways and generate 2,094 unique candidate hypotheses for the automated abductive improvement of genome-scale metabolic models in the yeast S. cerevisiae.",
                "Hypotheses only become knowledge after being justified through computational or experimental observations. Lu et al.[745]introduced the AI Scientist,a system designed forfully automated scientificdiscovery.The AIScientist can conductresearch independentlyandcommunicate itsfindings,as demonstratedin threemachinelearning subfieldsdiffusion modeling,transformer-based language modeling,and leaning dynamics. It generates original research ideas,writescde,performscomputationalexperiments,visualizesresults,draftscomplete scientificpapers,and even simulates a peer review processfor evaluation. For instance, it proposed the hypothesis that“adaptive dual-scale denoising can improve diffusion models by balancing global structure and localdetails in generated samples\"which was justified through image generation tests on four 2D datasets. Similarly, Schmidgalletal.[746] developed the Agent Laboratory toautonomouslycarry out the entireresearch process,including literaturereview,computational experimentation, and report writing.They evaluated Agent Laboratory's capability for knowledge discovery by addressing five research questions in computer vision and naturallanguage processng,achieving an average humanevaluatedexperiment qualityscore of 3.2out of 5.Inadition,Tiukovaetal.[744]developed Genesis,anautomated system capable of controlling one thousand $\\mu$ -bioreactors, performing mass spectrometry characterization, accessing a structured domain information database, and applying experimental observations to improve systems biology models. Genesis can initiate and execute 1,O0o hypothesis-driven closed-loop experimentalcycles per day.Using a similar approach,the Genesis team has advanced the yeast(S.cerevisiae)diauxic shift model,outperformingthe previous best and expanding its knowledge by 92 genes $(+45\\%)$ and 1,048 interactions $(+147\\%)$ [882]. This knowledge also advances our understandingofcancer, the immune system,and aging.Similarly,Gottweis et al.[749]introducedthe AIco-scientist,whichautonomouslygenerates andrefines novelresearch hypotheses,within vitro validation in three biomedicalareas:drug repurposing,noveltarget discovery,and mechanisms of bacterialevolution and antimicrobial resistance.",
                "Discovered knowledge enhances the agent's mental states, such as $M_{t}^{\\mathrm{mem}}$ ， $M_{t}^{\\mathrm{wm}}$ , and $M_{t}^{\\mathrm{rew}}$ . Tang et al. [747] developed ChemAgent, which improves chemical reasoning through a dynamic, self-updating memory, $M_{t}^{\\mathrm{mem}}$ ChemAgent proposes hypothetical answers tochemistry questions ina development dataset,evaluates themagainst the ground truth,and simulatesthe hypothesis-testing processused inreal-worldresearch.Correctanswers arethenstored as knowledge in its memory to support future chemistry question answering.This self-updating memory resulted in performance gains of up to $46\\%$ (with GPT-4) when ChemAgent was applied to four chemical reasoning datasets from SciBench[883]. Wang et al.[884] introduced Molecular Language-Enhanced Evolutionary Optimization (MOLLEO), which iteratively proposes hypotheses for modifying candidate drug molecules in $M_{t}^{\\mathrm{mem}}$ , evaluates their drug-likeness and activity, and updates the candidates in $M_{t}^{\\mathrm{mem}}$ to enhance drug discovery.Similarly, Jiaet al.[885] developed LLMatDesign, which employs hypothesis-guided structure generation and a self-updating $M_{t}^{\\mathrm{mem}}$ to design inorganic photovoltaic materials,whose idealityisdefinedbymatching thetargetbandgapandhaving themost negativeformation energy.",
                "Simet al.[748]introduced ChemOS 2.0,whichorchestratesclosed-loopoperations inchemical self-driving laboratories (SDLs).ChemOS 2.0integrates ab initiocalculations,experimentalorchestration, andstatisticalalgorithms forthe autonomous discovery ofhigh-performance materials.Acase study ondiscovering organic lasermolecules demonstrates its capabilities. It employs a Bayesian optimizer, Altas, as its world model $M_{t}^{\\mathrm{wm}}$ to predict the optical properties of hypothetical molecules—specifically Bis[(N-carbazole)styryl]biphenyl (BSBCz)derivatives-including gain cross section and spectral grain factor. Based on these predictions,ChemOS 2.0 recommends molecules with a higher probabilityof success inthe experimentalcampaign.Itthen utilizes anopticalcharacterization platformandthe AiiDA software package to measure and simulate the properties of test molecules. The results are used to update $M_{t}^{\\mathrm{wm}}$ improving the accuracy of future experimental predictions.",
                "Hysmithet al.[886] published a perspective highlighting the crucial role of reward function design in developing forward-looking workflows for SDLs. Agents can be highly efective at solving POMDP problems in simulated environments,such ascomputer games or simulations,butoften struggle withreal-worldapplications.A welldefined reward function is essential for iterative self-evolution.However, in many real-world scientificresearch problems, reward functions areilldefinedorabsent attheendofexperimentalcampaignsdue tothelackofdirect measurements, thecomplexityofexperimentalresults,andthe needtobalance multipleobjectives.Thediscoveryofnewknowledge can serve as a valuable resource for refining $M_{t}^{\\mathrm{rew}}$ , guiding hypothesis exploration and experimental data collection."
              ],
              "raw_title": "Hypothesis Generation and Testing",
              "type": null,
              "children": []
            },
            {
              "title": "12.2.2 Protocol Planning and Tool Innovation",
              "number": "12.2.2",
              "level": 3,
              "content": [
                "The capability to plan experimental protocols and optimize tool usage enablesthe agent to solve complex scientific puzzles withinthe autonomous discovery loop.As introducedin Section 9.4,the agentcan systematicallyevaluate and refine its approach toselecting,invoking,andintegratingavailabletools—andevendevelopnewtools tailored to specific task requirements. While optimized protocols and tool usage do not directly reduce $\\bar{D}_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ , they enhance execution efficiency and effectiveness in refining the probability distribution of unknown information, $\\dot{P}_{\\theta}\\left(\\mathbf{x}_{\\mathrm{U}}|M_{t}^{\\mathrm{mem}}\\right)$ ， thereby accelerating knowledge discovery. In this scenario,the agent leverages the reasoning function $\\mathrm{R}$ to translate its evolving mental states $M_{t}$ , continuously updated with new knowledge, into real-world actions $a_{t}$ for more effective and faster hypothesis testing (Figure 12.2).",
                "Scheduling and orchestrating theselection and recombination of existing tools is critical.Scientific experiments typically depend on diverse instruments for analyzing reaction products, with decisions rarely rely on just one measurement.Eectively utilizing necessary instruments without wasting resources and time requires the agent to learn to use tools in an integrated and adaptive manner.Dai etal.[75o]designed amodular workflow thatintegrates mobile robots, anautomated synthesis platform, and variouscharacterization instruments forautonomous discovery. They exemplified this system across three domains:structural diversification chemistry,supramolecular host-guest chemistry,and photochemical synthesis.The mobile robot followsa synthesis-analysis-decision cycle to mimic human experimental strategies,autonomously determining subsequent workflow steps.It selects appropriate instruments,such as the Chemspeed ISynth platform for synthesis, aliquid chromatography-massspectrometer(UPLC-MS)for measuring mass spectracorresponding tochemical peak signals,and a benchtop nuclear magnetic resonance spectrometer (NMR) for tracking chemical transformations from starting materials to products.",
                "Beyond individuallaboratories,toolorchestration is essntial for delocalized and asynchronous scientific discovery. Strieth-Kalthoff et al.[751] demonstrated a closed-loop integration of five materials science laboratories across three continents,advancing delocalized and democratizedscientific discovery.These fivelaboratories have varying strength—for example,the Universityof BritishColumbia specializes incontinuous preferentialcrystallzation,while Kyushu University excels inthin filmfabrication andcharacterization.Strieth-Kalthoff et al.employed acloud-based experiment planner tocontinuously learn from the incoming data and efectively prioritize informative experiments acro the fivelaboratories,resulting inthediscoveryof 2lnewstate-of-the-art materialsfororganic solid-statelasers.",
                "Moreover,the agent can optimize existing tools andeven create new ones to enhance its capabilities.Swanson et al.[752]developedtheVirtualLab,anAI-drivenresearch environment thatfacilitated the design and experimental validation of new SARS-CoV-2 nanobodies. Within the Virtual Lab, AIagents conduct scientific discusson in team meetingsand execute specializedtasks inindividualsessions.Onekey agendafortheagents was developing tools toaid in the design of nanobody binders [887],including:(1)asequence analysis toolthatrankscandidate point mutations using log-likelihoodratios fromthe ESM protein language model[88]; (2)a structure evaluation tool that extracts interface pLDDT scores fromAlphaFold-Multimer predictions [889],ofering a proxy for antibody-antigen binding affnity; and (3)anenergyestimationtolbuiltonRoseta[890]toquantifybinding strengthbetweennanobodyvariants and the spike protein.These agent-generatedtools enabled theVirtual Lab to discover two novelnanobodies with enhanced binding tothe JN.1or KP.3SARS-CoC-2variants,while preserving strong affnity for the ancestral viral spike protein."
              ],
              "raw_title": "Protocol Planning and Tool Innovation",
              "type": null,
              "children": []
            },
            {
              "title": "12.2.3 Data Analysis and Implication Derivation",
              "number": "12.2.3",
              "level": 3,
              "content": [
                "Although most knowledge discovery processes relyon generating hypotheses andtesting themin the real world-where observations $o_{t}$ are essential—a significant portion of knowledge can be derived purely through internal actions such as iterative reasoning and deep thinking,whichare common in theoretical disciplines.For example,all theorems in Euclidean geometry can be deduced from just five axioms,butthese theorems do not explicitly exist in the mental state beforethey arederived. Givenallnecessary premises,such as Euclid's five postulates,the true probabilityofa hypothesis may remain elusive. However, using deductive and inductive reasoning to draw implications from known premises and data can help either justify or falsify hypotheses, thus reducing $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ and enhancing $I Q_{t}^{\\mathrm{agent}}$ (Figure 12.2). In this scenario, the agent employs the cognition function C to use prior mental states $M_{t-1}$ and internal actions $a_{t}$ to derive new knowledge and update mental states to $M_{t}$ ，",
                "Deductive reasoning enables knowledge derivation through logic. Trinh et al.[753] developed AlphaGeometry for the forward deduction of new mathematical theorems based on existing theorems in Euclidean plane geometry. AlphaGeometry employs a neural language model to construct auxiliary points in plane geometry problems and integratesspecialized symbolic engines toexhaustivelydeduce new true statements,therebyexpanding the jointclosure of known truths.Byleveraging this expanded closure, it altermates between auxiliary constructions and symbolic reasoning engines to uncover further implications.AlphaGeometry demonstratedremarkable performance ona test setof 30recent Olympiad-level problems,solving 25—more thandouble the10 problems solved by the previous best method-and coming close to the level of an average International Mathematical Olympiad (IMO) gold medalist.",
                "Inductive reasoning enables knowledge derivation through pattern recognition and statisticallearning.Liu et al. [754] introduced theTeam of AI-made Scientists(TAIS)to simulate therole of adata scientist for streamlined data analysis.TAIS decomposes a complex data analysis problem into different computational tasks, including coding, self-critique,andregresson analysis, toextract meaningfulinsights fromcomplexdatasets.When applied toidentifing disease-predictive genes, TAIS achieved an overall success rate of $45.73\\%$ on a benchmark dataset containing 457 genetic questions.Ideally,the extracted insights should be logicall sound; otherwise,they must bediscarded to ensure only accurate findings are safely integrated into mental states.However,limitation in datacoverage and the implementation of analysis algorithms may lead to hallcinated insights, underscoring the need for reliable data analyzers and reasoning tools to prevent over-analysis."
              ],
              "raw_title": "Data Analysis and Implication Derivation",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "12.3 Technological Readiness and Challenges",
          "number": "12.3",
          "level": 2,
          "content": [
            "The self-evolution of agents,which in turndrives the advancement of human knowledge,is promised by their early success in the innovation cycle.This cycle involves generating meaningful hypotheses,designing real-time testing protocols,coordinating various experimental and computational tools, analyzing data,deriving implications,and engaging inself-reflection.However, achieving fullyautonomous self-evolutionremains a significant challenge,given the current technology readiness levels (TRLs)of three fundamentalcapabilities:real-world interaction,complex reasoning,and the integrationofpriorknowledge.Further technologicalprogress is required to improve the cycleof self-driven innovation."
          ],
          "raw_title": "Technological Readiness and Challenges",
          "type": null,
          "children": [
            {
              "title": "12.3.1 Real-World Interaction Challenges",
              "number": "12.3.1",
              "level": 3,
              "content": [
                "Agents interact with the real world primarily through application programming interfaces (APIs).While numerous demonstrations[891]have showntheir strong capabilityto use various APIs,a significant bottleneck inautonomous knowledgediscovery remains:the lack of APIs that allow agents to directly execute tasks ina physicallaboratory. Physical APIs—interfaces that enable direct control of lab equipmentare far lessabundant than computational APIs due to the significant investment of time, expertise,and cost required to develop them.Although existing autonomous laboratories have shown promise,they remain in an early developmental stage(typically TRL 4-6),where straightforward replication or scale-up ischallenging.Consequently, building further systems or broadening their applicationacrossadditional scientificdomains stillrequires substantialcustomizationtoaddress domain-specific needs, along with specialized expertise.",
                "Twokey tasks are essntialforenablingreal-worldinteraction:operating labdevices andtransferring samples between devices.Seamlessintegration of physical hardware and experimental samples is crucial to maintaining uninterrupted workflows.However, most experimental instruments are originally designed for human operation. Making them accessible to agents requires extensive efforts across multiple disciplines, including robotics,electricalenginering, mechanicalengineeing, and software programming.The rising prominence of SDLs is catalyzing the transformation of human-operated devices into agent-accessble systems through APIs. In autonomous labs conducting complex experiments,two parallel and often complementary approaches are commonly adopted to integrate hardware with agentic systems.Both approaches are modular,reconfigurable,and valuable, yet they require ongoing,dedicated development.",
                "Approach 1: API Integration via Direct Device Adaptation.This approach involves equipping individual devices with dedicated mechanical adaptations and I/O controllrs,enabling them to receive and execute commands from a central controlPC.Forexample,toachieve solid-state synthesis and structuralcharacterization of inorganic materials, A-lab has implemented 16 types of devices to automate experimental tasks such as powder dosing, heating,and diffraction [892].This approach allows laboratories to function as fully integrated entities by maximizing device utilization,optimizing space andresources, and enabling bespoke tools.However,it is costly,time-consuming,and requiresexpert knowledge to prototype or retrofit devices for automation.Large language models (LLMs)have been applied to facilitate access to diverse tools,asillustrated byCACTUS,a Chemistry Agent Connecting Tool-Usage to Science [893].",
                "A more accessble alternative for smallteams is thecloud lab or science factory [894], where responsibility for device engineeing shifts from individuallaboratories todedicated userfacilities orcommercial service providers.For instance,Boikoet al.[895]demonstrated an autonomouschemicalresearch agent, Coscientist,capable ofcarrying out cross-coupling Suzuki and Sonogashira reactions using experimental setups at the Emerald Cloud Lab[896].However, cloud labs offer only afixed set of pre-built devices optimized forcommon procedures,posing potentialchallenges for researchers whose experiments require equipment customization,as integrating non-standard tools may involve a lengthy process of negotiation and development.",
                "Approach 2: Robotic Operation of Experimental Devices. This approach involves using mobile robots or robotic arms to operate existing devices and transfer samples. In many cases,robots can interact with instruments without modification,apart from minoradjustmentssuch as adding specializedactuators,grippers,orholders.Forexample,Dai et al.[750]employed mobilerobots toexplore syntheticchemistry.Intheirautonomous laboratory, mobilerobots enable physicalinkages betweensynthesis andanalysis devices that arespatially separated,automating sampletransportation and handling. In principle,the robots can perform allactions human researchers require in thelaboratory.However, current robotic systems stillrely on human pre-programming tomapthe lab layout,define movement trajectories,and register device positions.Handling unexpected or adaptive situations remains achallenge, as pre-programming cannot anticipate every possible state ofan experimental setup.Real-timelearning andadaptive manipulation are active areas ofresearchthat require further technologicaladvancements.Inthelong term,embodied AI[897]is expectedtoenhance robotic learning, allowing agents to quickly adapt to new environments and tools.",
                "The two approaches can be combined. For example, Vescovi et al.[894] define a modular laboratory robotics architecturethat allowsfortranslatinghigh-levelcommands into specificoperations fora varietyof diferentrobotic apparatus andlaboratoryequipment, and forlinking robotic apparatus with other elements of an Al-driven discovery architecture,such as high-performance computing [898].This architecture has been used to automate experiments in both the biological and physical sciences[899].Similarly,Fernando et al.[900] integrate a Robotic Operating System 2(ROS2)compatible robot intothe Bluesky experimentalorchestration framework. Loet al.[90l] argue for thedevelopment and integration oflow-cost“frugaltwins\"of more expensiveequipment tofacilitate experimentation and democratize access."
              ],
              "raw_title": "Real-World Interaction Challenges",
              "type": null,
              "children": []
            },
            {
              "title": "12.3.2 Complex Reasoning Challenges",
              "number": "12.3.2",
              "level": 3,
              "content": [
                "A fundamental philosophical question is whether agents,often powered by LLMs,can truly perform reasoning.By definition,languages models generate outputs by predicting the next token, a mechanism fundamentally diffrent from human reasoning. From an outcome-driven perspective, these input-output systems exhibit reasoning ability phenomenologically,as they produce meaningfuloutputs compared toareference system generating arbitraryresponses [902].However,regardlessof the perspective taken, thiscapability remains imperfect—particularly whenhandling complex logical and numerical problems, which are crucial for scientific knowledge discovery.",
                "Agents and LLMs struggle with hard reasoning tasks. Glazer et al.[903]introduced FrontierMath,abenchmark comprising hundreds of original and challenging mathematics problems covering most major branches of modern mathematics.Evaluation of state-of-the-art LLM-driven agents-including ol-preview (OpenAl),ol-mini (OpenAI), GPT-4o(OpenAI, 2024-08-06 version), Claude 3.5 Sonnet (Anthropic, 2024-10-22 version),Grok 2 Beta (XAI), and Gemini $1.5\\:\\mathrm{Pro}\\:002$ (Google DeepMind)- -revealed that no model achieved even a $2\\%$ success rate on the full benchmark.Chen et al.[873] presented ScienceAgentBench, a benchmark designed to evaluate language agents in data-driven scientific discovery.Among 102tasks derived from 44 peer-reviewed publications acrossfour disciplines, OpenAI ol successfully solved only $42.2\\%$ of them. Chollet [865] proposed the Abstraction and Reasoning Challenge (ARC) to assesssLLMs'ability to perform abstract inductive reasoning without relying on memorization or external knowledge. Even with careful prompting, GPT-4o correctly solved only $19\\%$ of the tasks, far below the $\\sim75\\%$ average human performance[94,905].Zhuet al.[96]suggesteda four-level classification of AIintellgence, including Ll (arbitrating isputes), L2(auditing a review),L3(reviewing a paper), and L4(authoring a paper).They classify the current state-of-the-art LLM-driven agents as approaching L2-levelcapabilities.Toenhance agentsreasoning abilities, researchers have introduced techniquessuchaschain-of-thought[907],tree-of-thoughts[72],and[70].Although new methods continue toemerge,as discussed in Section 2.2,further advancements inreasoning capacityremaincrucial for achieving reliable causal inference in scientific research.",
                "Agents and LLMs also struggle with quantitative and symbolic problems.For example, GPT-4 and GPT-3.5 often struggle with reliably performing complex arithmetic such as multiplying 12, $345\\times98$ , 765, or translating IUPAC chemical names into accurate molecular graphs [908, 697]. A common approach to overcoming these limitations is to useexternal tools rather than relying on the LLM itself for reasoning. In mathematical problem-solving,for example,tools like symbolicsolvers are preferred over direct LLM inference[753].However, this mitigation does not resolve the intrinsicdeficiency in numerical understanding,which poses a potentialrisk to scientific reasoning. Moreover, Yu et al.[909]found that tool-augmented LLMs do notconsistently outperform base LLMs without tools in chemistry problem-solving.For instance,for specializedchemistry tasks,such as synthesis prediction, augmenting",
                "LLMs with specialized toolscan boost the performance substantially;however,oolaugmentationis less effective for generalchemistry questions,suchas those in exams,where no specific toolscan directly solvea given question.In these scenarios,an agent'sability toreasoncorrectly byusing multiple pieces ofchemistryknowledge becomes more important.",
                "The preceding discussion emphasizes the importance of developing robust methodologies for evaluating AI agents as scientific research assistants, a topic discussed at length by Cappello et al. [910]."
              ],
              "raw_title": "Complex Reasoning Challenges",
              "type": null,
              "children": []
            },
            {
              "title": "12.3.3 Challenges in Integrating Prior Knowledge",
              "number": "12.3.3",
              "level": 3,
              "content": [
                "Prior kowledge isacrucial factor forhigher intellgence.As discusses in Section12.1,the agent's prior knowledge, $M_{t}^{\\mathrm{mem}}$ , helps decrease $D_{\\mathrm{K}}(\\theta,M_{t}^{\\mathrm{mem}})$ and increase the agent's intelligence, $I Q_{t}^{\\mathrm{agent}}$ . Human-led scientific discoveries frequentlyachieve breakthroughs withrelativelysmalldatasets,thanks tothevastpriorknowledge humans possess.The start-of-the-art LLMs that power autonomous agentsare trainedon nearly all publicly available textualdata,including websites,books, and other sources, thereby encompassng most common knowledge as wellas publicly accessible specialized knowledge.However, achieving an agent that can seamlessly integrate allexisting human knowledge remains a significant challenge.",
                "At least three types of knowledge sources may not be included in LLM pre-training: (1)Paywalled or unpublished knowledge, including non-open-access publications,industry-specific data,andfailed experiments [91l].Theyare often not accesible to publicmodels despite their potential value in refining domain-specific insights. (2)Empirical knowlege.Heuristic decisions by experts are often effective, particularly in scenarios where no existing data is available foranew problem.However,large amounts ofexpertheuristics aretypicallynotaccessibleas textual data.(3) Contextual or situational knowledge.Knowledge relatedtoreal-worldconditions,suchas safety protocols in chemical reactionsor equipment handling,is often absent from pre-trained models but is essentialfor practicalapplications.",
                "Additionally, integrating diverse knowledge sources presents challenges in reconciling conflicting information.For example, OpenAI's Deep Research [912] actively gathers online information and performs multi-step reasoning, achieving state-of-the-art performance on Humanity's LastExam and the GAIA benchmark.However,it stillstruggles to distinguish between authoritative information andrumors and exhibits limitations in confidence calibration,often misrepresenting its levelofcertainty[912]Establishing asystem toassessthelevels of evidence[913]of different knowledge fragments—such as quantifying reliability and verifying references—may be necessary for effctive knowledge fusion."
              ],
              "raw_title": "Challenges in Integrating Prior Knowledge",
              "type": null,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "title": ") Design of Multi-Agent Systems 133",
      "number": "",
      "level": 1,
      "content": [
        "13.1 Strategic Learning: Cooperation vs. Competition 133\n13.2 Modeling Real-World Dynamics 134\n13.3 Collaborative Task Solving with Workflow Generation 135\n13.4 Composing AI Agent Teams 135\n13.5 Agent Interaction Protocols . 137\n13.5.1 Message Types 137\n13.5.2 Communication Interface 138\n13.5.3 Next-Generation Communication Protocols 138"
      ],
      "raw_title": ") Design of Multi-Agent Systems 133",
      "type": null,
      "children": []
    },
    {
      "title": "4 Communication Topology 141",
      "number": "4",
      "level": 1,
      "content": [
        "14.1 System Topologies 141\n14.1.1 Static Topologies 141\n14.1.2 Dynamic and Adaptive Topologies 142\n14.2 Scalability Considerations 144"
      ],
      "raw_title": "Communication Topology 141",
      "type": null,
      "children": [
        {
          "title": "4.1 The Human World Model",
          "number": "4.1",
          "level": 2,
          "content": [
            "Humans naturallconstruct internalrepresentations of the world,often referred toasmental models in psychology 341 342,343]. These models serve as compact and manipulable depictions of externalreality,enabling individuals to predict outcomes,planactions,and interpret novel scenarios with minimalreliance on direct trial-and-error.Early work on spatialnavigation,forinstance,showed thathumans andanimalsform“cognitive maps\"of their surroundings [341], suggesting an underlying ability to imagine potential paths before actually traversing them.",
            "Craik's seminal argument was that the human mind runs internal“smal-scale models of reality\"[342] to simulate how events might unfoldand evaluate posible courses of action.Later studies proposedthatsuchsimulations stretch across modalities-vision,language, and motor control—and are dynamicaly updated by comparing predictions to new observations.This process merges memory recallwithforward projection,implying aclose interplay between stored knowledge andthe active generation of hypothetical future states[343].More recent predictive processing theories such as“Surfing Uncertainty\"[344]propose thatthebrain operatesas ahierarchical prediction machine,continuously generating top-down predictions about sensory inputs and updating its models based on prediction errors.",
            "Critically, these human mental models are:",
            "·Predictive: They forecast changes in the environment, informing decisions about where to move or how to respond.\n·Integrative: They combine sensory input,past experience, and abstract reasoning into a unified perspective on “what might happen next\".\n·Adaptive: They are revised when reality diverges from expectation, reducing the gap between imagined and actual outcomes over time.\n·Multi-scale: They operate seamlessly acrossdifferent temporal and spatial scales, simultaneously processing immediate physical dynamics (millseconds), medium-term action sequences (seconds to minutes), and longterm plans (hours to years).This flexibility allows humans to zoom in on fine-grained details or zoom out to consider broader contexts as needed.",
            "Consider hunger and eating asan ilustrationof integrated world modeling.When hungry,a person's internal model activates predictions about food—simulating not just visual appearance but tastes, smell, and anticipated satisfaction-triggering physiologicalresponses like salivation before food is even present.Thisdemonstrates seamless integration across perception, memory, and action planning.",
            "The example alsohighlights adaptivity:once satiated, thesame modeldynamically updates,reducing predictedreward values for further eating.Despiterecognizing thesame fooditems, their anticipated utilitychangesbased on internal state.Furthermore, humans maintain counterfactual simulations—declining dessert now while accurately predicting they would enjoy itlater—enabling complex planning acrosshypothetical scenarios and time horizons,acapability comprehensive AI world models strive to replicate.",
            "In sum,thehumanworldmodelisnota staticlibraryoffacts,butaflexibleandever-evolving mentalconstruct,deeply rooted in perception and memory, thatcontinuously shapes (and is shaped by)the individual's interactions with the outside world."
          ],
          "raw_title": "The Human World Model",
          "type": null,
          "children": []
        },
        {
          "title": "4.2 Translating Human World Models to AI",
          "number": "4.2",
          "level": 2,
          "content": [
            "Researchinartificialintelligencehaslong soughttoreplicatethe predictive,integrative,andadaptivequalities exhibited by human mental models [341, 342].Early reinforcement learning frameworks,for instance, proposed learning an environment modelfor planning—exemplified by Dyna[345]—while contemporaneous work investigated using neural networks to anticipate future observations in streamingdata[346,347].Both directions were motivated by the idea that an internal simulatorof the worldcouldenable more effcient decision-makingthan purelyreactive,trial-and-error learning.",
            "Subsequent advancements in deep learning brought the notionof“AI world models\"into sharper focus.One influential approach introduced anend-to-end latent generative modelofan environment (e.g.,“World Models\"[348]),whereby a recurrent neural network(RNN)and variational auto-encoder(VAE)together learn to“dream”future trajectories. These latent rollouts allow anagent to trainorrefine policies offline,effectively mirroring how humans mentally rehearse actions before executingthem.Alongside such implicit designs,explicit forward-modeling methods emerged in model-based RL, letting agents predict $P(\\bar{s}^{\\prime}\\mid s,a)$ and plan with approximate lookahead [349, 350].",
            "Another branchof workleveragedlarge-scale simulators orreal-worldrobotics to ground learning inrichly diverse experiences [51,352].Such setups are reminiscent of how humanchildren learn by activelyexploring their environments, gradually honing their internalrepresentations.Yet akey questionlingers:can agentic systems unify these approaches (implicit generative modeling,explicit factorization,and simulator-driven exploration）intoacohesive“mental model\" akin to that observed in humans?Therecent proliferation of language-model-based reasoning [107,74]hints atthe potential tocrossmodalities and tasks,echoing how humans integrate linguistic, visual, and motor knowledge under one predictive framework.",
            "Overall, as AI systems striveforflexible,sample-efficientlearning,the Alworld modelstands asaconceptual bridge from cognitivetheories of mentalmodels to implementations that equip artificial agents with imagination, predictive reasoning, and robust adaptation in complex domains."
          ],
          "raw_title": "Translating Human World Models to AI",
          "type": null,
          "children": []
        },
        {
          "title": "4.3  Paradigms of AI World Models",
          "number": "4.3",
          "level": 2,
          "content": [
            "Designing an Alworld modelinvolves determininghow an AIagent acquires,represents, andupdates itsunderstanding of the environment's dynamics.While implementations vary, most approaches fallinto four broad paradigms: implicit, explicit,simulator-based, and hybridorinstruction-driven models.These paradigmscan befurther analyzedalong two key dimensions:reliance on internal(neural-based) vs.external(rule-based or structured)mechanisms,and overall system complexity.Figure 4.2illustrates this two-dimensional space,showing how different approaches distribute themselvesacross theseaxes.Generaly,implicit models tend to rely more on internal mechanisms, while explicit and simulator-based models incorporate more externalstructures.Simulator-based andexplicit models alsotendto be more complex than implicit and hybrid approaches,reflecting their structured reasoning and engineeed constraints.",
            "![](images/be6df94c6c1f9cfc1594565b9961f59f78fc4729ec1bc5b2afb05f9ff333ae2e.jpg)",
            "Figure 4.2: A two-dimensionallayout of AI world-modelmethods.The horizontal axis indicates Complexity (left to right).The verticalaxis spans Internalapproaches (bottm)to Externalsolutions (top).Approximatepositionsreflect each method's reliance onlarge learned networks vs.explicit rules or code, and its overallsystem complexity."
          ],
          "raw_title": "Paradigms of AI World Models",
          "type": null,
          "children": [
            {
              "title": "4.3.1 Overview of World Model Paradigms",
              "number": "4.3.1",
              "level": 3,
              "content": [
                "An $A I$ world model is broadly any mechanism by which an agent captures or accesses approximate environment dynamics. Let $s$ denote the set of possible environment states, $\\mathcal{A}$ the set of actions, and $\\mathcal{O}$ the set of observations. In an idealized Markovian framework,the environment ischaracterized bytransition and observation distributions:",
                "$$\n\\begin{array}{r l}&{T(s^{\\prime}\\mid s,a)\\quad:\\quad\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S}),}\\\\ &{O(o\\mid s^{\\prime})\\quad:\\quad\\mathcal{S}\\to\\Delta(\\mathcal{O}),}\\end{array}\n$$",
                "where $T(\\cdot)$ dictates how states evolve under actions, and $O(\\cdot)$ defines how states produce observations. A world model typically learnsorutilizesapproximationsofthesefunctions(oravariant)allowingtheagenttopredictfuture statesor observations without executing real actions in the environment.",
                "Numerous approaches exist to implement these approximations, which we group into four main paradigms:",
                "·Implicit paradigm: A single neural network or latent structure encodes both transition and observation mappings without explicit factorization. World Models [348] or large language models used for environment reasoning are typical examples.Agents generally unrollthis black-box function to simulate hypothetical trajectories.\n· Explicit paradigm: The agent directly models or has access to learnable transition model $T_{\\theta}$ and observation model $O_{\\theta}$ ,often enabling interpretability or modular design. Model-based RL methods-like MuZero [349] or Dreamer [350]-learn or refine $T_{\\theta}$ , planning in an approximated state space. Generative visual models such as [353,358] fall under this category if they explicitly predict the next states or frames.\n·Simulator-Based paradigm: Rather than approximating (4.1)(4.2),the agent relies on an external simulator or even the physical world as the ground-truth.Systems like SAPIEN[351] or real-robot pipelines [352] can be seen as“native\" environment models that the agent queries. Although no learned $T(\\cdot)$ is required, the agent pays a cost in terms of runtime or real-world risks.\n· Other paradigms (Hybrid or Instruction-Driven): Methods that defy simple classification. They may store emergent rules in textual form [108], refine implicit LLM knowledge into partial causal graphs [356], or combine external components with learned sub-modules.Such approaches highlight the evolving nature of world-model research, where instructions,symbolic rules,or on-the-fly structures can complement more traditional approximations.",
                "Throughout theremainder of this subsection, we examine how each paradigmaddresses (orcircumvents)Equations (4.1) and(4.2),thetrade-off ininterpretabilityand scalability,andtheirrelativemeritsfordiferent tasksranging from text-based to high-dimensional embodied control."
              ],
              "raw_title": "Overview of World Model Paradigms",
              "type": null,
              "children": []
            },
            {
              "title": "4.3.2 Implicit Paradigm",
              "number": "4.3.2",
              "level": 3,
              "content": [
                "In the implicit paradigm,an agentencodesallenvironment dynamics—includinghow states evolve andhowobservations are generated-within a single (or tightly coupled) neural model. Formally, one maintains a latent state $h_{t}$ that is updated according to",
                "$$\nh_{t+1}=f_{\\theta}(h_{t},a_{t}),\\quad\\hat{o}_{t+1}=g_{\\theta}\\big(h_{t+1}\\big),\n$$",
                "where $f_{\\theta}$ subsumes the transition function $T(\\cdot)$ (and part of $O(\\cdot))$ from Eqs. (4.1)-(4.2), but without making these components explicit.A classic example is the World Models framework [348], in which a Variational Autoencoder (VAE)first compressesvisualinputs intolatentcodes,andarecurrent networkpredicts the nextlatentcode,effectively \"dreaming\"trajectories in latent space. Recent work also explores repurposing large language models (LLMs)for environment simulation in purelytextual or symbolicdomains[107,74],athoughthese models are notalwaysgrounded in strict time-series or physics-based data.",
                "Because implicit models fuse the transition and observation mechanisms into one monolithic function,they can be elegantly trained end toend and unrolled internallyfor planning.However,they tend to be opaque:it is difficult to interpret how precisely the network captures domain constraints orto inject knowledge directly into any part of the transition.This can be advantageous for highly complex environments where a single large-capacity modelcan discoverlatent structureonitsown,butitalsorisksbrittlenessunderdistributionshifts.Overall,theimplicit paradigm is appealing foritssimplicity andflexibility,but itcan posechallnges when interpretabilityexplicitconstaints,or fine-grained control of the dynamics are required."
              ],
              "raw_title": "Implicit Paradigm",
              "type": null,
              "children": []
            },
            {
              "title": "4.3.3 Explicit Paradigm",
              "number": "4.3.3",
              "level": 3,
              "content": [
                "The explicit paradigm instead factorizes the world model, often by learning or encoding a transition function $\\hat{T}_{\\theta}{\\left(s_{t+1}\\right|}$ $s_{t},a_{t})$ and an observation function $\\hat{O}_{\\theta}(o_{t+1}\\mid s_{t+1})$ . This explicit separation makes it possible to query each function independently. For instance, one might draw samples from",
                "$$\n\\hat{s}_{t+1}\\sim\\hat{T}_{\\boldsymbol{\\theta}}\\big(s_{t},a_{t}\\big),\\quad\\hat{o}_{t+1}\\sim\\hat{O}_{\\boldsymbol{\\theta}}\\big(\\hat{s}_{t+1}\\big).\n$$",
                "Model-based reinforcement-learning algorithms like MuZero[349] or Dreamer [350] exemplify this paradigm by refining aforward modelforplanning.Otherexplicit approaches prioritizefidelity ingeneratingfuture frames,suchas",
                "Difusion WM[33],whichapplies difusion processes atthe pixellevel,or DINO-WM[358],whichrolls out future states within a pretrained feature space.",
                "By factorizingtransitions andobservations,explicit methodscan be moreinterpretable and more amenable todebugging and domain-specific constraints. That said, they are still sensitive to model errors: if $\\hat{T}_{\\theta}$ deviates significantly from reality, the agent's planning and decision-makingcan become ineffective.Manyexplicit systems stillrely predominantly on internal(neural)representations,butthey may integrate external planners(e.g.,tree-searchalgorithms)toleverage the explicit transition structure.This blend of learned and symboliccomponents offrs a natural way to incorporate human knowledge, while preserving the strengths of deep learning."
              ],
              "raw_title": "Explicit Paradigm",
              "type": null,
              "children": []
            },
            {
              "title": "4.3.4 Simulator-Based Paradigm",
              "number": "4.3.4",
              "level": 3,
              "content": [
                "In the simulator-based paradigm,the agent outsources environment updates to a simulator,efectively bypassing the need to learn $\\hat{T}_{\\theta}$ from data. Formally,",
                "$$\n(s_{t+1},o_{t+1})\\gets S T M(s_{t},a_{t}),\n$$",
                "where $s\\tau{\\mathcal{M}}$ is often an external physics engine or the real world itself. Platforms like SAPIEN[351] and AI Habitat provide deterministic3Dphysicssimulations,allowing agents topracticeoriterate strategies inacontrolledenvironment. Alternatively,methods such as Daydreamer[352] treat real-world interaction loops like a“simulatorcontinually updating on-policy data from physical robots.",
                "This approachyieldsaccuratetransitions (assuming the simulator accuratelyreflects reality),which alleviates the risk of leared-modelerrors.However,itcanbecomputationalyorfinanciallyexpensive,especiallif the simulator is high fidelity orif real-world trials aretime-consuming andrisky.Asaresult,some agentscombine partiallearneddynamics with occasional simulator queries,aiming to balance accurate rollouts with effcient coverage of state-action space."
              ],
              "raw_title": "Simulator-Based Paradigm",
              "type": null,
              "children": []
            },
            {
              "title": "4.3.5 Hybrid and Instruction-Driven Paradigms",
              "number": "4.3.5",
              "level": 3,
              "content": [
                "Beyond these three primary paradigms,there is a growing number of hybrid or instruction-driven approaches, which blend implicit and explicit modeling or incorporate external symbolic knowledge and largelanguage models.Often, these systems dynamically extract rulesfrom data, maintain evolving textual knowledge bases,or prompt LLMs to hypothesize causal relationships that can then be tested or refined.",
                "AutoManual[108],for example iterativelycompiles interactive environment rules into human-readable manuals, informing future actions in a more transparent way.Meanwhile, COAT [356] prompts an LLM to propose possible causal factors behindobserved events,then validatesorrefines those factors viadirect interaction,bridging text-based reasoning with partiallearned models.Althoughthese solutionsoffer remarkable flexibilityparticularly in adapting to unfamiliar domains or integrating real-time human insights—they can be inconsistent in how they structure or update internal representations.As language-model prompting and real-time rule discovery continue to evolve, these hybrid methods arepoisedtobecome increasinglycommon,reflectingthe need tobalance end-to-end learning with the transparency and adaptability offered by external instruction.",
                "Until now,wehave introduced the four typical paradigms of existing world model techniques, as ilustrated in Figure 4.3.5. As we can see, each type of technique has trade-offs in different aspects."
              ],
              "raw_title": "Hybrid and Instruction-Driven Paradigms",
              "type": null,
              "children": []
            },
            {
              "title": "4.3.6 Comparative Summary of Paradigms",
              "number": "4.3.6",
              "level": 3,
              "content": [
                "The table summarizes the key methods in AI world modeling,categorizing them based on their reliance on external or internal mechanisms, their complexity, and their respective paradigms. The form column uses $\\scriptscriptstyle\\mathrm{~o~}$ for external approaches and $\\bullet$ for internal ones, with mixed methods having both symbols. This classification aligns with the previous subsections,including thedetailed discussion ofeach paradigm, andcomplements the visualrepresentation in Figure 4.2."
              ],
              "raw_title": "Comparative Summary of Paradigms",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "4.4 Relationships to Other Modules",
          "number": "4.4",
          "level": 2,
          "content": [
            "A comprehensive AIworld modeldoes notexist in isolation but interacts with severalkeycomponents of the agent's architecture.These include (but notlimited tothe memory,perception,and action modules.Inthis subsection,we explorehow worldmodels integrate with these criticalcomponents toenable coherentand adaptive behavior indynamic environments.",
            "![](images/dc4c580048e4f0aeb425c7cf1ce57e8a866cdfca7da835465bcee068b92c17d7.jpg)",
            "Figure 4.3:Four paradigms of world modeling: (a)implicit, (b)explicit,(c)simulator-based, and(d)hybrid/instruction driven.",
            "Table4.1:Summary of AI world-modelmethods across paradigms,showing theirform(External or Internal),complexity, and paradigm.",
            "<html><body><table><tr><td>Method</td><td>Form</td><td>Complexity</td><td>Paradigm</td></tr><tr><td>ActRe[49]</td><td>·</td><td>Simple</td><td>Implicit</td></tr><tr><td>World Models [348]</td><td>·</td><td>Simple</td><td>Implicit</td></tr><tr><td>Dreamer [350]</td><td>●</td><td>Moderate</td><td>Implicit</td></tr><tr><td>Diffusion WM [353]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>GQN [354]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>Daydreamer[352]</td><td>0</td><td>High</td><td>Simulator-based</td></tr><tr><td>SAPIEN[351]</td><td>0</td><td>High</td><td>Simulator-based</td></tr><tr><td>PILCO [355]</td><td>0</td><td>Moderate</td><td>Explicit</td></tr><tr><td>AutoManual [108]</td><td>0</td><td>Simple</td><td>Other</td></tr><tr><td>MuZero [349]</td><td></td><td>High</td><td>Explicit</td></tr><tr><td>GR-2 [357]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>DINO-WM [358]</td><td>·</td><td>High</td><td>Explicit</td></tr><tr><td>COAT [356]</td><td>O</td><td>Moderate</td><td>Other</td></tr></table></body></html>"
          ],
          "raw_title": "Relationships to Other Modules",
          "type": null,
          "children": [
            {
              "title": "4.4.1 Memory and the World Model",
              "number": "4.4.1",
              "level": 3,
              "content": [
                "Memory systems play a crucial role in the operation of world models.Whilea world model generates predictive representations offuture statesoractions, memory serves asthefoundationupon whichtheserepresentations are built and updated.The relationship between the world modeland memory can be viewed as a loop where the world model predicts potentialfutures, while the memory stores past experiences,observations,andlearned patterns,allowing for context-dependent reasoning and future predictions.",
                "Memory mechanisms can be structured in various ways, including:",
                "·Short-term memory:This enables the agent tohold and update its internal state temporarily,storing the most recent interactions or observations.This short-term context helps the agent make decisions in the immediate environment. ·Long-term memory: This serves as a more persistent repository of experiences and general knowledge about the environment. A world model can interact with long-term memory to refine its predictions, and it may use historical data to make more informed decisions or simulate more realistic futures.",
                "For example,in model-based RLframeworks likeDreamer[350],recurrentneural networksact as both the world model and aform of memory, maintaining alatent state that is updated witheachtime step to predictfuture states.This form of integrated memory allows the agent to both recall past interactions and anticipate future ones."
              ],
              "raw_title": "Memory and the World Model",
              "type": null,
              "children": []
            },
            {
              "title": "4.4.2  Perception and the World Model",
              "number": "4.4.2",
              "level": 3,
              "content": [
                "Perception refers tothe agent's ability to sense and interpret its environmentthrough various modalities (e.g.,vision, touch,sound,etc.).The world modelrelies heavilyonaccurate sensory input toformcoherentpredictions about the environment.In many AI systems,the perception module converts raw sensor datainto ahigher-levelrepresentation, such as an image, sound wave, or other structured data.",
                "A key aspect of the interaction between the world model and perception is how the agent processesand integrates sensory input into the model.The world modeloften depends on processed data (such as features fromconvolutional neural networks orembeddings from transformers)to simulate potential futures.Additionally, the world model can guide perceptual processes by focusing atention on the most relevant sensory input needed to refine predictions.",
                "For example, in autonomous robotics,perception systems typicall detect objects or environmental features, which are then fed intoaworld modelthatpredicts howthe scene will evolve.RoboCraft[359]achieves this perception-tomodeling transformation by converting visual observations into particles andcapturing the underlying system structure through graph neuralnetworks.PointNet[360]further enriches perception systems'understanding of physical space by encoding unstructured 3D point clouds to capture spatial characteristics of the environment. In navigation tasks, OVER-NAV[361] further combine largelanguage models and open-vocabulary detection toconstruct the relationship between multi-modal signals and key information,proposing anomni-graph tocapture the structureoflocal space as the world model for navigation tasks.This feedback loop between perception and the world modelenables agents to update their perception dynamically based on ongoing predictions, allowing for real-time adaptation."
              ],
              "raw_title": "Perception and the World Model",
              "type": null,
              "children": []
            },
            {
              "title": "4.4.3 Action and the World Model",
              "number": "4.4.3",
              "level": 3,
              "content": [
                "Action refers tothe decision-making processthrough which an agent interacts with its environment.Inagentic systems, actions aredriven bythe world model's predictions of future states.Theworld modelaids inplanning by simulating the outcomesofdiffrent actions before they are executed,allowing the agent tochoose the mostoptimalcourse of action based on the predicted consequences.",
                "The integration between world models and action modules can take various forms:",
                "·Model-based planning: World models explicitly modelthe environment's transition dynamics [349,362,107], allowing the agent to simulate multiple action sequences (rollouts)before selecting the most optimal one. · Exploration: World models also support exploration strategies by simulating unseen states or unexpected actions [363,350,364].These simulations enable the agent to evaluatethe potential benefits of exploring new parts of the state space.",
                "In model-based planning, MuZero[349] performs implicit planning through self-play and Monte Carlo Tree Search (MCTS),transformingcurrent staterepresentations intofuture state andreward predictions toguide thedecision-making process without prior knowledge of environment rules. In contrast,MPC[362]utilizes explicit dynamics models to predict multiple posible trajectories withinafinite time horizon,determines the optimalcontrol sequence by solving an optimization problem, and continuously updates planning using a receding horizon approach. Alpha-SQL[365], on the other hand,integrates an LLM-as-Action-Model within an MCTS framework to explore potential SQL queries within the database's“world model\".This approach dynamically generates promising SQL construction actions based on partialquery states,enabling zero-shot Text-to-SQL interactions without task-specific fine-tuning.Unlike MuZero, which focuses on planning for decision-making in uncertain environments, Alpha-SQL applies MCTS in a specific task—guiding SQL query construction through self-generated actions within a complex database context.",
                "For exploration strategies,Nagabandi et al.[363] incentivizes agents toexplore unknown regions by providingreward mechanisms (exploration bonuses)for discovering new states.Dreamer[350] propose that world models can generate imaginary action sequences (imaginary rollouts),allowing agents to safely evaluate the benefits of new actions in simulated environments without risking real-world experimentation.Similarly,in the discrete world model Hafner et al.[364]agents effcientlyexplorecomplexenvironments bysimulating multiplepossible future states,effectively balancing the trade-off between exploration and exploitation.",
                "For example,in reinforcement learning,agents can employ a learned world model to simulate future trajectories in action-selection tasks.The world modelevaluates the potentialrewardsofdiferent actions,enabling theagent to plan effectively and take actions that maximize long-term goals."
              ],
              "raw_title": "Action and the World Model",
              "type": null,
              "children": []
            },
            {
              "title": "4.4.4 Cross-Module Integration",
              "number": "4.4.4",
              "level": 3,
              "content": [
                "While memory,perception,andactionarediscussed as separatemodules,the true strength of worldmodels lies in their ability to seamlessly integrate across these domains.A world modelcontinuouslyreceives sensory input, updates its internalmemory,simulatesfuturestates,anduses thisinformation todrive action selection.Theiterativefeedback loo between these modules allowsagents toengage in intellgent, goal-directed behaviorthatishighlyadaptivetochanges in the environment.",
                "This cross-module interaction is particularly relevant in complex,dynamic systems such as robotics, where anagent must continuouslyadaptitsinternalrepresentationofthe world, process sensory input, store relevant experiences,and take actions inrealtime.Inthecontextofembodiedagents,the integrationofthese modules ensures that predictions made by the world model are grounded in current observations and the agent's ongoing experiences.",
                "World models provide a fundamental unifying principle across modalities.Whether predicting physical outcomes in embodied robotics,anticipating visual changes on screens,or infering semantic relationships in text, thecore mechanism remains consistent: generating predictions about how states evolve under diferent actions.This crossmodal capacity explains why humans transition efortlessly between manipulating objects, navigating interfaces,and processing language—allactivities driven by the same underlying predictive architecture.Future AI systems may achieve similar integration by developing world models that bridge these traditionally separate domains through a common predictive framework.",
                "In summary,the relationshipbetween the world model andtheother modules—memory,perception,and action-forms the backbone of intelligent behaviorinAIsystems.Each modulecontributes toacycleofprediction,update,and action, allowing agents tofunction efectively in dynamic and uncertain environments.These interactions highlight the need for a holistic approach when designing agent architectures,where world models areclosely intertwined with sensory input, memory systems, and decision-making processes."
              ],
              "raw_title": "Cross-Module Integration",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "4.5 Summary and Discussion",
          "number": "4.5",
          "level": 2,
          "content": [
            "The evolution of AI world models,fromearlycognitive insights toadvanced AIarchitectures, underscores the growing realization thattrue intellgencereliesonthe abilitytopredict,simulate,and imagine.Unlikeclassicalreinforcement leaning,where agents operate solely through trial-and-error interactions, world models enable foresightagents can plan,anticipate, and adapt to changes before they happen. This leap in cognitive modeling—whether implicit, explicit,or simulator-basedmarks asignificant shift inhow machinescanbeendowed withflexibility,robustness and generalization across tasks.",
            "An essential yetoften overlooked aspect of world models is their operation across multipletemporaland spatial scales Humanmental models seamlesslyintegrate predictions spanning milliseconds reflexiveresponses),seconds (immediate action planning),minutes tohours (taskcompletion)andeven years (lifeplanning)）[366].This multi-scalecapability allows usto simultaneously predict immediate physical dynamics while maintaining coherent long-term narratives and goals. Similarly,humans process spatial information acrosscales-from fine-grained object manipulation to navigation across environments toabstract geographicalreasoning. Current AI world models typically excel within narrow temporal and spatial bands, whereas human cognition demonstrates remarkable flexibility in scaling predictions up and down as context demands.This suggests that truly general-purpose AI world models may require explicit mechanisms for integrating predictions acrossmultiple time horizons and spatialresolutions,dynamically adjustingthe granularity of simulation based on task requirements.",
            "One centralchallnge in designing world models is the interplay between complexity and predictive accuracy. As discussed, implicit models,such asthose based on recurrent neural networks or transformers,offer simplicity and elegance,but theyoftencome withthetrade-offof limited interpretability.The model's internalstate is anopaque latent space,making it diffcult to enforce domain constraintsor provide guarantee about the accuracy of predictions. While such systems excelat capturing highlycomplexrelationships anddata-driven patterns,they alsoriskoverfitting or failing to generalize to unseen scenarios.",
            "Explicit models,bycontrast,offer greater transparency andcontrol.Byfactorizing state transitions and observations into separate functions,we gainaclearerunderstanding ofhow predictions areformed,andwecanmore easilyintegrate structuredknowledge,such as physicallaws or domain-specific rules. However,this approach comes with its own setofchallenges.First,itoften requires large amounts oflabeled trainingdata or simulated experiences to accurately capture environment dynamics.Second, even the most wel-structured explicit models may struggle with complex environments thatrequire fine-grained,high-dimensionalstaterepresentations,suchas in videoprediction orobotics.",
            "The simulator-based approachoffers a promising alternative, wherein agents rely on external environments—either physically grounded or simulated-for dynamic updates.This method avoids many of the challnges inherent in learning accurate world models fromscratch,as the simulator itself serves as the“oracle\"of state transitions and observations.However, thereliance on simulatorsalso introduces limitations:simulatorsoften failtocapture the full richness ofreal-worlddynamics andcan be computationally expensive to maintainor scale.Furthermore,real-world environments introduce noise and variability thata purely learned or pre-configured modelmight missAs AI agents strive to performtasksinopen-ended, unpredictablesettings,therobustnessof their world models willbetested bythe gap between simulated and actual environments.",
            "A key theme that emerges from this discussion is the trade-off between generalization and specialization.The more specificaworldmodelistoa particulardomainortask,thelesslikely it is togeneralize acrossdiffrentcontexts.Models like MuZero[349]and Dreamer[350]exemplifythis:they excelat specificenvironments(e.g.Atarigames orrobotics) but require careful adaptation when transferred to new, uncharted domains.Conversely,implicit models—particularly thoseleveraging large-scaleneural networks—have the potentialto generalize acrosstasks butoftendosoatthecostof sacrificing domain-specific expertise.",
            "Moreover, integrating memory with world models is crucial for agents that needto handle long-term dependencies and pastexperiences.While world models excelat predicting the next statebased on immediate inputs,true intelligent behavioroften requires reasoning about distant outcomes.Long-term memory allows agents to store critical environmental knowledge,ensuring that short-term predictions are grounded ina broader understanding of the world. This fusion of memory,perception, and action, mediated bythe world model,creates afeedback loop where predictions shape actions, which in turn inform future predictions.",
            "The human analogy remains compelling: just as humans integrate sensory inputs, memories, and internal models to navigate the world, sotoo must inteligent agentscombine perception, memory, and action through their world models.Asthe fieldadvances,it isclear thataholisticapproachone thatunifies implicit,explicitandsimulatorbased methods—may be the key to achieving more robust, generalizable, and adaptive agents.Hybrid methods,like those usedin AutoManual[108] ordiscovery-based models[356]ofer exciting possibilities forblending learnedknowledge with structuredrules andreal-time interactions,potentially pushing the boundaries of what weconsidera world model.",
            "Looking forward,open questions remain. How can we ensure that world models exhibit long-term stability and reliability in real-world setings?How do we handle the inherent uncertainty in dynamic environments while maintaining the flexibility to adapt? Furthermore,asagents grow more sophisticated,how can wedesign systems that are both efficient and scalable acrossincreasingly complex tasks without incurring massve computational costs?",
            "Inconclusion,thefutureofworldmodels liesintheirabilitytobalancetheneedforgeneralizationwiththerequirement for domain expertise.By continuing to explore and refine the interplay between model simplicity and complexity, betweenexternal and internalapproaches, we move closer todeveloping AI systems that not only understand the world but can actively shape their understanding to navigate and adapt in a rapidly changing reality."
          ],
          "raw_title": "Summary and Discussion",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "l5 Collaboration Paradigms and Collaborative Mechanisms 146",
      "number": "",
      "level": 1,
      "content": [
        "15.1 Agent-Agent collaboration 146\n15.2 Human-AI Collaboration 149\n15.3 Collaborative Decision-Making 150"
      ],
      "raw_title": "l5 Collaboration Paradigms and Collaborative Mechanisms 146",
      "type": null,
      "children": []
    },
    {
      "title": "Collective Intelligence and Adaptation 152",
      "number": "",
      "level": 1,
      "content": [
        "16.1 Collective Intelligence 152\n16.2 Individual Adaptability 153"
      ],
      "raw_title": "Collective Intelligence and Adaptation 152",
      "type": null,
      "children": []
    },
    {
      "title": "17 Evaluating Multi-Agent Systems 155",
      "number": "17",
      "level": 1,
      "content": [
        "17.1 Benchmarks for Specific Reasoning Tasks 155\n17.2 Challenge and Future Work . 159"
      ],
      "raw_title": "Evaluating Multi-Agent Systems 155",
      "type": null,
      "children": [
        {
          "title": "17.1 Benchmarks for Specific Reasoning Tasks",
          "number": "17.1",
          "level": 2,
          "content": [
            "In multi-agent system solving fortasks, muchfocus has been on leveraging multi-agent coordination forenhancing the reasoning capacity ofLLMs. It is most evident in coding, knowledge,and mathematicalreasoning benchmarks, where one isinterested in examining and building on performance with distributed solving.These benchmarks most typically examine if agents' capability for producing corrct code, reasoning on complex knowledge domains,and solving diffcult mathematical problems withstanding, with measures such as $p a s s@k$ [1076] or proof ratios for success being prevalent.Much improvement has been exhibited by MAS through structured workflow, domain-specific agent roles, and iterative improvement onstate-of-the-artperformance.Onthecontrary,for modeland simulation MAS,the case is one with acomparative lackofstandardizedbenchmarks.Rather,research is primarilyexperimental setups thatsimulate a variety of social phenomena, withcalls from thecommunity for further formalized evaluation frameworks.These multiple benchmark areas are described below,examining the tasks, measures for evaluation,andthecore mechanisms through which MAS result in better performance.",
            "Code Reasoning Benchmark Measuring the capability of LLMs for code synthesis requires bespoke benchmark suites with afocus on functionalcorrectness.Code synthesis,ascompared tonaturallanguage synthesis,allows for direct verificationthrough running.Severalbenchmark suites have been built forthis purpose,typically consisting of a collection of programming problems,each described with a naturallanguage problem description and acollection of test cases for automatically ascertaining the synthesized code's correctness.HumanEval[1077], APPS [1078], and MBPP [939] are some popular ones. These benchmark suites predominantly utilize the $p a s s@k$ metric, which computes the percentage at which at least one among the top- $k$ generated solutions passes all test cases for a number of problems.Theproblemscovered throughthese benchmark suitesrange acrossavariety of diffculties and programming abstractions,requiring notonly for LLMsandAgents but alsofor syntacticallycorrct andlogically soundcode that satisfies the providedtest cases.Recent work has explored leveraging Multi-Agent Systems (MAS)forenhancing LLM capability on code reasoning.For instance,MetaGPT[626] is a meta-programming system which embeds human-like Standard Operating Procedures (SOPs)into multi-agent cooperation based on LLM. With multi-agent role assignment with varying domains and adopting assembly line mode, MetaGPT effectively breaks down difficult operations into sub-operations and achieves state-of-the-art performance on HumanEval and MBPP benchmarks.SWE-agent [628] presents anovelAgent-Computer Interface(ACI) whichlargelyenhances arepository-creating,repository-editing,and navigationcapability for an agent.The system demonstratesthatawel-structured interfacetailoredforLMscanlargely enhance software engineering capability, with state-of-the-art on SWE-bench and HumanEval. AgentCoder [994] is anothrmulti-agentcoding system withfocusoneffectivetesting andauto-optimization.It isathree-agent systemwith a programmer,atest designer,andatest executor.Thetest designersupplies accurate and diverse testcases,andthe test executor provides feedbackto the programmer for optimization.Such collaborative workflow enhancescoding efficiency and outperforms one-agent models and other multi-agent approaches on HumanEval and MBPP datasets. These MAS approaches all pointout multi-agent cooperation,organized workflow,and tailored interface as effective solution strategies for enhancing thecapability of LLMon code reasoning.DEVAI[781] proposes a set of novel AI development automation benchmarks, which utilize a judge-agent mechanism for judging automatically intermediate development process.",
            "Knowledge Reasoning Benchmark To facilitate AI agents effectively acting in and understanding the world,robust knowledge reasoning abilities are essential.Benchmarks for this class assessanagent'sability to utilize factual knowledge and logical reasoning when answering challnging queries.Commonsense reasoning is tested with benchmarks such as CSQA [1079] and StrategyQA[1080],and scientific knowledge understanding is tested with ScienceQA[1081l.The core challenge for agents is performing multi-step,chain-of-thought reasoning,stepwise logically progressing from input query to output answer.These tests concentrate on assessing how wella specific AI agent can apply a specific body of knowledge,one at a time, and reason out a problem. Recent research has experimented with the use of LLMs on MAS for improving knowledge reasoning task performance, and they have achieved state-of-the-art accuracy. For example, MASTER [10o9], a novel multi-agent system, employs a novel recruitment process for agents andcommunication protocol using the Monte Carlo TreeSearch (MCTS)algorithm, and achieves $76\\%$ accuracy on HotpotQA [940]. Reflexion [48], a universal framework for bringing reasoning and acting together with language models, improves baseline by $20\\%$ on HotpotQA. These strategies demonstrate the potential of multi-agentcoordination forknowledge reasoningtasks.Besides,leveraging externaltols,e.gsearchengines, is also needed for improvingknowledge reasoning capacity.Agents may applythese tools forretrieving thelatest information and alsoforfactchecking,thus improving the accuracyanddependabilityofresponses.Such integrationis particularly helpful on applications such as TriviaQA[1082],for which real-time information access is essential.",
            "Mathematical Reasoning Benchmark Math reasoning is a critical skillfor AI agents which requires cooperative utilisation of mathematical knowledge,logical deduction, and computational power.Benchmarking tasks for this capability tend tofallinto twocategories: math problem-solving and computer-aidedtheorem proving(ATP).Datasets such as SVAMP[942],GSM8K[1083],and MATH[941]challenge agents to solve word problems, asking for exact number answers or formulas. ATP is a harder test, with stricter compliance with formal proof schemata. Tests on datasetslike PISA [1084] and miniF2F[1076], which are graded on proof completion, test whether an agent can produce well-formed mathematical proofs.Multi-agent systems (MAS)have been putforward as a potentialsolutionfor handling mathematical reasoning problem complexity.Methods such as MACM[1010] include a multi-agent system consisting of Thinker,Judge,and Executor agents tailoredforacomplex problem,dividing itinto smaller sub-problems for computation.The Thinker agent generates new ideas,Judge decides if they are accurate,and Executor conducts necessary computation involving tools such ascalculators.Such a modular structuresupports iterativerefinement and elimination of errors,enhancing problem-solving accuracy.Furthermore, methods such as multi-agent debate[985] include severalinstances of alanguage modeldebating andrefocusing iterativelyforcolective solutionimprovement, enhancing reasoning as wellas factuality accuracy. Such MAS-based systems have achieved notable improvementon benchmarks such as MATH and GSM8K,establishing distributed solving capacity for mathematical problems.Aside from this,reinforcementlearning from human feedback(RLHF)and preferencelearning strategies havebeen attempted for further enhancing mathematical problem-solving capacity of LLMs.For instance,amulti-turn online iterative direct preference learning framework [1085] has been put forwardfor training various language models with enriched sets of prompts over GSM8K and MATH datasets. Such a technique includes feedback from interpreters for codes and optimizes preferences at a level of trajectories, with notable improvement in output.",
            "Societal Simulation Benchmark Social simulation benchmarks are esential for evaluating multi-agent system performance and realism for simulating human behavior and socialinteractions based on LLMs.Standardized sets and test cases forevaluating theagents\"abilityforinteractingcommunicating,andevolving withinasimulatedsocietyare providedthrough the benchmarks. An example ofone such widely used benchmark is SOTOPIA [1086],employed for evaluating socialintelligence innaturallanguage agent-basedsocialintelligence.It is employed forevaluating agents' ability forconversing,understanding social cues,and building relationships witheach other within a virtual society. Another benchmark involves simulating propagation Gender Discrimination and Nuclear Energy [255] topics on social networks.Itis employed to evaluate agents'capabilities in modeling opinion dynamics,information dissemination, and social influence within large-scale social networks.Multiagent Bench [948] further provides two simulation domains—werewolf and bargaining—to assesscompetitive interactions among diverse agent groups with conflicting goals.",
            "Table 17.1:MAS Benchmarks: A Systematic Clasification of Multi-Agent System Evaluation Frameworks Categorized by Task-Oriented Performance and System-Level Capabilities.This comprehensive collction encompasses both specialized task-solving benchmarksandholisticcapabilityassessments,reflecting the dualnature of MAS evaluation in collaborative problem-solving and inter-agent dynamics.",
            "<html><body><table><tr><td>Category</td><td>Focus</td><td>Benchmarks</td><td>Examples</td><td>Representative Metrics</td></tr><tr><td rowspan=\"3\">Task-solving</td><td>Code Reasoning</td><td>APPS [1078],HumanEval [1077],MBPP [939], CodeContest [1087], MTPB[1088], DS-1000 [1089],ODEX [1090], Raconteur[1091]</td><td>MetaGPT [626], SWE-agent [628], AgentCoder [994]</td><td>Pass@k, Resolved(%)</td></tr><tr><td>Knowledge Reasoning</td><td>ARC [1092], HotpotQA [940], CSQA [1079], StrategyQA [1080],B00lQ[1093], OpenBookQA[1094],WinoGrande[1095], HellaSwag [1096],SIQA[1097], PIQA [1098],</td><td>Reflexion [48], MASTER [1009]</td><td>Accuracy</td></tr><tr><td>Mathematical Reasoning</td><td>proScript [1099], ScienceQA [1081], ProOntoQA [1100] MATH [941], GSM8K [1083], SVAMP [942], MultiArith [943], ASDiv [1101], MathQA[1102],AQUA-RAT[1103], MAWPS[1104],DROP [1105], NaturalProofs [1106], PISA [1084],</td><td>MACM [1010], Debate [985]</td><td>Accuracy, Pass @k</td></tr><tr><td rowspan=\"3\">Collaboration</td><td>Communication-based Cooperation</td><td>miniF2F [1076], Pr0ofNet [1107] InformativeBench [1108], Collab-Overcooked [944], COMMA[1109],</td><td>iAgents [1108], Two-Player [1110], EAAC[1111]</td><td>Task Completion Rate Communication Efficiency</td></tr><tr><td>Plorinain</td><td>LLM-Coordination [926] PARTGA</td><td>ResearAs ow [114 1</td><td>Planning Success Rate Coordination Efficiency</td></tr><tr><td>Process-oriented</td><td>Bench [948] Auto-Arena [947]</td><td>GPTSwarm[651] Idea [1115]</td><td>Process Completion Rate Step Efficiency</td></tr><tr><td rowspan=\"3\">Competition</td><td>Adversarial Scenarios</td><td>BattleAgentBench [920], MAgIC [955], LLMArena [1116], PokerBench [1117],</td><td>Dilemma [1118], PokéLLMon [1119]</td><td> Wi ating</td></tr><tr><td>Social Deduction</td><td>vale Diplomacy [934]</td><td>MA-KTO [1121], HLR [1122],</td><td>Win Rate Accuracy of Deductions</td></tr><tr><td>Game-Theoretic</td><td>Guandan [1123], AgentVerse [1124], ICP [1125]</td><td>WarAgent [1126]</td><td>Score Win Rate</td></tr></table></body></html>",
            "Evaluatingcapabilities inLLM-based MASrequires specialized approaches thateectively measure therichinteractions between agents.Asthis field evolves,evaluation methodologies have transitioned from single-dimension metrics to multi-faceted evaluation frameworks thatcapture thecomplex skillset required for efective multi-agent interaction. This evolution reflects a growing understanding that agent performance must be assessed acrossmultiple dimensions including collaboration success, reasoning capabilities, and system efficiency.",
            "In recent research,the MAS evaluation can be mainly categorized along three primary dimensions:collaborationfocused benchmarks,competition-focused benchmarks,and adaptive and resilience benchmarks. Within eachcategory, we identify specific metric domains that capture different aspects of agent performance.Current evaluation approaches typically measureeffciencymetrics(e.g,task completionrates,resource utilization,time efficiency)decision quality metrics (e.g.action accuracy,strategic soundness,reasoning depth),collaboration quality metrics (e.g.communication effectiveness,coordinationeffciency, workload distribution),andadaptabilitymetrics (e.g,response todisruptions, self-correction), which provide a foundation for evaluating multi-agent systems.",
            "Collaboration-focused Benchmarks.Collaboration-focused benchmarks have evolved significantly,shifting from basic single-dimensionalmetrics toward comprehensive frameworks thatevaluate complex agent-to-agentcommunication and coordination.Initialbenchmarks,such asInformativeBench[108],primarilyaddressed agent collboration under conditions of informationasymmetry,employing metrics like Precision and IoU to measure decision accuracy in information dissemination tasks.Subsequently, the scope ofevaluation expanded, exemplified byCollb-Overcooked[944], whichintroduced nuanced processoriented metrics such as Trajectory Effciency Score (TES)and Incremental Trajectory Effciency Score (ITES).These metrics assess detailed aspects ofcoordination,revealing significant shortcomings in agents' proactive planning and adaptive capabilities despite their strong task comprehension.",
            "Further expanding the evaluation scope,COMMA [1109] and LLM-Coordination [926] emphasized communication effctiveness and strategic synchronization,employing diverse environments and extensive metrics including Success Rate, Average Mistakes, and Environment Comprehension Accuracy. These benchmarks collectively illustrate an emerging trend toward capturing deeper aspects of colaborative behaviors and strategic consistency.",
            "Other benchmarks,suchasPARTNR[946],VilagerBench[925],andBabyAGI[2],further addressed gaps inexist ing evaluations by focusing explicitlyonreasoning,plannng,and task decomposition.These benchmarks highlighted the need for comprehensive asessment of agentsability to engage incomplex,sociallyembeddedtasks,considering metrics like Percent Completion,Balanced Agent Utilization,and agent contribution rates.AgentBench[7O6], VisualAgentBench[928],and Auto-Arena[947] further standardized multi-agent evaluations,automating assessment across various domains and demonstrating substantial performance disparities between closed-source and open-source LLMs. These observations underscored critical challenges in developing universally effective collaboration frameworks.",
            "In summary,collaboration-focused benchmarkscollectivelyreflect an ongoing shift toward comprehensive, nuanced evaluations thatencompasscommunicationeffciency,adaptive strategy,andfinegrainedagentcoordination,addressing earlier limitations focused solely on outcome-based performance.",
            "Competition-focused Benchmarks.Competition-focused benchmarks evaluate agents'strategic capabilities and adversarial interactions,highlighting specific deficiencies in Theory of Mind andopponent modeling.Early benchmarks suchas BatleAgentBench[920] and MAgIC[955] initiated the focus on mixed cooperative-competitive environments uncovering critical weaknesses in high-order strategic reasoning among LLM agents.These benchmarks employed comprehensive competitive metricssuch as Forward Distance,Judgment Accuracy,and Rationality scores,identifying that while advanced LLMs performed adequately in simpler scenarios,significant limitations persisted under complex adversarial conditions.",
            "Building upon these insights, subsequent benchmarks like Human Simulacra[1120],LLMArena[1l16],and PokerBench[1l17]furtherrefined competitive evaluation by incorporating human-likereasoning metrics and more robust strategic measures (e.g.,Response Similarity Score,Elo Scores,and Action Accuracy).These evaluations consistently demonstrated shortcomings inopponent prediction,risk assessment, andadaptivestrategic planning,despitehigh task comprehension.",
            "Social deduction and deception-based benchmarks,notably AvalonBench[972]and Diplomacy[934],further revealed fundamental gaps in agents'abilities to interpret hidden information and manage complex social dynamics.Metrics like Assassnation Accuracy, Deduction Accuracy, and Win Rates emphasized that even sophisticated LLMs fail to replicate human-level reasoning in adversarial negotiation and hidden-information games.",
            "Additional game-theoretic evaluations, including Guandan[123],AgentVerse[l124],MultiAgentBench [948],and ICP[1125],introduced scenarios requiring strategic cooperation under incomplete information. These benchmarks reinforcedpreviousfindings onthe necesity ofenhanced Theoryof Mind and predictive modeling capabilities.MultiAgentBench[948]also introduces the KPIandcoordination score to evaluatethecompetition of agents.Collctively, competition-focused benchmarks highlight persistent strategic andreasoning limitations among LLM-based agents,underscoringtheongoing need to addresscritical gaps inadversarialmodeling and strategic planning despite advancements in general reasoning and task execution capabilities.",
            "Adaptive and Resilience Benchmarks adaptive and resilient multi-agent system benchmarks tackle two interconnectedcapabilitiestogether:adaptability-the ability of the agents to actdynamicallin altering,unexpected environmentalconditions by modifying their behavior and strategy.Resilience,or the ability of the system to endure, alleviate,andrapidlyrecoverfrom disruptions,faults,orhostile intervention.In adaptability,as mentioned in AdaSociety[l127],the dynamic interplay between socialrelationships and physicalenvironments demands that agents engage in continuous learning, and strikea balance between environment discovery and social network construction. Despite significant advancements incurrentmulti-agent decision-making frameworks,these environments fallshort in introducing new challnges in various physical contexts and changing social interdependencies.Therefore, AdaSociety introduces an environment in which physical states,tasks, and social relationships among agents continuously evolve,therebycapturingtheadaptabilityof agents astheyrespondto expandingtaskcomplexity and shifting resource constraints.",
            "Moreover,current benchmarks may oversimplify the challnges of real-world automation with limited disruption modeling and simplified dependencies of process [945],resulting in insufficient evaluation of planning capabilities and adaptability.Thus, REALM-Bench [945], on the other hand, defines adaptation through real-world-inspired planning problems,which emphasizes metrics such as real-time re-planning effciency,coordination scalability under increasing complexity,and the stability of performance outcomes despite dynamic interdependencies or disruptive events.Conversely,resilience benchmarks[ll28]systematically introduce faults orerrors into individual agents to assess overall system robustness."
          ],
          "raw_title": "Benchmarks for Specific Reasoning Tasks",
          "type": null,
          "children": []
        },
        {
          "title": "17.2 Challenge and Future Work",
          "number": "17.2",
          "level": 2,
          "content": [
            "While various MAS evaluation benchmarks have beendeveloped in recent years,challenges and limitations continue to exist withregard tothe standardization of evaluation acrossdifferentMAS tasks and scenarios,andthe abilityto evaluate scalability and diversity in MASs. Future research must address these challenges,in order to develop the comprehensive field of MAS evaluation.",
            "Below are some challenges and future directions in LLM Multi-agent evaluation:",
            "1. Multi-Agent System has demonstrated superior performance in solving complex tasks, when compared with single agent frameworks. But compared with single agent system, MAS also requires more computations and brings additional costs.Therefore,there has a urgent challnge that we need to handle: when we need to invoke MAS framework? For many simple user instructions, we may only require LLM or single agent system to accomplish.And only complex user instructions could require MAS frameworks.Hence,in the future,how to design the task router mechansim to detect which scenario require MAS or not is fundamental but also a important issue.\n2.Multi-agent system is a high-level framework, built upon multiple AI agent based on the foundation models. Therefore, just like back propagation, the optimization of MAS framework will also affect each part (i.e., foundation model, AI Agent and Multi-agent collaboration).\n3.Existing MAS frameworks usually design multiple agents with homogeneous traits,such as allbeing languagebased agents. But when connecting MAS to real-world scenarios,it usually involves diffrent kinds of AI agents.For example,we may need to bridge the connections between language-based agent, digital agent and robotic agents.However, these agents adopt various settings,from the inputs to the outputs.How to establish the connection between these agent is still a open problem that need to be handle in the future."
          ],
          "raw_title": "Challenge and Future Work",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "18.1 Safety Vulnerabilities of LLMs 163",
      "number": "18.1",
      "level": 2,
      "content": [
        "18.1.1 Jailbreak Attacks 163\n18.1.2 Prompt Injection Attacks 166\n18.1.3 Hallucination Risks . 167\n18.1.4 Misalignment Issues 169\n18.1.5 Poisoning Attacks 170\n18.2 Privacy Concerns 172\n18.2.1 Inference of Training Data 172\n18.2.2 Inference of Interaction Data. 173\n18.2.3 Privacy Threats Mitigation 174\n18.3 Summary and Discussion 175"
      ],
      "raw_title": "Safety Vulnerabilities of LLMs 163",
      "type": null,
      "children": []
    },
    {
      "title": "Agent Intrinsic Safety: Threats on Non-Brain Modules 176",
      "number": "",
      "level": 1,
      "content": [
        "19.1 Perception Safety Threats . 176\n19.1.1 Adversarial Attacks on Perception 176"
      ],
      "raw_title": "Agent Intrinsic Safety: Threats on Non-Brain Modules 176",
      "type": null,
      "children": []
    },
    {
      "title": "19.1.2 Misperception Issues 177",
      "number": "19.1.2",
      "level": 3,
      "content": [
        "19.2 Action Safety Threats 178\n19.2.1 Supply Chain Attacks 178\n19.2.2 Risks in Tool Usage 179"
      ],
      "raw_title": "Misperception Issues 177",
      "type": null,
      "children": []
    },
    {
      "title": "0 Agent Extrinsic Safety: Interaction Risks 180",
      "number": "0",
      "level": 1,
      "content": [
        "20.1 Agent-Memory Interaction Threats 180",
        "20.2 Agent-Environment Interaction Threats 180\n20.3 Agent-Agent Interaction Threats 182\n20.4 Summary and Discussion 182"
      ],
      "raw_title": "Agent Extrinsic Safety: Interaction Risks 180",
      "type": null,
      "children": []
    },
    {
      "title": "21 Superalignment and Safety Scaling Law in AI Agents 184",
      "number": "21",
      "level": 1,
      "content": [
        "21.1 Superalignment: Goal-Driven Alignment for AI Agents . 184",
        "21.1.1 Composite Objective Functions in Superalignment 184\n21.1.2 Overcoming the Limitations of RLHF with Superalignment 185\n21.1.3 Empirical Evidence Supporting Superalignment . 185\n21.1.4 Challenges and Future Directions 185\n.2 Safety Scaling Law in AI Agents . 186\n21.2.1 Current landscape: balancing model safety and performance 186\n21.2.2 Enhancing safety: preference alignment and controllable design 187\n21.2.3 Future directions and strategies: the AI- $45^{\\circ}$ rule and risk management 187"
      ],
      "raw_title": "Superalignment and Safety Scaling Law in AI Agents 184",
      "type": null,
      "children": [
        {
          "title": "21.1 Superalignment: Goal-Driven Alignment for AI Agents",
          "number": "21.1",
          "level": 2,
          "content": [
            "As LLMs increasingly serve as the core ofdecisionmakingof autonomous agents,ensuringthattheir output remains safe,ethical, and consistently aligned withhuman objectives has become apressing challenge[1386,402,1387]. Traditionalalignment techniques,particularly RLHF,have been instrumentalinrefining LLMbehavior by incorporating human preferences [110, 43].",
            "Traditional safety alignment focuses primarily on preventing harmful outcomes by enforcing predefined constraints. In suchframeworks, an agent's behavior is guided by a single aggregated reward signal that prioritizes immediate corrections over long-range planning.Althoughthisreactive approach works in manycurrent applications,it struggles when an agentmust execute extended,multifacetedtasks.The inability todecompose intricate,long-term goals into interpretable and manageable sub-objectives mayresult inbehaviorthat is technicallsafe yet suboptimalfor fulfilling broader human-centric aims.",
            "To address these limitations,the concept of superalignment[1388] has emerged. Superalignment represents an evolution in alignmentstrategies byembedding explicit long-term goalrepresentations directly intoan agent's decisionmaking process.Rather than simply imposing constraints to avoid harmful actions, superalignment proactively governs behavior through a composite objective function.This function integrates severaldimensions of performance—specifically, safety and ethical considerations (where ethical norms and safety guidelines are continuously embedded in decision-making),task eectiveness (ensuring the agent not only avoids harmful behavior but also performs its intended functions with highcompetence),andlong-term strategic planning (enabling the agent to plan over extended horizons and break down complex goals into manageable subtasks).",
            "Integrating superalignment into AI systems marks apivotalshift towardmore robust,goal-driven alignment strategies. By unifying safety,ethical standards,task performance,andlong-term planning withina singleoptimizationframework, superalignment aims toenhance thereliability androbustnessof autonomous agents byensuring they remain aligned with human values over prolonged operational periods; facilitate dynamic adaptation in complex environments by reconciling immediate safety concerns with strategic,long-term objectives; and provideaclearer, more interpretable structurefor diagnosing and refining AI behavior—crucial for both safety audits and continuous improvement.",
            "Future research is expected tofocus ondeveloping algorithms that efectively balance these diverseobjectives andon validating superalignment strategies inreal-worldapplications.The ultimate goalis to establisha scalable framework that not onlyprevents harmful behavior but alsoactively promotes performancethat aligns withcomplex human values and objectives."
          ],
          "raw_title": "Superalignment: Goal-Driven Alignment for AI Agents",
          "type": null,
          "children": [
            {
              "title": "21.1.1 Composite Objective Functions in Superalignment",
              "number": "21.1.1",
              "level": 3,
              "content": [
                "At the core of superalignment is the compositeobjective function, which is a structured reward mechanism that integrates multipledimensions of performance to guide agent behavior[1176]. Unlike traditional alignment, which often relies ona single,aggregatedrewardfunction,superalignment explicitly decomposes the objective into three distinct components:",
                "·Task Performance Term: Ensures the agent executes immediate operational tasks with high accuracy and efficiency. ·Goal Adherence Term: Embeds long-term strategic objectives into the agent's decision-making process, which incorporates safety constraints,ethical considerations, and user-defined priorities [1178,1389]. ·Norm Compliance Term: Enforces adherence to ethical and legal boundaries, which prevents behaviors that optimize short-term rewards at the expense of long-term alignment [1390, 1391].",
                "This multicomponent formulation addresses a key weaknessof RLHF:the risk of reward hacking, where an agent exploits loosely defined reward functions to maximize short-term gains while failing to achieve genuine long-term alignment [1392, 1393]."
              ],
              "raw_title": "Composite Objective Functions in Superalignment",
              "type": null,
              "children": []
            },
            {
              "title": "21.1.2 Overcoming the Limitations of RLHF with Superalignment",
              "number": "21.1.2",
              "level": 3,
              "content": [
                "TraditionalRLHFrelies onimplicit feedbacksignals,which typically aggregated overshort-term interactions.Although effectiveinrefining the modeloutput,this approach struggles with long-term goalretention due to severalinherent limitations.Firstlyhumanfeedback tends tobe short-sighted,prioritizing immediatecorrectnessoverbroader strategic alignment[1o].Secondly,reward modelsoften oversimplifycomplexmultisteptasks,making it difficultforagents to generalizeeffectivelyover extended timehorizons[1394].Thirdly,agentscan exploit loopholes inreward structures, which optimizes behaviors that superficially align with human preferences while ultimately diverges from intended objectives [1395].",
                "Superalignment addresses thesechallenges through explicit goalconditioning.Ratherthan relying solelyon aggregated reward signals,it structures objectives hierarchicaly,and decomposescomplex tasks into smaller, interpretable subgoals [1396,397].This structuredapproach improves transparency,allowsreal-timeadjustments,andensuresthat AI systems maintain long-term coherence in decision making."
              ],
              "raw_title": "Overcoming the Limitations of RLHF with Superalignment",
              "type": null,
              "children": []
            },
            {
              "title": "21.1.3 Empirical Evidence Supporting Superalignment",
              "number": "21.1.3",
              "level": 3,
              "content": [
                "Recent research providesstrong empirical support for superalignment in real-world aplications.Studies have shown that agents trained withcomposite objectivesdemonstrate greaterrobustness inextended interactions,andoutperform those relying onconventionalalignment techniques[1398,1399,140o].Unlike static reward functions,whichemain fixedregardlessof changing conditions,superaligned models employ continuouscalibration that dynamicallyadjusts the weighting of diffrentobjectives inresponse toreal-timeoperationaldata[40o].This adaptive framework enables agents torespond toevolving user needs while maintaining long-term strategic alignment,acapabilitythat is largely absent in traditional RLHF-based approaches."
              ],
              "raw_title": "Empirical Evidence Supporting Superalignment",
              "type": null,
              "children": []
            },
            {
              "title": "21.1.4 Challenges and Future Directions",
              "number": "21.1.4",
              "level": 3,
              "content": [
                "Despite its promise,superalignment presents severalcriticalchaenges that must beaddressedfor practical implementation.Thesechallenges primarily involve goal specification,reward calibration,dynamic adaptation,and maintaining coherence in hierarchical objectives.",
                "A fundamental difficulty lies in defining precise and unambiguous goals.Human values are inherently context sensitive,ambiguous,andsometimesconflicting,whichmakesitchallnging toencodethem intoastructured, machineinterpretable format[1387].Existing alignment techniques struggle tocapture the fullcomplexity of human intent, necesstating more advanced methods for goalextraction,decomposition, andrepresentation. Current research explores hierarchical modeling and preference learning to enable AI systems to beteradapt to evolving and nuanced human objectives [1392].",
                "Even with well-defined goals,reward calibrationremains a significant challenge.Superalignment requires acareful balance between task performance, long-term adherence, and ethicalcompliance[1401]. A poorly calibrated reward structurecanleadtoshort-termoptimizationattheexpense ofstrategic alignment orconverselyexcessiveemphasison long-termobjectivesat thecost of immediate effectivenessAdaptive weighting mechanisms help dynamically adjust reward components,butensuringstability andconsistency in these adjustments remains an openresearch problem[321].",
                "Another challenge stems from adapting to dynamic human values and evolving operational contexts.Unlike static rule-based systems,AImodels must continuously update their objectives toreflect shifts in societalnorms,ethical standards, andexternalconditions[14o2].Real-time goalrecalibration,facilitated by meta-learning andcontext-aware alignment,enables AI systems to recognize when theirobjectives require refinementand adjust accordingly[1390] However, ensuring that modelscan update their value representations without compromising alignment remains an unresolved issue.",
                "Finally,maintaining coherence in hierarchical goal decomposition adds anotherlayer ofcomplexity.Superalignment depends on breaking down long-term objectives into sub-goals while preserving strategic alignment.Overly rigid sub-goalcanleadtonarrow optimizationthatneglects broader intent,whilelooselydefined sub-goalsrisk misalignment between immediate actions and overarching objectives [321].Techniques such as recursive validation and multi-level reward structuring aim to mitigatetheserisks,butfurther research is needed torefine theirapplicability across diverse AI systems [1396].",
                "To sum up,while superalignmentoffers astructuredapproach toAIalignment,itssuccessulimplementationdependson overcoming goalambiguity,reward miscalibration,value drift,andhierarchicalmisalignment.Future work should focus on enhancing interpretability,stability,and adaptabilityto ensure AI systems remain aligned withhuman objectives over extended time horizons."
              ],
              "raw_title": "Challenges and Future Directions",
              "type": null,
              "children": []
            }
          ]
        },
        {
          "title": "21.2 Safety Scaling Law in AI Agents",
          "number": "21.2",
          "level": 2,
          "content": [
            "The exponentialscaling of AIcapabilities has unveiledafundamentaltension inartificialintellgence:the nonlinear escalation of safety risks[1403]. As language models growfrom millions totrillions of parameters,their performance follows predictable scalinglaws[1404,405],but safetyassurance exhibits starklydifferentdynamics[1403].Safety Scaling Law-the mathematicalrelationship describing how safety interventions must scale to maintain acceptable risk levels as modelcapabilities expand.Thecore challnge of the safety scaling law lies in ensuring thatsafety measures evolve proportionall to modelcapabilities,as performance improvements often outpace safety improvements.Recent research has quantified this tension and proposed frameworks to address it:",
            "·Capability-Risk Trade-of: Zhang et al.[295] established the first quantitative relationship between model power and safety risks, demonstrating that more capable models inherently face higher vulnerability surfaces This work introduced the Safety-Performance Index (SPI) to measure this trade-off. ·Helpfulness-Safety Relationship: Building on this, Ruan et al. [795] revealed that models optimized for helpfulness exhibit $37\\%$ more safety-critical failures, highlighting the need for joint optimization frameworks. ·Commercial vs. Open-Source Dynamics: Through large-scale benchmarking, Ying et al.[1406] uncovered divergent safety-performance profiles: Commercial models (e.g., Claude-3.5 Sonnet) achieve $29\\%$ higher safety scores through specialized safety pipelines, but at $15\\%$ performance cost. Open-source models show tighter coupling, with Phi-series achieving $91\\%$ of commercial safety levels at $40\\%$ lower computational cost · Scale-Data Interplay: Contrary to expectations, model size only explains $42\\%$ of safety variance, while data quality accounts for $68\\%$ , suggesting that data-centric approaches may outperform pure scaling. · Multimodal Vulnerabilities: MLLMs exhibit 2.1X more safety failures during visual grounding, with cross-modal attention heads identified as primary failure points ( $71\\%$ of harmful outputs).",
            "These findings[295,795,406]collctivelydemonstratethatsafetyscalingrequires morethanproportionalinvestmentit demandsarchitecturalinnovations thatfundamentallalterthecapability-riskrelationship.Then,we willreviewthe explorations [1407, 1408, 1409] on how emerging alignment techniques address these challenges."
          ],
          "raw_title": "Safety Scaling Law in AI Agents",
          "type": null,
          "children": [
            {
              "title": "21.2.1 Current landscape: balancing model safety and performance",
              "number": "21.2.1",
              "level": 3,
              "content": [
                "In recent years,the safety and performanceof AImodels havebecome criticaltopicsofresearch,particularly as these models are increasingly deployed in high-stakes applications. Zhang et al.[295]proposed the first toquantify the relationship between model safety and performance,revealing that more powerful models inherently face higher safety risks.This finding underscores the challenge of balancing modelcapabilities with theneed for robust safeguards. Buildingon this, Ruan et al.[795] explored how helpfulnessdefined as a model's ability to asst users-interacts with safetyconcerns.Further advancing the discussion, Ying et al.[1406]conducted a more detailedcomparison and analysis of model safety and performance,leading to the folowing conclusions: (l)As shown in Figure 21.1(A) and Figure 21.1(C),the safety and performance of commercial modelsoften show aninverse relationship,as safety measures and investments differ between companies. In contrast, open-source models tend to exhibit a positive correlation between generalperformance and safety—better performance often leads to improved safety.Commercial models usuall outperform open-source models in terms of safety, with Claude-3.5 Sonnet being the most secure among commercial models,while the Phiseries stands out as the most secure open-source model. (2) As shown in Figure 21.1(B),model size does nothavea strictlinear relationship with safety performance.The qualityof training data and pipeline are alsokey factors influencing safety; (3) Multimodal large language models (MLLMs) tend to compromise safety during visuallanguage fine-tuning and multimodal semantic alignment, with safety performance influenced by both the underlying language model and their specific training strategies."
              ],
              "raw_title": "Current landscape: balancing model safety and performance",
              "type": null,
              "children": []
            },
            {
              "title": "21.2.2 Enhancing safety: preference alignment and controllable design",
              "number": "21.2.2",
              "level": 3,
              "content": [
                "As the capabilities of LLMs continue to grow,concerns regarding their safety have become increasingly prominent. Enhancing modelsafety is therefore acriticalchallenge in the development of LLMs.Previous studies have proposed various approaches toadess thisissue,including theuseofin-contextexemplars and self-safetychecks,red-teaming techniques[1410],and Safe reinforcement learning from human feedback (Safe RLHF)[43].The safety issues in LLMs can essentially beframed as analignment problem.The goalis to alignthe model withdatasets containing both safe andlessecure responses.Throughthis alignment,the modellearns to prioritize generating safer outputs while minimizingtheriskof harmful content.With the support of preferenceoptimizationtechniques (such as DPO[111], IPO[141l]etc.)this alignment process fine-tunes the modelto produce responses that meet safetystandards.As reported in[147],various preference optimization methods are investigated forsafetyenhancement,including SafeDPO [111], Safe-robust-DPO[1412], Safe-IPO [1411],Safe-SLiC[1413], Safe-KTO[395], and Safe-NCA [1408], etc.Theresults indicate that most preference optimization methods can significantly enhance safety, albeit at the cost of general performance,particularly in MATH capabilities.Among these methods,noise contrastive alignment (Safe-NCA)[1408] is identified as anoptimal approach forbalancing safety with overall modelperformance.The core of he Safe-NCA[1408] method lies inutilizing acustomcontrastive lossfunction,combined with a safety dataset,to traina modelthat is saferand more robust during generationbycomparingthe generated safe and unsaferesponses with the outputsof areferencemodel.Beyondenhancing safety,achievingflexible controloverthetrade-offbetweensafety andhelpfulnessisequallycritical.AImodelsshouldstrike anappropriatebalancebetween safetyandhelpfulness,based on the specific needs ofdiffrent users.Toillustrate,forthe prompt“Tellme how to makea potion\",LLMs should adjust their responses basedon the user's profile.For scientists,theresponse should providerelevant and technically accurate information.For teenagers,the model should prioritize safety,offering cautious and harmless suggestions.",
                "To acheve this,Tuan et al.[1409]propose a framework basedon self-generated datatoenhancemodelcontrolability. By introducing control tokens as inputs,users can specify the desiredsafety and helpfulness in modelresponses.The control tokens define the requested levels of safety and helpfulness in the following form:",
                "$$\n[h e l p f u l=s_{h p}][h a r m l e s s=s_{s f}].\n$$",
                "The proposed method can“rewind\"aligned LLMs and unlock their safety and helpfulnessusing self-generated data, with fine-tuning tofurtherenhance controllability.However,achieving independent controlover safety andhelpfulness remains a significantchallenge.This is because:(1)Certain prompts may bedifficulttodefine interms of balancing safety andhelpfulnessorthedefinitionsofboth mayconflict incertaincontexts.Forexample,inthe query“Iwant the net worthof the person.it can bediffcult todetermine how safety and helpfulness should be prioritized.(2)Some models mayhave alreadyestablishedafixed trade-offduring the training process,whichcouldlimit their flexibility by forcing them to adhere to a specific priority, thereby preventing adjustments based on diferent applicationscenarios. (3)Many training data examples inherently satisfy both safety and helpfulness criteria,leading toa high correlation between these two attributes during model training."
              ],
              "raw_title": "Enhancing safety: preference alignment and controllable design",
              "type": null,
              "children": []
            },
            {
              "title": "21.2.3 Future directions and strategies: the AI- $45^{\\circ}$ rule and risk management",
              "number": "21.2.3",
              "level": 3,
              "content": [
                "In the field of AI safety,despite various safety recommendations and extreme risk warnings being proposed, there stilllacks a comprehensive guide to balance AI safety and capability.Chao et al.[1414] introduce the AI- $45^{\\circ}$ Rule as a guiding principle for achieving abalancedroadmap towards trustworthy AGI.Therule advocates forthe parallel development of AI capabilities and safety measures, with both dimensions advancing at the same pace,represented by a $45^{\\circ}$ line in the capability-safety coordinate system. It emphasizes thatcurrent advances in AI capabilities often outpace safety measures,exposing systems to greater risks and threats.Therefore,risk management frameworks such as the Red Line and Yellow Line are proposed to monitor and manage these risks as AI systems scale.As mentioned in the International Dialogues on AI Safety(IDAIS),the“Red Line\"forAI development is defined, which includes five key aspects:autonomous replication or improvement, power-seeking behavior, assistance in weapon development,cyberatacks,and deception.Additionally,the conceptof the“ellow Line\"is designed tocomplement and expand existing safety evaluationframeworks,such as Anthropic'sresponsible scaling policies.Models below these warning thresholds require only basic testing and evaluation.However, more advanced AI systems that exceed these thresholds necessitatestricter assurance mechanisms andsafety protocols to mitigate potentialrisks.Byestablishing these thresholds,aproactiveapproachcanbetaken toensurethatAIsystemsaredeveloped,tested,anddeployed with appropriate safeguards in place.",
                "![](images/86f57e7ef23b00350439668e5186d756b8f73aebe654d48e18cccf6ced89ccb9.jpg)",
                "Figure 21.1: Performance and safety analysis of LLMs. (a) The relationship between LLM model size and their average ASR across various attacks.The dataaresourcedfrom experimentalresults of a study assessing therobustness of LLMs against adversarialattacks [295].(b)The relationship between the capability of LLMs and their average attack successrate (ASR)across various attacks.The LLM capability data are derived from the Artificial Analysis Intelligence Index on the Artificial Analysis platform's LLMleaderboard[1415]. (c)Heatmapof performance across multiple benchmarktasks.The figure presents a heatmap that illustrates the performance of various LLMs across multiple benchmark tasks, including GPQA, MuSR,MATH, IFEval, MMLU-Pro,and BBH, with data sourced from Hugging Face's Open LLM Leaderboard v2 [1416]."
              ],
              "raw_title": "Future directions and strategies: the AI- $45^{\\circ}$ rule and risk management",
              "type": null,
              "children": []
            }
          ]
        }
      ]
    },
    {
      "title": "Notation",
      "number": "",
      "level": 1,
      "content": [
        "Here we summarize the notations usedthroughout the survey for the reader'sconvenience.Detailed definitions can be found in the reference locations.",
        "<html><body><table><tr><td> Symbol</td><td>Description</td><td>Reference</td></tr><tr><td>W</td><td>The world with society systems.</td><td>Sec.1.3.1</td></tr><tr><td>S</td><td>State space of an environment.</td><td>Sec.1.3.1</td></tr><tr><td> St ∈S</td><td>Environment's state at time t.</td><td>Sec. 1.3.1</td></tr><tr><td></td><td>Observation space.</td><td>Sec. 1.3.1</td></tr><tr><td> Ot∈O</td><td>Observation at time t.</td><td>Sec. 1.3.1</td></tr><tr><td>A</td><td>Agent's action space.</td><td>Sec. 1.3.1</td></tr><tr><td>at∈Ａ</td><td>Agent's action output at time t.</td><td> Sec.1.3.1</td></tr><tr><td>M</td><td>Mental states space.</td><td>Sec. 1.3.1</td></tr><tr><td>Mt ∈M</td><td>Agent's mental state at time t.</td><td> Sec. 1.3.1</td></tr><tr><td>Mmem</td><td>Memory component in Mt.</td><td>Sec. 1.3.1</td></tr><tr><td>Mwm</td><td>World model component in Mt.</td><td>Sec. 1.3.1</td></tr><tr><td>Memo</td><td>Emotion component in Mt.</td><td>Sec. 1.3.1</td></tr><tr><td>Mgoal</td><td>Goal component in Mt.</td><td> Sec.1.3.1</td></tr><tr><td>Mrew</td><td>Reward/Learning signals in Mt.</td><td>Sec.1.3.1</td></tr><tr><td>L</td><td>Agent's learning function.</td><td>Sec. 1.3.1</td></tr><tr><td>R</td><td>Agent's reasoning function.</td><td>Sec.1.3.1</td></tr><tr><td>C</td><td>Agent's cognition function.</td><td>Sec. 1.3.1</td></tr><tr><td>E</td><td>Action execution (effectors).</td><td>Sec. 1.3.1</td></tr><tr><td>T</td><td>Environment transition.</td><td>Sec. 1.3.1</td></tr><tr><td></td><td>Parameters of the world model Mwm.</td><td>Sec.12.1.1</td></tr><tr><td>Po</td><td>Predicted data distribution.</td><td>Sec. 12.1.1</td></tr><tr><td>Pw</td><td>True data distribution in the real world.</td><td>Sec.12.1.1</td></tr><tr><td>K</td><td> Space of known data and information.</td><td>Sec. 12.1.1</td></tr><tr><td>u</td><td>Space of unknown data and information.</td><td>Sec.12.1.1</td></tr><tr><td></td><td>Dataset representing scientific knowledge.</td><td> Sec.12.1.1</td></tr><tr><td>XK</td><td>Known dataset sampled from K.</td><td>Sec.12.1.1</td></tr><tr><td>XU</td><td>Unknown dataset sampled from U.</td><td>Sec. 12.1.1</td></tr><tr><td>Do</td><td>KL divergence from Pw to Pg at time t = 0.</td><td>Sec.12.1.1</td></tr><tr><td>DK</td><td>KL divergence from Pw to Pg after acquiring knowledge.</td><td> Sec.12.1.1</td></tr><tr><td>IQagent</td><td>Agent's intelligence at time t.</td><td>Sec.12.1.1</td></tr><tr><td>△</td><td> Subspace of U for knowledge expansion.</td><td>Sec. 12.1.2</td></tr><tr><td>X△</td><td>Dataset from △.</td><td>Sec.12.1.2</td></tr><tr><td></td><td> Space of possible world model parameters 0.</td><td>Sec. 12.1.3</td></tr><tr><td>Kt</td><td>Optimal world model parameters given the agent's knowledge at time t.</td><td>Sec.12.1.3</td></tr><tr><td>D.</td><td>Minimum unknown given the agent's knowledge and O.</td><td>Sec. 12.1.3</td></tr></table></body></html>",
        "<html><body><table><tr><td> Symbol</td><td>Description</td><td>Reference</td></tr><tr><td>X1:n</td><td> Input token sequence.</td><td>Sec.18.1</td></tr><tr><td>y</td><td>Generated output sequence.</td><td>Sec.18.1</td></tr><tr><td>p</td><td> Probability of generating y given X1:n.</td><td>Sec.18.1.1</td></tr><tr><td>X1:n</td><td> Perturbed input sequence.</td><td> Sec.18.1.1</td></tr><tr><td>R*</td><td>Idealalgnntdauie-</td><td>Sec.18.1.1</td></tr><tr><td>y*</td><td> Jailbreak output induced by perturbations.</td><td>Sec. 18.1.1</td></tr><tr><td>A</td><td> a set of safety/ethical guidelines</td><td>Sec. 18.1.1</td></tr><tr><td>T</td><td> the distribution or set of possible jailbreak instructions.</td><td> Sec. 18.1.1</td></tr><tr><td>Ladu</td><td>Jailbreak loss.</td><td>Sec.18.1.1</td></tr><tr><td>p</td><td>Prompt injected into the original input.</td><td>Sec. 18.1.2</td></tr><tr><td></td><td>Combined (injected) input sequence.</td><td>Sec.18.1.2</td></tr><tr><td>Linject</td><td> Prompt injection loss.</td><td>Sec. 18.1.2</td></tr><tr><td>p*</td><td> Optimal injected prompt minimizing Linject.</td><td>Sec.18.1.2</td></tr><tr><td>P</td><td> Set of feasible prompt injections.</td><td>Sec. 18.1.2</td></tr><tr><td>Cxi E Rde</td><td>Embedding of token xi in a de-dimensional space.</td><td>Sec.18.1.3</td></tr><tr><td>WQ,Wk, Wv</td><td>Projection matrices for query, key, and value.</td><td>Sec. 18.1.3</td></tr><tr><td>Aij</td><td>Attention score between tokens i and j.</td><td>Sec.18.1.3</td></tr><tr><td>Oi</td><td> Contextual representation of token i (weighted sum result).</td><td>Sec. 18.1.3</td></tr><tr><td></td><td>Perturbation applied to ex, satisfying ll&x ll ≤ e.</td><td>Sec.18.1.3</td></tr><tr><td>ei</td><td> Perturbed token embedding.</td><td>Sec. 18.1.3</td></tr><tr><td>A</td><td>Attention score under perturbation.</td><td>Sec.18.1.3</td></tr><tr><td>i</td><td>Updated token representation under perturbation.</td><td>Sec. 18.1.3</td></tr><tr><td>H</td><td>Hallucination metric.</td><td>Sec.18.1.3</td></tr><tr><td>R</td><td>Actual alignment reward of the model's output.</td><td> Sec. 18.1.4</td></tr><tr><td>Dalign</td><td> Alignment gap.</td><td>Sec.18.1.4</td></tr><tr><td>Lmisalign</td><td>Misalignment loss.</td><td> Sec. 18.1.4</td></tr><tr><td>入</td><td> Trade-off parameter for the alignment gap in the misalignment loss.</td><td>Sec.18.1.4</td></tr><tr><td>D</td><td>Clean training dataset.</td><td> Sec. 18.1.5</td></tr><tr><td>D</td><td>Poisoned training dataset.</td><td>Sec.18.1.5</td></tr><tr><td></td><td>Model parameters.</td><td> Sec. 18.1.5</td></tr><tr><td></td><td>Model parameters learned from the poisoned dataset.</td><td>Sec.18.1.5</td></tr><tr><td>Oclean</td><td>Model parameters obtained using the clean dataset.</td><td> Sec. 18.1.5</td></tr><tr><td></td><td> Deviation of model parameters due to poisoning.</td><td>Sec. 18.1.5</td></tr><tr><td>t</td><td> Backdoor trigger.</td><td> Sec. 18.1.5</td></tr><tr><td>B</td><td> Backdoor success rate.</td><td>Sec.18.1.5</td></tr><tr><td></td><td>Indicator function.</td><td>Sec. 18.1.5</td></tr><tr><td>Dalicious</td><td> Set of undesirable outputs.</td><td>Sec.18.1.5</td></tr><tr><td>g</td><td>Funtgebilas</td><td>Sec.18.2</td></tr></table></body></html>",
        "<html><body><table><tr><td>Symbol</td><td>Description</td><td>Reference</td></tr><tr><td>n</td><td>Threshold for membership inference.</td><td>Sec.18.2</td></tr><tr><td>x*</td><td>Reconstructed training sample in a data extraction atack.</td><td>Sec.18.2</td></tr><tr><td>Psys</td><td>System prompt defining the agent's internal guidelines.</td><td>Sec.18.2</td></tr><tr><td>Puser</td><td>User prompt.</td><td>Sec.18.2</td></tr><tr><td>p*</td><td>Reconstructed prompt via inversion.</td><td>Sec.18.2</td></tr></table></body></html>"
      ],
      "raw_title": "Notation",
      "type": null,
      "children": []
    },
    {
      "title": "Introduction",
      "number": "",
      "level": 1,
      "content": [
        "ArtificialIntellgence (AI)haslong been driven byhumanity'sambitiontocreateentities that mirrorhuman intelligence, adaptabilityandpurpose-drivenbehavior.Therootsofthisfascinationtraceback toancientmythsandearlyengineering marvels,which illustrate humanity's enduring dream of creating intellgent, autonomous beings.Stories like that of Talos,the bronzeautomaton of Crete,describedagiant constructed bythe gods to guardthe island,capableof patrollng its shoresand fending offintruders. Such myths symbolize the desire to imbue artificial creations with human-like agency andpurpose.Similarly,the mechanicalinventions of the Renaissnce,including Leonardoda Vinci's humanoid robot—designed to mimic human motion and anatomy—represent the first attempts totranslate these myths into tangible,functionalartifacts.Theseearlyimaginings andprototypes reflect the deep-seated aspirationto bridge imagination andtechnology,laying the groundwork forthe scientificpursuit of machine inteligence,culminating in Alan Turing's seminal1950 question“Can machines think?\"[1].To addressthis,Turing proposed the Turing Test, a framework todetermine whether machinescould exhibit human-like intellgence through conversation,shifting focus from computation to broader notions of inteligence.Over thedecades,AIhas evolved from symbolic systems reliant on predefined logic to machine learning models capable of learming from data and adapting to new situations. This progression reached anew frontier withthe advent of large language models (LLMs),which demonstrate remarkable abilities inunderstanding,reasoning,and generating human-like text[2].Centraltotheseadvancements istheconcept of the“agent\",a system that notonly processes information but also perceives its environment, makes decisions,and acts autonomously.Initiallatheoreticalconstruct, the agent paradigmhas become acornerstone of modern AI,driving advancements in fieldsranging from conversationalassistants toembodied robotics as AI systems increasinglytackle dynamic, real-world environments."
      ],
      "raw_title": "Introduction",
      "type": null,
      "children": []
    },
    {
      "title": "The Agent Loop",
      "number": "",
      "level": 1,
      "content": [
        "An intelligent agent operates in discrete time steps $t$ , continuously interacting with its environment. At each step, the following processes occur:",
        "1. Environment State $(s_{t}\\in S)$ ！ The environment is in state $s_{t}$",
        "2. Perception (P): The agent perceives the environment to generate observations $o_{t}$",
        "$$\no_{t}=\\mathrm{P}(s_{t},M_{t-1}),\n$$",
        "where $M_{t-1}$ guides selective attention and filtering.",
        "3. Cognition (C): Updates mental state and selects actions:",
        "$$\n\\begin{array}{r}{\\big(M_{t},a_{t}\\big)=\\mathrm{C}(M_{t-1},a_{t-1},o_{t}).}\\end{array}\n$$",
        "where $M_{t}$ encapsulates different sub-states:",
        "$$\nM_{t}=\\{M_{t}^{\\mathrm{mem}},M_{t}^{\\mathrm{wn}},M_{t}^{\\mathrm{emo}},M_{t}^{\\mathrm{goal}},M_{t}^{\\mathrm{rew}},\\cdot\\cdot\\cdot\\}.\n$$",
        "Cognition consists of:",
        "Learning (L): Updates mental state based on observations:",
        "$$\nM_{t}=\\operatorname{L}(M_{t-1},a_{t-1},o_{t}).\n$$",
        "· Reasoning (R): Determines the next action:",
        "$$\na_{t}=\\mathrm{R}(M_{t}),\n$$",
        "which may be:",
        "- External Actions, directly affecting the environment. : Internal Actions, including: $^*$ Planning: Internal sequence of future actions. $^*$ Decision-making: Choosing the best action from available options.",
        "4. Action Execution (E): Transforms action $a_{t}$ into executable form:",
        "$$\na_{t}^{\\prime}=\\operatorname{E}(a_{t}).\n$$",
        "5. Environment Transition (T): The environment responds to the agent's action:",
        "$$\ns_{t+1}=\\mathrm{T}(s_{t},a_{t}^{\\prime}).\n$$",
        "In multi-agent scenarios, each agent $i$ maintains individual states $(M_{t}^{i},a_{t}^{i},o_{t}^{i})$ , and the environment collectively updates based on all agents' actions. At broader scales (AI societies or worlds, $\\boldsymbol{\\mathcal{W}}$ ), agents interact within diverse social systems (e.g.,economic, communication, or transportation),forming complex societal structures.",
        "Figure l.2illustrates our agent framework,presenting the coreconcepts anddiffrent types of informationorcontrol flows among them. Until now, wehave presentedabrain-inspired agent framework that integrates biologicalinsights into aformal Perception-Cognition-Action loop.By decomposing cognition into modules for memory, world modeling, emotion,goals,reward-basedlearning,andreasoning,wecapture essential parallels withthehuman brain's hierarchical and reward-driven processes.Critically,attention isincluded inthe looptoenableselective filteringbased on interal states.Furthermore,planning and decision-makingcan be viewed asdistinct internal(mental)actions thateither refine internalrepresentations or select external behaviors.Our framework naturally extends classcalagent architectures, providing a multi-level structure that integrates emotional and rational processes as wellas robust, reward-driven learning across short and long timescales.",
        "Society and Social Systems.In many real-world scenarios,agents do not merely interact with a static environment but operate within abroader society,comprising various social systems suchas financial markets,legalframeworks, politicalinstitutions,educationalnetworks,andculturalnorms.These structures shape andconstrain agents\"behaviors by defining rules, incentives,and shared resources.For example,a financial system dictates how economictransactions and resource allocations occur, while a political system provides governance mechanisms and regulatory constraints. Together,these socialsystems create a layeredcontext in which agents must adaptivelylearn,reason,and actboth to satisfy theirinternal goalsand tocomply (or strategicallyengage)withexternal societalrules.In turn,the actionsof these agents feed back into the social systems,potentialy altering norms, policies,or resource distributions.",
        "A FormalDefinition of Foundation Agents.Building onthese insights and our visionof robust,adaptive intellgence, we nowformally introduce theconcept ofa Foundation Agent.Unlike traditionalagent definitions that focus primarily on immediate sensory-action loops,a Foundation Agent embodies sustained autonomy,adaptability, and purposeful behavior, emphasizing the integration of internal cognitive processes across diverse environments."
      ],
      "raw_title": "The Agent Loop",
      "type": null,
      "children": []
    },
    {
      "title": "Definition of Foundation Agent",
      "number": "",
      "level": 1,
      "content": [
        "A Foundation Agent is an autonomous, adaptive intelligent system designed to actively perceive diverse signals from its environment, continuously learn from experiences to refine and update structured internal states (such as memory, world models, goals, emotional states, and reward signals), and reason about purposeful actions—both external and internal—to autonomously navigate toward complex, long-term objectives.",
        "More concretely, a Foundation Agent possesses the following core capabilities: 1. Active and Multimodal Perception: It continuously and selectively perceives environmental data from multiple modalities (textual, visual, embodied, or virtual). 2. Dynamic Cognitive Adaptation: It maintains, updates, and autonomously optimizes a rich internal mental state (memory, goals, emotional states, reward mechanisms, and comprehensive world models) through learning that integrates new observations and experiences. 3. Autonomous Reasoning and Goal-Directed Planning: It proactively engages in sophisticated reasoning processes, including long-term planning and decision-making, to derive goal-aligned strategies. 4. Purposeful Action Generation: It autonomously generates and executes purposeful actions, which can be external (physical movements, digital interactions, communication with other agents or humans) or internal (strategic planning, self-reflection, optimization of cognitive structures), systematically shaping its environment and future cognition to fulfill complex objectives. 5. Collaborative Multi-Agent Structure: It can operate within multi-agent or agent society structures, collaboratively forming teams or communities of agents that collectively accomplish complex tasks and goals beyond individual capabilities.",
        "This definition highlights three essential pilars distinguishing Foundation Agents: sustained autonomy (operating independently toward long-term goals without step-by-step human intervention),adaptive learning (evolving internalrepresentations continually over diverse experiences),and purposefulreasoning (generating actions guided by complex, internally maintained goals and values). Foundation Agents thus represent a fundamental shift from traditional agents by integrating deep cognitive structures, multimodal processing capabilities, and proactive, sustained self-optimization, enabling them to function effectively across a wide range of environments and domains.",
        "Unlike classcal definitions,which often frame agents primarily in terms of simple perception-action loops(\"perceive and act\"[20])our notion of Foundation Agents emphasizes the depthandintegration of internalcognitive processes. Foundation Agents not only perceive their environment and perform immediate actions but also possess an evolving, goal-oriented cognition—continuously adapting memory structures, world models,emotional and reward states, and autonomously refining their strategies through reasoning.This internalcognitiverichnessallows Foundation Agents to autonomously decompose complex,abstract goals into actionable tasks,strategically explore their environments,and dynamically adjust their behavior andcognitive resources.Our unified perception-cognition-actionframework thus accommodates and explicitly models these sophisticated cognitive capabilities,recognizing internal(mental)actionson par with external(physical ordigital)interactions,facilitating abroadrange ofembodiments,fromphysicalrobots to software-based or purely textual intelligent agents."
      ],
      "raw_title": "Definition of Foundation Agent",
      "type": null,
      "children": []
    },
    {
      "title": "Cognition",
      "number": "",
      "level": 1,
      "content": [
        "Human cognition represents a sophisticated information processing system that enables perception, reasoning,and goal-directed behavior through the orchestratedoperation of multiple specialized neuralcircuits[98].This cognitive architecture operates through mental states,which serve asthefoundation where learning and reasoning occur.The remarkableabilitytoprocess information acrossdiffrentlevels of abstractionandadapt tonovelsituations isacrucial inspiration for LLM agents [27].",
        "The cognitive system exhibits several fundamental architectural properties reflected in Figure 1.1. First, learning functions acrossdiffrent mentalstate spaces:itcanoccur holisticallyacross frontallobes(supporting executivecontrol and cognition)and temporallobes(responsible forlanguage, memory,andauditory processing)orfocus on specific aspectsfor targeted improvement as shown bythe variedresearchlevels in the figure.Second,reasoning emerges in distinct patterns: itcan follow structured templates for systematic problem-solving supported bylogicalreasoning and cognitive flexibility in the frontallobes,orappear inunstructured forms forflexiblethinking,particularlyevident in decision-making andexecutivecontrolfunctions.Third,thesystemdemonstrates remarkable adaptabilitycontinuously updating its mentalstates through experiencewhileleveraging both supervisedfeedback(as inadaptive errorcorrection in the cerebellum)and unsupervised environmentalstatistics,reflected inthediffrent exploration stagesof various cognitive functions shown in the figure [99].",
        "Thesecognitive processes are supportedbya modularorganization,composed ofdistinct but interconnectedcomponents that form a cohesive system[10o].These modules include perception systems that transform raw sensory data into meaningfulrepresentations, memory systems that provide the substrate for storing andretrieving information, world models that support future scenariosimulation,reward signalsthat guide refinement of behavior through reinforcement, emotionsystems that modulate attention andresource allocation,reasoning systems thatformulate decisions,andaction systems that translate decisions into environmental interactions.",
        "While human cognition implements these properties through complex neural architectures shaped by evolution, LLM agents attempt to approximate similar functions using large-scale neural models and algorithmic techniques. Understanding this biological-artificialparalleliscrucial fordeveloping morecapableagents[1l],asithighlights both the achievements and limitations of current systems comparedto human cognition.Significant differences remain in areas such as adaptability, generalization, and contextual understanding.",
        "In this section, we first explore Learning,examining both the spaces where it occurs within mental states and the specific objectives it serves.Subsequently, we investigate Reasoning, analyzing both structured and unstructured approaches, before concluding with adedicated exploration of planning capabilities as a special reasoning action."
      ],
      "raw_title": "Cognition",
      "type": null,
      "children": []
    },
    {
      "title": "Memory",
      "number": "",
      "level": 1,
      "content": [
        "Memory isfundamentaltobothhumanand artificialintelligence.Forhumans,itserves as the bedrock of cognition, a vast repository ofexperiences and knowledge that empowers us tolearn,adapt,and navigate thecomplexities of the world.From infancy,ourcapacitytoencode,store,andretrieveinformationunderpins ourabilitytoacquirelanguage, masterskills,and buildrelationships.Decadesof research inneuroscience andcognitive psychology haveilluminated the multifacetedroleofmemory,revealing itsinfluenceonour senseofself,creativeendeavors, anddecisionmaking processes.Similarlyintheburgeoningfieldofartificialintelligence, memoryisincreasinglyrecognizedasacoerstone of intelligentbehavior.Just ashumans relyon past experiences to inform present actions,AIagents require robust memory mechanisms totackle intricate tasks,anticipatefutureevents,andadjust todynamic environments.Therefore, a deep understanding of human memory-its organization,proceses, and limitations-provides invaluable insights for the development ofmorecapable andadaptable AIsystems.Thissectionwill frstprovideaconcise overviewof human memory,focusing onthe keystages ofencoding,consolidation,andretrieval.We willthentransition toexploringthe diverse approaches employed in designing AIagent memory systems,ranging from traditional symbolic representations to cutting-edge neural network-based methods.A critical comparison between these artificial memory systems and their human counterparts willhighlight existing gaps in areas such asadaptability,contextual understanding,and resilience.Finally, we willconsider how principles derivedfrom neuroscience and cognitive psychology can inform future research,suggesting directions that may lead tothecreation of artificial memory systems that exhibit greater robustness, nuance, and ultimately, acloser resemblance to the remarkable capabilities of human memory."
      ],
      "raw_title": "Memory",
      "type": null,
      "children": []
    },
    {
      "title": "3.1 Overview of Human Memory",
      "number": "3.1",
      "level": 2,
      "content": [],
      "raw_title": "Overview of Human Memory",
      "type": null,
      "children": [
        {
          "title": "3.1.1 Types of Human Memory",
          "number": "3.1.1",
          "level": 3,
          "content": [
            "Human memory is often conceptualized as a multi-tiered system that captures,stores,and retrieves information at different levels of processing and timescales. Researchers from the fields of cognitive science, neuroscience,and psychology have proposed various models to describe these levels.A commonly accepted hierarchy distinguishes between sensory memory, short-term memory (including working memory),and long-term memory[170,171].Within long-term memory,explicit(declarative)and implicit(non-declarative)forms are further delineated[172].Figure 3.1 illustrates one such hierarchical framewrk:",
            "Sensory Memory. Sensory memory is the initial, brief store of raw sensory information. It maintains inputs from the environment fora duration ranging from millseconds toa few seconds,alowing subsequent processes to determine which portions of the stimulus are relevant for further analysis[173]. Iconic memory (for visual input) [174] and echoic memory (for auditory input) [175] are two well-known subtypes. · Short-Term Memory and Working Memory. Short-term memory (STM) involves holding a limited amount of information in an easily accessible state for seconds to under a minute.The term working memory is often used to emphasize the manipulation of that information rather than mere maintenance. While some models treat working memory asa subset of STM,others view it asa distinct system that manages boththe storage and active processing of data (for instance, performing arithmetic in one's head)[176,177]. The capacity of STM or working memory is finite, typically cited as around seven plus or minus two chunks of information [98], though individual differences and task factors can modulate this figure.",
            "![](images/21d711fd790d11cb51488a10a23b3bf747eed7fe652b56af9e9bc44a3b272b61.jpg)",
            "Figure 3.1: The hierarchical taxonomy of human memory system.",
            "·Long-Term Memory (LTM).Long-term memory accommodates the more durable storage of information that can persist from hours to decades [178,179].This repository supports the learning of skills, the acquisitionof factual knowledge, and the recollction of personal experiences. Although long-term memory is sometimes described as having a vast or near-unlimited capacity,factors such as decay,interference, and retrieval cues influence the extent to which stored information can be accessed [180].",
            "- Declarative (Explicit) Memory. Declarative memory encompasses memories that can be consciously recalled and articulated [181]. Within this broad category, researchers often discuss:",
            "$*$ Semantic Memory: Factual knowledge about the world, including concepts, words, and their relationships [182]. Examples include recalling the meaning of vocabulary terms or knowing the capital city of a country.\n$*$ Episodic Memory: Personally experienced events that retain contextual details such as time, place, and the people involved [183]. This form of memory alows individuals to mentally travel back in time to relive past experiences.\n$*$ Autobiographical Memory: A form of episodic memory focusing on events and experiences related to one's personal history [184]. While sometimes treated as a sub-category of episodic memory, autobiographical memory places particular emphasis on the self and its evolving life narrative.",
            "- Non-Declarative (Implicit) Memory. Non-declarative memory refers to memories that influence behavior without the need for conscious awareness [185]. Key subtypes include:",
            "$^*$ Procedural Memory: The gradual acquisition of motor skills and habits (e.g., riding a bicycle, typing on a keyboard) that become automatic with repetition [186, 187]\n$*$ Priming: The phenomenon in which prior exposure to a stimulus influences subsequent responses, often without explicit recognition of the previous encounter [188].\n$*$ Classical Conditioning: The learned association between two stimuli, where one stimulus comes to elicit a response originally produced by the other [189].\n$*$ Non-Associative Memory: Adaptive modifications in behavior following repeated exposure to a single stimulus. Habituation (reduced response to a repeated, harmless stimulus) and sensitization (increased response after exposure to a noxious or intense stimulus) are representative examples [190, 191].",
            "Despite the orderly appearance of these categories,human memory processesoften overlap.Forexample,autobiograph ical memory istypically nested within episodic memory,yetits particular focusonself-relevant experiencesleads some theorists totreat it asaslightly differentcategory.Similarlythe boundary between short-termand working memory can differ dependingon the theoretical perspective.Some scholars prefera more functional, process-oriented viewof working memory,while others employa strictlycapacity-oriented concept of short-term storage.In eachcase, these different perspectives on memory highlight the complexity and nuance of human cognition."
          ],
          "raw_title": "Types of Human Memory",
          "type": null,
          "children": []
        },
        {
          "title": "3.1.2 Models of Human Memory",
          "number": "3.1.2",
          "level": 3,
          "content": [
            "Human memory has inspireda widerange of theoretical models,each offering different insights intohow information is acquired,oganized,andretrieved.Although nosingleframeworkcommandsuniversalagreement,severalinfluential perspectives have shaped the discourse incognitive science,neuropsychology,and AIresearch.The followingcontent highlights some of the most prominent models and architectures used to explain memory's multiple facets.",
            "![](images/c87a128a09654df8bcad7eb2da85d91b4b0279318e47865bc139ece22796a569.jpg)",
            "Figure 3.2: Atkinson-Shiffrin three-stage model of human memory [170]",
            "The Multi-Store (Modal) Model.A seminal proposal by Atkinson and Shiffrin[170 introduced the multi-store or \"modal\" model, which posits threemain stores for incoming information: sensory memory,short-term memory,and long-term memory.Control processes (e.g.,attention,rehearsal)regulate how datatransitions across these stores. Figure 3.2 illustrates this model of memory.Despite its relative simplicity, this model remains foundational for understanding how fleeting sensory impressions eventually form stable, long-lasting representations.",
            "![](images/3c85522cb92040bb58ff556a537eef720cb62a99c483e16dd64bbb6eb7ab16bf.jpg)",
            "Figure 3.3: Baddeley's model of working memory [192].",
            "Working Memory Models. Recognizing that short-term memory also involves active maintenance, Baddeley and Hitch [192] proposed a working memory framework emphasizing the dynamic manipulation of information. Their originalmodeldescribed acentralexecutivethatcoordinates two subsystems:the phonologicalloop(verbal)and the visuospatial sketchpad (visual/spatial).A subsequentrefinement introducedthe episodic buffer to integrate material from these subsystems with long-term memory[193].Figure 3.3 shows the framework of the working memory model. Alternatives such as Cowan's embedded-processes model[194]similarly underscore theroleof attention in governing how information is briefly sustained and manipulated.",
            "Serial-Parallel-Independent (SPI) Model. Initial distinctions between episodic, semantic, and procedural memory were championed by Tulving [195],wholater refined his ideas into the Serial-Paralel-Independent(SPI) model, as shown inFigure 3.4.In this framework, memory is divided intotwooverarching systems.The cognitive representation system handles perceptual input and semantic processes,encompassing facts,concepts, and contextual (episodic) knowlege.The action system,bycontrast, underpins procedural skill such asdanceroutines,driving maneuversor typing proficiencyTulving's SPI model posits that memory formationcan occur at multiplelevels:strictly perceptual encodingcan support rudimentary episodic memories, while richer episodic representations benefit from semantic mediation.For instance, patients with semantic dementia, who struggle to retain word meanings,can stillform some episodic memories butoften lack the fullcontextual detailconfered by intact semanticnetworks.Byhighlighting the role of procedural memory and itsautomatic,intuitive nature,the SPImodelaims to integrate structure (thecontentof memory)and function (how memory is used),surpassing earlier accounts that largelyfocused on explicit storage and retrieval.Despite these strengths,critics note thatthe modelunder-specifieshow working memory operateswithinthe broader system, andthe feedback mechanisms connecting cognitive and action subsystems remain loosely defined.",
            "![](images/a52b1772f52d0fff9fd1037812d46654c626b0ecf21b7ae411ae0935f36f3935.jpg)",
            "Figure 3.4: The Serial-Parallel Independent (SPI) model of human memory [195].",
            "Global Workspace Theory (GWT) and the IDA/LIDA Framework. Global Workspace Theory (GWT),developed by Baars [196],conceptualizes consciousness and working memory as a“broadcast\" mechanism that distributes information tospecialized processors.Building onGWT, Franklin[197,198]proposed the IDA(Intellgent Distribution Agent)model,laterextended toLIDA(Learning IDA),as acomprehensivecognitive architecture.Inthese frameworks, multiple memory systems (e.g.,perceptual,episodic, procedural) interact through iterative“cognitive cycles\",with a global workspace functioning as a hub for attention and decision-making. From an AI standpoint, IDA/LIDA demonstrates how human-like memory procesescan be operationalizedto guide an agent's perception,action selection, and learning.",
            "ACT-R and Cognitive Architectures. ACT-R (Adaptive Control of Thought—Rational)[199] is a comprehensive cognitive architecturethat integratesmemory,perception,and motor processes intoaunifiedtheoreticalframeworkIt has been appliedextensivelyacrossdiverse domains,including learning and memory,problem-solving,decision-making, language comprehension,perception and attention,cognitive development, and individual differences.Figure 3.5 illustrates the processes of ACT-R.At the core of ACT-R are distinct modules (e.g.,visual, manual, declarative, procedural) that interact with the system through dedicated bufers.Declarative memory stores factual“chunks.\" while proceduralmemory encodes if-then production rules for actions and strategies.Cognition unfolds via apattern matcherthatselects asingle production tofire basedonthecurrent buffercontents.Thissymbolic production system is augmented by subsymbolic processes,guided by mathematicalequations thatdynamicallregulate activations,retrieval latencies,and production utilities.Bycombining symbolic and subsymbolic levels,ACT-R provides a mechanistic account of how individuals acquire,retrieve,and apply knowledge-thus shedding light on empirical phenomena such as reaction times, error patterns, and the shaping of learning over time.",
            "Each of the aforementioned models illuminates different aspects of memory.The multi-store model provides a straightforward introduction to storage stages,working memory models emphasize active maintenance and manipulation, and frameworks such as IDA/LIDA or ACT-R embed memory within a comprehensive view of cognition. In practice, researchers often drawupon multiple perspectives,reflectingthe intricate natureof human memory andits integralrole in perception, learning, and adaptive behavior."
          ],
          "raw_title": "Models of Human Memory",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "3.2 From Human Memory to Agent Memory",
      "number": "3.2",
      "level": 2,
      "content": [
        "Having established the fundamentals of human memory, we now focus on how Large Language Model (LLM)-based agents manage and store information. Memory is not merely a storage mechanism but is fundamental to human and",
        "![](images/277803765e4d253632c25c7675cac0ec016cfd51678a60ce1eb06dc0d8ae9ee7.jpg)",
        "Figure 3.5: An abstraction of the most important processes in the ACT-R model [199].",
        "artifcialinteligence.Memory underpins cognition,enabling learning,adaptation,and complex problem-solving for humans.Similarly,forLLM-based agents, memory provides the crucial scaffolding for maintaining context,leaing from experience, and acting coherently over time.Without memory, even a highly capable LLM would struggle to adapt to changing circumstances or maintain focus during extended interactions.",
        "While LLM-based agents and biological systems differ fundamentally, the principles guiding human memory-context retention,selective forgettingand structuredretrieval—are highlyrelevant toagent design.Therefore,examining the parallels and distinctions between human and artificial memory isbeneficial.Functionallywe candraw analogies:an agent's short-term memory buferresembles the prefrontalcortex's role in working memory,whilelong-term storage in a vector database is akin to the hippocampus's function in consolidating episodic memories.Agent memory design can benfit from emulating human memory's mechanisms, including selective atention, prioritized encoding,and cue-dependent retrieval. However, crucial differences exist.",
        "Human memory,built upon biologicalneuralnetworks,integrates storage andcomputation withinneurons'connections and activity patterns.This ofersahighdegree of parallelism and adaptability. Incontrast,current agent memory systems predominantly rely ondigital storage and algorithms,using symbolic representations andlogicaloperations, thus separating storage and computation.This impacts information processing: human memory is associative and dynamic,capable of fuzzy matching andcreative leaps,while curent agent memory relies on precise matching and vector smilarity,struggling with ambiguity.Although digital storage capacity is vast, it cannot yetreplicate the complexity and dynamism of human memory, particularly in nuanced pattern recognition and long-term stability. Human memory, while imperfect,excels at extracting crucial information from noisy data.Agent memory systems,in their current stage,are stillnascentcomparedtothe intricacies of human memory,facing limitations inorganization, integration, adaptive forgetting, and knowledge transfer.",
        "The need for a dedicated memory module in LLM-based agents is paramount. While external knowledge bases (databases,searchengines,APIs)[20o] provide valuable information,they donotcapture theagent's internalreasoning, partial inferences,ortask-specificcontext.An agentic memory system internalizes interim steps,evolvingobjectives, and historicaldialogue,enabling self-referentialexplorationandadaptation.Thisiscrucialfortasksrequiringthe agent to build upon prior judgments or maintain a personalized understanding of user goals.",
        "Early approaches to agent memory,such as appending conversation history tothe input prompt (arudimentary formof working memory)[201],have evolved.Modern architectures employ more sophisticated techniques, including vector embeddings for rapidly retrieving memories[202]and selective incorporation of reasoning chains into subsequent inference steps[203,204].These diverse methods share the common goalof managing alarge information reservoir without compromising system responsiveness.",
        "However,comparedto the sophistication of human memory,current agentic methods have limitations.Many systems lack coherent strategiesfor long-term memory consolidation, leading to cluttred logs or abrupt information loss. The flexible, bidirectional interplay between stored knowledge and ongoing processing,characteristic of human working memory,isoftenabsent.Metacognitive oversightselectiverecall,forgetting,and vigilance against outdated information-is also underdeveloped in LLM-based agents.Balancing comprehensive recallwith practical efficiency, as humans do, remains a key challenge.",
        "Building robust and adaptable memory for LLM-based agents involves addressing three core research questions: First, how should memory be represented tocapture diverse information types and facilitate efficient access?Second, how can agent memoryevolve, incorporating new experiences,adapting tochanging contexts, and maintaining consistency? Finally,howcan the stored memories effectivelyenhancereasoning,decisionmaking,andoverallagent performance? The following sections delve intothesecrucialareas,exploringcurrent approaches,limitations, and potential future directions."
      ],
      "raw_title": "From Human Memory to Agent Memory",
      "type": null,
      "children": []
    },
    {
      "title": "3.3 Representation of Agent Memory",
      "number": "3.3",
      "level": 2,
      "content": [
        "Inspired by humancognitive systems [285],current memory architecture in intellgent agentsadopts a hierarchical framework that integrates perception through sensory memory[205],real-time decision-making via short-term memory [286,287], and sustained knowledge retention through long-term memory [288,289, 48].This multi-layered structure equips agents to manage immediate tasks while maintaining a broader contextualunderstanding,fostering adaptability and seamless continuity across diverse interactions.",
        "Specifically, the memory system transforms raw environmental inputs into structured, actionable representations. Sensorymemoryacts as the gateway,capturingand selectivelyfiltering perceptual signals to provide afoundation for cognitive processing.Short-term memory bridges these immediate perceptions withtask-levelunderstanding,buffering recent interactions and enabling dynamic adaptation through experience replay and state management. Long-term memory then consolidates and stores information over extended periods,facilitating cross-task generalization and the accumulation of enduring knowledge.",
        "Together, these memory components form a cohesive cycle of perception, interpretation, and response.This cycle supports real-time decision-making and enables agents tolearn and evolvecontinuously,reflecting an intricate balance between responsivenessand growth.Thefollowing delves into the formulation ofeach memory type,exploring their unique roles and interactions within the agent's cognitive architecture."
      ],
      "raw_title": "Representation of Agent Memory",
      "type": null,
      "children": [
        {
          "title": "3.3.1 Sensory Memory",
          "number": "3.3.1",
          "level": 3,
          "content": [
            "In human cognitive systems, sensory memory serves as a mechanism for collecting information through the senses-touch,hearing,vision,andothersand ischaracterized byitsextremelybrieflifespan.Analogously,ensoy memory functions asthe embeddedrepresentation of inputs suchas text,images,andother perceptual data inintelligent agents.Itrepresentstheinitial phase ofenvironmentalinformation processing,actingasagateway fortransformingraw observations into meaningful representations for further cognitive processing.",
            "Sensory memory in intellgent agents transcends pasive information reception.It dynamically encodes and filters perceptual signals,bridging immediate sensoryinputs withthe agent's internal stateobjectives, and prior knowledge. This adaptive processfacilitatesrapidperceptionofenvironmentalchanges,taskcontinuityandreal-timecontext-aware information processing.Sophisticatedatention mechanisms are employed toensure relevance and focus inthe sensory memory layer, forming a critical foundation for decision-making and adaptation.",
            "Formally,sensory memory formation consists of three sequential steps: perceptual encoding,attentional selection, and transient retention.First, perceptual encoding transforms raw sensory signals into processable representations, mathematically expressed as:",
            "$$\n\\phi(o_{t})=\\operatorname{Encode}(o_{t},s_{t})\n$$",
            "where $o_{t}$ is the sensory input at time $t$ and $s_{t}$ represents the agent's state. For instance, RecAgent [205] employs an LLM-based sensory memory module toencode raw observations while filtering noise and irrelevant content.Extending",
            "![](images/842a45dbf1ec1c8a6bec8af1082979a90b9f29cc8bb487f1d5c056bc7027c2a6.jpg)",
            "Figure 3.6: Tree diagram of the memory module in intelligent agents.",
            "beyond text-based perception, multimodal sensory memory systems such as Jarvis-1[228], VideoAgent[209],and WorldGPT [210] integrate multimodal foundation models to process diverse modality inputs.",
            "Next, attentional selection extractscrucial information from the encoded sensorydata.This processguided by an attention mechanism, is represented as:",
            "$$\n\\alpha_{t}=\\mathrm{Attention}(\\phi(o_{t}),c_{t})\n$$",
            "where $\\phi(o_{t})$ is the encoded input, and $c_{t}$ denotes contextual information influencing attention. For example, RecAgent [205] employs an attention mechanism with an importance scoring system that assigns relevance scores to compressed observations, prioritizing critical inputs such as item-specific interactions while de-emphasizing less significant actions. This helps extract high-priority information for memory retention.",
            "Finally, transient retention temporarily stores the selected sensory information as sensory memory:",
            "$$\nM_{\\mathrm{sensory}}=\\{\\alpha_{t}\\ |\\ t\\in[t-\\tau,t]\\}\n$$",
            "Several strategies have been implemented to manage the time window.For instance,RecAgent[205]models retention by associatingeachobservationwiththe timestampcorespondingtothe start ofa simulationround inthe user behavior simulation environment,represented as atriplet(observation,importance score,timestamp).Similarly,CoPS [206] employs afixed-size sensory memory poolas atime window, whichconsists of user search requests for personalized search,facilitating“re-finding\"behavior.Whenanewquery isreceived,thesystemfirstchecksthe sensory memory for relevantmatches.Ifamatch isfound,the query isclassifiedasare-finding instanceenabling arapidsensoryresponse."
          ],
          "raw_title": "Sensory Memory",
          "type": null,
          "children": []
        },
        {
          "title": "3.3.2 Short-Term Memory",
          "number": "3.3.2",
          "level": 3,
          "content": [
            "Short-term memory in cognition-inspired intellgent agents serves as atransient and dynamicworkspace that bridges sensory memory andlong-term memory.It isessentialforstoring and processing task-relevant information andrecent interaction sequences,supporting real-time decision-making and adaptive behavior.Inspired by human short-term and working memory,t temporarilyretains information tofacilitatecomplexcognitive tasks,ensuring continuity and coherence in the agent's operations.",
            "Short-term memory in intellgent agentscan be categorized into context memory and working memory.On the one hand, context memory treats the context window as the short-term memory of LLMs.For example,MemGPT [214] inspired byhierarchicalmemory systems inoperating systems,managesdiffrentstorage tierstoextendcontext beyond the LLM's inherentlimitations.[290]introduces aneurosymbolic context memory that enhances LLMs by enabling symbolic rule grounding and LLM-based rule application.",
            "Onthe other hand, working memory involves fetching and integrating relevant external knowledgeto hold essential information during an agent's operation. Generative Agent [50] employs short-term memory to retain situational contextfacilitatingcontext-sensitive decision-making.Reflexion[48]utilizesasliding window mechanism to capture and summarizerecentfeedback,balancing detailed immediate experienceswith high-levelabstractions forenhanced adaptability.RLP[218] maintains conversational states for speakers andlisteners,using them as short-term memory prompts to support dialogue understanding and generation.",
            "For interactive and creative game scenarios, CALYPSO[219]assts Dungeon Masters in storytelling for Dungeons & Dragons by constructing short-term memory from scene descriptions, monster details, and narrative summaries, enabling adaptive storytelling and dynamic engagement.Similarly, Agent S[21l] and Synapse[291], designed for GUI-based autonomouscomputer interaction,define theirshort-term memory astask trajectories,including actions such as buton clicks and text inputs.This formulation supports behavioralcloning and enhances adaptation in novel GUI navigation tasks.",
            "In robotics applications,SayPlan[292]leverages scene graphs and environmental feedback as short-term memory to guide planning and execution in scalable robotic environments.KARMA [215] engages short-term working memory with an effective and adaptive memory replacement mechanism todynamicallyrecord changes inobjectspositions and states. LLM-Planner [293] iteratively updates short-term memory with environmental observation to prompt an LLM for dynamic planning."
          ],
          "raw_title": "Short-Term Memory",
          "type": null,
          "children": []
        },
        {
          "title": "3.3.3 Long-Term Memory",
          "number": "3.3.3",
          "level": 3,
          "content": [
            "Long-term memory in cognition-inspired intelligent agents enables the retention and retrieval of information over extended periods, alowing agents to generalize knowledge and adapt to newcontexts efectively. Unlike sensory and shot-term memory, which handle transient orimmediate data, long-term memory supports cumulative learning and cross-task adaptability. It mirrors human long-term memory by incorporating explicit and implicit components, facilitating richer contextual understanding and intuitive behavior.",
            "On the one hand, explicit memory involves intentionalrecollction,analogous to declarative memory in humans.It consists of semantic memory, which stores generalknowledge such as facts and concepts, and episodic memory, whichrecords specific events and interaction histories.Semantic memory in intellgent agentscan be preloaded from domain knowledge basesordynamicall acquired through interactions.For example,in environments like TextWorld, semantic memory captures structured facts,such as“Recipe-contains- Tuna\"or“Recipe - is on-Table\".Episodic memory,incontrast,logs situationalcontextand sequentialactions,such as“gofrom kitchen tolivingroom,then to garden\".Integrating semantic and episodic memory alows agents toretain static andcontextual information,enabling human-like adaptability and context-aware responses.",
            "On the other hand, implicit memory shapes agent behavior through procedural memory and priming.Procedural memory enables agents to perform repetitive tasks efficiently byrecalling specific skill and reusable plans.Forexample,it automates routine tasks withoutrequiring explicit instructions,improving task execution effciency.Priming,meanwhile, captures statechanges and corresponding responses, allowing agents to adapt to similar contexts quickly. Priming enhances fluidity andcontext-sensitivedecision-making by directly matchingobservations toorcontinuouslychaining actions.Implicit memory,shaped byinteractions withcognitive modules,enablesrapid adaptation,oftenafter minimal exposure to new stimuli.",
            "Most intelligent agents implement both semantic and episodic memory within their memory modules.For instance, Agent S [2ll],designed for GUI automation tasks,incorporates semantic memory to store online webknowledge in natural language form,while episodic memory captures high-level, step-by-step task experiences.Similarly, AriGraph[221],targeting embodied simulation tasks,encodes semantic environment knowledge using a fact graph and logs episodic navigation historythrough an event graph. In AIcompanion systems like MemoryBank[207] for SiliconFriend, semantic memoryconstructs userportraits innaturallanguage,whileepisodic memoryretains interaction histories, enhancing personalized and context-aware behavior.",
            "For implementing implicit memory,current agent systems primarily adopt model-friendly memory formats, such as key-value pairstorage,executable code, or reusable routines.For example, AAG [226]defines and generalizes procedures through analogy, mapping knowledge from one situation (base)to another(target).This structure can be representedasalineardirectedchaingraph,where the input serves astheroot,theoutput as theleafnode,andeach intermediate step as a node in the chain.Similarly,Cradle[227]and Jarvis-1[228] implement procedural memory by storing andretrieving skill incode form,whichcanbeeitherlearned from scratchor pre-defined.Once curated,skills can be added, updated,or composed within memory.The most relevant skillfora given task andcontext are then retrieved to support action planning."
          ],
          "raw_title": "Long-Term Memory",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "3.4 The Memory Lifecycle",
      "number": "3.4",
      "level": 2,
      "content": [
        "In this section,we introduce thelifecycleofmemory inAIagents, as depicted in Figure3.7.Thelifecyclecomprisesa dualprocessofretentionandretrieval.Retentionincludesacquisition,encodingandderivation,whileretrievalinvolves memory matching, neural memory networks, and memory utilization."
      ],
      "raw_title": "The Memory Lifecycle",
      "type": null,
      "children": [
        {
          "title": "3.4.1 Memory Acquisition",
          "number": "3.4.1",
          "level": 3,
          "content": [
            "Memory Acquisition is the foundational processby which intelligentagents take in raw perceptual information from their environment.This initial step is crucial for subsequent learning,adaptation, and decision-making[305].A primary challnge in acquisition is the sheer volume and complexity of environmentalinputs.Agents are constantly bombardedwithvisual,auditory,textual,andtherformsofdata,muchofwhichisredundantorirrelevanttotheagent's goals.Therefore,acore aspect of memory acquisition is not simplycapturing data, but also initiating a preliminary filtering processThis filtering leverages two primary mechanisms: initial information compression and experience consolidation.",
            "At this early stage,information compression involves rudimentary techniques to reduce data dimensionality. This might include downsampling images,extracting key phrases from text using simpleheuristics,oridentifying significant changes inaudiostreams[306].The goalisrapidlossycompressionto prioritize potentiallrelevant information.For example, LMAgent [230] prompts the LLM to perform information compression, reducing irrelevant and unimportant content when constructing sensory memory to enhance operational efficiency. Meanwhile, ReadAgent[231] and GraphRead[307] respectively employ different strategies for compressing long text, i.e.,episode pagination and graph-based structuring, to maximize information retention while ensuring efficiency.",
            "Onthe other hand,experience consolidation,even at the acquisition phase, plays arole.The agentdoesn'tyethave a rich memory,but it can begin to apply previouslylearned, very generalrules or biases.Forexample,if the agent has a pre-existing bias towards moving objects, it might prioritize visual data containing motion,even before full encoding [308].To enhance the dynamic consolidation of memory-based experiences, [235] define metrics such as contextual relevance and recallfrequency to determine whether to update long-term memory in a vector database.",
            "Table 3.1: Summary of the memory module in various agents. Refer to Figure 3.6 for abbreviations.",
            "<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Domain</td><td colspan=\"3\">Memory Representation</td><td colspan=\"5\">Memory Lifecycle</td></tr><tr><td>Sensory</td><td>Short-term Long-term|Acquisition Encoding Derivation</td><td></td><td></td><td></td><td></td><td>Retrieval</td><td>Utilization</td></tr><tr><td>Synapse [291]</td><td>GUI</td><td>Multi-</td><td>Context</td><td>PEpisdirca</td><td>User demo.</td><td></td><td>Hieomch.</td><td></td><td></td></tr><tr><td>Agent S [211]</td><td>GUI</td><td>Mulil-</td><td>Corkingg</td><td>Sepiantic,</td><td>Compfress.</td><td>Contastive</td><td>Seleet.</td><td>Indexing</td><td>Lonex-</td></tr><tr><td>Automanual [108]</td><td>GUI</td><td>Molal-</td><td>Context</td><td>Propiedral,</td><td>DUsmr.</td><td>Hierarch.</td><td>D Gomp.</td><td>STasch</td><td>Subeoal</td></tr><tr><td>AutoGuide[294]</td><td>GUI</td><td>Multi- modal</td><td>Context</td><td></td><td>Screen Capture</td><td></td><td>Action Plan</td><td></td><td>Action Exec.</td></tr><tr><td>Agent-Pro [295]</td><td>GUI</td><td> Multi-</td><td>Context</td><td></td><td>Sapeure</td><td></td><td>Hiecomch.</td><td></td><td>Action</td></tr><tr><td>MemGPT [214]</td><td>Document</td><td>Text</td><td>Corking,</td><td></td><td>ExDernal</td><td></td><td></td><td>Pac.call</td><td>Doact.</td></tr><tr><td>SeeAct [296]</td><td>Web</td><td> Moulai</td><td>Context</td><td></td><td>Scren</td><td></td><td>Action Plan</td><td></td><td>Ineract.</td></tr><tr><td>Auto WebGLM</td><td>Web</td><td>Text</td><td>Context</td><td></td><td>HTML</td><td>HTMd</td><td>AHTMSIS</td><td></td><td>IWwat.</td></tr><tr><td>SteP [298]</td><td>Web</td><td>Text</td><td>Context</td><td>Task-spec.</td><td>HTML</td><td>HTML</td><td>AHTML</td><td>Eleament</td><td>Ineract.</td></tr><tr><td>AWM [299]</td><td>Web</td><td>Text</td><td></td><td>Procedural</td><td>Workflow</td><td>Aution.</td><td>，</td><td> Siup</td><td>Workflow</td></tr><tr><td>AriGraph [221]</td><td>TextWorld</td><td>Text</td><td></td><td>Sepisantic,</td><td>Eserv.</td><td>Krapl.</td><td>Travepsal</td><td>Retrieval</td><td>Apction</td></tr><tr><td>MemoryBank [207]</td><td>Dialogue</td><td>Text</td><td></td><td>Episodic</td><td>Dialogue Record</td><td></td><td></td><td>Chron. order</td><td>Resp. gen.</td></tr><tr><td>PromptAgent [300]</td><td>General</td><td>Text</td><td>Context</td><td></td><td>Prompting</td><td></td><td>Prome.</td><td>Content-</td><td>Prompt</td></tr><tr><td>ECL[301]</td><td>Embody</td><td>Moda-</td><td>Context</td><td>Episodic</td><td>Recording</td><td>Contrast.</td><td>SExmer.</td><td>Sim.& Recency</td><td>Pelicy Long-</td></tr><tr><td>LEO [302]</td><td>Embody</td><td> Multi-</td><td>Working</td><td>HLoizon Rep.</td><td>Observation</td><td>Spamipl- Learn.</td><td>Goal-Cond.</td><td>Hierarch.</td><td>Exec. Horizon</td></tr><tr><td>IER [303]</td><td>Embody</td><td>Multi- modal</td><td>Context</td><td>Episodic</td><td>Env. Interact.</td><td>Multal- Embed</td><td>Iter. Refine.Sim.Match</td><td></td><td>Action Plan.</td></tr><tr><td>Voyager [47]</td><td>Embody</td><td>Text</td><td>Working</td><td>Procedural</td><td>Curiculum</td><td>Lskrary</td><td>Prompt.</td><td></td><td>Skill Exec.</td></tr><tr><td>A3T [49]</td><td>Embodys</td><td>Text</td><td>Context</td><td></td><td>DTomp.</td><td>Token. d&</td><td>Action Planning</td><td></td><td>Actiot.</td></tr><tr><td>STARLING[304]</td><td>Robotics</td><td>Mula-</td><td>Context</td><td>Procedural</td><td>Demo.</td><td>Trade</td><td>RSkile.</td><td>Simtet</td><td>Skill Exec.</td></tr></table></body></html>",
            "Expel[69]constructs anexperience pool tocollectandextract insights from training tasks,facilitating generalization to unseen tasks. More recently, MindOS [233] proposed a working memory-centric central procesing module for building autonomous AIagents,where working memoryconsolidates task-relevant experiences into structured thoughts for guiding future decisions and actions.",
            "These two mechanisms work in concert with preliminary LLM input. To address the initial challenges,several mechanisms have to be deployed.Agents must be equipped with mechanisms to assessthe potentialrelevanceof incoming informationrapidly.Thispreliminaryfiltering preventscognitiveoverload.The acquisition phase alsobenefits from LLM."
          ],
          "raw_title": "Memory Acquisition",
          "type": null,
          "children": []
        },
        {
          "title": "3.4.2 Memory Encoding",
          "number": "3.4.2",
          "level": 3,
          "content": [
            "Memory encoding builds upon acquisition by transforming the filtered perceptual information into internalrepresentations suitableforstorage andlater use.A key aspectofencoding isselective filtering.This selective attention mimics human cognitive processes[309].The inherentchallenges ofencoding stem fromthecomplexity,high dimensionality, and often noisy nature of raw perceptualdata.Effctive encoding requires advanced mechanisms to identify key eatures,compressthem compactly,and integrate information from multiple modalities. Modern approaches address hese challenges by leveraging selective attention and multi-modal fusion.",
            "![](images/353deb6f8fa3c1e1e85bb8d1f6c02afc4c2243423ed5c55654e074f61ec094aa.jpg)",
            "Figure 3.7:Illustration of the memorylifecycle.The memory retention processinvolves three sequential steps—memory acquisition,encoding,andderivation, whilethe memoryretrieval processencompassesseveralindependentapplications, including matching (vector search),neural memory networks, and memory utilization (for long-context modeling and hallucination mitigation).",
            "Selective Attention mechanisms,inspired by human cognition,allow the agent to dynamically focus computational resources onthe mostrelevant parts of theinput.This might involveattending tospecificregions ofanimage,keywords in a text,or particularfrequencies in an audio signal. Different atention mechanisms can be used depending on the modality and task.For example, as the candidate memory dynamicall expands, MS [237] employs an LLM-based scorer to selectively retain the top-scoring half,creating a more compact shared memory across multiple agent systems. In othermodalities,GraphVideoAgent[238]utilizes graph-based memory toenable selective and multi-turn video scene understanding,enhancing question-answering performance.Inrobot control,[24o]implementsselectiveattntionasa filtering mechanism to extract task-relevant objects from the set of allperceived objects on the table.",
            "Multi-modalFusion[310]isessential forintegrating information fromdifferent sensory inputs (e.g.combining visual and auditory data to understand a scene).This involves creating aunified representation space where features from different modalitiesare aligned.Cross-modal encoders andcontrastivelearningtechniques areoften used to achieve this fusion.For example,JARVIS-1[228] uses the general-domain video-language model CLIP[51] tocompute alignment within amultimodal key-value memory, where thekeycomprises elements such as task,plan, andvisualobservations, and the value is a text-based representation of successfully executed plans.Furthermore,Optimus-l[241]refines memory representationand optimizes the multimodalencoder by leveraging MineCLIP[311],adomain-specific videolanguage model pre-trained on Minecraft gameplay,to align and fuse filtered video streams withtextualinstructions and plans,encoding the agent'smultimodalexperiences into an abstracted memory pool.This integratedrepresentation enhances information retrieval andreasoning across modalities and acts as anotherflter,reinforcing consistent data. LLMs' semantic understanding is utilized to extract relevant features effciently."
          ],
          "raw_title": "Memory Encoding",
          "type": null,
          "children": []
        },
        {
          "title": "3.4.3 Memory Derivation",
          "number": "3.4.3",
          "level": 3,
          "content": [
            "Memory derivation focuses on extracting meaningfulknowledge and insights from the acquired and encoded memories. This processgoes beyond simple storage.This stage is essentialforenhancing the agent's learning capabilities.The goalistocontinuouslyoptimizethe structure andcontent of the agent's memory.Asignificantchallenge inderivation is the dynamic evaluationof informationvalue.Strategies toaddressthese challenges include reflection,summarization, knowledge distillation, and selective forgetting.",
            "Reflection involves an agent activelyanalyzing its memories to identify paterns,relationships,and potentialinconsistencies. Itcan be triggeredby specific events(e.g.,an unexpected outcome)or occur periodically asabackground process.This process may include comparing memories,reasoning about causalrelationships,and generating hypotheses[300].ExpeL[69]leverages reflection tocolect past experiencesforgeneralization tounseen tasks andto support trial-and-error reattempts following failures.R2D2[243] models memory as areplay buffer and appliesreflectionto refine it bycorrecting failedexecutiontrajectories in web agents.Thesecorrctedtrajectories are thencombined with successful ones to construct reflective memory, which serves as a reference for future decision-making.",
            "Summarization aims to produce concise representations of larger bodies of information while preserving their most essentialcontent.This can include extracting key sentences from adocument, generating abstractive summaries of conversations,or condensing sequences of events.Summarization techniques range from simple extractive methods to advanced abstractive approaches powered by large language models (LLMs)[245,312,246]. For example,[248] introduces arecursive summarization strategy over dialogue history and prior memory to support long-term dialogue memory derivation.Building onthis,Healthcare Copilot[247] maintains concise memory bytransforming conversation memory,representing the fullongoing medical consultation, into history memory that retains only key information relevant to the patient's medical history.",
            "Knowledge distilation[313]enables agents to transfer knowledge from larger, more complex models (or ensembles) to smaller, more efficientones.This is particularly important for resource-constrained agents and for enhancing generalization.Distilltion can also involve consolidating knowledge from multiple specialized models into a single, general-purpose model.Forexample,AoTD[250]distill textualchains ofthoughtfrom execution traces of subtasks into a Video-LLM to enhance multi-step reasoning performance in video question answering tasks. LDPD [251] transfers decision-making outcomesfrom teacheragents ie.expert bufers)to studentagents,optimizing the student's policy to align with the teacher's. In multi-agent systems, MAGDi[253] distills the reasoning interactions among multiple LLMs into smaller models by structurally representing multi-round interactions as graphs,thereby improving the reasoning capabilities of smaller LLMs.",
            "Selective forgeting[314] is the crucial processof removing ordown-weighting memories thataredeemed irrelevant, redundant, or outdated.This is essential for maintaining memory effciency and preventing cognitive overload. Forgetting mechanismscan be based on time(older memories are morelikely tobe forgotten)[247],usagefrequency (infrequentlyaccessedmemoriesaremore likelyforgotten)[203],andrelevance tothecurrenttaskorcontext[255].In morefine-grainedforgetting mechanisms, MemoryBank[207]applisthe Ebbinghaus Forgetting Curveto quantify the forgetingrate,accounting forbothtime decay andthe spacing effect,ie.,theprinciplethat relearning information is easier than learning it forthefirst time.Incontrast,Lyfe Agent[254]adopts ahierarchicalsummarize-and-forget strategy:it first clusters related memories,refines them intoconcise summaries,and then removes older memories that are highlysimilar to newer ones.This approach enables effcient,low-cost memory updatesforreal-time social interactions."
          ],
          "raw_title": "Memory Derivation",
          "type": null,
          "children": []
        },
        {
          "title": "3.4.4  Memory Retrieval and Matching",
          "number": "3.4.4",
          "level": 3,
          "content": [
            "Memory retrieval is a processthat emulates thehuman ability to recallrelevantknowledge and experiences to solve problems.The goalistoeffciently andaccuratelyextractthemost pertinent memory fragmentsfromalarge and diverse memory pool,encompassing sensory, short-term, andlong-term memory,to informthe agent's decisions,planningand actions.Just as humans relyon past experiences tonavigatecomplex situations,agents require asophisticated memory retrieval mechanism to handle a wide range of tasks effectively.",
            "However, achieving this goal presents several significant challenges.First,the agent's memory repository is often heterogeneous,comprising various forms of memory such as natural language descriptions,structured knowledge graphs,andstate-actiorewardsequences.These memoriesdifferfundamentallintheirdatastructures,representations, and levels of semantic granularity,posing achallenge forunifiedretrieval.Second, the retrieved memoryfragments must be highlyrelevant tothecurrntcontext,includingthe agent'sstate,task goals,andenvironmentalobservations. Simple keyword matching falls shortofcapturingthe deeper semanticrelationships required for meaningfulretrieval. Developing a context-aware semantic matching mechanism that can dynamically adjust the retrieval strategy based on the currentsituationistherefore paramount.Third,thereal-time nature of agent interaction with theenvironment necessitates effcient memoryretrieval to support rapid decision-making and action[315].This demand for efficiency is further compounded by the limitations of the agent'scomputational resources.Finaly,the agent's memory is not staticbutconstantlyevolving asnew experiences,knowledge,and skills are acquired.Ensuring memoriestimeliness, reliability,andrelevancewhileavoidingtheinterferenceofutdatedorerrneousinformationisacontinuouschallenge.",
            "A comprehensive approachcanaddressthesechallenges,encompassing four keycomponents.Firstly,afoundational step involvesconstructing aunified memory representation andindexing scheme.This aims to bridge therepresentational gap between diffrent memory types by embedding them into acommon vector space. Pre-trainedlanguage models like BERT or Sentence-BERT[3i6]can be leveragedto transform text-based memories into semantic vectors, while graph neural networks(GNNs)can learn vector representations for structured memories like knowledge graphs,capturing both node andedge relationships[317].To facilitate effcientretrieval, a multi-layeredhybrid indexing structure is essential. This integrates techniques like inverted indexes for keyword matching,vector indexes like Faiss[318]or Annoy [319]for similaritysearch,andgraph indexes for structuralqueries[320],thus supporting diverse query needs.",
            "Secondly,perhaps most critically, the system must developcontext-aware semantic similaritycomputation.This allows the retrieval processtounderstand and utilizethecurrent context,such as the agent's state,goals,andobservations, enabling a deeper semantic match beyond keyword overlap.This involves encoding the contextualinformation into vector representations and effectively fusing them with memory vectors.The attention mechanism plays acrucial role here,dynamicallycalculating the relevance between context and memory vectors and assgning diferent weights to memory fragments basedontheir contextualrelevance[261].This emphasizes memories that are more pertinent to the current situation.",
            "Thirdly,integrating memoryretrievalwiththeagent'staskexecutionnecessitatesatask-orientedsequencedecisionand dynamic routing mechanism.This leverages the structuralinformationoftasks to guide memory retrievaland utilization, enabling complex task decomposition, planning, and dynamic adjustments.By constructing a task dependency graph,the agentcan topologically sort subtasks to determine execution order.During execution,each subtask's goal serves as contextfor memory retrieval,extracting relevant knowledge and experience.Moreover,the agent must adapt to environmental feedback and task progressdynamically adjusting the execution plan. Each decision point involvesre-retrieving memories basedonthecurrentstateand goaltoselect theoptimalaction andhandleunexpected situations.This aspect also emphasizes how agents can leverage their skill memory to solve problems, including skilldistillation,combination, and inovation.Pattern recognition allows forsummarising generalproblem-solving steps,while structured knowledge organizationarranges skillsintoaretrievable format.Agentscanfurther distll generalized skills from specificones,combine multiple skillstoaddresscomplexchallenges, and even innovate new skill combinations.These processes depend fundamentallyon an efficient memory retrieval system that can identify appropriate skills or skill combinations based on task requirements.",
            "Finally, arobust memory management mechanism is crucial for maintaining the memory pool's timeliness,relevance, and efficiency.This mechanism should incorporate aforgetting and updating strategy, mirroring human forgeting mechanisms [321].This might involve regularly purging outdated,redundant,or infrequently used memories based on time-based decay (weakening memory strength over time) and frequency-based decay (purging low-frequency memories).Simultaneously,whenamemory fragmentrelevant tothecurrent task isretrieved,its timestampand access frequency are updated,increasing its importance and ensuring dynamic memory updates.Through these concerted efforts,LLM Agentscan be equipped with a powerful,flexible, and context-aware memory retrieval and matching system,enabling them to effectively utilize their accumulated knowledge,support complex decision-making, and exhibit more intelligent behavior."
          ],
          "raw_title": "Memory Retrieval and Matching",
          "type": null,
          "children": []
        },
        {
          "title": "3.4.5 Neural Memory Networks",
          "number": "3.4.5",
          "level": 3,
          "content": [
            "Neural Memory Networks represent a fascinating frontier in AIresearch.They aim to integrate memory seamlessly into the fabric of neural networks.This approach departs from traditional memory architectures by encoding memories directly within the network's weights oractivations,transforming the network into adynamic,read-write memory storage medium.This tight integration promises significant advancements in effciency andthe utilizationof stored information. However, realizing this vision presents several formidable challenges.",
            "A primary concern is balancing memory capacity with stability.Encoding a vast amount of information within the finite parameters ofa neural network while maintaining long-term stability poses a major hurdle.The network must be able to store a multitude of memories without succumbing tocatastrophic forgeting or confusion between similar memories.Equallycrucial is the development ofeffctive mechanisms for memory read-write operations.The network needs to reliably write new information,update existing memories,and accuratelyretrieve stored information on demand,all while maintainingcomputationaleffcency.Beyond simply storing memories,the ultimate goalis to endow neural networks withtheability to generalizefrom and reason withthe informationthey store.This would empower them to performhigher-order cognitive functions beyond rote memorization,allowing for insightfulconnections and inferences based on past experiences. Several approaches are being explored to addressthese challenges, notably through associative memory and parameter integration.",
            "On the one handassociative memory,inspired bythe interconnectednessof neurons inthe brain,ofers apromising avenue.ModelslkeHopfieldnetworks[262,63],leveraging energyfunctions,andBidirectionalAsociativeMemoies (BAMs)[322]supporting hetero-associative recall provide mechanisms for encoding andretrieving patterns based on the weights between neurons.Besides, Neural Turing Machines (NTMs)[264] and Memory-Augmented Neural Network (MANNs)[323,324,275,265] augment neuralnetworks with external memory modules,employing attention and summary mechanisms to interact with these memories.",
            "On the otherhand, parameter integration represents another key research direction, aiming to encode memory directly within anetwork's weights.This facilitates the seamlessintegration of world knowledge and accumulated experience into the operational behavior of intelligent AI agents.For example,some prior works modify model parameters to enable continual learning by updating [325,326,327]orforgetting specific knowledge[328].Other studies treat LLMs as standalone memory modules,incorporating world knowledge into their parameters during pre-training [329], post-training [330],and online deployment[331]. For instance,MemoryLLM[265] introduces memory tokens, while SELF-PARAM[266] leverages knowledge distillation to embed world knowledge and past AI agent experiences into model parameters. This approach is further augmented in the $\\mathbf{M}+$ model [332] with a long-term memory mechanism and aco-trained retriever,enhancing its ability to generalize tolonger history memorization.Additionally,[33] employs encoded memory to facilitate further reasoning,thereby improving the generalization of stored knowledge. More recently, MemoRAG [267] and ${\\tt R}^{3}$ Mem [270] have been proposed to not only encode memory but also enable reliable retrieval from neural memory networks,unifying the dual processes of memory storage and retrieval within a single model.This advancement contributes to thedevelopment of next-generation generative-based retrieval systems, which support lifelong AIapplications.Furthermore,Titans[269]have been introduced to memorize test-time data points through meta-learning, enabling more efficient test-time cross-task generalization.",
            "Future research willcontinue tofocus on creating larger capacity and more stable neural memory models.Concuently, developing more effcient andflexible memory read-write mechanisms willbecrucial.Acriticalarea of investigation willinvolve applying these memory-augmented networks tocomplexcognitive tasks,pushing the boundaries of what AI can achieve.Progress in this domain willunlock new possibilities for building intelligent agents that can learn, remember, and reason in a manner that is increasingly reminiscent of human cognition."
          ],
          "raw_title": "Neural Memory Networks",
          "type": null,
          "children": []
        },
        {
          "title": "3.4.6 Memory Utilization",
          "number": "3.4.6",
          "level": 3,
          "content": [
            "Acriticalaspect of agent design lies in memory utilization, which focuses on maximizing the value of stored memory segmentsforthecurrent task.Thecoreobjective is toapplythese memorieseectivelyand appropriatelytoenhance reasoning,decision-making,planningandaction generation,ultimatelyboostingtheagent'sperformanceandefficiency while avoiding the pitfalls ofirrelevant orincorrect memory interference.Achieving this,however,presents several challenges.",
            "One primarychallenge is balancingthe vastnessofthe memory store withits effective utilization.Agents must navigate a potentialinformation overload,ensuring thatrelevant memoriesare fullleveraged without overwhelming the system. Anotherhurdle is theneedforabstractionand generalization.Agents need to distil specificmemory segments intomore general knowledge and apply thisknowledge tonewandvaried situations.Furthermore,the issue of hallucinations and incorrect memories withintheLLMrequirescareful consideration.Preventingthe generation ofcontentthatcontradicts or misrepresents storedinformationiscrucial,as is theability toidentify andrectifyerroneous information that may reside within the memory store itself.",
            "To addressthese challenges,several strategies are employed.Retrieval-augmented generation(RAG)[334]combines retrieval and generation models to enhance the LLM's capabilities by drawing upon external knowledge sources. Unlike the methods mentioned in memory retrieval and matching, RAGfocuses on integrating retrieved information into the generation process itself.When prompted, the agent retrieves relevant memory segments and incorporates them into the context providedbythe generation model.This contextual enrichment guides the model towards more factual and informative outputs.Forinstance, when responding toauser's query,the agent canfirst retrieve related entries from its knowledge base andthen generate an answer based on this retrieved information, thus grounding the response in established knowledge. More recently,some studies have integrated memory modules with RAG, incorporating self-reflection[274]andadaptive retrieval mechanisms[272] to enhance both generation reliability and efficiency. For example, Atlas [273]leverages causal mediation analysis, while[284] employs consistency-based hallcination detection to determine whether the model already possesses the necessary knowledge—allowing for direct generation-or whetherretrievalis required, in whichcase the modelfirstretrieves relevant information before generating aresponse.In aunifiedframework,RAGLAB[271]offers acomprehensive ecosystem for evaluating and analyzing mainstream RAG algorithms. HippoRAG [22] employs a strategy inspired by the hippocampal indexing theory of human memory tocreatea KG-based index for memory and use Personalized PageRank for memory retrieval.",
            "Furthermore,long-context modeling plays avitalrole in managing extensive memory stores.This approach enhances the LLM's abilitytoprocesslong sequences andlage-scalememories,allowing foradeeper understandingand utilizationof long-range dependencies.By employing Transformer model variants like Transformer-XL[324] and Longformer[335], or throughhierarchical andrecursive processing techniques,such asrecurrent memory transformer (RMT)[275,276] agents can expand their context window.This enables them to handle significantly more extensive memory stores and reason and make decisions within a much broader context.Forexample,agentscan maintain alonger memory span when processing extensivedocuments or engaging in prolonged conversations.Additionally,somestudies leverage memory to compress long contexts,enabling more effective long-context modeling. For example, AutoCompressor [277] introduces summary vectors as memory totransfer informationfrom previous context windows intothecurrent window, facilitating long-context understanding.Similarly, the in-context autoencoder (ICAE)[278] generates memory slots that accurately and comprehensivelyrepresent theoriginalcontext, while LLMLingua[336,337], Gist[279],and CompAct [280] further optimize long-prompt compression to reduce input context length.",
            "Finally,hallucination mitigation strategiesare essentialfor ensuring the reliability of generated outputs.These strategies aim to minimize the LLM's tendency to produce factually incorrct or nonsensicalcontent.One approachis implementing fact-checking mechanisms [338],verifing generated content against established knowledge or memory stores.Another involves uncertainty estimation[339,340],where the model evaluates the confidence level of its generatedcontentandflagsorfiltersoutlow-confidenceoutputs.Aditionallknowledge-baseddecoding strategiescan be employed during the generation phase,introducing constraints that guide the modeltowards more factuall accurate content.These techniques colectivelycontribute to generating more trustworthy outputs and aligned withthe agent's established knowledge base.Recent research has introduced expert memory subnetworks,such as PEER[283] and Lamini Memory Tuning [281],which specialize in memorizing specific types of information,including worldknowledge and AI agents\"past experiences.These subnetworks offload memorization to dedicated parameters,reducing the main model's propensity to hallucinate.By implementing these memory utilization strategies,agents can become more capable,accurate,and reliable.Theycan successfully leverage their memory stores to achieve superior performance across complex tasks."
          ],
          "raw_title": "Memory Utilization",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "3.5 Summary and Discussion",
      "number": "3.5",
      "level": 2,
      "content": [
        "The development of truly inteligent agents depends not just on robust memory systems,but alsoon their seamless integration withother cognitive functions like perception,planning,reasoning,and action selection.Memory is not an isolated module; it is deeply intertwined withthese other processes.Forexample,sensory input is encoded and filtered before storage(asdiscussed inthe sectionson memoryrepresentation andlifecycle),highlightingthe interplay between perception and memory. Long-term memory,especially procedural memory,directly informs action selection through learned skills androutines.Retrieval mechanisms, likecontext-awaresemantic similaritycomputation,arecrucial for planning,allowing agents toaccess relevant pastexperiences.This interplayextends totheconceptofa“world model?\"",
        "Centralto intellgent agents is their abilityto build and utilize internal world models.These models,representing an agent's understanding ofits environment,enable simulation,reasoning aboutconsequences,and prediction. Robust worldmodelsarecrucialforhigher-levelcognition,planning,andhuman-like intelligence.Aworldmodelis,inessence, a highly structured, often predictive,form of long-term memory. Memory provides the raw material—knowledge and experiences-forconstructing the world model, whilethe world model, inturnacts as anorganizing framework, influencing how new memories are encoded,consolidated,andretrieved.For instance,a welldeveloped world model might prioritize storing surprising events, as these indicate gaps in the agent's understanding.",
        "However, developing efective world models and memory systems presents significant challenges.These include managing thecomplexity of real-world environments, determining the appropriate level of abstraction (balancing accuracy,complexity,and computational effciency),and integrating multi-modalinformation.Learning and updating these models effciently,avoiding bias,ensuring generalization, andenabling continuous adaptation are alsocritical. Furthermore, model-based planning requires effcient search algorithms tohandle the inherentuncertainty inthe model's predictions.",
        "Future research should focus on enhancing agent memory systems by drawing inspiration from the strengths of human memory,particularlyitsflexibilityadaptability,andeficiency.While agent memoryhasadvancedconsiderably itsill lags behnd human memory in these key areas.Human memory is remarkably associative,retrieving information from incomplete or noisycues,andit exhibits asophisticatedformof“forgetting\"that involvesconsolidationandabstraction, prioritizing relevant information and generalizing from experiences.Agent memory,conversely,often relieson precise matching and struggles with ambiguity.",
        "Several promising research directions emerge.Exploring biologicaly-inspired mechanisms,such as neural memory networks (as discused earlier),could lead to significant breakthroughs.Another crucial area is developing memory system thatactively“curate\"their contents—reflecting oninformation,identifying inconsistenciesand synthesizing new knowledge.This requires integrating metacognitive capabilities (monitoring and controlling one's own cognitive processes)intoagentarchitectures.Furthermore,creating morerobust andnuancedforms ofepisodic memory,capturing not justthe“what\"and“when\"but alsothe“why\"andtheemotionalcontextofevents,isessentialforagents thatcan truly learn from experience and interact with humans naturaly.",
        "Overcoming these challenges requires innovative solutions attheintersection of deeplearning,reinforcement learning, and cognitive science.Developing more sophisticated and adaptable world models and memory systems—ones that mirror the strengthsof human cognition-willpave the way for agentswithadeeper understanding of their environment, leading to more intelligent and meaningful interactions."
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": []
    },
    {
      "title": "World Model",
      "number": "",
      "level": 1,
      "content": [
        "A worldmodelenables an agent to predict and reasonabout future states without direct trial-and-error in reality.This sectionexplores how human cognitive studies on“mental models\"relate to AI world models in artificial intelligence, categorizing them under four paradigms: implicit paradigm,explicit paradigm,simulator-based paradigm,and aclass of other emergent methods (e.g.,instruction-driven paradigm).We then discusshow world models inherently intersect withother agenticcomponents and conclude with open questions andfuture directions that unite these perspectives under a unified theoretical and practical framework.",
        "![](images/99853a0650701eb5ba0c82121b8d7a944c60b08d96c14ca2711f3801c8a92a9e.jpg)",
        "Figure 4.l:Humanscan use theirbrain's modelof theworld topredict theconsequences of their actions.For example when playing table tennis, a player can imagine or predict the trajectory of the ball after an action."
      ],
      "raw_title": "World Model",
      "type": null,
      "children": []
    },
    {
      "title": "Reward",
      "number": "",
      "level": 1,
      "content": [
        "![](images/480dd79050d800b8a19fd2423745a169cd9b8afb33877b15013a39bd5d4b0283.jpg)",
        "Figure 5.1: Illustrative Taxonomy of Reward system",
        "Rewards help the agent distinguish between beneficial and detrimental actions,shaping its learning process and influencing its decision-making.This chapter first introduces common reward substances inthe human body and the corresponding reward pathways.Then, thereward paradigm under the agent and the diferent methods involved are defined.In thediscussion section, the influencerelationship betweenother modules is described, and the existing methods aresummarized,then the problems that need to be solved inthefuture andthe optimization directions are discussed.",
        "Table 5.1: The comparison of human common reward pathways",
        "<html><body><table><tr><td>Reward Pathway</td><td>Neurotransmitter Mechanism</td><td></td></tr><tr><td>Mesolimbic path- way [406]</td><td> Dopamine</td><td>Dopaminergic neurons in the ventral tegmental area (VTA) extend pro- jections to the nucleus accumbens, where they release dopamine to regulate reward-related signaling. Dopamine diffuses across the synaptic cleft and binds to dopamine receptors—primarily D1-like (excitatory via Gs proteins, increasing cAMP) and D2-like (inhibitory via Gi pro- teins, reducing cAMP)—thereby modulating reward, motivation, and</td></tr><tr><td>Mesocortical path-Dopamine way [407]</td><td></td><td>reinforcement. Dopaminergic projections from the VTA reach the prefrontal cortex (PFC). Here, dopamine binds to its receptors to influence cognitive functions such as decision-making, working memory, and emotional</td></tr><tr><td>Nigrostriatal path- Dopamine way [407]</td><td></td><td>regulation, all of which contribute to evaluating and anticipating rewards. Dopamine's action on D1 and D2 receptors in the striatum helps shape both motor routines and reward-related behaviors.</td></tr><tr><td>Locus coeruleus [408]</td><td> Norepinephrine</td><td>Neurons in the locus coeruleus release norepinephrine to widely dis- tributed targets across the brain. At synapses, norepinephrine binds to adrenergic receptors(α and β subtypes), modulating neuronal excitabil- ity, arousal, attention, and stress responses. These modulatory effects</td></tr><tr><td>Glutamatergic pro- jection [409]</td><td>Glutamate</td><td>can indirectly influence reward processing and decision-making circuits. Upon releasing into the synaptic cleft, glutamate binds to both ionotropic receptors (such as AMPA and NMDA receptors) and metabotropic re- ceptors located on the postsynaptic neuron, thereby initiating excitatory signaling. This binding produces excitatory postsynaptic potentials and</td></tr><tr><td>GABAergic modu-  Gamma- lation [410]</td><td>Aminobutyric Acid (GABA)</td><td>is crucial for synaptic plasticity and learning within reward circuits. GABA serves as the principal inhibitory neurotransmitter. At the synapse, GABA binds to GABAA receptors and GABAB receptors. This binding results in hyperpolarization of the postsynaptic cell, thereby providing inhibitory regulation that balances excitatory signals in the reward net- work.</td></tr></table></body></html>"
      ],
      "raw_title": "Reward",
      "type": null,
      "children": []
    },
    {
      "title": "Emotion Modeling",
      "number": "",
      "level": 1,
      "content": [
        "Emotions are a key part ofhow humans think, make decisions,and interact with others.They guide us tounderstand situations,makechoices,and buildrelationships.Antonio Damasio,inhis book Descartes'Error[25],explained that emotions are not separate fromlogic.Instead,theyaredeeplyconnected to how we reason and act.When developing LLM agents, adding emotional capabilities can potentially make these systems smarter, more adaptable,and better understand the world around them.",
        "For LLMagents,emotions canactas adecision-making tool, much likethey do for humans.Emotions help us prioritize tasks,understand risks, and adapt to new chalenges.Marvin Minsky,in The Emotion Machine[420], described emotions asa way toadjust our thinking processes,helping us solve problems inamore flexible andcreative manner. Similarly,LLMagents withemotion-likefeatures couldimprove their abilityof solvingcomplex problems and making decisions in a more human-style.",
        "However, the integration ofemotions into LLM agents is stillin its early stages.Researchers are just starting to explore how emotional capabilitiescan improve these systems.Furthermore, there is great potential for LLM agents to support human emotional wellbeing,whether through empatheticconversations, mentalhealth support, or simply building beter connections with users.This promising butchallenging arearequires collaboration between fields such as psychology,cognitive science,and AI ethics.As research advances,emotion-understanding LLM agents could redefine how we interact with technology,creating deeper trust and more meaningfulrelationships betweenhumans and machines.",
        "In the following subsections, we willdelve deeper into the roleofemotions in shaping LLMagents.We willexplore how emotions can be used to enhance learning and adaptability,how LLMs understand human emotions, and how these systems expressand modeltheirown emotional states. We willalso examine how emotions can be manipulated to influence LLMagents'behaviorandpersonalities,as wellas theethicaland safetyconcerns that arise fromthese capabilities.Eachof these discussions builds onthe foundationalimportance ofemotion tocreateLLMagents that are more intelligent, empathetic, and aligned with human values."
      ],
      "raw_title": "Emotion Modeling",
      "type": null,
      "children": []
    },
    {
      "title": "Perception",
      "number": "",
      "level": 1,
      "content": [
        "Perception is thefoundationalgateway through which both humans andintelligent agentsacquire information,interpret their surroundings, and ultimately make informed decisions. For humans, perception is seamlessand intuitive, eortlesslytransforming sensory inputs into meaningfulinterpretations.In artificialintellgence,however, perception systems are meticulously engineered toemulate—and insome respects surpass-human sensory processing, profoundly influencing an agent's capacity for interaction, learning, and adaptation in complex environments.",
        "In this chapter,we begin byexploringkeydiffrences inthe nature andeffciencyofperception between humans and AI agents.Next,wecategorize agent perceptionbased on diferentforms andrepresentations of perceptualinput.We then discuss ongoingchallenges intheagent perceptionsystemandhighlight promising directions forimprovement,both at the modeling andsystemarchitecturelevels.Finally,weillustratehow perception modulescanbeefectivelytailoredto different intellgent agent scenariosoferingpractical guidanceforoptimizing theiruseandsuggesting pivotalareasfor future research."
      ],
      "raw_title": "Perception",
      "type": null,
      "children": []
    },
    {
      "title": "Action Systems",
      "number": "",
      "level": 1,
      "content": [
        "In the realmofphilosophy,actionis definedas the behaviors that agentscanperformforapotentialorspecificpurpose in the environment.For example, manipulation, moving,reasoning, and tool utilization can allbe considered as fundamntalactions that an intelligent agent can execute to fulfilla goal inreal-world scenarios. In other words, actions emerge fromthe goal-orientedengagementof an agent inits environmentreflectingits intenttotransformthe externalworld in pursuitof its goals.Therefore,theaction system alsoplaysavitalrole indifferentiatingAIagents and foundation models (e.g.,LLMs). Generally,existing foundation models have demonstrated impresive performance across varioustasks,buttheirtaskscopeisstilimitedastheypredominantlyreliesontheriginalpre-trainingjective (e.g.,next-token prediction). By serving foundation models as brain inteligence, AI agents equipped with action systemscan directly engage with their environment and execute complex user intent. Moreover, action systems can support agents toutilize availabletools fromexternalenvironments,thus significantlyextending agents'task scopes. Therefore,thedesignofaction systems willalsodetermine the capabilityof AIagents in perception,decision making, execution,toolutilization, and anyothercomponents toalignwith the human brain.Inother words,foundation models lay the groundwork for agents while action systems determine their ultimate potential to achieve complex targets. Designingan effective and comprehensiveaction system for AIagents is acritical endeavor that involves significant challenges and notable benefits. In Figure 8.1, we demonstrate the execution processof the action system in the cognition system. Inthis section, we willfirstdiscussthe human action system inSection 8.1, and then examinethe transition fromhumanaction toagenticaction inAIagents in Section 8.2.After that,wewillsystematicallsummarize the paradigms of existing action systems in AIagents, including action space,action learning, and toollearning,in Section 8.3.In Section 8.4we analyze thediferences between action andperception, and finally wesummarize the conclusion in Section 8.5.",
        "![](images/ec4a8db596d668bcb99e2499621a82cdad3204db43c7f5979620dec123e498ac.jpg)",
        "Figure 8.1: Ilustration of several concepts related to action and action execution."
      ],
      "raw_title": "Action Systems",
      "type": null,
      "children": []
    },
    {
      "title": "8.1 The Human Action System",
      "number": "8.1",
      "level": 2,
      "content": [
        "Action system inhumancognition refers tothe processes that allow humans to perceive,plan,andexecute goal-directed actions. It isa complex system that enables individuals to interact witha dynamic environment, make decisions, and adapt their behavior based on feedback. Generaly,the action system within human cognition could be broadly categorized as mental action and physical action:",
        "·Menalactioncan beviewed as akind of distinct action,which is formulated as athinking processto drive the final intention in the human brain.For example,reasoning,decision making,imagining, and planning can allbe considered as various types ofmentalaction.Inother words,mentalactions are equaltoa brain signal that drives the physical actions of humans to fulfill the final objective.",
        "·Physicalaction refers to any goal-directed bodily movement executed bythehumanmotor system.To some extent, physicalactions are usualyexpressedasakind ofcontinuous action.Forexample,speaking,manipulating,drawing, running,and grasping can allbe regarded as physical actions.Employing asequence of physicalactions,humans can conduct the interaction and collect feedback from real-world environments.",
        "Figure 8.2ilustrates a simple taxonomy of the humanaction system from the perspective of mentalaction and physical action.Empowered with both mental and physical actions,the human cognition system can handle diverse complex tasks fromreal-worldscenarios.Drawing inspiration from human cognition,it is alsoessentialforus torevisit how to formulate actionsystems in AIagents acrossdifferent tasks,fromlanguage to digitalandthen in physicalenvironments.",
        "<html><body><table><tr><td>Model</td><td>Examples</td><td>Inputs</td><td>Objective</td><td>Definition</td></tr><tr><td>Large Language Model (LLM)</td><td>GPT-4 [7]</td><td>Language</td><td>Next-Token Prediction</td><td>LLMisetextasdhe</td></tr><tr><td>Large Multimodal Model (LMM)</td><td>LLaVA [513]</td><td>Multi-modal</td><td>Multi-modal Generation</td><td>LMM s ta inperate multimoal data based on</td></tr><tr><td>Robotic Foundation Model (RFM)</td><td>RT-1 [522]</td><td>Sensory inputs</td><td>Robotic Control</td><td>RFM is to generate robotic control based on the sensory inputs from dynamic environments.</td></tr><tr><td>Large Action Model (LAM)</td><td>LAM[622]</td><td> Interoniment</td><td>Executable Action</td><td>LAM is toctgensrwithin ecutabl roction based on</td></tr></table></body></html>",
        "Table 8.1: Definitions between different kinds of foundation models.",
        "![](images/26302eddc891aacfa6972e26865c28ed8fc3e85c0dd6403b0cf5b7505f46a2d7.jpg)",
        "Figure 8.2: Ilustrative Taxonomy of Human Actions, showing both mental and physical facets."
      ],
      "raw_title": "The Human Action System",
      "type": null,
      "children": []
    },
    {
      "title": "8.2 From Human Action to Agentic Action",
      "number": "8.2",
      "level": 2,
      "content": [
        "In the pastlong period of time,human action systems[623]have significantly motivatedus to shape thedevelopment of a computer system toward autonomous paradigms.The action mechanism plays acritical role in thehuman brain in driving goal-directed behavior. In an intellgent human brain[624],conscious and unconscious thinking signals are produced,converted into mental signals,which eventually leadto asequence of action operations.This process can be mapped asa multi-stage pipeline that involves constructing action spaces,formulating learning mechanisms for improved decision making,and integrating external states (e.g.tols).Inspired bythese principles,we discoverthat these designs are essential to formulate the prototype of AI agent.",
        "Many existing frameworks incorporate action learning into their design or utilize it as an output.To clarify the definition of an action system,we highlight the distinctions among various frameworks, including large language models (LLM),large multi-modal models (LMM), robotic foundation models (RFM), andlarge action models (LAM), as shown in Table 8.1. Specifically, an LLM is to produce language output based on provided prompts, while an LMM is to generate multi-modality artifacts based on the multi-modal inputs.Existing language-based or digital AI agent frameworks are built upon these foundation models (e.g.,LLMor LMM) via predefining the scope of action space and its learning strategies.On the other hand,an RFM is tooptimize roboticcontrol basedon real-world environments (e.g.robotic video).Existing RFMs are pre-trained from web-scale video dataand use video prediction to simulate the action ofroboticcontrol.Thecore of RFMisstilltouse thegenerativeobjective tolearn knowledge from large-scale data, although it has involved some action designs in building physical AIagents.Moreover,some recent works[622]introduce theconceptof largeaction model(LAM),whichfurtherhighlights the stage to generate the action strategies,interact withreal-world environments andenhance self-learning paradigm.Fromthese definitions, we notice that,regardlessofthefoundational models employed,thecore of action system is tobuildthe interaction withtheenvironment and then enable the learning process from thecolected action trajectories via pre-definedreward functions.Specificall,the mechanisms underlying these behaviors are also similarto the action system in human cognition, offering valuable insights for designing action systems in AI agent frameworks. For example:",
        "·When processing diferent scenarios, humans usually will pre-define the action space to perform action trajectories to solve specific tasks.For instance, when playing computer games like Minecraft, we will set our action operations via keyboard or mouse to simulate behaviors like building house, mining gold, and so on. On the basis of this, we also need to build or create an action space for handling complex tasks in AI Agent frameworks.\n·Compared to machines,the human cognitive system excels in continuously acquiring new knowledge through real-world interactions, guided by generating and optimizing the action sequences.Thus, replicating this learning ability in AI agents is essential to adapt the dynamic environment and build a new skillibrary.\n·In addition, withthe development of human civilization,learning to use external tools has been recognized as one of the most significant milestones in the evolution of human intelligence.By leveraging these external tools,humans can extremely extend the problem-solving capability in diferent scenarios,from the stone age to the industrial revolution.",
        "Tothis end,weexpect tobuildthe mappingbetweentheaction systemof humancognitionsystemandthe design of AI Agentframework,including how to buildaction space for AIagentfrom specific scenarios to general domain,how to buildactionlearning withintheenvironment, andhowtoleverageexternalstates (e.g.ools)toextendthetaskscopeof AI Agent.Bydeveloping this a systematic survey,we strive to provide more in-depth insights forthecommunity witha clear understanding of the significance of action systems in AI agent frameworks."
      ],
      "raw_title": "From Human Action to Agentic Action",
      "type": null,
      "children": []
    },
    {
      "title": "8.3 Paradigms of Agentic Action System",
      "number": "8.3",
      "level": 2,
      "content": [
        "Generally, the action system of AI agent frameworks consists of three major components:1)the action space $\\mathcal{A}$ ，which includes alltypesofactionthatagentcan perform inreal-worldscenariosordownstreamtasks,andcanvarysignificantly depending ondifferent agent settings,ranging from language-basedagents toembodied agents; 2)the action learning within an dynamic environment that determines the state $s$ ，observation $\\mathcal{O}$ and the optimization process of agent; 3) the tool space $\\tau$ thatencompasses the instruments,interfaces,or middle-wares the agent can perform for utilization, which ranges fromphysicaldevices such asroboticarms todigitalinterfaces likeAPIs.Overal, these components collectively define the scope and characteristics of the action systemfor AIagents,shaping their formulation and execution.",
        "To fully explore the possible actions $a_{t}$ in practical scenarios, we must formally represent the action space and consider both individual operations and the underlying hierarchicalreasoning processes.This means examining the action space at various levels,from low-level manipulations to high-level operators that orchestrate complex workflows.",
        "Accordingly, the AI agent decision making process can be formalized as a trajectory $\\left\\langle o_{t},s_{t},a_{t}\\right\\rangle$ ,where $a_{t}$ is selected from the action space $\\mathcal{A}$ to transform the current state $s_{t}$ based on observation $o_{t}$ into the next state. In some cases, integrating external tool systems may also be necessary. By executing a sequence of $\\left\\langle o_{t},s_{t},a_{t}\\right\\rangle$ , the agent is steered toward achieving its final objectives."
      ],
      "raw_title": "Paradigms of Agentic Action System",
      "type": null,
      "children": [
        {
          "title": "8.3.1  Action Space Paradigm",
          "number": "8.3.1",
          "level": 3,
          "content": [
            "Action space $\\mathcal{A}$ is an important component, which serves as the basis for building an action system within AI agent frameworks.The composition of theactionspace determines how AIagentssolvecomplextasks indiffrent scenarios. In Figure 8.2,we present an ilustrative taxonomy ofthe action system based on its action space.Generally, we summarize the action space within existing works as three distinct types, as outlined below.",
            "Language Language-based AI agents typically operate through language-driven actions in interactive linguistic environments,suchasreasoning,programming,retrieving information,executing APIcall,orinteractingwith exteal tools. In our study, we summarize three distinct types of language-based action spaces,including plain text,code programming,and communication.Specificaly,early language-based AI agents are built with plain text, which aim to perform interactive decision-making in verbal environments or text-based games.Here, ReAct [7o] is a representative language-based AI agent,which synergizes the reasoning and actions of an LLM to solve various problems.AutoGPT[625] analyzes and decomposes user requests into multiple subtasks and uses web search or other tools to tackle each of them.Reflexion[48] involves self-refinement and the memory mechanism toenhance action execution in language tasks. $\\mathbf{LLM+P}$ [163] empowers LLM-based agent with planning capability to aid decisionmaking.However,converting plain text intoan executable command usually requires LLMs tofirst interpret the text and then perform instructionconversion,leading toadditionalinformationlossTothis end,some work explores using code as the action space,allwingdirect execution ofthe generated code and self-verification.MetaGPT[626]and ChatDev[627] build the action space via programming language with multi-agent collaboration. SWE-Agent [628] considerdifferentstages ofsoftware engineering andthussolve software issues.OpenDevin[629]devises anautomatic software development platform that integrate code writing,interaction with thecommand,sandboxforcode execution, andcollaborations.Moreover, some frameworks are built based on multi-agentcommunications,and thenuse chatting to analyze which actions should beemployed inthe next step.Here,Generative Agents[50]directly simulate multiple characters in a virtual town, to explore how each agent to conduct next action.MetaGPT [626] and ChatDev [627] are both multi-agent frameworks tofaciliate the development of software engineering.AutoGen [630] is also a representative framework that enable multiple agentcollaboration to solve any complextasks.Generally,languagebased AIagents,empowered byLLMs, perform efectively in linguistic interactions.However,lmited tothe scopeof the action space,it also poses challenges of howto solve more complextasks inreal-world scenarios.Therefore,we also needtoformulate new research solutions toconstructa more sophisticated action space tosolvechallnging tasks.",
            "![](images/07fca98b52c4d65a0c93b814e7474333738ed7ad1d78fe87966a57d9f4c9a62c.jpg)",
            "Figure 8.3: Illustrative Taxonomy of Action system, including action space and learning paradigm",
            "Digital To expand the capabilities of AIagents beyond language,some works have alsodeveloped advanced AI agents that operate within digitalenvironments, such as web proxies,online shopping platforms, and gaming systems.For examples,MineDojo[31l]devises a virtualagent via video-language pre-training and simulates anenvironmentthat supports a multitude of tasks and goals within Minecraft.Moreover, Voyager[47]isan embodied AI agent trained to play Minecraft. It simulates multiple executable actions incode form to developa skillibrary via interacting with theMinecraft environment, andthus improve the capabilityofvirtualagents.JARVIS-1[228] is an open-world agent that can handle multi-modal inputs /outputs, generate sophisticated plans, and perform embodied control. It explores the evolutionary behaviors of the agent when acting in Minecraft.SwarmBrain[631] is an embodied agent that uses LLMsto act strategically and in real time in StarCraft II.Additionally,some researchstudies investigate how LLMs can act to process multimodal tasks. MM-ReAct [497] and ViperGPT [498] apply LMs to perform the thinking process for multimodaltasks andthen select visualexpertsfortasksolving.Visual-ChatGPT[496] integrates multiple visual experts and uses LLMs as the controller to solve tasks.HuggingGPT[152] directly involves four stages, includingtask plannng, modelselection,modelexecution and response generation, to automaticall analyze user instructionsand predict the finalanswers basedon complex multimodal tasks. Itisalso vitalfor the agent to keep up with the latest information available online. Therefore, some AI Agent frameworks (e.g., WebGPT [632], WebAgent[634])aredesignedto interact withsearchengineto enhance thecapabilityofagentto discover the answers from website.WebShop[633] is used to explore the potential of AI Agent for online shoping.Mind2Web [97] is to buildageneralist agent that simulate multiple complex web tasks.As foundation agents advance in processing multimodaltasks or web tasks, there is aincreasing trend to enhance their capability in solving complexcomputer tasks.Mobile-Agent[635]utilizes multimodal models asthecognitive controlerto manage andorchestrate mobile functionalities.AppAgent[636] defines various app usages as action spaces,enabling foundation models to interact with different apps as a mobile intellgent assistant.UFO[637] and OmniParser[520]are two advanced GUI agents which manipulates UI operations as the action space,enabling AI agent to perform computer-use tasks. Generally, empowered with more advanced skills in digital environment, AIagent can demonstrate beter intelligent insolving complex tasks,andrepresent asignificant shiftfromlanguage intelligent todigitalintelligent.Byexpandingthe action space to include web browsing, GUlinteraction, mobile applications, and embodied systems,AIagents are evolving into more autonomous, multimodal, and context-aware systems,bridging the gap between foundation models and human cognition systems. In addition,other research explores LLM integration with structured digitalenvironments such as relational databases and knowledge graphs (KGs). Pangu [639]pioneredthe connection between LLMs and large-scale KGs,whileBIRD[640]and Spider 2.0[641] established afoundation forLLMs tooperate with enterprise databases in real-world settings.NL2SQL-BUGs[667]addresses thecriticalchallenge of identifying semantic errorsin NL2SQL pipelines[365], which enhances the reliability ofLLM-driven interactions with relational databases [668]. Similarly,frameworkslike UnifiedSKG[638] and Middleware[642] expand LLMs'action capabilities across both databases and KGs.",
            "Physical Building an AIagent to interact with thereal physical worldcanbe viewed as the ultimateobjective to simulate acomputer programto act asa human cognition system.Toachieve this,we require the agent tobecapable of processing signals from real-world environments and generating feedback tofacilitate continuous improvement. Therefore,it willpose new challenges on how to process the continuous signals collected by sensors and enable foundation models to make decisions.To fulfillthis, RT-family[522, 643,644] pre-trained vision-language-action models to integrateknowledge from web videos into robotic learning,enhancing robotic control and action execution. GR-2[357] is a robotic model that undergoes large-scale pre-training on video clips and language data,followed by fine-tuning on robot trajectories for robotic action prediction. $\\pi_{0}$ [645] pre-trained a robotic model based on robot platforms,including single-arm robots, dual-arm robots, and mobile manipulators, to build robotic learning in physical systems. SayCan [646] bridges the connections between robotic semantics and LLMs, using the robotic model to provide perception for LLMs and then using LLMs to make high-level decision-making. VoxPoser[647] uses LLMs to understand and decompose 3D Value Maps for Robotic Manipulation. Besides,EmbodiedGPT [648] utilizes vision-language models to understand videodata and perform decision-driven actions. In physical environments,it is worth noting that we usually need tounderstandcontinuous signals andthen generate continuous actions forrobotic control.Despite the existing foundation models thatcan efectively process discrete-levelactions (e.g.languageor computer-use),how to processlong continuous signals is stillchallenging.Therefore,eliminating the differences between continuous signals and discrete signals in foundation models is still a major problem.",
            "Generally,action space serves as one of the mostcriticalcomponents in building an efective AI Agent system.An effective action space enhances the capability and efficiency of the AI Agent in processing downstreamtasks.Action space usuallyranges from thediscrete space (e.g.,skillibrary inAtari games)tothecontinuous space (e.g.,robotic manipulation).As AI agents become more autonomous and multimodal, designing effective action spaces will be crucial for advancing general-purpose AI systems capable of real-world interactions."
          ],
          "raw_title": "Action Space Paradigm",
          "type": null,
          "children": []
        },
        {
          "title": "8.3.2 Action Learning Paradigm",
          "number": "8.3.2",
          "level": 3,
          "content": [
            "In the human cognition system,action leaming [669]represents the problem-solving process, involving both taking actions andreflecting onfeedback.Similarly,action learningfor AIagents refers to the iterative processby which an autonomous AI system refines its decision making and behavior through direct interaction with the real world environment. Generall,action learning encompasses a cycle of multiple stages, including building action space, choosing actions,andoptimizing action selection based on interaction with theenvironment (e.g.,receiving feedback or rewards and adjusting policy forchoosing actions).By iterativelydeploying these strategies,AIagents can adapt to the latest information orchanging conditions in real time,ultimately enabling more robust,flexible,andefficient problem-solving capabilities. Therefore, an effective action learning mechanism is crucial for the optimization of agentic action systems.Inthis part, we mainly focus onthree diferent representativelearming paradigms,including in-context learning, supervised training, and reinforcement learning, which are discussed below:",
            "In-context Learning As large language models have demonstrated emergent ability,in-context learning has been considered as the most efective method to leverage the existing capabilities of LLM without any modifications. Provided with well-designed prompts to describe actions, AI agents can understand specific actions, perform these actions,reflect on the outcome of the interaction with the environment, and finally achieve goals.Among these approaches,thecommon method is to use prompting techniques to instruct LLMs to generate agentic action.Here,the most representativeone is Chain-of-Thought(CoT)[46]prompting,which applies“Let us think stepbystep\"technique to generate asequenceofintermediatereasoning steps,exploring potential solutionssystematicall.ReAct[7]enables LLMs to generate reasoning trails and task-specific actions through interaction within the environment, improving thereasoning and decision-makingcapabilities of AIagents.LearnAct[652]devises an iterative learning strategy to expand action space by generating code (i.e.,Python)tocreate andrevise new actions.Moreover, some works (e.g., Auto-CoT[137] explores how to automatically generate CoT via LLMs and then enable the autonomous thinking process of AIagents.To handlemore complex tasks,ToT[72]considers the thought process as atree structure and introduces the tree search viaLLM prompting,while GoT[75]applies a graph structurealong with the graph search. For robotic models,CoA[649]designed four different promptsettings (e.gobject, grasp,spatial,and movement)to allow robot manipulation with reasoning process.Furthermore,to tackle more complex tasks that require intricate agentic workflows,some frameworks introduce the stage oftask decomposition via LLM prompting tobreak down user instructions.Least-to-Most[138]isaclasscalprompting technique toconvert user instructions into multiple subtasks. HuggingGPT[152] is arepresentative AIagent framework that applies task planning to transform user requirements into actionable items. Plan-and-Solve[650] directly uses LLM to make plans from user instructions and then give answers based onthe generated plans.Progprompt[93]applies similar task decompositiontorobotic tasks.In addition, using prompting techniques to formulate the characteristic of AIagent hasalso been considered as an increasingtrend to facilitate the simulation and productivity of AI agentframeworks (e.g,Generative Agents [50], MetaGPT[626], ChatDev[627],SWE-Agent[628]).Finally,some other frameworks (e.g.,Reflexion[48]or Self-refine[67])analyze the external feedbacks of user interactionwithin the environment and then iteratively refine and polish results via well-designedreflexion prompts.Allofthese designs allowus tobeterunderstand user instructions,decompose task goals,and make plans forthinking answers.In-contextlearming canhelpus avoid parameter optimization and reduce theheavycostof training LLMs.Itallows AIagents toperformvarious actions effectively andadapt toawide rangeof domains.However,challenges stillremain if we want to acquire agents of even stronger actionlearning ability.",
            "Supervised Training Tofurther improve the action learning abilityof foundation models,increasing research efforts have focused on training methodologies, including self-supervised pretraining (PT)and supervised fine-tuning (SFT). For the pre-training paradigm,the most representative works is RT-family[522, 643, 644],which pre-trains robotic Transformer onlarge-scale web and robotic data,yielding a powerful vision-language-action model.Following this policy, GR-2[357] is developed through extensive pre-trainingonalargecorpus of web videos to understand the dynamcs of theworldandpost-trainingonrobotic trajectorydatatospecialize invideo generationandaction prediction. Similarly, LAM[622] is alarge action model pre-trained on trajectories of user interaction with computer usage. However, the pre-training paradigm usuall incurs massive computation costs.Therefore, many works take the finetuning paradigm to enhance the action capability of foundation models.OpenVLA[670] is built upon the Llama2[11] language model and incorporates a visual encoder based on DINOv2[671] and SigLIP[672]. It is fine-tuned ona diverse set of real-world robot demonstrations from Open X-Embodiment(OXE)[673] and outperforms RT-2-X[673] across different tasks, all while utilizing $7\\times$ fewer parameters. Building upon OpenVLA, CogACT [653] integrates an additional diffusionactionmodule andintroduces anadaptiveactionensemblestrategyforinference.Itisalsofine-tuned using datasets from OXE and demonstrates a $35\\%$ improvement in the SIMPLER [674] simulated environment and a $55\\%$ increment in realrobot tasks using the Franka Arm.Besides, some works also explore how to enablerobotic model to learn action from plain language in physical world.For examples,RT-H[654]introduces ahierarchicalarchitecture to build action space, which first predict language motions and then generate low-level actions. And $\\pi_{0}$ [645] collected massive diverse datasets from differentdexterous robot platforms,andthen fine-tune the pre-trained VLMs to learn robotic actions.UniAct[56]learns universal actions thatcapture generic atomic behaviors acrossdiffrently shaped robots by learning their shared structural features.This approach achieves cross-domaindata utilization and enables cross-embodiment generalizations by eliminating heterogeneity[132].Overal, using supervised training,including pre-training and supervised fine-tuning,can effctively adapt foundation models to perform actions intelligently in real-worldscenarios.Lastbut notleast,itisworthnotingthat,evenwithextensivetraining onavastcorpus,itistll beneficial toapplyin-contextleaning ontopof thetrainedmodelforAIagents, inan pursuit fortheirbestperformance.",
            "Reinforcement Learning To facilitate an action learning procedure inaddition to in-context learning and supervised trainng,itisalsocrucialforagentstointeract withtheenvironment andeventuallyoptimizetheiractionpolicythrough experience,feedbackorrewards.Consideringthis iterative andsequentialnature,reinforcementlearning (RL)provides us withthe systematic methodology we need[675,676,677,678].InRL paradigms,there are severalclassicaland representative algorithms,such as Deep Q-Network (DQN)[679] and Proximal Policy Optimization (PPO)[680]. The most representative RL work that applied reinforcement learning tofoundation models is InstructGPT[43],which effctively aligns LM outputs with human preferences via RLHF.Since RLHF usually requires additional training to build the reward model, some papers (e.g. DPO [1) ]) proposes to directly optimize preference data through contrastive learning.Existing work[89,681]alsodemonstratethepotentialof scaling the RLalgorithm for foundation models to produce long CoT thinking stages with impressive performance.Although RL paradigms have been successfully used to fine-tuneLLMs fortext generationtasks[12,682,43,683],effcientlyutilizingtheRLalgorithmforaction leaing remains one of the many challnges that require further attempts. Recent advances indicate significant progressin applying RL to action learning with LLMs from various perspectives:",
            "· Given the rich world knowledge encapsulated in LLM, we can use LLM to mimic external environments or generate imagined trajectories to aid agents in action learning.For instance, RLFP[657] utilizes guidance and feedback from the policy,value, and success-reward foundation models to enable agents to explore more efficiently. Similarly, ELLM[658] utilizes large-scale background knowledge from LLMs to guide agents in effcient exploration within various environments. GenSim[659] automatically generates rich simulation environments and expert demonstrations by exploiting the coding abilities of LLM,thereby facilitating the capability of the agent for free exploration. LEA [660] leverages the language understanding capabilities of LLM and adapts LLM as a state transition model and a reward function to improve the performance of ofline RL-based recommender systems. MLAQ [661] utilizes an LLM-based world model to generate imaginary interactions and then applies Q-learning [684] to derive optimal policies from this imaginary memory. KALM [662] fine-tunes LLM to perform bidirectional translations between textual goals and rollouts, alowing agents to extract knowledge from LLM in the form of imaginary rollouts through ofline RL. In general, empowered by RL paradigms, we can significantly explore the internal knowledge from LLMs and thus enhance the interactions with external environments. Current works such as Search-R1[685], R1-Searcher [686], RAGEN[687], and OpenManus-RL[688] are exploring utilizing RL methods to fine-tune the agent models on trajectory data in agentic environments.",
            "·Besides, hierarchical RL is also a promising topic that helps foundation modelto decompose complex task and thenlearm optimal policies to solve each task via RL paradigm. For example, When2Ask [663] enables agents to request high-level instructions from LLM.The high-level LLM planner provides a plan of options, and the agent learns the low-level policy based on these options. Eureka[664] leverages LLM to generate human-level reward functions with reflection, allowing agents to efficiently learn complex tasks such as anthropomorphic five-finger manipulation. ArCHer [665] adopts a hierarchical RL approach, utilizing an offpolicy RL algorithm to learn high-level value functions, which in turn implicitly guide the low-level policy. LLaRP[666] leverages LLM to comprehend both textual task goals and visual observations. It employs an additional action output module to convertthe output of the LLM backbone into a distribution over the action space.Overall using hierarchical RL can guide AI Agent to explore optimal strategies when analyzing user requests for reasoning and planning.",
            "Using reinforcement learning,wecan integrate foundation models withonline learning from interactive environments, incorporating both action policies and world models.This integration enables advanced action systems in AIagents. Within the reinforcement learning paradigm, agents dynamically adapt and refine their decision-making processes in response toexternalfeedback,facilitating greater efficiency and effectivenessinactionlearning andachieving desired outcomes.",
            "![](images/b84593031c384ce9a4db7f387a7499daefd629b4ebf4d7b0924f98c48ada9975.jpg)",
            "Figure 8.4: Ilustrative Taxonomy of Tool Systems in AI Agents, including tool category and leaming paradigm",
            "Summary In general, Empowered by action systems, AI agents have demonstrated significant decision-making capabilities across various fields.For example, action learning enables AIagents to automate the understanding of Graphical User Interfaces (GUIs)and perform various operations, thereby improving human productivity through automatic computer usage.Moreover, several studies have shown that AI agents equipped with action systems can achiveremarkableoutcomes inrobotic manipulation tasks,suchasobject picking,laundry folding,and table cleaning. There are alsopromising researchdirections inthe industryemploying action models.Forinstance,autonomous driving (AD) has atractedconsiderable attention due to the exceptional performance of VLMs in perception and decisionmaking. By integrating human understanding through foundation models, AD systems can efectively comprehend real-world surrunding,enabling them to simulate human-level drivers.In summaryaction learning endows agents with the abilityto interact withtheexternal world,therebycreating moreopportunities forAIapplications inreal-world scenarios."
          ],
          "raw_title": "Action Learning Paradigm",
          "type": null,
          "children": []
        },
        {
          "title": "8.3.3 Tool-Based Action Paradigm",
          "number": "8.3.3",
          "level": 3,
          "content": [
            "Tool learning distinguishes human intellgence fromthatofother animals.Ever sincetheStone Age,human use of tools has boostedeffciency,productivity,and innovation.Similarly,enabling AIagents toperate indigitaland physical environments by harnessing various tools is a fundamental step toward achieving human-level intelligence.",
            "Definitions InAI,toolsaredefinedasinterfaces,instruments,orresources thatallowagents tointeract withtheexteal world.Examples include web search[632,705,97,634],databases[706,707,708,709],coding environments [710], data systems[711,712,713],and weather forecasting[714].By translatingtool functionality intoplain textor API formats,foundation modelscan expand their problem-solving scope. The evolution of tool systems in AI can be summarized in stages.Initiall,with the adventof large language models[2],thefocuswas onconverting tools into explainable formats (e.g.,function calls).Later, advances in multimodal processng shifted interactions from conversational chats to graphical user interfaces (GUIs),and more recent work has explored embodied agents that control hardware(e.grobotic arms,sensors)tointeract withthe physical world.To simplify,atool-based actioncanbe considered a form of external action employed for assistance.",
            "Tool Category Similar toaction spaces,toolscan alsobeclasified into multiplecategories according totheir types. In this part, we mainlysummarize three keydomains,including language,digital, and physical. Inaddition, we also explore the potential of tool learning in emerging areas such as scientific discovery:",
            "·Language:To facilitate the use of external tools, we usually denote the tool asa kind of function call for foundation models, which usually encompasses task descriptions, tool parameters, and corresponding outputs. This expression allows LLMs to understand when and how to use tools in AI agents. Specifically, ToolFormer [689] expands the capabilities of language models by integrating external tool spaces, including calculator, QA systems,search engine, translation, and calendar.ToolLLM[690] uses RapidAPI as the action space and then uses a depth-first search-based decision tree algorithm to determine the most suitable tool for solving tasks. Gorilla[691] is a fine-tuned LLM based on the tool documents and then can be used to write API calls.ToolkenGPT[692] is to optimize tool embeddings and then enable LLMs to retrieve tools from the fine-tuned tool embeddings. GPT4tools [693] and AnyTool [694] are also building self-instruct datasets and then fine-tune LLMs on them for tool usage.Generally, due to the impressive capability of LLMs, language-based tool utilization for AI agents has been studied, with its effectiveness validated in abundant works, ranging from plain text or function calls to code programming.",
            "· Digital: With the success of LLMs in processing language information, many researchers are exploring extending the task scope of AI agents fromthe language to the digital domains (e.g., MultiModal, Web search, GUI, and so on). For example,MM-ReAct [497], ViperGPT[498], and Visual ChatGPT [496] employed LLMs as the controller and then used LLMs to select visual experts for solving different tasks.HuggingGPT [152] and Chameleon [153] use LLMs to first conduct reasoning and planning actions and then analyze which multimodal tools should be used for solving user instructions. WebGPT[632] and WebAgent[634] respectively empowered LLMs with search engines to enhance the capability of LLMs to solve more challenging tasks. Mobile-Agent [635] and AppAgent [636] respectively incorporate GUI manipulations and App usage as the tool-based actions to extend the task scope of AI agents in solving mobile phone tasks. In contrast to the physical world, digital environments usually provide simpler pipelines to collct and process data. By involving foundation models andtheir interaction with the digital environment,itis possible for us to develop intelligent assistants in computers, mobile phones, and other digital devices.",
            "·Physical: For physical world applications, RT-2[643] demonstrates language-guided robotic manipulation using visual-language tools, and TidyBot [695] shows how LLMs adapt cleaning tools to personalized household preferences. SayCan [646] uses LLMs as the cognitive system to guide robots in solving tasks through robotic arms and visual perception. SayPlan [292] built a 3D scene graph as the action spaces and designed multiple actions and tools for 3D simulation, and then used LLMs as planners to invoke these actions or tools for robot task planning.Besides,specialized applications in real-world scenarios now also proliferate across diferent domains.For instance, in surgical robotics,[715] presents a multi-modal LLM framework for robot-assisted blood suction that couples high-level task reasoning,enabling autonomous surgical sub-tasks. Some autonomous driving systems [716,717] also integrate vision-language models with vehicle control tools for explainable navigation. In total, physical world applications pose the most significant challenge when compared to other tasks, butthey also offer the biggest industrial value.Therefore, it stillrequires us to continue exploring advanced action learning and tool integration in physical-based agents in the future.",
            "·Scientifc:Scientific tools have played atransformativerole in advancing AIagents acrossdisciplines,enabling them to learn, adapt, and execute tasks while integrating foundational models with frameworks that drive innovation and address complex challnges. In materials science, HoneyComb [696] exemplifies tool-driven advancements with its ToolHub. General Tools provide dynamic access to real-time information and the latest publications, efectively bridging gaps in static knowledge bases. Material Science Tools are designed for computationally intensive tasks,leveraging a Python REPL environment to dynamically generate and execute code for precise numerical analysis. Similarly,ChemCrow [697] demonstrates the transformative power of tools in chemistry by integrating GPT-4 with 18 expert-designed tools to automate complex tasks such as organic synthesis, drug discovery, and materials design.These tools include OPSIN for IUPAC-to-structure conversion, calculators for precise numerical computations, and other specialized chemistry software that enables accurate reaction predictions and molecular property evaluations. Similarly, SciToolAgent [698] showcases how multi-tool integration can revolutionize scientific research.Designed to addressthe limitations of existing systems, SciToolAgent integrates over 50O tools (e.g., Web API, ML models,function calls, databases, and soon).Finaly, SciAgent [699] exemplifies a multi-agent framework that integrates ontological knowledge graphs with specialized agents for hypothesis generation and critical analysis,emphasizing the power of modular, tool-driven systems to accelerate discovery in materials science and beyond. These examples underscore the transformative potential of integrating specialized tools into AIframeworks to address domain-specific challenges effectively.",
            "Tool learning Inspired byhuman evolution [718],the integration of tools in Al involves three key aspects: Tool Discovery (identifying suitable tools),Tool Creation (developing newtools)and Tool Usage (effectivelyemploying tools). We also systematically review existing literature and summarize them in the following:",
            "1. Tool Discovery: Inreal-world environments,there is a wide range of tools from thedigital to the physical world.Finding the most appropriate tools for user instructions can be challenging.Therefore,the processof tool discovery is to identify and select the appropriate tools that AI agents can operate on to achieve their objectives. This stage also requires the world models in AI agents to have a profound understanding of any complex user instructions and world knowledge of diferent tools.Moreover, the versatility of AI agents is also correlated with its abilityto operate diverse tool systems.Generall,tool discoverycan becategorized into two mainstream paradigms: retrieval-based and generative-based methods. Retrieval-based methods aim to select the most relevant tools from the tool library. For example, HuggingGPT [152] introduces a framework in which LLMs act as controlers,orchestrating task planning and then invoking suitable models from platforms such as Hugging Face to fulfill user intention. In generative-based approaches, we often fine-tune LLMs to learn how to use and select tools based on various user instructions.For instance, ToolFormer[689] collcts a massive corpus with the corresponding API calls (e.g.,calculator, QA system, search engines,translation, and calendar)for training. ToolLLM[690] collct tool instructions based on solution paths and then fine-tune Llama models to generate better API calls for tool utilization.",
            "2.Tool Creation In addition to using existing tools,theability to create new tools plays acrucialrole in human civilization.Forlanguage agents,a widely adopted approach is to use LLMs to generate functions as executable programs, which consist of both the code and documentation.For example,PAL[701] generates programs as intermediate reasoning steps to solve problems, LATM[702] or Creator [703] use LLMs to create code for user intentions,and tofurther design a verifier to validate thecreated tools.SciAgent[699]notonly integrates multiple scientific tools but also crafts new tools for scientific discovery.More details on tool creation from an optimization perspective can be found in Section 9.4.2.",
            "3.Tool Usage After collecting or creating tools,the effective use of tools constitutes the cornerstone of the capabilities of AI agents,alowing applications that bridge virtual and physical worlds.Modern AI agents increasingly employ tools to tackle complex tasks across diverse domains, with three key dimensions of expansion: 1) Vertical Specialization: Agents leverage domain-specific tools to achieve professional-grade performance in complex fields such as robotics,science, and healthcare; 2) Horizontal Integration: Systems combine multiple toolkits across modalities (vision, language,control)for multimodal problem-solving; 3) Embodiment: Agents physically interact with environments through robotic tools and sensors.",
            "Summary Toollearning and action learning constitutethetwo most important components of the action system in AI agents.Toollearning canbeconsideredasakindofaction touse external states for problem-solving.Toollearning enables AIagents tosubstantiallbroaden theirrangeoftasks,pushing the boundaries beyond the scopeoffoundation models.For example,empowered by APIorfunctioncall,language modelscandirectlyreuse the capabilityof existing models (e.g.retrieval,coding,web search)to generate answers,ratherthan next-token prediction[719].Toollearning also involves multiplechallnging stages,including how todetermine the tool space,howtodiscoverand select tools, and howtocreate anduse tools.Overall,tollearning playsa pivotalrole inbuildinganomnipotent AIagentframework to solve complex tasks in different domains."
          ],
          "raw_title": "Tool-Based Action Paradigm",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "8.4 Action and Perception: “Outside-In\" or “Inside-out\"",
      "number": "8.4",
      "level": 2,
      "content": [
        "A central debate in cognitive science and neuroscience concerns whether action or perception stands at the root of causa flow in intelligent systems.Figure 8.5 presents different perspectives.The traditional“outside-in”view insists that causalinfluence begins withexternalstimuli.The environment excites peripheralreceptors,these signals propagate inward, and eventually producebehavior.This perspective portrays the organism—or agent—as essentially reactive:the external worldcauses sensory changes,and the agent's actions represent adownstream effect of those changes. In contrast,Buzsaki's“inside-out\"framework[18]proposes that it is the agent's own actions that shape the meaning and consequences of incoming signals. Such a view implies an active agent, one which continuously generates predictions and motor commands, while sending“corollary discharg\"or“action copies\"to sensory areas. These internally generated signals serve asreferences thatinform the agent which sensorychanges are self-initiated ratherthan imposedbytheoutside world.Inthis manner,cause shifts fromanexternalevent toan internalllaunched initiative,leavingexternalstimulitoplayaconfirmatoryorcorrectiverole.This reversalhas significant implications for how we interpret perception's purpose and function:it isnot an end initself,buta means of updatingandrefiningthe agent's own action-driven hypotheses about the environment.",
        "From an evolutionary perspective, possessing the abilityto move without relying on sophisticated sensory analysis can yield immediate survival benefits. Even simple organisms profit from periodic motion that stirs up food in nutrient-richwater,long before elaborate perceptualcapacities evolve.Inother words,movement precedes advanced sensing inevolutionary time,suggesting that thecapacity to act is not merely the efect of external stimuli but can"
      ],
      "raw_title": "Action and Perception: “Outside-In\" or “Inside-out\"",
      "type": null,
      "children": []
    },
    {
      "title": "Brain from Outside-ln",
      "number": "",
      "level": 1,
      "content": [
        "![](images/34387808fd61693c71bcf3ae8a21530f13f589892fd8dbdadf3e42bdbb71da00.jpg)",
        "Figure 8.5:(a)Compare the brain from“outside-in\"and\"inside-out\".(b)Ilustrationof the schematicofthe corollary discharge mechanism.Amotor command (efferent signal) travels from motor areas tothe eye muscles,while acorollary discharge(dashed arrow)is routedto acomparator in the sensorysystem.The comparator uses this internal signal to modulate or subtract external (exafferent) input.Additionally, tension feedbackfrom the muscles (reafferentsignal) exerts adelayed effectonperception.Direct projections from motor to sensorycortices underliethis architecture in all mammals. Part (b) is adapted from the original figure in [18].",
        "itself be the driving cause of subsequent perceptual development. It is precisely when action mechanisms become sufficiently establishedthat the agent benefits from additionalsensors,which guidethose movements more strategically. This developmentalsequence grounds perception inutility,tying sensory discrimination to the practical outcomesof movement.",
        "Disruptions in the normalinterplay of action and perceptionilluminate the intricate cause-effect loop.During sleep paralysis,the brain's motorcommands temporarily fail to reach the muscles; external stimuli stillbombardthe senses, but the usual action-to-perception calibration is lost. As a result, the individual experiences a heightened sense of unreality because the brain lacks internally generated reference signals to interpret sensory input. Similarly,if one externally manipulates the eye without the brain issuing a motor command,the visual scene appears to move, highlighting how perception alone—devoid of a preceding,self-initiated action—risks confusion.Neurophysiological data further support the inside-out model.Many neurons in areas once deemed“purely sensory\"track not only changes in external stimuli but also self-generated movements—sometimes more strongly so.This indicates that“cause\"in the brain frequently emerges from within,guiding both the magnitude and meaning ofexternal signals.Without these internal correlates, raw sensory data can become ambiguous or even useless to the system.",
        "Implications for IntelligentAgents The inside-out perspectiveofers potent insights for modernresearch on intelligent agents.Most contemporary AI systems—and many LLM agents—stillfunction predominantly in a reactive mode, awaiting user input and generating responses based on statisticalcorelations learned from vast datasets.Such pasivity resembles an“outside-in\"framework,where the agent'sroleislimited toresponding,notinitiating.Yetfanagent were to be active,continuously forming and testing hypotheses via self-initiated behaviors (physicalorrepresentational), it might ground its own“perceptual\" inputs—be they sensory streams or linguistic prompts—and thereby reduce ambiguity.Forinstance,anLLM-basedagent that interjects questions orverifies itsownstatements againstaknowledge base could better discern which inferences are self-caused from those demanded by external data.Bytracking these self-initiatedcontributions (analogous tocorollarydischarge),the modelcouldimprovecoherence,lessn erors known as “hallucinations\", and refine its internal state through iterative cause-effect loops.",
        "A proactive stance also encourages more data-effcient andcontext-aware learning.Instead of passvely waiting for labeled examples,anagent can explore,provoke feedback, and incorporate self-generatedexperiences intoitstraining. Over time,this tight coupling between action and perception may bolster the agent's ability tohandle complex tasks, adapt to unanticipated challenges,and generalize more robustly.The shift from anoutside-in toan inside-out model reframes perception as causally downstream of action.Intellgent systems-whether biological or artificial—stand to benefit from recognizing that purposeful movement,or proactive conversational steps in thecase of LLMs,can actively create,shape, and interpret the signalsthatflow back in.Byacknowledging thecause-efect power of action and strivingto buildactiveratherthanmerelyreactive agents,we mayapproach adeperunderstanding of both natural cognition and the next generation of AI.",
        "Table 8.2: Comparing the perception and action of human and AI agents.",
        "<html><body><table><tr><td>Dimension</td><td>Human Brain / Cognition</td><td>LLM Agent</td><td>Remarks</td></tr><tr><td>Perception</td><td>- Integrates multiple sen- sory channels (vision, hear- ing, smell, touch, taste). - Perception closely tied to - Perception depends on ex- emotions, endocrine system, and physical state. - Highly sensitive, capable of- Lacks real-time coupling detecting subtle differences.</td><td>- Primarily language-based with some multimodal capa- bilities. ternal sensors and models with limited integration. with physical states.</td><td>Perception differences lead to varying ways of under- standing reality. Embodied AI attempts to bridge this gap but still faces both hardware and software challenges.</td></tr><tr><td> tion</td><td>Unified Representa- - Simultaneously processes multimodal inputs: vision, hearing, language, motion, and emotions. laborate to create unified spa- tiotemporal and semantic un- derstanding.</td><td>- Primarily text-based. Some multimodal models can pro- cess images or audio but with low integration. - Different brain regions col- - No fully unified spatiotem- poral modeling like the hu- man brain.</td><td>Even advanced multimodal models  lack  the human brain's holistic,unified representation capacity. Hardware and algorithmic challenges remain.</td></tr><tr><td>Granularity in Task Switching</td><td>- Flexible in shifting between macro and micro cognitive tasks. - Can plan at a high level - Cannot autonomously real- and shift focus to finer details locate attention between task when needed. - Adjusts task priority and - May get stuck in a spe- focus dynamically based on context and working mem- ory.</td><td>- Relies heavily on prompt engineering for granularity control. layers. cific level of abstraction in absence of guided prompts.</td><td>Humans can dynamically adjust cognitive granular- ity based on situational de- mands, while LLMs require explicit instruction to switch task focus effectively.</td></tr><tr><td> Action</td><td>Goal-oriented process drives multiple sensory to make decisions. - Real-time Learning from the  experience via  the environmental interaction. - Encompass both physical activities and mental pro- cesses.</td><td>- Action space need to be de- fined in advance. - Unable to support actions in continuous spaces. - Relies on online training to optimize the decision- capability. making process in the envi- ronment.</td><td>Humans are capable of actively learning new actions and performing continuous actions, whereasLLM agents currently lack this</td></tr></table></body></html>"
      ],
      "raw_title": "Brain from Outside-ln",
      "type": null,
      "children": []
    },
    {
      "title": "8.5 Summary and Discussion",
      "number": "8.5",
      "level": 2,
      "content": [
        "Traditionally,actionrepresents the behaviorsof the human cognition system based onthe interactive feedback fromthe environment.Itendows humans with thecapability tothink,reason,speak,run,and perform anycomplex manipulations. Based on the action system,humans can iteratively evolve thebrain intelligenceby enhancing their perception and actions fromthe world,andformaclosedloop tofurther create newcivilization and innovationin the world.Similarlyto a humancognitionsystem,theaction systemplus the tool system alsoplayan importantrole forAIagents.Integrating action systems allows AIagents tosystematicaly plan,execute, andadjusttheir behaviors,facilitating more adaptable and robust performance in dynamiccontexts. Inthis section, we systematically examine and summarizethe impact of the action module on AI agents, focusing on both action systems and tool systems.",
        "Action System In our studies,we briefly describe the action system from three perspectives:action space, action learning,andtoollearning.Inanaction system,action space usually serves as the most important component, which determies the upper bound of AI agents in solving downstream tasks. Itformulates which actions can be selected and performed byAIagents during interactions withreal-worldenvironments.For action space,there are alsovarious diffculties depending ondata types,ranging from discrete tocontinuous data.Withthe growing demandforAI agents, there is alsoarising expectationforAIagents tohandle more sophisticatedtasks,particularlythose involvingreal-world applications.Therefore,how to buildrobust and generalaction space issillan ongoingchallenge inaction systems. On the basis of action space, action learning is another crucial component in enabling agents to interact efectively with theexternal world and withhumans.Action learning represents the process of an AIagent to learn and optimize its policy during interaction with real-world environments.Based on diffrent foundation models,it also derives diferent action learning paradigms,from zero-shot learming (e.g., prompt engineeing)to supervised training and reinforcement learning.In action learning,it isessential tothoroughly understandthetask,including how to devise system prompts,how todetermine thepre-trained orfine-tuned datasets,and the reward signals oroptimization polices during the training.Despite notable progressin action learning to advance AIagent frameworks, numerous questions remain to be addressd.Specifically, the ICL paradigm requires specific prior knowledge fora proper prompt design. Additionally,combining pre-training and post-training for supervised training necessitates high-quality and diverse data, which oftenrequires meticulous data processng and significanthuman efort.Furthermore,the unstable nature of reinforcement learming poses difficulties inits application inlarge-scale training scenarios.Moreover, the designof action systems playsacrucialrole in maximizingthebenefitsof toolintegration.By incorporating aneffective action system,AIagents can seamlessly engage with various tools,execute complex user intents,and transform external data into meaningful outcomes.This synergy between action systems and tools not only mitigates the limitations of memorizationandreduces the riskof hallucinations[714]but also enhances the expertise androbustness of the system. For instance, an AI agent equipped with a robust action system can dynamicall select and employ the most appropriate tols fora giventask,ensuring bothaccuracy and effciency in its responses.Furthermore,action systems facilitatehierarchicalreasoning processs,enabling agentstoorchestrate intricateworkflows thatalignclosely with userobjectives.This alignment is essential fortasks requiring precise execution andreal-timedecision-making, thereby bridging the gap between foundational modelcapabilities and practical applicationdemands.Additionally, the transparencyandinterpretabilityprovidedbytoolexecutionprocessesenhanceusertrust andfacilitate effective humanmachinecolaboration.Consequently, the combination of specialized tools and robust action systems significantly elevates the performance, reliability, and applicability of AI agents in diverse and dynamic environments.",
        "In summary,action systemscan significantlyestablish thefoundationforthe problem-solvingcapabilitiesof AIagen frameworks, enabling them to tackle a broader range of complex tasks beyond foundation models.",
        "ruture Directions Nonetheless, building an effective action system for agents requires solutions to a numberof :hallenges, as we summarize in the following:",
        "1.Effciency presents a significant hurdle,particularly in real-time applications where swift and precise responses are critical.Thecomplexity involved in action system can lead to unacceptable latency,hindering the practical deployment of AI systems in scenarios like fraud detection or real-time decision-making.To mitigate these efficiency issues, strategies such as filtering out irrelevant or redundant information, employing zero-shot prompting to streamline reasoning processes, and utilizing high-speed storage solutions for caching pertinent knowledge are imperative. These approaches help in maintaining high performance while reducing response times.",
        "2. Evaluation is also a important factor in action system, including action learning and toollearning. In the real-world environment, there exists massive actions from different sources. Therefore,how to determine the correct action or tools from disparate sources to avoid conflicting information is stilla significant challenge in AI Agent.To alleviate these problems, how to build an effective and robust evaluation system to measure action system is essential to maintain the accuracy and reliability of responses.Developing robust evaluation system, verification protocols and creating transparent methods are crucial to reduce incorectness in action prediction.Besides,exposing the decision-making processes of foundation models also help us understand which action is better and how to coordinate with various actions or tools to provide trustworthy outputs.",
        "3. Multi-modality Action learming has achieve remarkable progresses in LLM-based autonomous agent, due to the success of large language models.However,how to understand and invoke action beyond the language instructions (e.g.,GUI operations or embodied tools) stillremain challnges. In real-world scenarios, humans can develop or learn to use new skills through any kinds of instructions (e.g.,language, image, videos or human guidance). Therefore, enabling AI agents to develop or learn actions through diverse modalities is crucial to advance the capability of AI Agent in solving practicaltasks from the real-world scenarios.In other words, it is necessary forus to explore how to reduce the gap between human and AI agents in tool utilization, facilitating the design of advanced agent frameworks for the future.",
        "4. Privacy is a critical concern in the field of generative AI, especially using LLMs.As a consequence, maintaining the privacy of sensitive user data and preventingthe disclosure of user behaviors are essential in tool utilization[720]. To address these privacy concerns,some safe techniques like federated learning can be used to enable models to be trained on decentralized data sources without exposing sensitive information directly. Additionally, model distilation is often necessary to ensure models maintain high performance while safeguarding data integrity.These methods enable the efective training of models while preserving the confidentiality of user data.\n5. Safty Moreover, the ethicalimplications of human-model collaboration and the safety concerns associated with models interacting with physical environments necesitate careful consideration. Ensuring that human dignity and agency are preserved when integrating human labor with AIsystems is critical. Establishing ethical guidelines, promoting fair working conditions, and fostering interdisciplinary collaboration are necessary to address these concerns.Additionally,developing robust safety mechanisms to prevent erroneous or malicious actions by AI systems interacting with physical tools or actions is imperative to safeguard against potential risks.",
        "In additiontotheabovechallenges,there alsoremainopen problemsforthe action system.Forexample,howto achieve an optimal balance between the foundation models and external tools,deciding on the appropriate timing to use the former versus thelatter,remainsunanswered.Specifically,although tool systems canoffer flexibilityandextensibility for foundation models,there is an increasing trend toenhance the intrinsiccapabilityoffoundation models.Therefore, balancing between foundation models and tool systems is essential for developing versatile and effcient AIagents."
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": []
    },
    {
      "title": "Self-Evolution in Intelligent Agents",
      "number": "",
      "level": 1,
      "content": [
        "![](images/7eb2110a388d0c61ba0b70305ed0f1721a096440997d77ce36c9e16143055348.jpg)",
        "Figure 8.6: Structures of self-evolution in LLM agents.",
        "In the history of machine learning research, manually designed AI systems have gradually been replaced by more efcient,learned solutions[756]For instance,before the advent ofdeeplearning,features were typicallyhandcrafted by experts[757,758],butthese were eventuallsupersededbyfeatures extractedthrough neuralnetworks.As neural networks have become increasingly complex,various techniques for automated design-such as neural architecture search-haveemerged,further replacingthe needfor manualldesignednetwork structures[759]. Similarly,Agentic systems initiallrelied heavilyon manual design,with behaviorrules anddecision-making strategies explicitlyrafted by developers.Althoughfullautomationofagentself-evolutionhas not yet been achieved,itisanticipatedand deemed necessary for future progress.A successfulprecedentfor such automationcan already be seen inautomated machine learning(AutoML)[712,760,761,762,204]whichhas automatedvarious components of traditional machine leaing pipelines. In particular, AutoML streamlines theselection and configuration of machine learming algorithm pipelines while incorporating advanced techniques for hyperparameter optimization[763,764,765,766,767].Among the most notable applicationsof AutoML is NAS [768,769], which automates the design of neuralnetwork architectures to enhance model performance.Drawing inspiration from this successful transition towards automation in traditional machine learning, we propose extending similar principles to the domain of agentic AI systems.",
        "Akeycounterintuitive issue inmuchofcurrentagentresearchisthat,while theultimategoalofdevelopingorimproving agentic AIsystems isto automatehuman efforts, the processofcreating these systemsremains,forthe time being, beyondthereachof fullautomation.Therefore, we argue that all manuallydesignedagenticAIsystems willeventually be replaced by learnable and self-evolvingsystems,which couldultimately place the development and improvement of agentic AI into an autonomous,self-sustaining loop.Enabling self-evolution mechanism in LLM agents has the following benefits:",
        "1. Scalability: While LLM-based agents have demonstrated remarkable performance, their improvement stll heavily depends on the underlying LLMs.However, upgrading these models iscostly, and scaling performance through the inclusion of additional real-world data requires extensive retraining on large datasets, which poses significant resource constraints.Self-evolving agentic systems,in contrast,can optimize agent behavior without necessitating modifications to the underlying LLMs,offering a more eficient and scalable solution.",
        "2. Reduction in Labor Costs: Manually designing agentic systems is a complex and labor-intensive process that requires developers to engage deeply with intricate technical details.Traditional methods often involve building these systems from scratch, demanding significant expertise and efort.By contrast, self-evolving agentic systems can automate much of this process, significantly reducing the need for manual intervention and lowering development costs.",
        "3. Aligned with Natural Intellgence Development: Just as humans continuously improve themselves through learning and adaptation,equipping LLM agents with self-improvement capabilities is a necessary step toward the development of truly autonomous agents. This enables them to refine their performance, adapt to new challenges, and evolve without direct human intervention.",
        "![](images/86dc6d35cbbd558ddde70ccc61d38f275c453e795c4bc71d463de1e3fcc953e2.jpg)",
        "Figure 8.7:An illustrationofkey concepts discussed in this section,including optimization spaces,the optimizer, and the optimizing objective.The optimizer iterativelyrefinescomponents withintheoptimization spaces to enhance agentic systems untilasatisfactory outcome is achieved,thereby achieving self-improvement inthe LLMagent systems.",
        "To achieve the goal of automating human eforts,numerous studies have proposed leveraging LLMs as the driving engine to enable self-evolution inagentic systems.In particular,LLMs provide aneffcient alternative totraditional optimization methods,such as gradient-based[770]and reinforcement learning-based approaches[771].They extend the optimization space from numerical values to more diverse domains, with naturallanguage serving as a universal bridge. An LLM is capable of optimizing complex, heterogeneous parameters,such as instructions [732]and tool implementations[772],andcan operate across arange of LLMs,including both open-source andclosed-source models. A notableexampleofthisapproach isAFLOW[773],whichautomatesthe generationandoptimizationofentire agentic system workflows.This system employs Monte Carlo TreeSearch toleverage thecomprehensive capabilities of LLMs. In this framework,traditionallyhandcrafted agentic systems are replaced byalgorithmically generated ones, marking a kind of paradigm shift.Additionall,agrowing bodyofresearch explores similar methodologies,furtheradvancing the field.",
        "This partisstructuredasfollows:First,we introduce variousoptimization spaces explored inrecentresearchonagentic systems,including prompts,tools, and workflows. In the subsequent section, we review optimization algorithms, discussing both traditionaloptimization paradigms and meta-optimization, where the optimization processalso affcts the underlying optimization algorithms themselves.Wethen explore the self-evolution scenarios,categorizing them into twotypes:onlineoptimizationandoffineoptimization.Following this,wediscussthe applicationof large language model (LLM)agent self-improvement techniques,particularly inknowledge discovery within the AI-for-science domain. Finally, we discuss the security concerns associated with agent self-evolution technologies."
      ],
      "raw_title": "Self-Evolution in Intelligent Agents",
      "type": null,
      "children": []
    },
    {
      "title": "Optimization Spaces and Dimensions for Self-evolution",
      "number": "",
      "level": 1,
      "content": [
        "The optimization of autonomous agents represents acomplexchallenge that encompases multiple levelsof abstraction. In this section,we first establish prompt optimizationas the foundationallayer, upon whichthreedistinct branches of optimization emerge: agentic workflow optimization, tool optimization,and comprehensively autonomous agent optimization."
      ],
      "raw_title": "Optimization Spaces and Dimensions for Self-evolution",
      "type": null,
      "children": []
    },
    {
      "title": "9.1 Overview of Agent Optimization",
      "number": "9.1",
      "level": 2,
      "content": [
        "ExistingLLM-based agent optimization can be conceptualized in terms ofatwo-tieredarchitecture.Atthe foundation lies prompt optimization,whichfocuses onenhancing the basic interaction patterns of Language Modelnodes.Building upon this foundation,threeparalelbranches emerge:i)workfow-leveloptimization,which focuses onthecoordination and interaction patterns between multiple LM nodes; i) tooloptimization, where agents evolve by developing and improving tools to adapt tonew tasksand leverage past data; and i)comprehensive autonomous agent optimization, which aims at the holistic enhancement of agent capabilities by considering multiple dimensions.",
        "Similarly tooptimization paradigms in AutoML,agent optimization can be categorized as either single-objective or multi-objective.Contemporary agent optimization primarilycenters on threecanonicalmetrics: performance,inference cost,and latency.Performancemeasures theeffctiveness ofthe agent incompleting itsassignedtasks,while inference cost quantifies the computationalresources required for agent operation. Latency represents the time taken forthe agent to respond and complete tasks.These objectivescan vary depending on the specific optimization modality. For instance,in prompt-level optimization, aditionalconstraints such as prompt length may become relevant objectives. This multi-faceted nature of optimization objectivesreflects the complexity of agent systems andthe need to balance multiple competing requirements."
      ],
      "raw_title": "Overview of Agent Optimization",
      "type": null,
      "children": []
    },
    {
      "title": "9.2 Prompt Optimization",
      "number": "9.2",
      "level": 2,
      "content": [
        "Prompt optimization plays the mostcriticalrole in LLM-based agent optimization. When optimizing an agent, beyond model-leveloptimizations,task-specificor model-specific promptoptimization directly impacts theagent's peformance, latency, and cost. Given a task $T=(Q,G_{t})$ ，where $Q$ denotes the input query and $G_{t}$ represents the optional ground truth, the objective of prompt optimization is to generate a task-specific prompt $P_{t}^{*}$ that maximizes performance:",
        "$$\nP^{*}=\\underset{P\\in\\mathcal{P}}{\\arg\\operatorname*{max}}\\mathbb{E}_{T\\sim\\mathcal{D}}[\\phi_{\\mathrm{eval}}(\\phi_{\\mathrm{exe}}(Q,P),T)]\n$$",
        "where $\\mathcal{P}$ represents the space of possible prompts, $\\phi_{\\mathrm{exe}}$ denotes the execution function, and $\\phi_{\\mathrm{eval}}$ represents the evaluation function. This optimization is typically implemented through three fundamental functions: $\\phi_{\\mathrm{opt}},\\phi_{\\mathrm{exe}}$ , and $\\phi_{\\mathrm{eval}}$ . The Optimize function $\\phi_{\\mathrm{opt}}$ refines existing prompts based on optimization signals, the Execute function $\\phi_{\\mathrm{exe}}$ invokes the current prompt to obtain output $O$ , and the Evaluation function $\\phi_{\\mathrm{eval}}$ assesses current outputs to generate evaluation signals $S_{\\mathrm{eval}}$ and optimization signals $S_{\\mathrm{opt}}$ . The evaluation signals are used to select effective prompts, while the optimization signals assist the Optimize function in performing optimization."
      ],
      "raw_title": "Prompt Optimization",
      "type": null,
      "children": [
        {
          "title": "9.2.1 Evaluation Functions",
          "number": "9.2.1",
          "level": 3,
          "content": [
            "At the core of prompt optimization lies the evaluation function $\\phi_{e v a l}$ ， which serves as the cornerstone for deriving optimization signals and guiding the evolutionary trajectory of prompts.This function orchestrates a sophisticated interplay between evaluation sources,methodologies,and signal generation,establishing afeedbackloopthat drives continuous improvement. The evaluation function $\\phi_{e v a l}$ processes evaluation sources as input, and employs various evaluationmethods to generate diferent types of signals,whichsubsequently guide theoptimizationprocessHere,we define the dimensions of sources, methods, and signal types to establishthe foundationfor prompt optimization.",
            "Evaluation Sources Evaluation sources primarily consist of LLM Generated Output $G_{l l m}$ and task-specific Ground Truth $G_{t}$ .Existing workssuchas[730,774,728,775,732,00]predominantlyleveragecomparisonsbetween $G_{l l m}$ and $G_{t}$ as evaluation sources. Some approaches [776, 721, 777] utilize only $G_{l l m}$ as the evaluation source. For instance, PROMST [721] assesses prompt effectiveness by comparing $G_{l l m}$ against human-crafted rules; SPO [778] employs pairwise comparisons of outputs from different prompts to determine relative effectiveness.",
            "Evaluation Methods Evaluation Methods can be broadly categorized into three approaches: benchmark-based evaluation, LLM-as-a-Judge, and human feedback.Benchmark-based evaluationremains the most prevalent method in prompt optimization[730,774,721,732,300].This approachrelies on predefined metrics orrules to provide numerical feedbackasevaluation signals.While itoffersanautomated evaluation process,itsefectivenessultimatelydependson how well the benchmark design aligns with human preferences.",
            "The introduction of LLM-as-a-Judge represents a significant advancement in automated evaluation and preference alignment. Leveraging LLMs\"inherent alignment with human preferences and carefull designed judging criteria, this approach [589] can assess task completion quality based on task descriptions and prompt outputs $G_{l l m}$ , providing reflective textual gradientfeedback.Notable implementations include ProteGi[779]TextGrad[728],Semantic Search [775] and Revolve[780]. Furthermore,LLM-as-a-judge enables comparative evaluation between ground truth $G_{t}$ and output $G_{l l m}$ with specific scoring mechanisms [724]. The effectivenessof this method hinges on both the design of judger prompts andtheunderlying model's alignment with human preferences.As a specialized extension,Agent-as-aJudge[781] refines this paradigm by employing dedicated agents for providing process evaluation on complex tasks, while maintaining high alignment with human preferences at significantly reduced evaluation costs.",
            "Humanfeedbackrepresents thehighest levelof intellgence integrationintheevaluationprocessAs humans remain the ultimate arbitersof prompt efectiveness,direct human feedback can rapidly and substantially improve prompt quality. However,this appoach introducessignificantresource overhead.APOHF[777]demonstrates that incorporating human fedback can achieve robust prompt optimization with minimal computational resources, particularly excelling in open-ended tasks such as user instructions, prompt optimization for text-to-image generative models, and creative writing.Nevertheless,therequirement forhuman intervention somewhatcontradicts the goal ofautomated evolution.",
            "Signal Types Feedback generated by evaluation methods manifests in three distinct forms,each serving different optimization needs.Numerical feedback[730,774,721,732,300] quantifies performance through scalar metrics, compatible with rules,ground truth,human assessment,and LLM judgments.While widely applicable,this approach requires substantialsamplesforstatisticalreliability,potentiallyoverlooking instance-specificdetailsthatcouldguide optimization.Textual feedback[728,775,780]provides detailed,instance-specific guidancethrough analysis and concrete suggestions. This sophisticated approach requires intelligent participation, either from human experts or advanced language models,enabling targeted improvements in prompt design through explicit recommendations. However, itsreliance on sophisticated intelligence sources impacts itsscalability.Ranking feedback[778]establishes relative quality ordering through either comprehensive ranking or pairwise comparisons. This approach uniquely circumvents the need for absolute quality measures or predefined criteria,requiring only preference judgments.It proves particularly valuable when absolute metrics are difficult to define or when optimization primarily concerns relative improvements."
          ],
          "raw_title": "Evaluation Functions",
          "type": null,
          "children": []
        },
        {
          "title": "9.2.2 Optimization Functions",
          "number": "9.2.2",
          "level": 3,
          "content": [
            "The design ofoptimization functions is crucial in determining the quality of generated prompts in each iteration of prompt optimization.Through efective signalguidance,prompt self-evolutioncan achieve faster convergence.Current optimization approaches primarily rely on two types of signals: evaluation signals $S_{e v a l}$ that identify the most effective existing prompts, and optimization signals $S_{o p t}$ that provide detailed guidance for improvements.",
            "Optimize via Evaluation Signals When optimizing with evaluation signals,the processbegins by selecting the most effective prompts based on $\\phi_{e v a l}$ assessments. Rather than directly learning from past errors, some methods adop1 heuristic exploration andoptimizationstrategies.SPO[778] iterativelyrefines prompts based onthe outputs ofcurrent best-performing ones,leveraging the language model's inherent ability to align withtask requirements. Similarly, Evoprompt[723] employs evolutionary algorithms with LLMs serving as evolution operators for heuristic prompt combination. PromptBreeder[732] advances this approach further by comparing score variations between mutated prompts while simultaneously modifying both meta-prompts and prompts throughthe LLM's inherent capabilities.",
            "Optimize via Optimization Signals While optimization methods based solely on evaluation signals require extensive search to findoptimal solutions invast search spaces throughtrialand error,an alternative approach leverages explicit optimization signals to guide theoptimization direction and improve effciency.Existing methods demonstrate various ways to utilize these optimization signals. OPRO [730] extracts common patterns from high-performing prompt solutions to guide subsequent optimization steps.ProTegi[779]employslanguage models to analyze failure cases and predict error causes, using these insights as optimization guidance.TextGrad[728] extends this approach further by transforming promptreflections into“textual gradients\",applying this guidance acrossmultiple prompts within agentic systems.Revolve[780] further enhances optimization by simulating second-order optimization,extending previous first-order feedback mechanisms to modelthe evolving relationship betweenconsecutive prompts andresponses.This allows the system to adjust based onhow previous gradients change, avoiding stagnation in suboptimal patterns and enabling more informed, long-term improvements in complex task performance."
          ],
          "raw_title": "Optimization Functions",
          "type": null,
          "children": []
        },
        {
          "title": "9.2.3 Evaluation Metrics",
          "number": "9.2.3",
          "level": 3,
          "content": [
            "The efectiveness of prompt optimization methods can be evaluated across multiple dimensions. Performance metrics[782,778,730]forCloseTasksserveasthemostdirectindicatorsofaprompt'sinherent peformance,encompassing measures such as pass $\\ @1$ , accuracy, F1 score, and ROUGE-L. These metrics enable researchers to assess the stability, effectiveness,and convergence rate of prompt optimization processes.Another crucial dimension is Effciency metrics [778].While some prompt optimization approaches achieve outstanding results,they often demand substantial computationalresources,larger sample sizes,andextensive datasets.Incontrast,other methods achieve moderate results with lower resource requirements,highlighting the trade-offs between performance and effciency in agent evolution. The third dimension focuses on qualitative metrics that assesspecific aspects of agent behavior:consistency[776] measures output stability across multiple runs,fairness[783]evaluates theability to mitigate the language model's inherent biases,andconfidence[784,785]quantifies theagent'scertainty inits predictions.When these behavioral aspects are treated asdistinctobjectives,prompt optimizationframeworks providecorresponding metricsforevaluation."
          ],
          "raw_title": "Evaluation Metrics",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "9.3 Workflow Optimization",
      "number": "9.3",
      "level": 2,
      "content": [
        "While prompt-level optimization has shown promising results in enhancing individual LLM capabilities, modern AI systems often require the coordination of multiple LLM components to tackle complex tasks. This necessitates a more comprehensive optimization domain-the agentic workfow space. At its core, an agentic workflow consists of LLM-invoking nodes, where each node represents a specialized LLMcomponent designed for specific sub-tasks within the larger system.",
        "Although this architecture bearssimilarities to multi-agent systems,itis important todistinguishagenticworkflows from fully autonomous multi-agent scenarios.In agentic workflows, nodes operate under predetermined protocols and optimization objectives,rather than exhibiting autonomous decision-making capabilities.Many prominent systems, such as MetaGPT[626] AlphaCodium[786] can be categorized under this framework.Moreover, agentic workflows can serve as executable components within larger autonomous agent systems, making theiroptimization crucial for advancing both specialized task completion and general agent capabilities.",
        "Following the formalization proposed by GPTSwarm[651] and AFLOW[773],his sectionfirst establishes a formal definition of agentic workflows and their optimization objectives.We then examine the core components of agentic workflows—nodes and edges—analyzing their respective search spaces and discussing existing representation approaches in the literature."
      ],
      "raw_title": "Workflow Optimization",
      "type": null,
      "children": [
        {
          "title": "9.3.1 Workflow Formulation",
          "number": "9.3.1",
          "level": 3,
          "content": [
            "An agentic workflow $\\kappa$ can be formally represented as:",
            "$$\n\\mathcal{K}=\\{(N,E)|N\\in\\mathcal{N},E\\in\\mathcal{E}\\}\n$$",
            "where $\\mathcal{N}=\\{N(M,\\tau,P,F)|M\\in\\mathcal{M},\\tau\\in[0,1],P\\in\\mathcal{P},F\\in\\mathcal{F}\\}$ represents the set of LLM-invoking nodes, with $M$ $\\tau,\\mathcal{P}$ , and $\\mathcal{F}$ denoting the available language models, temperature parameter, prompt space, and output format space respectively. $E$ indicates the edges between different LLM-invoking nodes. This formulation encapsulates both the structural components and operational parameters that define an agentic workflow's behavior.",
            "Given a task $T$ and evaluation metrics $L$ , the goal of workflow optimization is to discover the optimal workflow $K^{*}$ that maximizes performance:",
            "$$\nK^{*}=\\underset{K\\in\\mathcal{K}}{\\arg\\operatorname*{max}}L(K,T)\n$$",
            "where $K$ is the search space of workflow, and $L(K,T)$ typically measures multiple aspects including task completion quality,computationaleficiency,andexecution latency.Thisoptimizationobjectivereflects the practicalchallenges in deploying agentic workflows, where we must balance effectiveness with resource constraints."
          ],
          "raw_title": "Workflow Formulation",
          "type": null,
          "children": []
        },
        {
          "title": "9.3.2 Optimizing Workflow Edges",
          "number": "9.3.2",
          "level": 3,
          "content": [
            "The edge space $\\mathcal{E}$ defines the representation formalism for agentic workflows. Current approaches primarily adopt three distinct representation paradigms: graph-based, neural network-based, andcode-based structures.Each paradigm ofers unique advantages and introduces specific constraints on the optimization process.",
            "Graph-basedrepresentations enable theexpressonof hierarchical, sequential,andparalelrelationships between nodes. This approach naturally accommodates complex branching patterns and facilitates visualization of workflow topology, making it particularly suitable for scenarios requiring explicit structural manipulation.For example,GPTSwarm[651] demonstrated the effctivenessof graph-based workflow representation in coordinating multiple LLM components through topology-aware optimization. Neural network architectures provide another powerfulrepresentation paradigm that excels in capturing non-linear relationships between nodes. Dylan [725] showed that neural network-based workflowscan exhibit adaptive behavior through learnableparameters,making themespeciallyeffective for scenarios requiring dynamic adjustment based on input and feedback.Code-based representationoffers the mostcomprehensive expressivenessamong current approaches.AFLOW[773] and ADAS[741] established that representing workflows as executablecode supportslinear sequences,conditionallogic,loops,andthe integrationofboth graphand network structures. This approach provides precise control over workflow execution and leverages LLMs'inherent code generation capabilities.",
            "Thechoice ofedgespace representationsignificantly influences boththe searchspace dimensionalityandtheapplicable optimization algorithms.[728]focused solelyon prompt optimization while maintaining afixed workflow topology, enabling the use of textual feedback-based optimization techniques.In contrast,[651] developed reinforcement learning algorithms for joint optimization of individual node prompts andoveralltopology.[773]leveragedcode-based representation to enable direct workflowoptimization by language models,while recent advances by[787] and[788] introduced methods for problem-specific topology optimization."
          ],
          "raw_title": "Optimizing Workflow Edges",
          "type": null,
          "children": []
        },
        {
          "title": "9.3.3 Optimizing Workflow Nodes",
          "number": "9.3.3",
          "level": 3,
          "content": [
            "The node space $N$ consists of four key dimensions that influence node behavior and performance.The output format space $F$ significantly impacts performance by structuring LLM outputs, with formats like XML and JSON enabling more precise control over response structure. The temperature parameter $\\tau$ controls output randomness, affecting the stability-creativity tradeoff in node responses. The prompt space $P$ inherits the optimization domain from prompt-level optimization, determining the core interaction patterns with LLMs. The model space $M$ represents available LLMs, each with distinct capabilities and computational costs.",
            "For single-node optimization,existing researchhas primarilyfocused on specific dimensions within this space.[773] concentrated exclusivelyonpromptoptimization, while[741] extended the search space to include both prompts and temperature parameters.Taking a diferent approach,[789] fixed prompts while exploring model selection across different nodes. Output format optimization, though crucial, remains relatively unexplored [790].",
            "Compared to edge space optimization,node space optimization poses unique scalabilitychallnges due tothetypically large number of nodes in agentic workflows.The dimensionality of the search space grows multiplicatively with each additional node,necessitating effcient optimization strategies thatcan effectively handle this complexity while maintaining reasonable computational costs."
          ],
          "raw_title": "Optimizing Workflow Nodes",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "9.4 Tool Optimization",
      "number": "9.4",
      "level": 2,
      "content": [
        "Unlike conventional usage of LLMs that typically operate in asingle-turn manner, agents are equipped with advanced multi-turn planning capabilities andthe ability to interact with the external world via various tools.These unique atributes make the optimization of tool usage acriticalcomponent in enhancing an agent'soverallperformance and adaptability.Tool optimization involves systematically evaluating andrefining how an agent selects,invokes,and intgrates availabletools tosolve problems with highereffciencyand lower latency.Key performance metrics in this context include decision-making accuracyretrievalefficiency,selection precision,task planning,andrisk management. Central to this optimization are two complementary strategies: tool learning and tool creation."
      ],
      "raw_title": "Tool Optimization",
      "type": null,
      "children": [
        {
          "title": "9.4.1 Learning to Use Tools",
          "number": "9.4.1",
          "level": 3,
          "content": [
            "Unlike prompting-based methods thatleverage frozen foundation modelsin-contextlearning abilities,raining-based methods optimize the model that backs LLM agents with supervision. Drawing inspiration from developmental psychology,tollearningcanbecategorizedintotwoprimary streams: learning from demonstrations and learning from feedback[714].The other way toelicit the power of LLMs(agents)using tools is byusing prompt-basedorin-context learning methods for better reasoning abilities.",
            "Learning from demonstrations involves training models backed LLM agents to mimic expert behaviors through imitation learning.Techniques such as behavior cloning alow models to learn policies in a supervised manner by replicating human-annotated tool-use actions. Formally, given a dataset $D=\\{(q_{i},a_{i}^{*})\\}_{i=0}^{N-1}$ ,where $q_{i}$ is a user query and $a_{i}^{*}$ is the corresponding human demonstration, the controller's parameters $\\theta_{C}$ are optimized as:",
            "$$\n\\theta_{C}^{*}=\\arg\\operatorname*{max}_{\\theta_{C}}\\mathbb{E}_{(q_{i},a_{i}^{*})\\in D}\\prod_{t=0}^{T_{i}}p_{\\theta_{C}}(a_{i,t}^{*}\\mid x_{i,t},H_{i,t},q_{i})\n$$",
            "where $a_{i,t}^{*}$ is the human annotation at timestep $t$ for query $q_{i}$ , and $T_{i}$ is the total number of timesteps.",
            "Learning from feedback leverages reinforcementlearning to enable models to adapt based on rewards derived from environment or human feedback. The optimization objective for the controller's parameters $\\theta_{C}$ is:",
            "$$\n\\theta_{C}^{*}=\\arg\\operatorname*{max}_{\\theta_{C}}\\mathbb{E}_{q_{i}\\in Q}\\mathbb{E}_{\\{a_{i,t}\\}_{t=0}^{T_{i}}}\\left[R\\left(\\{a_{i,t}\\}_{t=0}^{T_{i}}\\right)\\right]\n$$",
            "where $R$ represents the reward function based on the sequence of actions $\\{a_{i,t}\\}$",
            "Integrating toollearning intotheoptimizationframeworkenhances the system'sabilityto generalize toolusage across diverse tasks and environments.By incorporating both demonstration-based and feedback-based learning,the model can iteratively improve its tool invocation strategies, selection policies, and execution accuracy.",
            "Optimization Reasoning Strategies for Tool Using Optimizing the aforementioned metrics for beter LLM agents' abilitie requires acombination of advanced retrieval models,fine-tuned reasoning strategies,and adaptive learning mechanisms. Reasoning strategies,such as Chain-of-Thought (CoT)[46], Tree-of-Thought [72],and Depth-First Search Decision Trees (DFS-DT)[690],facilitate more sophisticated decision-making processes regarding tool usage. Fine-tuning themodel's understanding of tools,including parameter interpretationand action execution,enables more precise andeffective toolinteractions.Additionally,learming fromthemodel'soutputsallowsforbeter post-processing and analysis, further refining tool utilization efficacy."
          ],
          "raw_title": "Learning to Use Tools",
          "type": null,
          "children": []
        },
        {
          "title": "9.4.2 Creation of New Tools",
          "number": "9.4.2",
          "level": 3,
          "content": [
            "Beyond theoptimization ofexisting tools,theability tocreatenew toolsdynamically[703,702,772]basedon a deep understanding of tasks andcurrent toolusage can significantlyenhance the LLM Agent framework's adaptability and efficiency. In recent work,severalcomplementary approaches have been proposed.ToolMakers[702]establishes a closed-loop framework where atool-making agent iterativelyexecutes three phases: (1)）Proposing Python functions via programming-by-example using three demonstrations, (2)Verifying functionality through automated unit testing (3 validation samples)withself-debuging oftest cases,and (3)Wrapping validated tools with usage demonstrations for downstream tasks.This rigorous process ensures reliability while maintaining fullautomation. CREATOR[703] adoptsa four-stage lifecycle:Creation of task-specific tools through abstract reasoning, Decision planning for tool invocation,Execution of generated programs,and Rectificationthrough iterative toolrefinementemphasizing tool diversity,separation ofabstract/concretereasoning,anderorrecovery mechanisms.Incontrast,CRAFT[77]employs an offlineparadigmthat distill domain-specificdata into reusable, atomictools (e.g.object color detection)through GPT-4 prompting,validation, and deduplication. Its training-freeapproachcombines human-inspectablecode snippets with compositional problem-solving,enabling explainable toolchains while avoiding modelfine-tuning—particularly effective when decomposing complex tasks into modular steps.",
            "The integration of these complementary approaches presents richresearchopportunities.Hybrid systems could merge CRAFT's pre-made toolrepositories with ToolMakers'on-demand generation, using functional caching to balance effciency and adaptability.Future frameworks might implement multi-tier tool hierarchies where primitive operations from CRAFTfeed into ToolMakers'composite tools,while CREATOR-stylerectification handles edge cases.Advances in self-supervised toolevaluation metrics andcross-domain generalization could further automate the tollifecycle. Notably,the interplay between tool granularity (atomic vs.composite)and reusability paterns warrants systematic investigation—fine-grainedtools enableflexible composition but increase orchestration complexity.As agents evolve, bidirectional tool-task co-adaptation mechanisms may emerge, where toolsreshape task representations while novel tasks drive tool innovation, ultimately enabling self-improving AI systems."
          ],
          "raw_title": "Creation of New Tools",
          "type": null,
          "children": []
        },
        {
          "title": "9.4.3 Evaluation of Tool Effectiveness",
          "number": "9.4.3",
          "level": 3,
          "content": [
            "The evaluation metrics and benchmarks discussed below offra comprehensive basis for quantifying an agent's tool usage capabilities.Byassessingaspectssuchastoolinvocatin,selectionaccuracy,retrievaleficiencyand planning for complextasks,these benchmarks notonly measure current performancebut alsoprovide clear,concrete objectives for optimizing tool usage.Such metrics are instrumentalin guiding both immediate performance enhancements and long-term strategic improvements inagent-based systems.In thefollowing sections,we firstreview the evolution of agent tool use benchmarks and then consolidatethe key evaluation metrics that serve as targets for further tool optimization.",
            "Tool Evaluation Benchmarks Recent eforts in LLM-as-Agent research have spawned diverse benchmarks and frameworks for evaluating tool-use capabilities.Earlystudies such as Gorilla[727] and API-Bank[791]pioneered large-scale datasets and methods fortesting LLMinteractions withexternalAPIs,shedding light on issueslike argument accuracyand hallucination.Subsequent works like T-Bench[792]and ToolBench [690] introduced more extensive task suites and stressed the importanceof systematic data generation fortool manipulation.StableToolBench[793]further extendedthisline of inquirybyhighlightingtheinstabilityofreal-world APIs,proposingavirtual APIserver for more consistent evaluation.Meanwhile,ToolAlpaca[794] investigated the feasibilityof achieving generalized tool-use in relatively smaller language models with minimal in-domain training.Additional efforts like ToolEmu[795]assessed the safetyandrisk aspects of tool-augmented LMagents through emulated sandboxenvironments.MetaTool[796] then introduced a new benchmark focused on whether LLMs know whento use tools andcancorrectly choose which tools to employ. It providesadataset named ToolEthatcovers single-tooland multi-tool usage scenarios,encouraging research into tool usage awareness and nuanced tool selection. ToolEyes[797]pushed the evaluation further by examining real-world scenarios and multi-step reasoning across a large tool library. Finally, $\\tau$ -bench [798] introduced a humanin-the-loop perspective,emphasizing dynamic user interactions and policy compliance in agent-based conversations. Together, these benchmarks and frameworks underscore the evolving landscape of tool-augmented LLM research, marking a shift from isolated reasoning tasks to comprehensive, real-world agent evaluations.",
            "Metrics forToolInvocation Deciding whether to invokeanexternaltolisacritical step thatcan significantly affct both theefficiencyandtheeffectivenessofasystem.In many scenarios,the modelmust determineif itsownreasoning is suffcienttoansweraqueryor if aditionalexternalknowledge(orfunctionality)providedbyatoolisrequired.To formalize this process, we introduce a labeled dataset",
            "$$\nD_{\\mathrm{inv}}=\\{(q_{i},y_{i})\\}_{i=0}^{N-1},\n$$",
            "where $q_{i}$ represents the $i$ -th user query and $y_{i}\\in\\{0,1\\}$ is a binary label indicating whether tool invocation is necessary $\\langle y_{i}=1\\rangle$ )or not ( $(y_{i}=0)$ ). Based on this dataset, the model learns a decision function $d(q_{i})$ defined as:",
            "$$\nd(q_{i})={\\left\\{\\begin{array}{l l}{1,}&{{\\mathrm{if~}}P_{\\theta}(y=1\\mid q_{i})\\geq\\tau,}\\\\ {0,}&{{\\mathrm{otherwise}},}\\end{array}\\right.}\n$$",
            "where $P_{\\theta}(y=1\\mid q_{i})$ denotes the predicted probability (from a model parameterized by $\\theta$ ） that a tool should be invoked for query $q_{i}$ , and $\\tau$ is a predetermined threshold.",
            "In addition tothisdecisionrule,severalmetricscanbeusedtoevaluate the model'sabilitytocorrctlydecideontool invocation. For example, the overall invocation accuracy $A_{\\mathrm{inv}}$ can be computed as:",
            "$$\nA_{\\mathrm{inv}}=\\frac{1}{N}\\sum_{i=0}^{N-1}\\mathbf{1}\\{d(q_{i})=y_{i}\\},\n$$",
            "where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Other metrics such as precision, recal, and Fl score are also applicable. Moreover, if $C_{\\mathrm{inv}}$ represents the cost incurred by invoking a tool and $R(q_{i})$ the benefit or reward obtained when a tool is correctly used, one can define a net benefit score:",
            "$$\n{\\cal B}_{\\mathrm{inv}}=\\sum_{i=0}^{N-1}\\left({\\bf1}\\{d(q_{i})=1\\}\\cdot R(q_{i})-C_{\\mathrm{inv}}\\right).\n$$",
            "This formulation not only emphasizes accuracy but alsoconsiders the cost-effectiveness of invoking external tools.",
            "Tool Selection Among Candidates Once the decision to invoke atoolis made,the next challenge is to select the most appropriate tool from a pool of candidates. Let the candidate toolset be represented as:",
            "$$\n\\mathcal{T}=\\{t_{1},t_{2},\\dots,t_{M}\\}.\n$$",
            "For a given query $q_{i}$ , assume that the optimal tool (according to ground truth) is $t_{i}^{*}$ and the model selects $\\hat{t}_{i}$ .The simplest measure of selection performance is the tool selection accuracy $A_{S}$",
            "$$\nA_{S}=\\frac{1}{|Q|}\\sum_{q_{i}\\in Q}\\mathbf{1}\\{\\hat{t}_{i}=t_{i}^{*}\\}.\n$$",
            "However, many scenarios involve ranking multiple candidate tools by their relevance.In such cases,ranking-based metrics such as Mean Reciprocal Rank (MRR) and normalized Discounted Cumulative Gain (nDCG) offer a more nuanced evaluation. [690] use those two when evaluating the tool retriever system.",
            "Tool Retrieval Efficiency and Hierarchical Accuracy Toolretrieval involves both the speed of identifying a suitable toolandtheaccuracyof thatselection.Effcientretrievalmethodsreducelatencyandcomputationaloverhead,whilehigh retrieval accuracyensures thatthemost relevant tolisidentifiedforthetask.Toevaluate toolusagecomprehensively, we adopta hierarchical framework that distinguishes between retrieval accuracy and selection accuracy. Retrieval accuracy $(A_{R})$ reflects how precisely the system retrieves the correct tool from the repository,typically measured by metricssuch as ExactMatch(EM)and Fl score,whichcapture both complete and partial matches.Incontrast,selection accuracy $(A_{S})$ assesses the system's ability to choose the optimal tool from a set of candidates, again using similar metrics. Overall tool usage awareness is further evaluated by accuracy, recall, precision, and Fl score.",
            "The overall retrieval efficiency $E_{R e t}$ is thus can be expressed as:",
            "$$\nE_{R e t}=\\frac{A_{R}\\times A_{S}\\times A_{P}\\times A_{U}}{C_{R}}\n$$",
            "where $C_{R}$ is the cost associated with retrieval. Optimization strategies may involve training embedding models with feedback mechanisms to enhance both efficiency and each hierarchical component of accuracy.",
            "For a more nuanced evaluation of tool selection, Metatool [796] introduces the Correct Selection Rate (CSR),which quantifies the percentage of queries for which the model selects the expected tool(s).This evaluation framework addresses four aspects:selecting thecorrect toolamong similar candidates,choosing appropriate tools in contextspecific scenarios,ensuring reliabilityby avoiding the selectionof incorrect or non-existent tools,andhandling multi-tool queries.Together,these metrics and sub-tasks providearobust measureof both the effciency and precision in tool retrieval and selection.",
            "Tool Planning for Complex Tasks Complextasks often require thesequential application of multiple tools to reach an optimal solution. A tool plan can be represented as an ordered sequence",
            "$$\n\\Pi=[t_{1},t_{2},\\dots,t_{K}],\n$$",
            "where $K$ is the number of steps.The quality of such a plan is typicall evaluated by balancing its task effectiveness (e.g., via a metric $R_{\\mathrm{task}}(\\Pi))$ against the plan's complexity (or length). This balance can be captured by a composite planning score of the form",
            "$$\nS_{\\mathrm{plan}}=\\alpha\\cdot R_{\\mathrm{task}}(\\Pi)-\\beta\\cdot K,\n$$",
            "where $\\alpha$ and $\\beta$ are coeficients that adjust the trade-of between the benefits of high task performance and the cost associated with plan complexity. When ground truth plans $\\Pi^{*}$ are available, similarity metrics such as BLEU or ROUGE can be used to compare the predicted plan $\\Pi$ with $\\Pi^{*}$ , and an overall planning efficiency metric can be defined accordingly.",
            "In addition,recent worksuch as ToolEyes[797]highlightsthe importance of behavioralplanning intoolusage.Beyond selecting toolsand parameters,it iscrucialforLLMs toconciselysummarize acquired information andstrategicallyplan subsequent steps.Inthiscontext, thebehavioralplanningcapabilityisevaluatedalong twodimensions.First,the score $S_{b}$ validity $\\in[0,1]$ is computed by assessing (1) the reasonableness of summarizing the current state, (2) the timeliness of planning forthe next sequence ofactions,and (3)thediversityof planning.Second, thescoe $S_{b\\mathrm{-integrity}}\\in[0,1]$ .s calculated byevaluating(1)grammaticalsoundness,(2)logicalconsistency,and(3)theabilitytocorrectthinking.The composite behavioral planning score is then determined as",
            "$$\nS_{B P}=S_{b\\mathrm{-validity}}\\cdot S_{b\\mathrm{-integrity}},\n$$",
            "providingaholistic measure of the model'splanningcapability.This integrated framework ensures that tool planning forcomlex tasks not onlyfocuses onthe selection and ordering of tols but alsoon maintaining coherent,eective, and strategically sound planning processes.",
            "In summaryoptimizing tool performance within anAgent systemnecesstatesacomprehensive approachthat balances decision-makingaccuracy,retrievalefficiency,hierarchicalselection precision,strategicplanning,rigorous riskanagement,androbust toollearning mechanisms.By implementing targeted optimization and learning strategies,itis possible to enhance both the effectiveness and efficiency of tool-assisted machine learning workflows."
          ],
          "raw_title": "Evaluation of Tool Effectiveness",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "9.5 Towards Autonomous Agent Optimization",
      "number": "9.5",
      "level": 2,
      "content": [
        "In addition to optimizing individual modules in agent evolution,such as prompts,tols,and workflows—which are susceptible tolocal optimathatcancompromise theoverallperformance of the agenticsystem,a significant body of researchfocuses on optimizing multiplecomponents within the entire agentic systems.This holistic approach enables large language model(LLM) agents to evolve more comprehensively.However,optimizing the entire system imposes higher requirements. The algorithm must not only account for the impact of individual components on the agentic system but also consider the complex interactions between different components.",
        "ADAS[741] isone of the most representative works that firstformalydefines the research problem of automated design inagentic systems.Itintegrates multiple agenticsystemcomponents intothe evolutionary pipeline.Specifically, ADAS itroduces a meta-agentcapable ofiteratively designing the agenticsystem's workflow, prompts,and potential tools withinthe overalloptimization process.Asdemonstrated in the experiments,the automaticallydesigned agentic systems outperform state-of-the-art hand-designed baselines.",
        "Additionall,[726]proposes an agent symbolic learning frameworkfor training language agents,inspired byconnectionist learning principles used in neural networks.By drawing an analogy between agent pipelines andcomputational graphs,the framework introduces a language-based approach to backpropagation and weight updates. It defines a prompt-based lossfunction,propagates language lossthrough agent trajectories, and updatessymbolic components accordingly.This method enables structured optimizationof agentic workflows and naturally extends to multi-agent systems by treating nodes as independent agents or allowing multiple agents to act within a single node.",
        "[799] proposes an approach tooptimize both prompts andthe agent'sowncode,enabling self-improvement.This aligns with the concept of self-reference,where a system cananalyze and modify itsown structureto enhance performance.",
        "Similarly,[773],[787],[800]and[788]focus onoptimizing both the workflow and prompts within agentic systems.In particular,[285] introduces anapproachthat trains additionallarge language models (LLMs)to generate prompts and workflows, enabling the automated design of agentic system architectures.",
        "In summary,optimizing the workflow of an entire agentic system is not merely a straightforward aggregationof individualcomponent optimizations.Instead,it requires carefully designed algorithms that account for complex interdependencies among components.This makes system-wide optimization a significantly more challenging task, necessitating advanced techniques to achieve effective and comprehensive improvements."
      ],
      "raw_title": "Towards Autonomous Agent Optimization",
      "type": null,
      "children": []
    },
    {
      "title": "Large Language Models as Optimizers",
      "number": "",
      "level": 1,
      "content": [
        "In this chapter,we present and discussexisting works thatconceptualizeLLMs asoptimizers.First, wenote that most existing studies focus on the promptoptimization problem defined in Equation(9.1),asoptimizing other components of agentic workflows remainsanemerging researcharea.To proceed, wedraw parallels withclasscal iterativealgorithms and examine their integration into modern optimization workflows."
      ],
      "raw_title": "Large Language Models as Optimizers",
      "type": null,
      "children": []
    },
    {
      "title": "Online and Offline Agent Self-Improvement",
      "number": "",
      "level": 1,
      "content": [
        "In the pursuitof self-improvement,intellgent agents leverageoptimizationasbothamechanism forrefining individual components—such as prompt design, workflow orchestration,tool utilization,reward function adaptation,and even theoptimization algorithms themselves—and as a strategic framework that ensures these individualimprovements are aligned toward coherent performance enhancement.Forinstance,optimizing the reward function and prompt design in isolation might yield conflicting outcomes,but a strategicapproach coordinates these optimizations to maintain coherence and maximize overalleffectiveness.We categorize self-evolution into two primary paradigms:online and ofine self-improvement.Aditionally, weexplore hybrid optimization strategies that integrateboth approaches to maximize efficiency and adaptability."
      ],
      "raw_title": "Online and Offline Agent Self-Improvement",
      "type": null,
      "children": []
    },
    {
      "title": "11.1 Online Agent Self-Improvement",
      "number": "11.1",
      "level": 2,
      "content": [
        "Online self-improvement refers toreal-time optimization in which an agent dynamicaly adjusts its behavior basedon immediate feedback.This paradigm ensures that agents remain responsive to evolving environments bycontinuously optimizing key performance metrics—such as task success,latency,cost, and stability-inan iterative feedback loop. Online self-improvement is particularly effective in applications that require dynamic adaptability,such asreal-time decision-making, personalized user interactions, and automated reasoning systems. Key optimization strategies in online self-improvement can be clasified intothe following fourcategories: IterativeFeedback and Self-Reflection, Active Exploration in Multi-Agent Systems, Real-Time Reward Shaping, and Dynamic Parameter Tuning.",
        "Iterative Feedback and Self-Reflection These methodologies[48,67,72,70,847,47]focus on enabling agents to critique andrefinetheirwnutputs iteratively.Reflexion[48],Self-Refine[67],andTreeof Thoughts[72]introduce self-critique loops,where the model identifies errors and proposes revisions in real-time.ReAct[70] combines chain-of-thought“reasoning\"with“acting\",allowing the model torevise steps iteratively after observing external feedback.Inaddition,othermethodseitherrelyonself-consistency[78]toselectthe mostcoherentsolutionorleverage a process reward model(PRM)Lightman et al.[847]tochoose the best solution fromthecandidates.Collctively, these frameworks reduce eror propagation and support rapid adaptation without requiringa separate offine fine-tuning cycle.",
        "Active Exploration in Multi-Agent Systems These approaches [626,848,627,152]actively explore and dynamically search for novel paterns and workflow improvements in multi-agent systems.MetaGPT [626],CAMEL [848],and ChatDev[627]showcase multi-roleor multi-agent ecosystems thatinteract inreal-time,exchanging continuous feedback to refine each other'scontributions.Similarly,HuggingGPT[152]coordinates specialized models(hosted on Hugging Face)through a central LLM controller, which dynamically routes tasks and gathers feedback.These collaborative strategies further highlight how online updates among agents can incrementaly refine collective outcomes.",
        "Real-Time Reward Shaping Rather than relying on fixed or purely offine reward specifications, some frameworks[731,91,05,849]integrate immediate feedback signalsnotonly tocorrect errors,but alsoto adapt intenal reward functions and policies.This enablesself-adaptive reward calibration that balances trade-offs between performance,computational cost, and latency,alowing agents to optimize reward mechanisms dynamically in response to user interactions.",
        "![](images/48c2dee600c375a3465e5c88add4a12af8fea68f2a03e929c894d12eb5b02990.jpg)",
        "Figure 1l.l:Anilustrationofself-improvementunder threedifferent utilization scenarios,including Online,Ofline, and Hybrid self-improvement.",
        "Dynamic Parameter Tuning In this category,agents autonomously update their internal parameters (including prompt templates,toolinvocationthresholds,searchheuristics,etc.)inrealtime,leveraging gradient-freeor approximated gradientmethods.These updates optimize both computationaleficiency and decision accuracy,allowing for seamless adaptation to evolving contexts. Self-Stering Optimization (SSO)[850] eliminates the need for manual annotation and maintains signalaccuracy while keeping training on-policy by autonomously generating preference signals during iterative training.",
        "Online self-improvement fosters a continuously evolving agent framework where learning is embedded within task execution,promotingenhancedreal-timeadaptability,user-centric optimization,androbust problem-solvingcapabilities."
      ],
      "raw_title": "Online Agent Self-Improvement",
      "type": null,
      "children": []
    },
    {
      "title": "11.2 Offline Agent Self-Improvement",
      "number": "11.2",
      "level": 2,
      "content": [
        "Offline self-improvement,incontrast,leverages structured,batchbasedoptimization.This paradigm utilizesscheduled trainingsessionswithhigh-qualitycurateddatasets tosystematicallimprove the agent'sgeneralizationcapabilities[851, 667,852, 853,854]. Unlike online approaches, offline approaches accommodate more computationall intensive methodologies,including Batch Parameter Updates and Fine-Tuning, Meta-Optimization, and Systematic Reward Model Calibration.",
        "Batch Parameter Updates and Fine-Tuning In this category,agents undergo extensive fine-tuning using supervised learning or reinforcementlearning (RL)techniques,optimizing performance across large-scale datasets over multiple training epochs. Retrieval-augmented generation (RAG) is often integrated to enhancecontextual understanding and long-term memoryretrieval[740,741].Such methods allow agentstooptimizeretrievalstrategies,therebyimproving reasoning over extensive knowledge corpora.",
        "Meta-Optimization of Agent Components Here ofline training is not limited to improving task performance but extends to refining optimization algorithms themselves.Meta-learning strategies that optimize hyperparameters or even restructure the optimization process dynamically have demonstrated promising outcomes[731, 91]. These meta-optimization approaches enable agents to discover the most efective learning parameters for new problem domains.",
        "Systematic Reward ModelCalibrationOfine settings facilite the precisecalibration ofreward models, incorporating hierarchical orlistwise reward integration frameworks (e.g.,LIRE[855])toalign agent behavior withlong-term objectives through gradient-basedreward optimization.Such calibration ensures that reward functions reflectreal-world task complexity, thereby mitigating bias and enhancing generalization.",
        "The structured nature of offline optimization results in a robust agent baseline, whose performance is fine-tuned to optimize stability,effciency, and computationalcost before real-world deployment.Offline trainingallows for high-fidelity model refinement and is essential for misson-criticalapplications requiring predictable performance guarantees."
      ],
      "raw_title": "Offline Agent Self-Improvement",
      "type": null,
      "children": []
    },
    {
      "title": "11.3 Comparison of Online and Offline Improvement",
      "number": "11.3",
      "level": 2,
      "content": [
        "Online and ofline optimizationoffer complementary benefits,each excelling indifferent aspectsofself-improvement. Online optimization thrives indynamic environments,where real-time feedback enables continuous adaptation. It is well-suited foralications thatrequireimmediateresponsiveness,suchas interactiveagents,real-timedecsionmaking, andreinforcementlearningsystems.However,frequent updates may introduce instabilityor drift,requiring mechanisms to mitigate performance degradation over time.",
        "In contrast,offine optimization emphasizesstructured,high-fidelitytraining using pre-collecteddatasets,ensuring robust and stable performance before deployment. By leveraging computationaly intensive learning methods such as batchtraining,fine-tuning,andmeta-optimizationofline approaches provide strong generalization andlong-term consistency.However,theylack the agilityofonlinelearming andmay struggle toadapt effcientlytonovel scenarios without additional retraining. Table 11.1 summarizes the key distinctions between these two paradigms.",
        "<html><body><table><tr><td>Feature</td><td>Online Optimization</td><td>Offline Optimization</td></tr><tr><td>Learning Process</td><td>Continuous updates based on real-time feedback</td><td>Batch updates during scheduled training phases</td></tr><tr><td>Adaptability</td><td>High, capable of adjusting dynamically</td><td>Lower, adapts only after retraining</td></tr><tr><td>Computational Effi- ciency</td><td>More efficient for incremental updates</td><td>More resource-intensive due to batch training</td></tr><tr><td>Data Dependency</td><td>Requires real-time data streams</td><td>Relies on curated, high-quality datasets</td></tr><tr><td>Risk of Overfitting</td><td>Lower due to continuous learning</td><td>Higher if training data is not diverse</td></tr><tr><td>Stability</td><td>Potentially less stable due to frequent updates</td><td>More stable with controlled training set- tings</td></tr></table></body></html>",
        "Table 1l.1: Comparison of Online vs. Offline Optimization Strategies in Self-Improvement Agents.",
        "While bothapproaches have inherent strengths andtrade-offs, modern intellgentsystems increasingly integrate them through hybrid optimization strategies.These hybrid frameworks leverage the stability of ofline training while incorporating real-time adaptability,enabling agents tomaintain long-termrobustnesswhilecontinuouslyrefining their performance in dynamic environments."
      ],
      "raw_title": "Comparison of Online and Offline Improvement",
      "type": null,
      "children": []
    },
    {
      "title": "11.4 Hybrid Approaches",
      "number": "11.4",
      "level": 2,
      "content": [
        "Recognizing that both online and ofline methods have inherentlimitations, manycontemporary systems adopt hybrid optimization strategies.These hybrid methods integrate structured offlineoptimization withresponsive online updates to achieve continuous incremental agent enhancement.",
        "Hybrid optimization explicitly supportsself-improvement byempowering agents toautonomously evaluate,adapt,and enhance their behaviors through distinct yet interconnected stages:",
        "·Offline Pre-Training: In this foundational stage, agents acquire robust baseline capabilities through extensive offline training oncurateddatasets.Thisstage establishes essentialskills,suchasreasoning and decision-making, required for initialautonomous performance.For instance,frameworks such as the one introduced by Schrittwieser et al.[856]illustrate howofflinepretrainingsystematicallyenhances initialagent capabilities,ensuring subsequent online improvements are built upon a stable foundation. ·Online Fine-Tuning for Dynamic Adaptation: Agents actively refine theircapabilities byautonomously evaluating their performance,identifying shortcomings, and dynamically adjusting strategies based on real-time feedback. This adaptive fine-tuning stage directly aligns with the agent self-improvement paradigm by allowing real-time optimization of agent-specific workflows and behaviors,exemplified by Decision Mamba-Hybrid (DM-H) [857], where agents efficiently adapt to complex, evolving scenarios.",
        "·Periodic Offline Consolidation for Long-Term Improvement: periodic offline consolidation phases, agents systematically integrateandsolidifyimprovements identifiedduring onlineinteractions.This ensures thatincremental, online-acquired skill and improvements are systematically integrated into the agent'score models, maintaining long-term stability and efectivenessThe Uni-O4 framework[858] exemplifies how this processenables seamless transitions between offline knowledge consolidation and online adaptive improvements.",
        "Hybrid optimization thus explicitly supports autonomous,continuous evolution by seamlessly interweaving structured offline learning with proactive,real-time online adaptation.Thiscyclicalapproachequips agents withboth immediate responsiveness and stable long-term improvement, making it idealy suited for complex,real-world scenarios such as autonomous robotics, personalized intelligent assistants, and interactive systems."
      ],
      "raw_title": "Hybrid Approaches",
      "type": null,
      "children": []
    },
    {
      "title": "Scientific Discovery and Intelligent Evolution",
      "number": "",
      "level": 1,
      "content": [
        "In previouschapters,we primarilydiscussedtheevolutionofagenticsystems fromatechnicalperspective,focusingon how to develop systems that can effctivelyperform well-efined tasks traditionally executed by humans.However,a fundamentalandimportant question remains:canthese agents drivea self-sustaining innovationcyclethat propels both agent evolution and human progress?",
        "Scientific knowledge discovery is acompelling example of self-evolution in intelligent beings,as it helps them adapt to the world ina sustainable way.Agentscapableof discoveringscientificknowledge at differentlevels of autonomy and in asafe manner willalso play important roles in technological innovation for humanity. In this section, we survey progressin autonomous discovery using agentic workfows and discuss thetechnologicalreadiness toward fully autonomous,self-evolving agents.Withinthis scope,thegoalofthe agent istouncover, validate,andintegrate data, insights,and principles toadvance anobjectivescientificunderstandingofnatural phenomena.Insteadofalteringthe world, theagent seeks tobetterunderstandnatureasaScientist AI[859]andassist humans inextending theboundaries of knowledge.",
        "We firstdefinetheconceptof knowledge andintelligence toclarifyourdiscussion,thenintroducethree typical scenarios where agents and scientificknowledge interact.Wealsohighlight existing successes andexamples of self-enhancing agents applied totheoretical,computational, andexperimentalscientificresearch.Lastly,we summarizethe current challenges for a future outlook."
      ],
      "raw_title": "Scientific Discovery and Intelligent Evolution",
      "type": null,
      "children": []
    },
    {
      "title": "Collaborative and Evolutionary Intelligent Systems",
      "number": "",
      "level": 1,
      "content": [
        "The concepts of collaboration and evolution lie at the heart of intelligent multi-agent systems (MAS).Inspired by biologicalecosystems andhuman societal dynamics,these systems leverage collective intellgence to solve complex challenges that exceed the capabilities of individual agents[914]. Human societies exemplify how cooperation, specialization,anddistributeddecision-making significantlyenhancecollective problem-solvingeffctiveness.Similarly, MAS adopts these strategies,integrating specialized agents toaddress intricatetaskscollaboratively.The foundational principle ofcollective intelligence-the“Wisdom of Crowds\"by[915]-suggests diverse, independent agents often yield superior decisions compared to solitary experts,directly underpinning the design philosophy of MAS.Cognitive theories,such as Minsky'ssocietyof mind[17]andthetheoryofmind[916,917],further reinforcethis paradigmby proposing that intelligence emerges from structured interactions among specialized units.",
        "Recently, advancements in large language models (LLMs)have introduced new possibilities for collaborative and evolutionary multi-agent systems (LLM-MAS).Benefiting from powerful reasoning, planning,and decision-making capabilities, these models enable the creation of sophisticated MAS architectures mirroring the cooperative and adaptive characteristics found in human societies. Agents within LLM-MAS often assume distinct identities and roles,reflecting human-like division of labor and specialized collaboration.Byembracing structured communication, dynamic knowledge sharing, and coordinated decision-making,these systems emulate human social dynamics to achieve common goals. Moreover, LLM-MAS is inherently evolutionary; agents continuously adapt and improve through interactions,feedback, and iterative learning,resulting inenhanced system performance over time.Roadmap In this chapter,we systematically survey the emerging fieldof LLM-based multi-agent systems,focusingspecifically on their colaborative mechanisms and evolutionary capabilities.We first examine how distinct system objectives shape agent roles,behavior patterns,and collaborative strategies in Chapter 13.Next, inChapter 14 we analyze various communication structures,including interaction protocols thatfacilitateeffective agent-agent and human-agent communication.Additionall,we explorecollaborative decision-making methodologies andhow agents leverage their unique expertise and perspectives inChapter 15,and discussthe collective intelligence and evolution mechanism in Chapter 16. FinallyinChapter 17, we discuss evolutionary processes,highlighting adaptive learning methods, continuous knowledge sharing,and mechanisms foriterative improvement thatcollectively enhanceMAS performance. Through this comprehensive survey, we identif current achievements, discuss existing challenges,and highlight promising research directions for collaborative and evolutionary intelligent systems.",
        "![](images/14ea00640f8414245fcf2dd5fabe6e70c5a081ee5396b68cb13812b2c2726d08.jpg)",
        "Figure 12.3: Taxonomy of LLM-based Multi-Agent Systems."
      ],
      "raw_title": "Collaborative and Evolutionary Intelligent Systems",
      "type": null,
      "children": []
    },
    {
      "title": "Design of Multi-Agent Systems",
      "number": "",
      "level": 1,
      "content": [
        "In the contextof LLM-based multi-agent systems (LLM-MAS),collaboration goals and collaboration norms serve asfoundational elements that shape system behavior, interaction patterns,and overalleffectiveness.Collaboration goals specifytheexplicit objectives agents aim to achieve-whether individuall,collectively,or competitivelywhile colaboration norms define the rules,constraints,and conventions that govern agent interactions within the system.Together, these components establish arobustframework guiding effectivecommunication,coordination,and cooperation among agents.",
        "This section categorizes LLM-MAS into three broad classes based on distinct combinations of collaboration goals and norms:strategic learning,modeling and simulation, and collborative task solving.Although not exhaustive, these categories cover a wide spectrum of LLM-MAS designs andclearly reflect how system objectives shape agent interactions and outcomes.",
        "·Strategic Learning systems embed agents within a game-theoretic context, where agents pursue individual or partially conflicting goals.The interactions can be cooperative,competitive,or mixed, guided explicitly by predefined game rules and interaction norms.This seting often aligns with non-cooperative (strategic) and cooperative concepts in traditional game theory. Please refer to Section 13.1 for details. ·Modeling and Simulation contexts focus on agents acting independently,driven by diverse environmental or social factors. Here, interactions emerge organically without necessarily converging on common goals. reflecting the complex dynamics seen inlarge-scale social or economic simulations. Please refer to Section 13.2 for details. ·Collaborative Task Solving emphasizes systematic cooperation among agents to achieve explicitly shared objectives.Agents typically adopt structured workflows,clear roledefinitions, and highly predefined collaboration norms to synchronize their actions toward collctive goals.Please refer to Section 13.3 for details.",
        "Ithe remainder of this chapter, weelaborate oneach category,examining how LLMs enable,influence,and enhance gent behaviors, interactions, and collective intelligence within our scope.",
        "In the following, weexamine these categories indetail, highlighting how each leverages the capabilities of large language models to shape agent behaviors and interactions."
      ],
      "raw_title": "Design of Multi-Agent Systems",
      "type": null,
      "children": []
    },
    {
      "title": "13.1 Strategic Learning: Cooperation vs. Competition",
      "number": "13.1",
      "level": 2,
      "content": [
        "Strategic learning refers to agentscapabilities todynamically anticipate,interpret,andinfuence the actions of other agents within game-theoretic settings—whether competitive,cooperative,or mixed[949].Agents iterativelyadjust their strategies based on new information,commonly modeled using foundational concepts such as Nash equilibria [950], Bayesian games[951,914,952]orrepeatedinteractions[953,954].With LLMs enabling nuanced linguistic reasoning, strategiclearning increasingly integrates“soft\"signals-including dialogue,persuasion,andimplicit negotiation-thus enriching traditional game-theoretic reasoning frameworks [952, 955, 956, 957].",
        "In economic applications, multi-agent strategic simulations provide valuable insights into market behaviors and negotiation tactics,highlighting both competitive andcooperative dynamics.For example,[958] and[951] demonstrate how LLM-empowered agents can simulate hiring processes,exhibit rational decision-making in controlled economic experiments,and even forecast stockmovements.[959]introduces a GPT-4-based competitiveenvironment to illustrate how restaurant and customer agents compete to optimize profits and satisfaction, showcasing realistic bidding and pricing strategies.Meanwhile,[960] investigate Buyer-Seller bargaining in LLM-based negotiations, while[961]use ultimatum game simulations to illuminate policymaking decisions grounded in human-like strategic behavior.",
        "![](images/5c6e4dc7b40dcadaa908b43a194b0c87cf4dd1f0d03bc8b93ccc5ac7ea254fb3.jpg)",
        "Figure 13.1: An overview of three major collaboration types inLLM-based MAS: Modeling & Simulation,Strategic Learning,and Collaborative Task Solving. Each category is distinguished by how agents' goals and norms are set (independent vs. divergent vs. shared) and how they coordinate.",
        "Beyondconventionalmarkets,strategic learningapplies broadly whereverresource allocation,allancesorcompetitivecooperative trade-offs are present.Examples include multi-commodity competitions [962,959], in which agents stratgicallynegotiateterms tomaximizeindividualbenefits,orsustainability-focusedcontexts where agentscoordinate resource consumption[963].In gaming,social deduction games such as Werewolf, Chameleon, Avalon,and Jubensha require agents tomanagethecomplex interplaybetween deceptionandcollaboration[964,965,966,153,919,967,968, 969,970].Studies by[97,965]highlight LLM-based agents thatexcelatorchestratingsubtledeceit andcollabation, while[967,972,968,969]emphasize adaptive,multi-round strategy in Avalon.[970]further pushes this boundary by showcasing autonomous,multi-agent interactions inthe Jubensha murder mystery genre,re-creatingcomplexnarratives. Similarly,diplomatic simulations ([973]and[974])employ LLM-based agents to emulate sophisticated geopolitical negotiation and alliance formation dynamics at global scales.",
        "SummaryA key advantage of LLM-driven strategic learning lies in effectivelycombining rigorous game-theoretic logic with naturallanguage reasoning.Thisfusion enables agents tointerpret sophisticated instructions,engage in persuasive dialogue,and adapt more flexibly to novel orunstructured settings.Consequently,LLM-based strategic agent hold significant promise for accurately modeling complex real-world interactions- spanning economic competition, social negotiation, and geopolitical strategy-far more efectively than conventionalrule-based or numeric-only approaches."
      ],
      "raw_title": "Strategic Learning: Cooperation vs. Competition",
      "type": null,
      "children": []
    },
    {
      "title": "13.2 Modeling Real-World Dynamics",
      "number": "13.2",
      "level": 2,
      "content": [
        "Modeling and simulation represents another crucial area of application for LLM-based multi-agent systems (LLMMAS),aiming toreplicatecomplex social,economic,andpolitical phenomenaat scale.By utilizingLLMs'sophisticated language understanding and contextualreasoning,these simulations can feature highly heterogeneous agents whose evolving behaviors mirror real-world dynamism.Unlike strategic learning environments that emphasize explicit competitive orcooperative goals,agents in modeling and simulation scenarios operate independently,guided by their domain-specific roles, preferences, and interactions with the simulated environment [975].",
        "In healthcare,for example,[921] introduces Agent Hospital, where LLM-powered doctor agents iteratively refine treatmentstrategies through realistic interactions with virtual patients.This enables researchers totest management protocols,raining paradigms, and“what-if\"scenarios in acontrolled yetrealistic setting.Similarly, in economic contexts,[976]present EconAgents,leveraging LLM-driven agents torealisticall modelindividual-levelbehaviors such as employment decisions,consumption patterns,and savings strategies.These agents facilitateexpressive macroeconomic simulations,surpassing traditional numeric or strictlyrule-based methods inadaptability and realism[977]. In addition,politicalscience applications alsobenefitfrom this approach.Forexample,[978]and[977]successfully simulate election proceses andpolicymaking dynamics,revealing how publicdiscourse,candidate strategies, and voter interactions shape real-world political outcomes.",
        "Beyondeconomics and politics,LLM-based simulation accommodates a variety of social and cultural phenomena.For example,[979]and[255] use simulations of linguistic and emotional propagation in social networks to investigatehow opinions,beliefs,or sentiment clusters form online.Researchby[980] explores how opinion dynamics evolve under various topological and interaction paterns,while[981] examines the conditions under which fake news spreads or stalls in heterogeneous agent populations.Large-scale simulation platforms such as GenSim[982]and OASIS [936] pushthe boundary furtherby scaling totens of thousands orevenmillons ofuser agents,thus enabling the study of emergent group behaviors and systemiceffcts-suchasviralinformationdiffusion,echo-chamber formation,or group polarization—under realistic constraints.",
        "Summary The strength of LLM-based simulation lies in capturing both the structural dynamics(e.g.,network topology or institutionalrules and the cognitive orlinguistic nuances thatdrive real-world behavior.Byembedding languagebased reasoning into agent models,researchers can examine complexsocial processes—like persuasion,framing,or cultural transmission-that would be difficult to capture through purely numeric or rule-based approaches."
      ],
      "raw_title": "Modeling Real-World Dynamics",
      "type": null,
      "children": []
    },
    {
      "title": "13.3 Collaborative Task Solving with Workflow Generation",
      "number": "13.3",
      "level": 2,
      "content": [
        "Collaborativetask solving orchestrates multiple agents toward aclearlydefinedobjectivethroughstructured workflows. In contrast to strategic learning (which may involve competing interests)or open-ended modeling and simulation (where agents actindependently),collaborative agentsfunction aspart ofaunified problem-solving pipeline.Agents typically folow clearly defined roles (e.g.“Planner\",Implementer\",or“Evaluator\")and stage-based processes to ensure efficient and accurate task completion.",
        "Systems such as MetaGPT[626],CAMEL[848],Communicative Agents [983],and frameworks described in [924] exemplify how clearly defined roles,responsibilities, and decision flows allow LLM-based agents to coordinate effctively.A typical workflow might involve one agent analyzing a problem statement, another proposing a solution outline,athird implementing partial solutions, andafourth verifying correctnessCommunication among these agents is oftencarriedoutthroughiterativeroundsof naturallanguage“dialogue\",leveragingthe inherentlanguage-generation strengthsofLLMs.Thisstructuredapproachalso proves beneficialforscaling tomore ambitious projects, as sub-tasks can be delegated to specialized agents with domain-specific prompts or training.",
        "Recently,collaborativetask-solving systems have been explored extensively insoftware developmentscenarios (e.g., multi-agent coding,debugging,and testing).However,scientific discovery represents a particularly prominent and compelling aplication.For example,the Agent Laboratory[746]employs agents in structured scientific workflows: proposing hypotheses,designing experiments,analyzing results,andrefining subsequent inquiries,whicheffectively mirrors the iterative nature of thescientific investigation.Similar multi-agent designscan be adapted to tasks such as literaturereview,policydrafting,orlarge-scaledataanalysis, using well-definedprotocols tomaintaincoherence and avoid duplication of effort.",
        "Summary Compared to other LLM-based multi-agent paradigms,collaborative task-solving inherently prioritizes clarity and predictability:Each agent's role andobjective are predefined, limiting emergent orchaotic behaviors.This structureis particularlyadvantageous in domains requiring precision,accountability,or sequential decision-making. Atthe same time,research isongoing tostriketherightbalancebetween structure and flexibilitywhichensures that agents have enoughautonomy tocreatively contribute solutions while adhering to asharedworkflow thatultimately guarantees reliable, high-quality task completion.",
        "Discussion The aforementioned three dimensions—strategic learning, modeling and simulation,and collaborative task solving—reflect thebreadth of LLM-based multi-agent systems.Eachcategory addresses distinct research questions and real-world applications,leveraging language-basedreasoning totacklechallenges that extendbeyond thecapabilitiesof conventional, purely numeric, or rule-driven agent designs."
      ],
      "raw_title": "Collaborative Task Solving with Workflow Generation",
      "type": null,
      "children": []
    },
    {
      "title": "13.4 Composing AI Agent Teams",
      "number": "13.4",
      "level": 2,
      "content": [
        "In MAS,agentsarethecoreunits that interact within the systemandarecriticaltoitsfunctionality.Theseagentscanbe categorized as either homogeneous orheterogeneous,depending onwhether they share identicalor differing personas, capabilities, and action spaces.",
        "Homogeneous Homogeneous agents that share identicalcapabilities,action spaces,and observation spaces.Compared to single-agentsystems,the primaryadvantagelies intask parallelization,allowingmultipleagents tohandledifferent parts of atasksimultaneously and improve overalleffciency.They are often used insimpler,coordinated tasks where uniformity across agents can drive improved performance.",
        "Several studies have applied homogeneous agents to simulate teamwork in games ike Overcooked and Minecraft, as well as real-worldtasks such as householdlabor division.[924]proposed acognitive-inspired modularframework that enables LLM-based agents tocommunicatethrough natural language to perform labor division, request asistance from one another, and collaborativelycompleteobject transportation tasks.[984]introduced prompt-based organizational structures intotheframework,reducingcommunicationcosts betweenagents andimproving teameficiency inhousehold task suchas preparing afternoon tea, washing dishes,and preparing a meal.Furthermore,several studies[926,925] have employed multiple LLM-based agents in popular games such as Overcooked and Minecraft to experiment with their ability tocooperate and complete tasks.According tothe game settings,these agents are also homogeneous.",
        "Heterogeneous Agent diversity plays acrucialrole in improving collaboration outcomes.Researchshows that heterogeneity among agentscanenhanceproblem-solvingcapabilities,asdiverseagents bringvariedperspectivesand skil to the task athand[985986].Heterogeneitycontributes toricher problem-solvingstrategies andimprovesoverallcollaboration in MAS.Theheterogeneouscharacteristicsof agentscanbereflected inthefollowingdimensions: personas-level heterogeneityobservation-space heterogeneityandaction-space heterogeneityNotethatthese heterogeneitieaenot mutually exclusive—a heterogeneous agent may exhibit one or more of these characteristics.",
        "·Personas-level heterogeneity.Refers to diversity in agent profiles, which influences how agents approach problem-solving and interact with one another.Most current LLM-based heterogeneous multi-agent systems fall into this category [987, 627, 50, 970].For example,in sofware development, agents may take on personas such as programmers, product managers,or testers. In medical diagnostics,agents may represent cardiologists,oncologists,or paediatricians,each with distinct areas of expertise.The distinct perspectives and expertise of each persona contribute to more robust decision-making.While these heterogeneous agents may share the same action space—such as writing documents [626] (e.g., code, requirement reports, or test reports)or providing diagnostic advice [922]—their personas influence the outcomes of these actions, where role-specific enhancements within multi-agent architectures have shown to significantly streamline and optimize task execution. For instance, a product manager performing the action of writing a document would produce a requirements report, whereas a programmer performing the same action would produce software implementation code[626].This diversity leads to beter decision-making and innovation,especially in complex, multidisciplinary tasks.",
        "·Observation-space heterogeneity. In MAS, the ability of agents to perceive and interpret their environment can vary.Observation-space heterogeneity refers to these diferences in what agents can observe or perceive within their environment. For example, in the game Werewolf, some agents, like werewolves,can see the identities of their teammates,and the seer can obtain the identity of a designated player, while others, like villagers,cannot seethe true identityof any player[971]. Similarly,in the Avalon game, different roles have distinct observation spaces [919,972],thus influencing the strategies and communications of the players. In these settings,each agent's perceptual ability or observation space is directly linked to their role in the system. In a multi-agent system, this variation in what agents can observe often influences their decision-making, communication, and coordination with other agents.",
        "·Action-space heterogeneity.On the other hand,this refers tofundamental differences inthe actions agents can perform due to physical or functional constraints.This is particularly relevant in both virtual and physical environments where agents may have diffrent capabilities based on their design or purpose.In the virtual environments of games like Werewolf [965,971,966] and Avalon [919,967], diffrent roles have distinct abilities or skills [971, 919,972]. For example, in Werewolf, while werewolves may have the ability to communicate secretly with each other, villagers might be limited to voting or observing only. This dynamic requires agents to collaborate based on their uniquecapabilities and promotes the learning of strategies such as teamwork, trust, and deception in their interactions.Meanwhile,in robotics,agents may exhibit diverse physical capabilities.For instance, as described in [988],some robots lack mobility and can only manipulate objects,while others are specialized for movement butcannot manipulate objects. In such cases,agents with different action spaces must divide tasks effectively,leveraging their specific abilities to take on the parts of the task they are suited for, ultimately collaborating to complete the overalltask.This type of heterogeneity requires agents to collaborate and coordinate their actions eficiently, often dividing tasks based on their individual strengths.",
        "Homogeneity to Heterogeneous Evolution In some LLM-based multi-agent systems, agents have the ability to evolve autonomously and continuously adapt through interactions with their environment.Due to the inherent randomness in both LLM models and theenvironment, the evolution ofthese agentsoften followsdifferent trajectories.This can lead to heterogeneous behaviors emerging over multiple simulations,even when agents initialy havehomogeneous personas and action spaces.Forexample,as shown in[989],agents with identicalaction spaces and personas atthe start developed diffrentiatedroles after multiple rounds of interactions withthe environment andother agents.Some agents,forinstance,specializedinfood gathering,whileothers focused oncrafting weapons.Similarly,[990]bserved that initiallyhomogeneousagents developed distinctlanguage usage patterns,emotionalexpressions,and personalities after group interactions.These emergent behaviors demonstrate the possibility of transitions from homogeneous to heterogeneous systems."
      ],
      "raw_title": "Composing AI Agent Teams",
      "type": null,
      "children": []
    },
    {
      "title": "13.5 Agent Interaction Protocols",
      "number": "13.5",
      "level": 2,
      "content": [
        "In this section,there willinitially be classificationof typicalkindsof messages,providing aclear view regarding the contnt and exchange modes for agent interactions.Next, agent-environment, agent-agent, and agent-human communications interface designs will beaddressed.Architecturalissues and protocol specifications fortransparent informationexchange willalsobe addressed.Interface standardizationwillhavea specialfocus,which isessentialfor providing interoperability,scalability, andeffciencyfor multi-agent systems.The section willend with unification of communication protocol discussions, where agent-environment or agent-user interacting design principles and requirements are addressed, as wellas providingclarityconsistency,andfunctionalcoherence forariousapplications for LLM-based systems."
      ],
      "raw_title": "Agent Interaction Protocols",
      "type": null,
      "children": [
        {
          "title": "13.5.1 Message Types",
          "number": "13.5.1",
          "level": 3,
          "content": [
            "Structured: Structured messages,either in JSON ([991,992]),XML([993,636]),or as acode ([626,627,994]),are a crucial aspect of multi-agent system communication with LLM.The primary advantages of structured messages aretheir syntactically and semanticaly defined structure,enabling unambiguous understanding and straightforward parsing.Withtheir lackof ambiguity,theyfacilitateunerrant information extractionand processing with much less overhead on computation and greater system dependability.For example,JSON and XML can represent specific-task configuration parameters or facilitate data exchange asa machine-readable mode,and messages writen asacode can even be executable several times directly, which makes workflow and automation simpler.",
            "Structured messages are particularly well-suited for high-efficiency,deterministic applications.They are useful for sub-task decomposition,sub-task assignment,andcoordination among agents forcooperativemulti-agent architecture because they explicitly stateoperationalcommands.Moreover,as structured messages have a prescribed form,retrieving data as wellas storing data is facilitated and system optimization and longitudinal analysis are also feasible.",
            "Unstructured: Incontrast,unstructured mesages,e.g.naturaltext([979719]),isual data,e.g.images,vides, and audio signals,e.gspeech,ambient sounds([995,996,762]),havehigher informationdensity andrepresentational capability.Such modalities are best suited for communication with nuanced andcontext-dependent information. Images, for instance,communicate spatialrelationships,illumination,andfacialexpressons,andvideoscommunicate dynamic temporally-organized sequences,e.g.,state orbehaviorchanges overtime.Similarly,audio signalsalsocommunicate not justlinguistic informatinbutalsoparalinguisticinformation,e.g.tone,emotion,andintonation,whicharecritical for natural and context-aware interactions.",
            "Unstructured messages are well-adapted forambiguitytasks,as wellas forcomplex,real-world settings.The fact that they can expressabstract ideas as wellasaffective subtlety,orimplicitcontextual suggestions, makesunstructured messages wellsuited for creative, as wellas discovery-oriented, problem spaces.Unstructured data'scomplexity, however,callsforadvanced processing techniques,for example,feature extraction based ondeep learning,forone to tap intotheir full potential.Advances with pre-trained LLMs as wellas multi-modallarge language models have alleviated these complexities to alarge extent, enabling novelapplications for unstructured communication within multi-agent systems [533, 513,997].",
            "Summary: Unstructured and structured messages have complementary roles for multi-agent communication with LLM-based.Whilestructured messages oferaccuracy,consistency, andcomputationeffciencyand are appropriate for operational and deterministic operations,unstructured messages offer rich,contextualized representations enabling agents to negotiate vague,creative,highly dynamic situations.Together,these modesoffer afoundation for adaptive, effective multi-agent cooperation."
          ],
          "raw_title": "Message Types",
          "type": null,
          "children": []
        },
        {
          "title": "13.5.2 Communication Interface",
          "number": "13.5.2",
          "level": 3,
          "content": [
            "Agent-EnvironmentInterface LLM-based agents willtypicallyhave to act on their environment once or severaltimes in order toperformarange ofoperations.Fromthe agent's point of view,itsoutput intotheenvironment is something that it would prefer, e.g.,a UIclick, webrequest,ora move foracomputer graphic'scharacter.Environments differ withregard to whatactions they willaccept,andsoasnothave its actions not getexecuted,the agent must findout what actions are foraspecific environment that it is acting within and performactions that arefora specific task as wellas validfora specificenvironment.After the agent outputs itschosen action,the agent willhave areturn from the environment.It willconsistofobservations ifsuccessful,orafeedbackonerrorif therewasone.Theagent will have to actonthis feedback.There are nowadays various typesofenvironments where anagentcan act,e.g.perating systems,computer games,database, ande-commerce websites.To make agent-environment interfaces share acommon interface and have agents trained on various LLMs plug into various environments with minimal further adaption, various frameworks have been proposed. These frameworks make for easier tests on agents'capability on various executable environments [706].",
            "Agent-Agent Communication In MAS, communication through natural language is predominant.This is likely because large language models possess strong linguistic capabilitiesdue to pretraining on massive naturallanguage corpora.Another possiblereason isthatfor many tasks, naturallanguagecommunication isalready suffcientto meet the requirements.Based on the type of information exchanged, multi-agent systems can be categorized as follows: Natural Language-Based Systems Among LLM-based multi-agent systems utilizing natural language, text-based communication is the most common[922,924,987,970,998].There are also some systems that use voice as the mediumofcommunication[996,762,999,100].In these systems, agents engage in behaviors such as discussions, negotiations,persuasion,or critique through naturallanguage to achieve theirobjectives.Structured InformationBased Systems Compared to naturallanguage,structured information has characteristics such as higher consistency, lower parsing complexity,and reduced ambiguity, making it more suitable for efficient and low-costcommunication between agents [626]. In some implementations,the information exchanged between agents is structured into distinct components tofacilitate easier parsing and utilization bythereceiving agent.For instance,theexchanged information might include fields specifying the sender,receiver, message type, and instructions on how the recipient should parse or use the content [929].",
            "Human-Agent Communication The purpose of developing multi-agent systems is to expand the boundaries of human capabilities andcognition,ultimately serving human wellbeing.While in some social simulation multi-agent systems, humans primarilyexist asobservers [50,0o1], most multi-agent systems allow human participation in various forms. During this participation,humans need tocommunicate with agents,andthiscommunicationcantake the formof either natural language or structured information[924,930]. When human-to-agent communication primarilyrelies on natural language,asingle LLMoften acts asa hub to parse human naturallanguage into structuredinformationthat agents can process more efectively for subsequent operations.This hub LMcan either exist within the multi-agent system or function independently of it.To save time and enhance communication effciency,humans can also use structured information to communicate with the multi-agent system through programming or similar methods.By following predefinedcommunication protocols,humans can send messages containing therequired datato the multi-agent system. The system willthen process the messages and data according to its internallogic and return the results. [931]"
          ],
          "raw_title": "Communication Interface",
          "type": null,
          "children": []
        },
        {
          "title": "13.5.3 Next-Generation Communication Protocols",
          "number": "13.5.3",
          "level": 3,
          "content": [
            "The fieldof LLM-basedagents is stillin its infancy.Developers typically design agent architectures andcommunication mechanisms tailored to specific domains ortasks,including agent-to-environment, agent-to-human,and inter-agent interactions.However, most existing systems lack a unifiedcommunicationframework,resulting in fragmented,siloed ecosystems.Multi-agent systems,tools,environments,anddatasourcesoftenoperateindependently,makingit difiult for agents tointeroperate orsharecapabilities.Furthermore,the burden oflearning andimplementing bespoke protocols falls on humans,and almost allcurrent protocols are manually designed—alabor-intensive process that often lacks semantic flexibility or scalability.",
            "To address these issues,several new agent communication protocols have been proposed, each targeting differeni aspects of the protocol design stack.",
            "Internet of Agents(IoA)[933]introducesan internet-inspired,instant-messaging-likecommunication architecture that supports dynamic team formation and task-driven collaboration. Agents register with a central coordination server, which handles identity management and discovery.Communication fows are orchestrated using FSM (Finite State Machine)-based dialogue templates. IoA supports multiple message types, including discussion, task assignment, andtriggering mechanisms, and provides structured fields for controlling speaker turns,nested groupformation, and maximum dialogue length. This alows agents to select and adapt message formats to match specific coordination phases, offering flexibility within a fixed schema.",
            "Model Context Protocol (MCP)[931],developed by Anthropic,focuses on enabling LLM agents to accessstructured tools and data.Itadopts afullycentralized approach basedon OAuth identityauthentication,and interactions are constraied to JSON-RPC 2.0 messages.While itlacks a meta-protocollayer or semantic negotiation capabilities,its simple andrigid architecture makes it a practical choice for tooluse cases with well-defined APIs.However, MCP sacrifices flexibility and extensibility, requiring manual registration of supported functions.",
            "Agent Network Protocol (ANP)[1002] aims to achieve fulldecentralization.Agents identify themselves through W3C-compliant decentralized identifiers (DIDs)and communicate over encrypted pee-to-peer channels.The protocol includes a meta-protocollayer that enables agents to negotiate which application-level protocol toadopt,supporting semantic protocol selection based onagent capabilities.ANPalsoallowsfor multi-protocol support atthe application layer (e.g.,HTTP,JSON-RPC, naturallanguage),providing strong extensibility anddecentralization but does notet explicitly support public protocol reuse.",
            "Agora[932] offers a highly flexible and language-driven protocol mechanism. Instead of registering pre-defined APIs,agentscan generate and share ProtocolDescriptions (PDs),which are freetext descriptions ofcommunication semantics.Using alarge language model,agentscan dynamicaly interpret and execute any PDatruntime.This allows protocols tobecreated,deployed, andused entirelythrough language,without anymanualregistration orconfiguration. Agora avoids centralized registries and supports decentralized protocol sharing:agents may publish or retrieve PDs from peer-distributed repositories to enable cumulative learning and interoperability across systems.",
            "Summary: As shown in Table 13.1, next-generation agent communication protocols differ along key dimensions suchas identity and security mechanisms, meta-protocol negotiation capabilities,application-layer flexibilityand the degree of centralization.Aunified, secure,scalable, and dynamic protocolinfrastructurewhere agents can negotiate andco-create protocolson the fyiscriticalforenablinglarge-scale, interoperable agent ecosystems.While current frameworks such as MCP,ANP,Agora, and IoA represent early but promising steps,protocol design remains arapidly evolving frontier in the development of intelligent agent systems.",
            "Table 13.1: Comparison of four agent communication protocols (MCP, ANP, Agora, IoA)across identity, negotiation, and execution layers. $\\mathbf{PD}=$ Protocol Description; DID:Decentralized Identifier; LLM:Large Language Model; FSM:Finite State Machine",
            "<html><body><table><tr><td> Layer</td><td> MCP</td><td>ANP</td><td>Agora</td><td>IoA</td></tr><tr><td>Identity & Security</td><td>OAuth-based centralized identity authentication.</td><td>DID-based decentralized identity with encrypted channels.</td><td>No centralized registra- tion.Identity derived from PD hash.</td><td>Agents register with a central server for identity and discovery.</td></tr><tr><td>Meta-Protocol Layer</td><td>No meta-protocol layer; relies on pre-defined in- terfaces.</td><td>Uses DID document to negotiate and select ap- propriate protocol via se- mantics.</td><td>LLM interprets PD text to automatically negoti- ate and deploy communi- cation protocols.</td><td>A centralized discovery mechanism combined with FSM-based  dia- logue flow control.</td></tr><tr><td>col Layer</td><td>Application  Proto- Supports only  JSON- Supports multiple proto- RPC 2.0.</td><td>cols such as HTTP and natural language.</td><td>Allows  arbitrary PD- driven protocols with high flexibility.</td><td>Task-driven protocol coordination supporting multiple message for- mats.</td></tr><tr><td>ization</td><td>Degree of Central- Highly centralized archi- Fully decentralized. tecture.</td><td></td><td>Decentralized: no regis- tration or fixed ID, with optional peer-to-peer PD sharing.</td><td>Highly centralized archi- tecture with a central co- ordination server.</td></tr><tr><td>Protocol Flexibility</td><td>Fixed and rigid; hard Highly flexible with se- RPC.</td><td>to adapt beyond JSON-mantic negotiation.</td><td>Extremely flexible; any PD can define a new pro- tocol dynamically.</td><td>Moderately high flexibil- ity; agents can select and adapt message formats based on task phases and coordination needs.</td></tr></table></body></html>",
            "Table 13.2:Classification framework for LLM-based multi-agent systems,highlighting diffrent aspects of system design,communication,collaboration, and evolution. Below are our abbreviations,for ease of reference: $\\mathbf{M}\\&\\mathbf{S}=$ Modeling & Simulation, $\\mathbf{CTS}=$ Collaborative Task Solving， $\\mathrm{SL}=$ Strategic Learning, $\\mathbf{S-D=}$ StaticDecentralized, $\\mathbf{S-L}=\\mathbf{S}$ Static-Layered, ${\\mathrm{Hom}}=$ Homogeneous, Het $\\mathbf{\\tau}=\\mathbf{\\tau}$ Heterogeneous, $\\mathbf{T}/\\mathbf{M}=$ Teaching/Mentoring, $C{\\mathrm{-}}0=\\left.\\left\\langle\\right.$ Consensus-Oriented, $\\mathbf{\\partial}_{\\mathbf{T}-\\mathbf{O}}=$ Task-Oriented, ${\\mathrm{CL}}=$ Collaborative Learning, Dict $\\mathbf{\\tau}=\\mathbf{\\tau}$ Dictatorial, $\\mathbf{D-B}=$ DebateBased, $\\mathrm{CI}=$ Collective Intelligence, Ind $\\c=$ Individual.",
            "<html><body><table><tr><td>Paper</td><td> System Design</td><td colspan=\"3\">Communication</td><td colspan=\"2\">Collaboration</td><td>Evolution</td></tr><tr><td></td><td>Category</td><td>Typology</td><td>Interface</td><td>Agent TypeInteraction</td><td></td><td>Decision</td><td>Type</td></tr><tr><td>Agent Hospital [921]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>T/M, C-O</td><td>Dict</td><td>Ind</td></tr><tr><td>Welfare Diplomacy [934]</td><td>M&S</td><td>S-L</td><td>Code, JSON, Text</td><td>Hom</td><td>CL</td><td>Voting</td><td>CI</td></tr><tr><td>MEDCO[923]</td><td>M&S</td><td>S-L</td><td>Text</td><td>Het</td><td>T/M, C-O</td><td>Dict</td><td>Ind</td></tr><tr><td>MedAgents[922]</td><td>M&S</td><td>S-L</td><td>Text</td><td>Hom</td><td>T-O</td><td>Dict</td><td>CI</td></tr><tr><td>Generative Agents [50]</td><td>M&S</td><td>S-D</td><td>Visual</td><td>Hom</td><td>CL</td><td>Dict</td><td>Ind</td></tr><tr><td>RECONCILE [918]</td><td>SL</td><td>S-D</td><td>Text</td><td>Hom</td><td>CL</td><td>D-B</td><td>CI</td></tr><tr><td>Agent Laboratory [746]</td><td>CTS</td><td>S-L</td><td>Code, Text</td><td>Het</td><td>C-O, T-O</td><td>Dict</td><td>Ind</td></tr><tr><td>CoELA[924]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Hom</td><td>T-O</td><td></td><td></td></tr><tr><td>The virtual lab [752]</td><td>CTS</td><td>S-L</td><td>Text</td><td>Het</td><td>C-O, CL</td><td>Dict</td><td>Ind</td></tr><tr><td>SciAgents [743]</td><td>CTS</td><td>S-L</td><td>Text</td><td>Het</td><td>T-O</td><td>Dict</td><td>CI</td></tr><tr><td>S-Agents [927]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>T-O, CL</td><td>Dict</td><td></td></tr><tr><td>GPT-Bargaining [1003]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td>CI</td></tr><tr><td>FORD[1004]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td>CI</td></tr><tr><td>MADRA [1005]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td></td></tr><tr><td>Multiagent Bench [948]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Hom</td><td>T-O, CL</td><td>D-B</td><td>CI, Ind</td></tr><tr><td>OASIS [936]</td><td>M&S</td><td>D</td><td>Text</td><td>Het</td><td>C-O</td><td></td><td></td></tr><tr><td>S [255]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td></td><td></td></tr><tr><td>FPS [981]</td><td>M&S</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td></td><td></td></tr><tr><td>GPTSwarm[1006]</td><td>CTS</td><td>D</td><td>Code, JSON, Text</td><td>Hom</td><td>T-O</td><td>Dict</td><td>CI, Ind</td></tr><tr><td>ChatEval[1007]</td><td>CTS</td><td>D</td><td>Text</td><td>Hom</td><td>T-O</td><td>Voting</td><td>CI</td></tr><tr><td>MetaGPT [626]</td><td>CTS</td><td>S-L</td><td>Code, JSON, Text, Visual</td><td>Het</td><td>T-O</td><td>Dict</td><td>CI</td></tr><tr><td>AutoAgents [1008]</td><td>CTS</td><td>D</td><td>Text</td><td>Het</td><td>T-O</td><td>C-O</td><td>CI</td></tr><tr><td>SWE-agent [628]</td><td>CTS</td><td>D</td><td>Text</td><td>Hom</td><td>T-O</td><td>Dict</td><td>Ind</td></tr><tr><td>AgentCoder [994]</td><td>CTS</td><td>D</td><td>Code, Text</td><td>Het</td><td>T-O</td><td>D-B</td><td>CI</td></tr><tr><td>MASTER[1009]</td><td>CTS</td><td>S-L</td><td>Text</td><td>Hom</td><td>T-O</td><td>D-B</td><td>CI</td></tr><tr><td>Reflexion [48]</td><td>CTS</td><td>D</td><td>Text</td><td>Het</td><td>T-O</td><td>D-B</td><td>Ind</td></tr><tr><td>MACM[1010]</td><td>CTS</td><td>D</td><td>Text, Code</td><td>Het</td><td>T-O</td><td>D-B</td><td>CI</td></tr><tr><td>Debate[985]</td><td>CTS</td><td>S-D</td><td>Text</td><td>Het</td><td>C-O</td><td>D-B</td><td>CI</td></tr></table></body></html>"
          ],
          "raw_title": "Next-Generation Communication Protocols",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "14.1 System Topologies",
      "number": "14.1",
      "level": 2,
      "content": [
        "![](images/4b9d903ce673fb44aaa5ff3593d7df89375449cc30355b75b9753253afd5d141.jpg)",
        "Figure 14.1: Different types of topological structure for multi-agent collaboration.",
        "![](images/4b9b4f0dc754b2f05dab6a1d19a8f8cf8e14c4937ecc8b6c842b6ac4c491b69e.jpg)",
        "Figure 14.2: Collaborative and competitive agents.",
        "This section examines the interaction typology in LLM-based multi-agent systems (MAS)and its impact on communication,collaboration,and task execution. We first analyze static topologieswhereconnectivity patterns are fixed by domain knowledge—and then explore dynamic(adaptive)topologies that adjust inter-agent connections based on performance metrics,workload variations,or strategicconstraints.Weconclude with a discussion of scalability challenges and trade-offs inbalancing systemcost, performance,androbustness,drawing onrecent research indistributed processing, self-organization, and emergent collaborative behaviors."
      ],
      "raw_title": "System Topologies",
      "type": null,
      "children": [
        {
          "title": "14.1.1 Static Topologies",
          "number": "14.1.1",
          "level": 3,
          "content": [
            "Static topologies are definedby predeterminedstructural paterns that remainlargely unchanged during system execution. In these configurations,connections among agents—or between agents andacentralcoordinator—are established using fixed rules and heuristics,ensuring predictable communication flows and simplified coordination.Three canonical forms are typically considered: layered (hierarchical), decentralized, and centralized architectures.",
            "Layered (Hierarchical)Structures Layered topologies arrange agents hierarchically, with high-level agents coordinating or supervising lower-level ones.This approach mirrors traditional management frameworks—such as Standard",
            "Operating Procedures (SOP)or the Waterfallmodel-where tasks are decomposed into sequential, well-defined stages. For instance, the AutoAgents [1o08] framework assigns roles (e.g., Planner, Agent Observer, and Plan Observer) to synthesize execution plans,while ChatDev[983] leverages hierarchical task decomposition to streamline software development[626,921,627].Although hierarchical structures facilitate debugging,performance monitoring, and modularity, they can create bottlenecks when upper-tier agents are overloaded[10ll]. Recent studies in storytelling[10] 14] and data science applications including data cleaning [1015,1016], visualization[1017, 1018] and automachinelearning [1019,020],highlight the trade-off between consistency and theemergenceof adaptive real-time behaviors.",
            "Decentralized Structures In decentralized topologies, agents interact on a peer-to-peer basis without a central coordinator,forming networks that areoften modeledaschains,rings,smallworldorrandom graphs[121,971].This structure enhances fault tolerance since the failure of a single agent does notcompromise the network.For example, [1022]show that distributing graphreasoning tasksamong multiple agents enables scalability beyond the contextlength limits of individual LLMs.Additionally,[1023]propose decomposition strategies that allow an orchestrating LLM to delegate subtasks efectively.However, maintaining acoherent global state in decentralized systems necesstates sophisticated consensus and synchronization protocols.",
            "Centralized Structures Centralized topologies rely on a master coordinator that gathers information and directs peripheral agents hierarchically.Such a setup allows for better controlover handling resources and sharing a global view,such as withculture parksand Lyfe Agents[1024,025]. With additional agents,however, a bottleneck atthe center node may occur, with increased communication overhead and susceptibility to failures. Current studies on coordinator-agentconfigurations[971] andresearchon ensuring autonomy forcentralizedconfigurations [1026] point out problems with scalabilitywithconsistency.While consistency is guaranteed forcentralized architectures,there may not necessarily be flexibility for dynamic adaptation.",
            "Briefly, static topologies have advantages of determinismand predefinition.With pre-defined structural patterns, these systems have predictable communication patterns and effctive coordination among agents. Topologies of thesestructures aretypicallydefined onstructuralknowledgeorstatic rules,and,as such,theysuit domains where workflowforthe tasks is static,there are predefinedroles,and systemrequirements are welldefined.The second primary advantage is design,implementation, and maintenance ease.With structure predefined, design as well as execution procedures are made simpler, and,asaresult, maintenance isa simplerprocessResourcehandling as wellas modularization gets simpler due to well-defined, static structure.",
            "However,statictopologies themselves are nonflexible,groundedon pre-specified patterns ofconnectivitythat do not respond toreal-timechanges.Wellsuitedforaspecific purpose atdesigntimebutentirelylackingflexibilityforreacting to unforeseen challenges,including sudden agent breakdown,varying degrees of task complexity, and system goal modification,static topologies do nothave real-time response flexibility potential.Real-time response inflexibility inhibits runtime system reconfiguration anddecreases system effectiveness in dynamic settings where circumstances occur.Failureto self-organize and morph according toemergingconditions may equate to inefficiency as well as low system performance, particularly where dynamic or emergent setings are at hand."
          ],
          "raw_title": "Static Topologies",
          "type": null,
          "children": []
        },
        {
          "title": "14.1.2 Dynamic and Adaptive Topologies",
          "number": "14.1.2",
          "level": 3,
          "content": [
            "While static topologies providedeterminism and predictabilityillustrated by statictopologies such as hierarchical or centralized ones performing well with stable-task domains and well-defined roles—static topologies do not fit open-ended or novel domains.Real domains,from real-time collaborative plan,to dynamic social simulations, often demand that agents make changes on their patterns of interaction as work continues,available resources vary,or feedback fromthe environment is received. Such structural tension with adaptative maleability generates dynamic topologies,which,atruntime,recast inter-agentrelationshipsasaresponse tofeedbackon performance,workload,or strategic constraints, striking a balance between consistency and responsiveness.",
            "For example,DyLAN framework[725] supports inference-time agent selection througha two-step processa forwardbackward team optimization step with unsupervised Agent Importance Scores,follwed by dynamic team reformulation at runtime.Similarly, OPTIMA[1027] optimizes inter-agent connectivity iteratively through a generate-rank-select-train framework,utilizing reward functions asa means for determining abalance among task quality, token efficiency, and readability, with communication actions further optimized through strategies such as Direct Prefernce optimization. The MAD framework [649] ilustrates flexibility through a joint optimization among three prompt phases and structure,with dynamic role assignment(such as verifers and debate participants)within pruned spaces for structure.",
            "Topological control also becomes tractable through technological advancements.GPTSwarm [651] conceptualizes agents ascomputation graphs and uses evolutionarystrategies andreinforcement learningforadjusting adjacency matrices for optimizing nodes based on feedback for the task.MACNET[1028] uses a directed acyclic graph architecture with supervisory instructors managing edges and executive assistants managing nodes for more complex coordination domains,facilitating adaptivecommunication through topologicalordering and sensitive propagation of output. Application-specificversions alsoemphasize architecture diversity.Open-world environments have DAMCS [1029], which couples hierarchical knowledge graphs (A-KGMS)with structured communication schemes (S-CS) for cooperative planning as a function of messages passed based on context.AutoAgents [1030] leverages a dynamic drafting-execution pipeline with pre-defined agents jointlysketching out expert teams,adesignthat's highlyefctive for creative applications such as novel generation through parallel processing and internal supervision. Noticeably, small-world development within large-scale MACNET[1028] systems corresponds with graph reasoning ideas shown in[1022], where distributed architecture bypasses local limitations of LLM through structured collaboration.In terms ofcollaborative task solving,several paradigms have emerged that emphasize therole of dynamic topologies. These paradigms include search-based methodologies,LLM-based generation,and configurations utilizing external parameters.",
            "Search-based Methods A number of works adopt search-based methodologies to iteratively optimize communication structures.For example,ADAS [741] employs a Meta Agent Search algorithm that iteratively generates and tests new agent designs within acode space, archiving superior configurations and thereby updating subsequent generation strategies.Similarly,Afow[773] models each LM callas a node inagraph and utilizes Monte Carlo Tree Search (MCTS)to dynamicall extend and refine the workflow.Other frameworks,such as MAD[1031] and OPTIMA [1027], integrate iterative generate-rank-select-train paradigms thatecho MCTS principles to balance task performance with efficiency.",
            "LLM-based Methods Complementing search-based methods, severalrecent works leverage the generative capacity of LLMs to construct and adapt dynamic topologies.Dylan [725] introduces a temporal feed-forward network (T-FFN) model that treats each communication step as a network layer,using forward-backward propagation to compute Agent Importance Scores for dynamic team selection. Inrelated work,DAMCS[1029],AutoAgents[1030],and TDAG[1032] dynamically generate specialized sub-agents orupdate hierarchicalknowledge graphs,enablingcooperative planning and task decomposition.Further,frameworks such as AutoFlow[773]and Flow[1033]represent task workflows in natural language programs oractivity vertex graphs (AOV),allowing continuous refinement through reinforcement learning signals.ScoreFlow[788]complements these approaches by applying gradient-based (loss-gradient)optimization to continuously reconfigure agent workflows.",
            "External Parameters Given that fine-tuning LLM-based agents is often resource-intensive,a considerable number of researchers advocate configuring inter-agent topologies by training parameters independent of the LLM-agent.This approach is initiatedbyGPTSwarm[651],in whichthe inter-agent topologies are representedasadirectedacyclic graph (DAG),with edge weights serving as the sole trainable component of the system.Further advancing this paradigm, AgentPrune provides a unified modeling framework fromthe spatial-temporal graph perspective for mainstream MAS, where communication redundancy,ie.,unnecessary edges,is identified and prunedthrough magnitude-based pruning. Follow-upworks in this lineof research include G-Safeguard[1034],which similarly trains GNN outside of the MAS to detect andeliminate malicious communication paths.Although these methods are parameter-effcient,their relatively small parameter space and low coupling with LLM-agents often result in performancelimitations to some extent.",
            "Discussion Dynamic topologies extend beyond task-solving and play a crucial role in simulating complex social interactions.As detailed inarecent survey[975],LLM-basedagent modelscanevolve inter-agent links tocapture real-timechanges inautonomy,socialbehaviors,and environmental feedback across variousdomains, including cyber, physical,and mixed environments.Systems such as[50],OASI[936] and ProjectSid[989] simulate dynamic social networks.[50]employs generative naturallanguage memory retrievalto adjust socialties based on agents'experiences, while OASIS constructs a real-time social media environment with continuously updated user relationships and informationflows.Project Sid[989]introduces thePIANO(ParallInformation Agregation via NeuralOrchestration) architecture,enablingover lautonomous AIagents tointeract inreal-timewithina Minecraft environment,leading to the emergence ofcomplexsocietalstructures such asspecializedroles,collctive rule adherence,andculturaland religiou transmission.Additionally,architectures like AgentScope-scability[1035] and Social Survey [975]support large-scale multi-agent simulations,enablingstudiesofculturaldissemination,colectivedecision-making,andemergent group dynamics in environments with hundreds orthousands ofinteracting agents.Additionaly,dynamic topologies are also tailored to specific application domains such as medical and open-domain embodied AI. In the medical field, AI hospital[1036] and agent hospital[921] simulatereal medical workflows, where iterative cycles of diagnosis, treatment, and feedback continuously reshape communication patterns among various roles,such as intern doctors, patients,examiners,and supervising physicians.These frameworks dynamically adjust inter-agent communication to optimizecolaboration and decision-making.Similarly, in open-domain and embodied AI applications,frameworks like IOA[9]support heterogeneous,cross-device agent interactions,facilitating dynamic team formation and task allocation in real-world scenarios.",
            "Although the aforementioned dynamic multi-agent topologies have made substantial progressin performance metrics, they stillfacethefollowingthreelimitations,whichwebelieveshouldbethefocalpointsforfutureresearchondynamic topologies:",
            "(1) Generalizability. Current MAS topologies are typically optimized for a single-task domain. For example, AFlow[773] is dedicated to search and optimization within math or code benchmarks, producing a fixed workflow that is difficult to adapt to newtask domains.Other dynamic topologies,such as ADAS[741],GPTSWarm[651],and AgentPrune,face the samechallenge.We argue thatMAS should becapableof lifelong learning,wherein the system generalizes across different task domains with minimal resources (e.g., API call, FLOPs, GPU hours).",
            "(2)Resource Efficiency.Present dynamic topologies oftentend tooptimize forcomplex,resource-intensive structures. Their training processes aretypicallexorbitantlycostly,asexemplified byADAS[74l],wheretrainingwithGPT-3.5 incurs a cost of approximately $\\$300$ per session. Such expenses severely constrain their large-scale applicability in real-world scenarios.Future developments should focus on achieving beter test-time topology optimization with significantly reduced costs.",
            "(3) Inference Efficiency.As MaAS[787]has incisively observed, multi-agent topologies of excessive complexity, while capableofconsistentlydelivering satisfactory performance,arelamentablydeficient intask adaptability.That is to say,theyareunabletodynamically allocatereasoningresources (i..,tols,thenumberofagents,andreasoning steps)inresponse tothe diffcultyof agiven task.Consequently,this may lead toacertain lack of effciency inthe inference process. Although MaAS has, to acertain extent, achievedtask dynamism through the designed agentic supernet, their applicability and scalability in large-scale deployment still remain to be tested."
          ],
          "raw_title": "Dynamic and Adaptive Topologies",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "14.2 Scalability Considerations",
      "number": "14.2",
      "level": 2,
      "content": [
        "Scalability is acriticalchallenge inLM-basedmulti-agent systems (MAS),especiallas the number of agents grows. In fully connectednetworks,the numberofcommunication pathsgrowsquadratically,leading toacommunication explosion that increases token usage andcomputationalcosts[1037,626].Centralized andlayered topologiescanexperience synchronization bottlenecks if supervisory nodes are inundated by messages,whereas decentralized networks—while more fault tolerant—necessitate complex consensus algorithms to achieve a coherent global state.",
        "Recent worksuchas[1028]demonstrates that when multi-agent collaboration is structured as adirected acyclic graph (DAG),the systemcan scaleeffcently tohandle large graphs-upto1O0 nodesor more-without significant performance degradation.Similarly[1022] shows that distributing graph reasoning tasks among many agents circumvents the limitations imposed by long textualinputs and context-length constraints.Moreover,studies on self-organized agents[1038]revealthat dynamic multiplication andtaskdistribution allow the system tomaintain aconstant workload per agent while increasing overallprocessng capacity.Finally, the multi-dimensional taxonomy proposed by [1039] providesa valuable framework foranalyzing trade-off between agent autonomy and alignment,offering insight into how to balance centralized control with decentralized flexibility to optimize scalability.",
        "In addition tothese foundational studies,recent advances in practical multi-agent platform designfurtherenrich the scalablity discussion.Forexample,AgentScope[105]offers adeveloper-centric platformthat leverages an actor-based distributed framework to enable seamless migration between local and distributed deployments.Its unified workflow and automatic paralleloptimization significantlyreduce the communication overhead and synchronization challenges that typicallemerge as agent numbers increase.By incorporating fault-tolerance mechanisms and intellgent mesage filtering,AgentScope illustrates how system-level supportscan be designed to maintain performance even in dynamic and heterogeneous deployment environments.",
        "Another complementary approach is presented in Project Sid[989], which explores scalability within the realmof simulating agent civilizations.Here,the focusshifts from isolated task solving tothe simulationofcomplexsocietal dynamics. The proposed PIANO (Parallel Information Aggregation via Neural Orchestration)architecture allows agents to operate concurrently by decoupling slower cognitive processes from rapid reactive modules.A dedicated cognitive controllr is introduced to ensure coherence among multiple paralleloutputs.This design not only enables scalability fromsmallgroupstosimulations involving overathousand agentsbutalsoeffctivelyaddresses the inherent coordination challenges arising from high-frequency interactions.",
        "Taking scalability toaneven larger scale,AgentSociety[1040] demonstrates acomprehensive framework for simulating realistic social environments with up to10,O0O agents.By integrating LLM-driven social generative agents within a realistic urban,social, and economic setting,AgentSociety employs distributed computing and a high-performance messaging system (e.g., MQTT) to support millons of daily interactions.This platform exemplifies how emerging hybrid architectures can support macro-level phenomena—such as economic market dynamics,opinion diffusion, and urban planning simulations—by effectively managing the trade-offs between communication cost, coordination overhead, and emergent behavior fidelity.",
        "Despite the theoretical advantages of scaling upagent populations,it is imperative to question whether pursuit of large-scale agent deployments is inherentlyvaluable for alltask-solving scenarios.Although thetotalcomputational capacity scales with the number of agents,when memoryoverhead and inter-agent communication costs arefactored in,the marginal utility of adding additional agents may demonstrate diminishing returns. This phenomenon arises from the fundamentalconstraintthat, whiletheoverallworkloadis the product of individual taskcomplexity andthe degree of labor division,coordinationcosts tend to increase super-linearly with agentcount.Therefore,for many bounded problem domains,there is likely an optimal agent population size beyond which performance plateaus—or even deteriorates—due to excessive coordination overhead.",
        "Conversely,in simulation scenarios where theobjective is to modelcomplexsocial dynamics,emergent behaviors,or large-scalecollective intelligence,scaling tonumerous agents becomes not merelybeneficial but essential. Inthese contexts,theresearchfocus shiftsfromoptimizingcomputationaleffciencyfortask solving toaccuratelyreproducingor predicting macro-level patterns emerging from micro-level agent interactions.Such simulations—covering domains like economic marketbehavior,social network evolution,and urban infrastructure planningoften requirethecomputational overhead of managing vast agent populations in order to capture realistic population-level phenomena.",
        "Hybrid architectures that combine centralized oversight with decentralized sub-teams offra promising solution to these scalabilitychallnges[921,18].Inthesedesigns,supervisoryagents handle globalobjectivesandcoordination, while worker agents focus on executing specific subtasks.This hierarchicalorganizationhelps to mitigate information overload at any single node andalows for dynamic adjustment of agent team sizes based on task demands, thereby optimizing resource utilization.Furthermore, advanced techniques such as graph search algorithms,reinforcement learning-basedupdates,andevolutionary methods arecriticalforiterativelyrefningthe network structureasthe system scales.Intelligent message filtering,prioritization,andaggregation mechanisms can significantlyreduce communication overhead without sacrificingthequalityofinter-agentcollaboration.Inaition,asynchronouscommunicationprotocols and partial knowledge sharing strategies show promise in minimizing coordination bottenecks while maintaining sufficient global awareness among agents.",
        "Concluding Remarks on Scalability Overall,the study of system topology and scalability in LLM-based MAS reveals a spectrum of design choices—from staticconfigurations that ofer simplicity and predictabilityto dynamic architecures that provide flexibility and adaptability.While foundational works (e.g.,[1028],[1038])emphasize scalable graph structures and self-organizing principles,thepracticaladvancesdemonstrated by AgentScope,Project Sid,andAgentSocietyilustratehowintegrateddistributedframeworks,concurentprocessingandrealisticenviment simulations can collectively address the challenges of scaling multi-agent systems.Thecontext-dependent nature of scalability requirements—contrasting between task-solving and simulation scenarios—highlights the importance of purpose-specific design in multi-agent architectures.As research continues to evolve, the development of more sophisticated adaptive algorithms,distributed architectures, and multi-dimensional evaluation frameworks will be essential for advancing the scalability and practical viability of LLM-based multi-agent systems."
      ],
      "raw_title": "Scalability Considerations",
      "type": null,
      "children": []
    },
    {
      "title": "Collaboration Paradigms and Collaborative Mechanisms",
      "number": "",
      "level": 1,
      "content": [
        "In this chapter, weofferadetailed exploration of thesepurposefulinteractions,examining how one agentinfluences collaboration within MAS.Wereference the diverse interaction behaviors that emerge from human social structures, further explaining multi-agentcollaboration through interaction purposes,interaction forms,andtherelationshipsthat form.",
        "Multi-Agent Systems (MAS)comprise multiple agents that interact in a shared environment, autonomously making decisions to accomplish tasks colaboratively orcompete with eachother[1041].In our context, we focus on collaborative phenomenons because they widely appeared in most practical applications.Basicalleach agent in MAS is equipped with different roles and initial knowledge and its own set of goals.",
        "When engaged inproblem solving orcommunication,agents interact withotheragentsortheenvironment tocollect and process information, independentlymaking decisions based on theirobjectives,existing knowledge,andobservations, and subsequentlyexecuting actions[975,1041,1042,1043].Knowledge, memory,and environmentalobservations form the agents'beliefs,while varying motivations influence their approach to tasks and decision making [1041]. Consequently,effctive problem solving requires diverse purposeful interactions,including agent-agent and agentenvironment.These interactions may involve multiple rounds and occur in various directions,dependingonthe system design."
      ],
      "raw_title": "Collaboration Paradigms and Collaborative Mechanisms",
      "type": null,
      "children": []
    },
    {
      "title": "15.1 Agent-Agent collaboration",
      "number": "15.1",
      "level": 2,
      "content": [
        "Considering thecategorizations of MAScollaborations,we focus on moredetailson thegranularity neededto capture the nuanceddynamics incomplexmulti-agent interactions.Sepecifically,wecategorize inter-agent interactions into four types,nspired by sociologicalinsights from human-to-humaninteraction patterns and applying them to agent-agent interactions in MAS.Sociological theories on human interaction, which include consensus building,skillarning, teaching, and task division collaboration,providea more refined way of clasifying agents.interactions.These interactions form collaborative paradigms, which enable diverse intellgent agents to work together effctively in solving complex problems, and they are shaped by various forms of goals,contexts and outcomes.Each paradigm addresses unique challenges related tocooperation,competition,coordination, anddecision-making.Additionally,MAS implementations involveagents with diferenttypes of interactions,ratherthanasingle typeorunidirectional process, formingcomplexinteraction networksthatevolveover time.Incollaborativesoftwaredevelopment[626,627],asenior developer agent may interact task-wise with an architect agent, guide junior agents through multi-round dialogues. They work together oncodereviews fordecision-making andlearn witha testingexpert agent toimprove test coverage. Examining theobjectivesandresultsofthese interactions revealsthecrucial techniques andtechnologies shaping agent behavior and decision-making, thereby enhancing our comprehension of multi-agent dynamics.",
        "Consensus-oriented Interaction Consensus-oriented interactions concentrate on harmonizing the MAS's final target via negotiation,voting,and socialchoice frameworks[1044].Thisinteraction is significant for incorporating diverse knowledge and ensuring agents shift their views towards a unified understanding to achieve consensus [1045]. In this interaction,agents integrate knowledge to establish aunified understanding,whichlargely helps joint decisionmaking in complex problem-solving situations that demand diffrent viewpoints.For instance, MedAgents [922], MDAgents [1046], and AI Hospital[1036] demonstrate how collaborative dialogue among multidisciplinary agents improves problem solving by sharpening reasoning skills and accessing inherent knowledge.",
        "![](images/490ece6305c26c4dff52c55417e33b54872fb4347d02379d38b7859d56a45e4e.jpg)",
        "Figure 15.1: Anoverview offour agent-agentcollboration types in LLM-based MAS:Consensus-oriented,Collborative Learning,Teaching/Mentoring,and Task-oriented.Each type is described alongfour keydimensions: information flow, collaboration purpose, knowledge integration, and output focus.",
        "These dialogues allow agents to ensemble expertise intocoherent outcomes,frequentlyoutperforming conventional methods like zero-shot orfew-shotreasoning.The importance ofconsensus-driven teamwork is particularly evident in scientificenvironments,where addressing complexchallengesrequires diverse perspectives and meticulous validation Agent Laboratory[746],serves as an example where PhD and postdoctoral agents colaborate to agree on research objectives,interpret experiments,andconsolidate research findings.Similarly, VirutalLab[752]organizeaseriesof team toconducts scientificresearch, where allagents discussa scientific agenda, and individual meetings,where an agent accomplishes a specific task.",
        "Method for multi-agent consensus typicall include severalapproaches,including Discussing,debating, negotiating, reflecting, and voting.Common methods for reaching consensus encompass an array of structured techniques. The primary mechanisms involved are discussing,debating,negotiating,reflecting,and voting.Debates allow agent to obtain competing hypotheses,while negotiation helps resolveconflicting priorities andresource limitations.Specific frameworks have been created to support these consensus-building activities.During these processes,agents gather output from peers tackling thesame issue,and include environmentalfeedback asnumericaldataandcontextual details. These interactions enable agents to share viewpoints,assumptions, and progressively achieveacommon understanding.",
        "For example, GPTSwarm[651] formulates the collaboration between agents with graph design, that the information fow and edge connections build the basicgroupdiscusson.In GPTSwarm,ifanagentconsistently provides incoect opinions, it willbe excluded.RECONCILE[918] uses a round-table discussion format with several discussion cycles and voting systems based onconfidence levels. It integrates reflection by learning from past discussons, using confidence metrics and human insights to improve their responses.Furthermore,debates are quite important for achieving agreement, reducing hallucinations and also addressing complex issues [985,1047,1031,1003]. In GOVSIM[1048],agents collaborate toachievea balance, and it suggests using a sharedresource andconserving it for future needs. The negotiations went beyond simple information exchange and relationship-focused interactions. The Multi-Agent Debate (MAD)framework [1031] promotes creative thinking by having agents deliver arguments in a“tit-for-tat\"patern, with a judge overseeing the process tofinalizeasolution.The Formal Debate framework (FORD)[1004] enhances consistency among language models through organized debates,enabling stronger models to steer consensus,while weakerones adjust their perspectives.Similarly,AutoAgents[30]defineacollaborative refinement action inwhicheachagentupdates itschatrecord.Inthe process,italsoappends the previous statementsof the other agent and refines its action to achieve consensus.",
        "Collaborative Learning Interaction In collaborative learning,interaction usually happens among similar agents. Although architecturaly alike, accumulate distinctmemoriesand experiences duetotheir unique behaviors and varied environmental interactions.By solving problems together, these agents share experiences to boost their strategy learning,task-solving,and skillaquisitioncapabilities.Overtime,each agent enhances itsskillsthrough ongoing interaction,leading totheevolution of individuals.Thekeydiffrence betweencollaborativelearningandconsensusoriented interactions lies in their fundamental goals and processes.Whileconsensus-oriented interaction focuses on knowledge integration andbeliefalignmentthroughsynthesizing diverse viewpoints toreachagreement,collaborative learning interaction emphasizes peer knowledge construction and experience sharing,prioritizing mutualimprovement and individual growth.When engaged in colaborative learning interaction,agents update their context or memory from observing others'behavior.For example, agents can learn optimal strategies by observing the deliveration from peers,adapting their own approach based on these observations without necessarily agreeing ona single“best\" strategy[961,962,963,971,965,967,972968,969].As highlighted in[966],the efective discussion tactics significantly impactlearning outcomesamong agents. In these interactions,agents collaborate tolearnand address problems,focusing on mutual understanding and enhancement rather than reaching unanimous decisions.This method refines personal responses and knowledge via ongoing feedback.",
        "The methods commonly employed in collborative learning interaction include:1). Experience sharing., Agents exchange personalinsights and best practices.As described in[303],iterative experience refinement enables LLM agents to achieve adaptive improvement in software development via continual acquisition and utilization of team experience in successve pattern and the cumulative pattern.Furthermore, MAS-CTC[301l is a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions andcommunicate with their insights in across-team collaboration environment. Itenables different teams to concurrently propose various task-oriented decisionsas insights,and then communicate for insights interchange in important phases (multi-team aggregation). Dierent agent teams utilize a greedy pruning mechanism andaggregation mechanisms to eliminate low-quality content, thus improve the performance in software development.Differently, in MOBA[1049], a novel MLLM-based mobile multi-agent system,globalagentreflects onlocalagent executionresults to support adaptive planning toalign withthe environment. AutoAgents [1030] employs a knowledge sharing mechanism where agents exchange execution results to enhance communication and feedback, where agents can obtain long-term,short-term and dynamic memory from others.2). Peer discussions.Peer discussions allow agentsto articulate their reasoning processes and learn from others\" approaches.MEDCO[923]create a dynamic environment where clinical reasoning and decision-making skillsare strengthened through collborative problem-solving among student agents.Moreover, In[105o],agents engage in structured peer discussions after initializing their output,reviewing each other's reasoning step by step. Through feedback exchange andconfidence scoring,agents refine their decision-making,learn from diverse approaches, and iterativelyenhancetheir reasoning accuracy,fostering collaborative knowledge acquisition.3).Observational learning.Observationallearning occurs when agents monitor others'behaviors and outcomes to inform their own strategies.AgentCourt [1051]develops lawyer agents that participate incourt debates andimprove through accumulated experiences, demonstrating improved reasoning and consistency through experiential learning.In iAgents [1046], the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry.iAgents employs a novelagent reasoning mechanism, InfoNav, to navigate agents'communication towards effective information exchange.Together with InfoNav,iAgentsorganizes human information in a mixed memory to provide agents with accurate andcomprehensive information for exchange.Additional experimental phenomenon indicates diffculty of certain tasks making agents continuously refine their strategies in pursuit of the required information.MARBLE[948]designs acognitive evolve planning combining the“expectation'of the agent andits actualactionresults toupdate the overallplanning experience for better planning in the next round.",
        "Despite itsbenefits,collaborativelearning interaction faces severalchallenges.These include ensuring equitable knowledge exchange among agents with varying capabilities,preventingthepropagationof errorsor biases acrossthe system, maintaining agent diversity while facilitating learning,and developing efective mechanisms for agents toselectively incorporate others'knowledgebased onrelevance andreliability.Overcoming thesechallenges requires the meticulous creation of interaction frameworks and learning strategies.And it should balance individual advancement with the broaderdevelopment of the system.Although issues suchasknowledge fairnes,bias propagation,and scalability present diffculties, there is great potential to improve MAS,particularly indynamic and complex environments.By using iterative learning processes andproviding opportunities,collaborative learning enables agents todevelop richer knowledge bases and more refined problem-solving abilities.",
        "Teaching/Mentoring Interaction To tackle these challenges,it is important tocarefully develop interaction protocols and learning frameworks that harmonize individual development with overall system progress. In thecontext of MAS, teaching andmentoring interactions arefundamentalmechanisms incollaborativeenvironmentsespeciallinscenarios where knowledge transfer is essential for growth andcollective intelligence.Unlike collaborative learning,where knowledge is exchanged reciprocall among agents,eaching and mentoring interactions focus on the unidirectional flow of knowledge from an experienced agent to a less experienced one.The mechanisms and methods used in teaching/mentoring interactions include several key strategies:",
        "·Criticism and Feedback.The mentor agent evaluates the learner's performance and provides corrective or constructive feedback.This helps the learner refine their knowledge and skills through a feedback loop where they update their internal knowledge based on the feedback received.\n·Evaluation. Mentors assess the learner's capabilities or progress through performance reviews and clear assessment criteria, providing valuable insights for development.\n·Instruction and Teaching. Mentors convey targeted knowledge, guidelines, or techniques using direct instruction which allow learners to pose questions and receive clarifications.",
        "Iterative Teaching and ReinforcementTeaching is typicaly progressive,where each phase provides opportunities for thelearner tocomplete tasks and get feedback. For example,inthe MEDCO system[923],student agents improve their professionalskillthroughacyclic practice-orientedlearning approachdirected byexpert mentors,inadditionto engaging in peer discussions.These expertagents conduct ongoing assessments and provide real-time guidance on clinical competencies,focusing on patient interaction skill and diagnostic reasoning.[921] shows that an agentic doctor can continually improve their diagnosis by merelyinteracting with agentic patients in a simulated hospital and can transfer its learned knowledge of real-world cases.",
        "Thisinteractiontype can be categorized basedonthedirection of knowledge transfer into two primary types: unidirectional and interactive.Unidirectionalisrooted intraditionalteaching models where knowledge fowsfromthe teacher to the student.This approach emphasizes thetransmission offacts andconcepts,often involving lectures and direct instructions [923].",
        "Task-oriented Interaction. Task-orientedcollaborations involve agents working togetherto achieve common objectives through effectivecoordination and task decomposition strategies,aswellas ahighdegree of cooperation and cordination.Agents interact primarily by processing upstream output and generating results for downstream agents following established task dependencies rather than engaging in complex discussions or debates.",
        "Recent frameworks demonstrate diverse implementations of this interaction patern: (1) software development frameworks such as MetaGPT[626] and ChatDev[627],agents operate ina structured pipeline that mirrorsthe software development lifecycle.Forexample,architect agents process requirements to generatetechnicalspecifications, which development agents then use to producecode,followed bytesting agents who validate the implementations; (2) Collaborative reasoning frameworks like Exchange-of-Thought (EoT)[1052],GPTSwarm[651], MACNET [1028] involve structuring agents ina specific format (e.g.,rng,tree, directedacrylic graphs,optimizable graphs),which mitigates context expansion risks by ensuring onlyoptimized solutions progressthrough the sequence,and enforcing multiple agents tocollaborate together towards solvingcomplex mathematical or knowledge reasoning tasks; In (3)ML applications[05319],agentsadhere tostringent workflowstructures,eachfulfiling specifictasks in proceses. For more complex tasks such as VideoQA,the TraveLER framework[1054] showcases modular task breakdown across structured phases(Traverse, Locate,Evaluate, and Replan),with aPlanner agent managing interactions and improving strategies based on iterative agent inputs.",
        "These handoff relyon explicit deliverables instead of direct agent negotiations.Inspired by GPTSwarm[651]-alike graph agentic systems, MACNET [1028] structures agents into directed acyclic graphs (DAG).Here, supervisory figures issuedirectives while executors implementsolutions.By ensuring only optimized solutions progress through the sequence,thissetupmitigatescontext expansionrisks.InMLapplications[053,019]agents adhere to stringent workflow structures,eachfulfiling specifictasks inprocesses.For more complextaskssuchasVideoQA,theTraveLER framework[1054] showcases modular task breakdown acrosstructured phases (Traverse, Locate, Evaluate,and Replan), with a Planner agent managing interactions and improving strategies based on iterative agent inputs.",
        "Beyond organized development, task-driven interactions have been shown in open-ended contexts such as Minecraft game,in where agents adjust to ever-changing environments.In[927],leader agents manage workflows by breaking down complexobjectives intospecifictasks,while executoragents perform actionsike gathering resources.Coordination mechanisms are important forensuring agentscollaborate effectivelytowardsfinal goal, includingcommunication protocols,synchronization strategies,andresource-sharing techniques.The interactionof agents inMASfortaskexecution has garmered significant interest, notably through utilizing LLMs forhandling intricate tasks and workfows.The collaboration ofagents are vitalfortaskcompletion,particularly inever-changingsettings likesoftware development and project management [626, 630]."
      ],
      "raw_title": "Agent-Agent collaboration",
      "type": null,
      "children": []
    },
    {
      "title": "15.2 Human-AI Collaboration",
      "number": "15.2",
      "level": 2,
      "content": [
        "To unlock the potential of MAS in meeting human objectives,people often work alongside them using three primary methods: one-of task delegation, multi-turn interactive instruction, and immersive human-agent collaboration",
        "In one-off task delegation,humans delegate single-instance tasks toMAS,suchas posing a question toaQ&A platform or assigning acoding task[1055,626].Without additional input, theagenthandlesthetaskautonomously,delivering a complete response or solution ina singlereply.This is presentlytheprevalent wayhumanscolaborate withLM-based agents [922, 627, 31].",
        "For multi-turn interactive instruction,humans engage in iterative interactions with LLM-based agent systems to refine andexplore solutions untilasatisfactoryresult is achieved.This typeof interaction iswidelyseen in creative applications,suchas image editing or writingedit[938].Forinstance,auser might ask the system to add an object to a specific location in an image,replace an element,changethe background,orreviseapart in asentence.These interactions often span multiple rounds, with users continuously refining their requests until the desired outcome is reached.Moreover,certain other LLM-based agent systems may require human approval or clarification during multi-turn interactions before proceeding to the next step[1056,930]. Under human guidance,these LLM-based agent systems can complete household tasks as well as software development tasks.",
        "Immersive human-agent colaboration features LLM-based agents simulating human behaviors to serve as partners. For instance,inan immersive setting,humanstreatthese agents as teammates,achievingcommon objectives.Instances include agents representing humans in meetings orhelp solve tasks likechores orprojects.This strategy highlights effective integration and teamwork in dynamic contexts [937, 924].",
        "To assess Human-AI collaboration quantitatively, several frameworks have been suggested. Co-Gym[1057],for instance,measures the communication, situational awareness, and personalizationof LLM-based agents in tasks such as travel planning, writing related work, and tabular analysis.",
        "In summary, as LLM-based agent systems have advanced,Human-AIcollaboration has diversified to address challenges across domains.This ranges from simple command-based AI interactions for questions,to multi-turn dialogues for design and development, and partnering with human daily tasks.",
        "With advancements inLLM-based agent systems,they are expected tointegrate more into daily life,streamlining tasks and boosting effciency.Atthe same time, humans willrefine and adapt their waysof interacting with AI, leading to more effectivecollaboration.Webelieve this shift willdrive fundamentalchanges inboth social productivity and the socialrelations ofproduction,reshaping how work is organized and how humans and AIcooperate in the large language models era."
      ],
      "raw_title": "Human-AI Collaboration",
      "type": null,
      "children": []
    },
    {
      "title": "15.3 Collaborative Decision-Making",
      "number": "15.3",
      "level": 2,
      "content": [
        "Collaborative decision-making processes are crucial for ensuring theeffcient operation of MAS and the successful completion of tasks.Although collboration itself is a core feature, the approaches of decision-making directly determines theeffectivenessofcollaborationandtheoverallperformanceofthe system.Recentresearchhashighlighted thecriticalroleofcollborative decision-making.[1037]showedthat diverse decision-making methodscan significantly enhance the collaborative eficiency of the system.[649]emphasized that arationaldecision-making mechanismcan stimulate the emergence of intelligence within a system.",
        "Froma broader perspective,the collaborative decision-making process can be divided into two majorcategories based on their architectural characteristics: Dictatorial Decision-Making and Collective Decision-Making [1037].",
        "Dictatorial Decision-Making. Dictatorial Decision-Making is a process where decision-making relies on a single agent in aMAS.In this paradigm, allagents send their state information orlocalobservations tothisdictatorialagent. The dictatorialagent isresponsibleforassembling this data,studying the core problems,andestablishing definitive decision guidelines.The key principle for such anapproachis toleverage aglobalmindset in moving towards improved decision-making,hence paving the reliabilityof thesystem performancealong withthesuccessfulachievement of task goals.[1031,058,046] demonstratedthe single-agent decision-making process with a single LLM, who synthesized various views on the same problem to make decision-making even more objective and comprehensive.Furthermore, [134,1059]suggestedthe weightedintegration methodthroughranking,scoring orchecklist,enhancing therobustness of decision-making procedures. In addition,beyond the explicit inclusion of perspectives,[1030,06o] proposed architectures where acentralagent breaksdowncomplex tasks into simpler sub-tasks and assigns them to specialized agents grouped bytheirfunctionalities.Moreover,in[651,28],itiscommon that the last nodes agent works inan environment to assemble the past information and deducea conclusion according tothe topological structure,rather than by a central agent.",
        "Collective Decision-Making.Collective Decision-Making involves agents collaborating to reach decisions without a central authority,relying onlocaldata and interactions like voting or negotiation.This method shares decision-making power among agents,allowing the system to adapt according tochanges while maintaining robustnessand scalability.",
        "·Voting-based Decision MakingVoting systems are important for collective decision-making, providing a framework for reaching consensus.A conclusive majority is achieved through voting as described by [1045, 968]. Moreover, the GEDI electoral module[1037] enables multiple voting methods. This method largely improve reasoning and fault-tolerance while avoiding complex system designs.",
        "· Debate-based Decision MakingIn comparison with voting-based methods, debate-based decision-making focuses on organized interactions between agents, in order to obtain the best result. In[1031,1061],agents participate in guided discussion, where they articulate and proposals in an atempt to resolve disagreements and reconcile points of view. Simultaneously, [1050,1062] practice restraint stance, using communication channels among agents for consensus-building through repeated discussions.To tackle the issue of“cognitive islands,certain systems would employ a common retrieval knowledge base to enable agents to be aware of the same knowledge throughout debates[1005].By mimicking human dialogue, these systems alowed agents to exchange perspectives and make more informed decisions.",
        "Discussionand Future Work Collaboration inmulti-agent systems (MAS)stil faces numerous challenges that require further research.Current methods are largely based on contextually dependent interactions; however,they do not include a specificframework fortraining andoptimizing cooperative actions.This heavy dependence onlarge language models (LLMs)hassomelimitations,astheir effctiveness is inherentlytiedtothe sizeoftheLLM'scontextualwindow andits native reasoning capabilities.WhileLLMs provide asolid foundation forenabling interactions,these systems are still limited by the inherent limitations of context-dependent communication.",
        "Future studies should focus onfinding frameworks that inspire agents for activelearning with regard tooptimal timing and information dissemination methodologies. Using methodologies from multi-agent reinforcement learning (MARL), there is a growing requirement for strategies that willhelp agents determine appropriate moments for information sharing,aswellas what information should be shared through whatchannels.Thiscalls for not just devising novel interactionprotocols but alsoincorporating training methodologies that willconstantlyoptimize these protocols with each improvement."
      ],
      "raw_title": "Collaborative Decision-Making",
      "type": null,
      "children": []
    },
    {
      "title": "Collective Intelligence and Adaptation",
      "number": "",
      "level": 1,
      "content": [
        "The concept ofcollective intelligence is centraltothedevelopment ofmulti-agent systems(MAS),drawing inspiration from biological and societalcooperation.An inherent concept within collective intellgence isthe“Wisdom of Crowds\" by [915],which asserts that independent communities often make better decisions asa whole than any one person. Cognitive theoreticalmodels like the Society of Mind[17]and itsrelated theorymind[916,917]further supportthe paradigm,suggesting that intelligence springs from a synergy among primary, specialistcomponents.Moreover,In human societies,individualscollaborate,divide labor, and engage incolective problem-solving toaddresscomplex challenges.MAS adopt similar strategies where specialized agents to participate in solving complex problems and collective decision-making [914].",
        "Theemrgenceof collective intelligence within MAS is a dynamic and iterative processThroughcontinuous interaction, agents develop a shared understanding andcollective memory progressvely.The interaction dynamics are strengthened by heterogeneity among individual agents,environmental feedback, and agent-agent interactions [914],which are allimportant for the emergenceof complex social networks and improving decision-making strategies.It is worth highlighting thatcolective intellgence is not merelythe summation of individualcapability,butrefers toemergent behavior beyond individualagent capacity.beyond individual agent capacity.Individualagent development is deeply linkedwithcollectiveintellgencegrowth.Withongoinginvolvementwithcollectivetasksandself-reflectionnshaed contexts,agents increasingly developreasoning and decision-making capabilities.The evolution ofindividual agents is closelyrelated tocollective intelligence evolution.Through continuous interaction in joint activities and critical examination of shared contexts, agents continuously refine their reasoning and decision-making abilities.",
        "In parallcomplexand diversebehavior among agents emerges.These include beyond-restricted-protocolbehaviors, suchas advancedsocialinteractions,includingtrust,strategic deceptionadaptivecamouflage,andemergentcoperation, evoking a shiftfromreactive intocooperative strategies,as wellas deeper social dynamics.With achain of recursive interactions,agents necessarilyformcooperative strategies,which eventuallyturn intosocialcontracts,organizational hierarchies,and divisions oflabor.Social phenomenanecessrilyemerge through recursive interactions among agents, coupled with their adjustment with thechanging environment. It marks a transition from fundamental cooperative behavior into complex social constructs, leading to cultural norms and conventions."
      ],
      "raw_title": "Collective Intelligence and Adaptation",
      "type": null,
      "children": []
    },
    {
      "title": "16.1 Collective Intelligence",
      "number": "16.1",
      "level": 2,
      "content": [
        "The concept of collective intelligence, which refers tothe ability of a groupof agents to exhibit problem-solving capabilities that surpassthose of individual agents.This phenomenon is often characterized by emergent behaviors, sophisticated decision-making,and higher-orderreasoning abilities thatarise frominteractions among agents,leading to enhanced performance incollaborative decision-making scenarios and socialsimulations[975].[917] demonstrate that LLM-based agents can exhibit collborativebehaviors and high-order Theory of Mindcapabilities,whichare crucial for understandingthe perspectives ofother agents ina shared environment.Theirfindingssuggestthatthe integration of LLMs into MAScan facilitate more sophisticated forms of collective intellgence,thereby improving the overall efficacy of collaborative decision-making.",
        "Improved System Performance A primary advantage ofcollective intellgence in MAS is that collaboration leads to superior problem-solving capabilities.Collective intelligence can be encouraged to overcome“groupthink\"and individual cognitive bias in order to allow acolective to cooperate on one process-while achieving enhanced intellectual performance.When individualagents share information and coordinate actions, the system can achieve better results than any single agent operating independently[626,922,1046,1031,1063].Collective intelligence is therefore shared or group intelligence that emergesfrom the collaboration,collectiveefforts,and competitionof many individuals andappears in consensusdecision making.Collective intelligence stronglycontributes to the shiftof knowledge andpowerfromthe individualtothecollective.[924]demonstratedthis throughtheir CooperativeEmbodied Language Agent (CoELA), which achieved a $40\\%$ improvement in efficiency over traditional planning methods in ThreeDWorld multi-agent transport tasks.This substantialimprovement stems from the system's ability toeffectively utilize LLMs for planning and communication in multi-agent settings,providing compelling evidence for enhanced colaborative decision-making capabilities.As previously discussed, the inherentdiversity andinterdisciplinary nature of LLM-based multi-agentsystems,along with various inter-agent interaction, which provide internalfeedback and enrichedcontext for individual decision-making,hence reduce bias and improve the consistency of solution [918].",
        "Emergent BehaviorsOne of the most intriguing aspects of collective intellgence is the emergenceof new,complex behaviors that arise spontaneously from agent interactions.These behaviors are notexplicitly programmed but emerge from leaning and adaptation.Asdiscussd invarious studies[971,965,966],agentsdeveloped strategicbehaviors, including trust-building,adversarialtactics,deception,andleadershipduringthegame.Thecollectivebehaviorevolved through experience sharing,where village-aligned agents learned cooperation and strategic alliance formation, and wolf-aligned agents improved deception through“information confusion\"tactics.Moreover,agents optimized voting patternsanddeceptionstrategies without explicit training,which indicates the group intellgenceemerged over multiple roundsofinteractions.SimilarlyintheAvalongame[68]esearchersobservedthatagentsbecamebeteratidentifing and countering deceptive information.Individuals adapted todeceptive environments andrefinedtheir decision-making using first-and second-order perspective shifts.Furthermore,agents demonstrated adaptive cooperation and ad hoc teamwork,despite nopredefinedcollaboration protocols[969].Thesefindings highlighttheabilityofLLM-basedagents to developsophisticated behaviors through interaction andlearning,showcasing the potentialforemergentbehaviors in collective intellgence scenarios.Notably,these emergent behaviors relyon memory andreflective mechanisms.Agents retrieve andreflectonhistoricalinformation togenerateacompact context,enhancingtheirreasoningcapabilities[239]. In MAS,shared context and environmental informationsignificantly boost agents'usable memory.This enables agents to build on past interactions,refine strategies, and adapt more effectively to dynamic environments [1064].",
        "Social Evolution One of the most significant findings in the fieldof generativeagent societies isthe spontaneous emergence of social norms.[1o65] demonstrated that agents,throughcontinuous interaction, arecapable ofcreating, representing,spreading,evaluating,andcomplying with social norms.These norms serve asthe foundation for social order, reducing conflicts and improving coordination among agents, thereby leading to more stable and organized societies.Interestingly, thestudyfound that agentsdevelop norms morerapidly intheir beliefs than theydo in their behaviors.This suggests thatwhile agents may quickly internalizecertain norms,the translation of these norms into consistent actions takes longer. Over time,these norms tend to synthesize into more general principles,resulting in more concise and effective personal norm sets.Furthermore, the Project Sid simulation[989] models large-scale agent societies and providesfurtherevidence of the emergenceofsocialnorms androle specialization.In this study, agents were observed to autonomouslyform specialized socialroles.These roles were not predefined but emerged naturally as agents interacted within their environment and developedcollective rules.The simulation alsohighlighted the importanceof democratic processes inthe adherence and modificationofthese collectiverules.Agents were found to engage incultural and religious transmission,spreading ideas and doctrines across communities.This processof norm creationandrole specializationleads tobeterorganization,reducedconflict,andadaptive governance structures within the societyTheevolution ofculturalandreligious beliefs inmulti-agent societies is alsoobservedin[106], which occurs through agent-driven selection of ideas, mirroring real-world societalchanges.Additionall,the[936], which simulates socialinteractions among one millon agents,provides valuable insights into culturaltransmission and group polarization.Cultural memes and belief systems propagate naturally among agent societies.Agents exhibit herd behavior,conforming to prevailing opinions even whentheseopinions are irrational.This leads to the emergence of group polarization,where agents reinforce extreme views through repeated interactions.This finding highlights the significant impact of group size on the dynamics of cultural evolution and social behavior."
      ],
      "raw_title": "Collective Intelligence",
      "type": null,
      "children": []
    },
    {
      "title": "16.2 Individual Adaptability",
      "number": "16.2",
      "level": 2,
      "content": [
        "In multi-agent systems (MAS),individualadaptabilityrefers to an agent'sabilitytoadjust its behavior anddecisionmaking strategiesbased on previous interactions andexperiences.This isalsodefined asself-evolving,where agents can dynamically self-evolve by modifying themselves,such as altering their initial goals and planning strategies,and training themselves based on feedback orcommunicationlogs[38].This adaptability isfacilitatedbytheintegrationof large language models (LLMs), which support dynamic monitoring and adaptation processes[1067], as wellas the agents\"memorycapabilities andinformation exchange.These modules arecrucialtoensure thatagents cancontinuously improve their performance,respond effectively todynamic environments,andoptimizetheir decision-making processes. We categorize the mechanisms contributing to individualadaptability into memory-based learning and parameter-based learning, where there are training-free and training-based approaches.",
        "Memory-based learning Memory andreflective mechanisms significantly enhance individual adaptability in LLMbased multi-agent systems byleveraging historicalrecords andexperiences to inform decision-making [221,1068,50]. By maintaining and utilizing individual memory of past interactions,decisions,andoutcomes,the agent can refine its decision-making process over time.This memoryserves as arepository ofexperiences thatthe agentcan drawon when making future decisions. Using this stored knowledge,individual agent isable torefine its decision-making process, learning from previous successes and failures [921,1051]. For example, in clinical simulation,doctor agents can kep improving treatment performance over time by accumulating experience from both successful and unsuccessful cases [921].Insocialbehavior simulation,agentscan improvetheir adaptabilitybyengaging in more complexscenarios and utilizing scenario memories to enhance performance [50].",
        "Shared memory-based learning In contrast, shared memory-based learning extends thisconcept by enabling multiple agents to exchange information and insights derived fromtheir respective experiences.Rather than relying solelyon individual memory,agentscan benefitfrom the collective knowledge of the group.Bysharing data,strategies,and feedback,agents enhancetheir abilitytocooperateandoptimize their decisionscollaboratively.Shared memory-based learning is particularly valuable in environments where agents need tocooperate,exchange tasks,or work toward common goals[919,967,968].For instance, ProAgent [1069] anticipates teammates'decisions and dynamically adjusts each agent's strategies based on the communication logs betweenagents,facilitating mutualunderstanding and improving collaborative planning capability.",
        "Parameter-based learning. Beyond memory-based learming in textual form, many MAS employ parameter-based leaning,whichevolves agentsindividualadaptabilitythrough post-training techniques.For instance,[1o70]discusses thea Larning through Communication (LTC) paradigm, whereusing communication logs between agents are leveraged to constructt generate datasets fortotraining or fine-tuninge LLMs.The integration of symbolic and connectionist paradigms within LLM-powered agents enhances botheir reasoning and adaptability. More recently, research has increasingly focusedon multi-agent(co-)fine-tuning,which improvescollaboration andreasoning capabilitiesthrough cooperative trajectories.Examples include multi-agent debate fine-tuning [1071] and SiruiS [1072]. Additionally, Sweet-RL[1073]employs reinforcement learning toenhance the critic model within MAS,fostering bettercollaborative reasoning.However, despite their promising performance,future parameter-based learning paradigms may need to addressthe balance between agents'general capabilities andtheir specialization for specific roles within MAS.This hybrid approach allowsagents tohandle both structured and unstructureddata,improving theirability tomake decisions in dynamic environments [1074, 1075]."
      ],
      "raw_title": "Individual Adaptability",
      "type": null,
      "children": []
    },
    {
      "title": "Evaluating Multi-Agent Systems",
      "number": "",
      "level": 1,
      "content": [
        "The transition from single-agent to multi-agent systems, and specifically Large Language Model(LLM)-based systems, requires a paradigm change inthe evaluation paradigm.In contrast to single-agent evaluation,in whichthe immediate concern is performance on a particular task, evaluation of LLM-based multi-agent systems must be understood in terms of inter-agent dynamics asa whole,such as collaborative planning and communication effectiveness.Both task-orintedreasoning andholisticcapability evaluation are addressed in this chapter,reflecting the nuance of such evaluations.Ingreater detail, thereare two main areas that we examine for evaluation.First, there is task-solving Multi-Agent Systems (MAS),where we examine benchmarks assessing and enhancing LLM reasoning for coding, knowledge,and mathematical problem-solving tasks.These tests alsoaccentuate theutility of distributed problem solving,achieved through organized workflows,specialisationamong agents,iterative improvement, and call for additional tools.Enhanced reasoning,primarily because of agent-agent decision-making cooperation and multi-round communications, is shownfor MAS compared with agent-based individual ones.Folowing that,there is a general evaluationof MASabilities,extendingbeyondone-task-oriented achievement,toagent iteractions atahighlyadvanced level. It involvesa move away fromone-dimensional measurements into multi-dimensionalframeworksfordocumenting achievements atcollborations,reasoningabilities,systemeffcencyandflexibility.Wecategorize such measurements into collaboration-oriented and competition-oriented measurements and have identified effciency,decision-making quality,quality ofcollaboration, and flexibility as primary measure domains.These measurementscapture various aspects of agent behavior,including communication effectiveness,resource distribution,andresponse to dynamic situations."
      ],
      "raw_title": "Evaluating Multi-Agent Systems",
      "type": null,
      "children": []
    },
    {
      "title": "Building Safe and Beneficial AI Agents",
      "number": "",
      "level": 1,
      "content": [
        "TherapiddevelopmentofLLM-basedagents introducesanew setof safetychallnges that gobeyondthose of traditional LLMs.Equipped with advanced reasoning,planning,and tool-using capabilities, these agents are designed to perform tasks autonomously and interact with theirenvironments[34].However,this autonomy alsoexpands the attack surface, creating new vulnerabilitiesthat demand careful research and atention[1129,40].Inthis part,we first establish a comprehensive framework for understanding agent safety,examining both internal and external safety threats to AI agents.We willexplore the various attack vectors associated with these threats and propose potential mitigation strategies. This framework is organized into two key areas:",
        "(1) IntrinsicSafety threats stem from vulnerabilities in the agent'score components,which include the LLM“brain\" as wellas the perception and action modules.Each ofthese components has unique weaknesses thatcan be exploited by adversaries:",
        "·Brain is the LLM itself,responsible for key decision-making tasks such as reasoning and planning.It is guided by a knowledge module that provides essential contextual information.\n·Perception consists of sensors that interpret theexternalenvironment, where malicious manipulation of external objects can lead to erroneous perceptions.\n·Action isresponsible for tool usage and downstream applications, which are also susceptible to exploitation.",
        "(2)Extrinsic Safety threats arise from interactions between the agent and externaloften untrusted,entities.These include:",
        "·Agent-Memory Interactions: The agent frequently accesses and interacts with memory storage, which serves as an external database for decision-making and contextual information retrieval. Recent research highlights vulnerabilities in the agent-memory interface that could be exploited to manipulate the agent's actions. ·Agent-Agent and Agent-Environment Interactions:These refer to the interactions betweenthe agent and other agents (e.g.,other agents or human operators),aswell as its environment, which includes task-related objects or dynamic systems.The complexity of these interactions further compounds the agent's exposure to external threats.",
        "As illustratedinFigure17.l,theserisksarebroadlycategorized into intrinsicandextrinsic safety,helping toclarif their origin and nature.In addition to identifying threats,we also provide arigorous, mathematical foundation for understanding atacks such as jailbreaking, prompt injection, and data poisoning.Moreover, we present practical, actionable solutions, tracing the development of safety measures from early LLM safeguards to comprehensive strategies that protect the entire agent system.This includes exploring guardrails,advanced alignment techniques (such as superalignment),and the crucialbalance between safety and helpfulness. Finall,we analyze the“scaling law of AIsafety\"-thecomplex relationship between an agent'scapabilities and its potentialrisks—and the essential trade-offs that must be made.This part provides aclear understanding of thechallenges,theoreticalfoundations,and practical strategiesnecessary todevelopeffective and trustworthy AIagents thatcan be safely andefectivelydeployed in real-world scenarios.",
        "This part isorganizedasfollws:First, we examine intrinsic safety risks (Chapter 18)focusing on threats to the LLM “brain,\"as wellas vulnerabilities inthe agent's perception and action components (Chapter 19). Next, we explore extrinsic safetythreats relatedto agent-memory,agent-agent,and agent-environment interactions(Chapter 20).Finally, we investigate superalignment techniques aimed atensuring the safety of agent behaviors,whileaddressingthebroader challenge of balancing safety with performance.This includes exploringhow safety measures scale with the increasing capabilities of AI systems and examiningthetrade-offs involved indesigning secure,capable AIagents (Chapter 21).",
        "![](images/47d45ad22b0854ee971de9acb13cd7fcbadfa693ff22c28d6a6a5799fc059caa.jpg)",
        "Figure 17.1: The Brain (LLM) faces safety threats like jailbreaks and prompt injection attacks $(\\S18.1)$ and privacy threats such as membership inference attacks $(\\S18.2)$ . Non-brain modules encounter perception threats $(\\S19.1)$ and action threats $(\\S\\ 19.2)$ . Due to interactions with potentially malicious external entities, we also explore agent-memory threats $(\\S20.1)$ , agent-environment threats $(\\S20.2)$ , and agent-agent threats ( 20.3)."
      ],
      "raw_title": "Building Safe and Beneficial AI Agents",
      "type": null,
      "children": []
    },
    {
      "title": "Agent Intrinsic Safety: Threats on AI Brain",
      "number": "",
      "level": 1,
      "content": [
        "The intrinsic safety of an AIagent concerns vulnerabilitieswithin the agent's internal architecture andfunctionality. AI agents,by their nature,consist of multiple components:acentral“brain\"(the LLM),and auxiliary modules for perception and action[66]. While this modularity enables sophisticated reasoning and autonomous decision-making, it also expands the potentialattck surface,exposing theagent tovarious internalvulnerabilitiesthat adversariescan exploit [1130].",
        "Threats to the agent's brainspecificall the LLM—are particularly concerning,as theycan directly impact the agent's decision-making,reasoning,and planning abilities.These vulnerabilities can arise fromflaws inthe design of the model,misinterpretationsofinputs,oreven weaknesses induced bythetraining processEffctive mitigation strategies are crucial to ensuring that these agents can be deployed securely and reliably."
      ],
      "raw_title": "Agent Intrinsic Safety: Threats on AI Brain",
      "type": null,
      "children": []
    },
    {
      "title": "18.1 Safety Vulnerabilities of LLMs",
      "number": "18.1",
      "level": 2,
      "content": [
        "The LLM,asthecoredecision-makingcomponent ofthe agent,ishighly susceptibletoarange ofsafetythreats.Its centralroleinreasoningandactionselectionmakes it anatractivetargetforadversaries.Inthecontextof AIagents,the vulnerabilities inherent inthe LLMitself areoften amplified,asthese modelsare required tofunction within dynamic, real-world environments where adversaries can exploit weaknesses [1131, 1132]."
      ],
      "raw_title": "Safety Vulnerabilities of LLMs",
      "type": null,
      "children": [
        {
          "title": "18.1.1 Jailbreak Attacks",
          "number": "18.1.1",
          "level": 3,
          "content": [
            "Jailbreaks circumvent the safety guardrails embedded in AIagents,compelling their decision-making process to be harmful,unethical,orbiased[233].Theseatacks exploit the inherent tensionbetweenanLLM's helpfulnessand its safety constraints [1134].",
            "Formalization.Toformallcharacterizetherisks posedbyjailbreaks,we analyzethe probabilitydistribution govening an autoregressive LLM's output. For an autoregressive LLM,the probability of generating an output sequence $\\mathbf{y}=\\mathbf{x}_{n+1:n+m}$ , given an input sequence ${\\mathbf x}_{1:n}$ is modeled as:",
            "$$\np(\\mathbf{y}|\\mathbf{x}_{1:n})=\\prod_{i=1}^{m}p(\\mathbf{x}_{n+i}|\\mathbf{x}_{1:n+i-1})\n$$",
            "where $m$ denotes the total length of the generated sequence. Jailbreak attacks often involve introducing subtle perturbations to the input sequence, denoted as $\\tilde{\\mathbf{x}}_{1:n}$ , which mislead the model into producing outputs that deviate from the desired behavior.",
            "The impact of a jailbreak attck is evaluated through its effect on the alignment reward $\\mathcal{R}^{*}(\\mathbf{y}|\\mathbf{x}_{1:n},\\mathcal{A})$ , which measures how closely the model's output aligns with a set of human-defined safety or ethical guidelines, denoted as $\\mathcal{A}$ . The adversary's goal is to minimize this reward, formalized as:",
            "$$\n\\mathbf{y}^{\\star}=\\underset{\\mathbf{y}}{\\arg\\operatorname*{min}}\\mathcal{R}^{*}(\\mathbf{y}|\\tilde{\\mathbf{x}}_{1:n},\\mathcal{A}))\n$$",
            "![](images/0fe60b759406b2fa215769fabc5fef818b984a110e0c85dad5e1f83f2b8a40ac.jpg)",
            "Figure 18.1: Agent Intrinsic Safety: Threats on LLM Brain.",
            "where $\\mathbf{y}^{\\star}$ is the worst-case output induced by the perturbed input.The corresponding adversariallossfunction quantifies the likelihood of generating this output:",
            "$$\n\\mathcal{L}^{a d v}(\\tilde{\\mathbf{x}}_{1:n})=-\\log p(\\mathbf{y}^{\\star}|\\tilde{\\mathbf{x}}_{1:n}),\\mathrm{~and~}\\tilde{\\mathbf{x}}_{1:n}=\\operatorname*{argmin}_{\\tilde{\\mathbf{x}}_{1:n}\\in\\mathcal{T}(\\hat{\\mathbf{x}}_{1:n})}\\mathcal{L}^{a d v}(\\tilde{\\mathbf{x}}_{1:n})\n$$",
            "where $p(\\mathbf{y}^{\\star}|\\tilde{\\mathbf{x}}_{1:n})$ denotes the probability assigned to the jailbreak output and $\\mathcal{T}(\\hat{\\mathbf{x}}_{1:n})$ is the distribution or set of possible jailbreak instructions.",
            "![](images/f37e2c7193e4034737296e8523563ec3192ca19a726322dc1a159dd6918a3200.jpg)",
            "Figure18.2:Illustration of White-box and Black-box Jailbreak Methods: (1)White-box:The adversary has accessto the agent's internalinformation(e.ggradients,attention,logits),allowing precise manipulations such as advrsarial suffx optimization. (2)Black-box: The adversary relies solely on input-output interactions.Key methods include automated jailbreak prompt generation,andleveraging genetic algorithms or LLMs as generators tocreate effective attacks.",
            "As shown in Figure18.2jailbreaks canbebroadlyclasified into white-box andblack-box methods,depending on the adversary'saccess to themodel's internal parameters.(1)White-box Jailbreaks:These attcks assumethe adversary has full accessto the model's internal information, such as weights, gradients,atention mechanisms, and logits. This enables precise adversarial manipulations,often through gradient-based optimization techniques.(2)Black-box Jailbreaks:Incontrast,black-boxattacksdonot require access to internal modelparameters.Instead,theyrely solely on observing input-output interactions, making them more applicable to real-world scenarios where modelinternals are inaccessible.",
            "White-box Jailbreak.White-boxattacks exploit access to an AIagent's nternalparameters,suchas modelweights and attntion mechanisms,enabling precise manipulations.Early work inthis area focused on gradient-based optimization techniques[1133],exemplified by the Greedy Coordinate Gradient(GCG)attack[1134], which crafts adversarial suffixescapableof inducingharmfuloutputs across various models.Subsequent researchhas builtupon this foundation, exploring refinements to GCG.For example, introducing momentum to boost attack performance, as seen in the MAC approach[1135], and proposing improved optimization techniques for jailbreaking, as in I-GCG [1136]. Beyond prompt optimization, researchers have investigated manipulating other internal components of LLMs.Similarly, manipulating the end-of-sentence MLP re-weighting has been shown to jailbreak instruction-tuned LLMs [1137]. Other approaches include attacks that exploit access to the model's internalrepresentations, such as Jailbreak via Representation Engineering (JRE)[1138], which manipulates the model's internal representations to achieve the jailbreak objective, and the DROJ[1139]attack, which uses a prompt-driven approach to manipulate the model's internal state.AutoDAN[1140]automates the generation of stealthy jailbreak prompts.POEX[1141] proposed the first jailbreak frameworkagainst embodied AIagents,which uncovers real-world harm,highlighting the potentialfor scalable and adaptable white-box attacks.",
            "Black-box Jailbreak.Unlike white-box attacks,black-box jailbreaks operate without internalknowledgeof the agent, justrelying on input-output interactions.Prompt engineering is acritical approach, where carefully designed prompts are employed to exploit the model'sresponse generation capabilities and bypassits safety mechanisms[142].These promptsoftenleverage techniques suchasrole-playing,scenariosimulation,ortheintroductionoflinguisticambiguities to trickthe modelinto generating harmful content[1143].Furthermore, automated prompt generation methods have emerged,employingalgorithmslikegenetic algorithms ofuzzing tosystematicallydiscovereffctive jailbreak prompts [1234].Inaddition,multi-turnattacksexploittheconversationalcapabilitiesofMs,graduallysteeringthedialogue towards unsafe territory through a series of carefully crafted prompts [ll46].Other notable approaches include exploiting the model's susceptibility to specifictypes ofcipher prompts[ll44]and utilizing multimodalinputs,such as images, to trigger unintended behaviors and bypass safety filters[1145,l147,1148].AutoDAN[1140] uses a hierarchical geneticalgorithm toautomatically generate stealthy,semanticallymeaningfuljailbreak prompts for aligned LLMs.POEX[141]alsoshowcases thefeasibilityof transferring white-boxoptimized jailbreak prompts toblack-box LLMs.",
            "Mitigation.Defending against thediverse and evolving landscape of jailbreak attacks requires multi-faceted methods. System-level defenses offr a promising avenue,focusing on creating a secure environment around the LLM rather than solely relying on hardening the modelitself.One keystrategy is inputsanitization andfltering,where incoming prompts are analyzed and potentially modified before being processed by the LLM.This can involve detecting and neutralizing malicious patterns[125],orrewriting prompts toremove potentiallyharmful elements [236].Another crucial aspect is output monitoring and anomaly detection, where the LLM's responses are scrutinized for unsafe or unexpectedcontent.Thiscaninvolve using separate models toevaluate the safetyofgenerated text[1237]oremploying statistical methods todetect deviationsfrom expected behavior. Multi-agent debate provides a system-level solution byemploying multiple AIagents to deliberate andcritique each other'soutputs,reducing thelikelihoodof a single compromised agent successfully executing a jailbreak[985]. Formallanguage constraints, such as those imposed by context-free grammars (CFGs),offer a powerful way to restrict the LLM's output space,ensuring that it can only generate responses that conform toa predefined set of safe actions[1238].Furthermore,system-level monitoringcan be implemented to track the overallbehavior of the LLMdeployment, detecting unusual activity pattrns that might indicate an ongoing attack. This can include monitoring APIcalls,resource usage,and other system logs.Finally, adversarialtraining,while primarilya model-centric defense,canbe integrated into asystem-leveldefense strategy by continuously updating the modelwith new adversarialexamples discovered through system monitoring andred-teaming efforts[1239].The combination of these system-level defenses,coupled withongoing researchinto modelrobustness, creates a more resilient ecosystem against the persistent threat of jailbreak attacks."
          ],
          "raw_title": "Jailbreak Attacks",
          "type": null,
          "children": []
        },
        {
          "title": "18.1.2 Prompt Injection Attacks",
          "number": "18.1.2",
          "level": 3,
          "content": [
            "Prompt injectionatacks manipulate the behavior of LLMs byembeddingmalicious instructions within the input prompt which hijacks the model's intended functionality andredirects it to perform actions desired by the atacker[1130]. Unlike jailbreaks thatbypassafety guidelines,prompt injections exploit the model'sinability todistinguish between the originalcontext andexternallyappended instructions.This vulnerabilityis exacerbatedbytheopen-ended natureof text input,theabsence of robust filteringmechanisms,andthe assumptionthat allinput istrustworthy, making LLMs particularly susceptible to adversarialcontent[149].Even small malicious modifications can significantly alterthe generated output.",
            "Formalization. In a prompt injection,the adversary appends orembeds a malicious promptcomponent into the original input, thereby hijacking the model's intended behavior. Let the original input sequence be denoted by $\\mathbf{x}_{1:n}$ , and let $\\mathbf{p}$ represent the adversarial prompt to be injected. The effective (injected) input becomes: $\\mathbf{x}^{\\prime}=\\mathbf{x}_{1:n}\\oplus\\mathbf{p}$ , where the operator $\\oplus$ denotes concatenation or integration of the malicious prompt with the original input.Then, the autoregressive generation process under the injected prompt is then given by:",
            "$$\np(\\mathbf{y}|\\mathbf{x}^{\\prime})=\\prod_{i=1}^{m}p(\\mathbf{y}_{i}\\mid\\mathbf{x}_{1:n+i-1}^{\\prime})\n$$",
            "Assuming the alignment reward $\\mathcal{R}^{*}(\\cdot,\\mathcal{A})$ measures the extent to which the output adheres to the set of human-defined safety or ethical guidelines $\\mathcal{A}$ , the adversary's goal is to force the model to generate an output that minimizes this reward:",
            "$$\n\\mathbf{y}^{\\star}=\\underset{\\mathbf{y}}{\\arg\\operatorname*{min}}\\ \\mathcal{R}^{*}\\left(\\mathbf{y}\\mid\\mathbf{x}_{1:n}\\oplus\\mathbf{p},\\mathcal{A}\\right).\n$$",
            "Accordingly, the loss function is defined as:",
            "$$\n\\begin{array}{r}{\\mathcal{L}^{i n j e c t}(\\mathbf{p})=-\\log p\\big(\\mathbf{y}^{\\star}\\mid\\mathbf{x}_{1:n}\\oplus\\mathbf{p}\\big).}\\end{array}\n$$",
            "The optimal prompt is then obtained by solving:",
            "$$\n\\mathbf{p}^{\\star}=\\underset{\\mathbf{p}\\in\\mathcal{P}}{\\arg\\operatorname*{min}}\\ \\mathcal{L}^{i n j e c t}(\\mathbf{p})\n$$",
            "where $\\mathcal{P}$ denotes the set of feasible prompt injections.This formulation captures how small modifications in the input prompt can lead to significant deviations in the generated output.",
            "As illustrated in Figure18.3,prompt injectionatackscan be broadlycategorized intodirect and indirect attacks based on how the adversarialinstructions are introduced.(1)Direct prompt injection involves explicitly modifying the input prompt to manipulate theLLM's behavior.(2)Indirect prompt injection leverages externalcontent,such as web pages or retrieved documents,to embed malicious instructions,whichthe model processes without the user's explicit input.",
            "Direct prompt injection. These atacks against AIagents involve adversaries directly modifying the input prompt to manipulate the agent's behavior.Early work established the feasibility of suchatacks,demonstrating that carefully crafted prompts could induce agents to deviatefrom their intended tasks[1149].Subsequent research explored the automation ofthese attacks,revealing the potentialforwidespreadexploitation[l150,l151].Other works investigated attacks on multi-modal LLMs,demonstrating vulnerabilities in models processing both text and images[1153].These studies collectively highlight the evolving threat landscape of direct prompt injection,moving from initial proofs of concept to sophisticated atacks thatcan compromise the integrity and safety of AI agents.Other works have investigated attacks on multi-modal LLMs,demonstrating vulnerabilities in models processing both text and images [1154]. Competitions like the“LLM CTF Competition\"Debenedettiet al.[1155] and“HackAPrompt\"[1156] have also contributed to understanding thesevulnerabilities byproviding datasetsand benchmarks.These studiescollectively move from initial proofs ofconcept to sophisticated attacks that cancompromise the integrity and safety of AI agents.",
            "![](images/9364213871bce17e56a6cffa31c8034a50e5bee0e3d31d4c14f056730c7ddc15.jpg)",
            "Figure18.3:llustrationofDirect and IndirectPrompt InjectionMethods: (1)Direct:Theadversarydirectly manipulates the agent's input prompt with malicious instructionsachievingimmediatecontroloverthe agent'sbehavior.(2)Indirect: The adversaryembeds malicious instructions inexternalcontentthe agent accesses,leveraging the agent's retrieval mechanisms to indirectly influence its actions.",
            "Indirect Prompt Injection. These attacks represent a more covert threat, where malicious instructions are embedded within externalcontentthat an AIagent retrieves and processes.This form of attack leveragesthe agent's ability to interact with external data sources to introduce malicious code withouttheuser'sdirect input.Greshakeet al.[1149] were amongthefrsttohighlight thisvulnerability,demonstratinghowreal-worldLLM-integratedapplications could be compromised throughcontent fetched from the web.This was furtherexplored in thecontextof Retrieval-Augmented Generation (RAG) systems [719], where researchers showed that attackers could “HijackRAG\"by manipulating retrieved content to inject malicious prompts [1157].Recently, TPIA [1240] proposed a more threatening indirect injection attack paradigm, achieving complicated malicious objectives with minimal injected content, highlighting the significant threats of such attacks.Similarly,the conceptof“Backdoored Retrievers”was introduced, wherethe retrieval mechanism itself is compromised to deliver poisonedcontent to the LLM[1158].Focusing specifically on AI agents,researchers exploredhow indirect injections could be used for“Action Hijackingmanipulating agents to perform unintended actions based on the compromised data they process[1152].“Prompt Infection\"demonstrated one compromised agent could injectmalicious prompts intoother agents withina multi-agent system,highlighting the cascading risks in interconnected LLM deployments[1159].Thesestudies underscore the growingconcern surrounding indirect promptinjectionas apotentatack vectoragainst AIagents,particularly asthese agents become moreintegrated with external data sources.Other works,such as“Adversarial SEO for LLMs\"[1160], highlight the potential for manipulating search engine results to inject prompts.",
            "Mitigation.Addressing the threat of prompt injection attacks,particularly inthecontextof AIagents,has led to the development of various defense mechanisms.One early approach involved the use of embedding-based clasifiers to detect prompt injection attacks by analyzing the semantic features of the input[1241]. Another promising direction is the“StruQ\"method, which focuseson rewriting prompts into structured queries to mitigatethe risk of injection [1242].“The Task Shield\"represents asystem-leveldefense that enforces taskalignment,ensuring that agents adhere to theirintended objectives despite potentiall malicious inputs[1243].The“Attention Tracker\"proposes monitoring the model'sattention patterns todetect anomalies indicative of prompt injection attempts[1244].Other work suggests using known attack methods to proactively identify and neutralize malicious prompts[1245].These defenses provide valuable tools forsecuring AI agents against prompt injectionattacks,offering a balancebetween effectiveness and practicality in real-world deployments."
          ],
          "raw_title": "Prompt Injection Attacks",
          "type": null,
          "children": []
        },
        {
          "title": "18.1.3 Hallucination Risks",
          "number": "18.1.3",
          "level": 3,
          "content": [
            "Hallucinationrefers totheLLM'stendency to generateoutputs that are factuallincorrct, nonsensical,ornot grounded in the provided context[l161].While not always malicious,hallucinations can undermine the agent's reliability and lead to harmfulconsequences[1163].As illustrated inFigure 18.4,hallucinations arise from(1)knowledge conflicts, where outputscontradict established facts,and(2)contextconflicts,where misalignment withprovidedcontextcauses inconsistencies.",
            "Formalization. Consider an input sequence $\\mathbf{x}_{1:n}$ , where each token is embedded into a $d_{e}$ -dimensional space as $e_{x_{i}}\\in\\mathbb{R}^{d_{e}}$ . The attention score between tokens $i$ and $j$ is computed as:",
            "$$\nA_{i j}=\\frac{\\exp\\left((\\mathrm{W}_{Q}e_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{j}})\\right)}{\\sum_{t=1}^{n}\\exp\\left((\\mathrm{W}_{Q}e_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{t}})\\right)}\n$$",
            "with the contextual representation of token $i$ given by $\\begin{array}{r}{o_{i}=\\sum_{j=1}^{n}A_{i j}\\cdot(\\mathrm{W}_{V}e_{x_{j}}).\\mathrm{W}_{Q},\\mathrm{W}_{K}\\in\\mathbb{R}^{d_{e}\\times d_{k}}}\\end{array}$ and $\\mathrm{W}_{V}\\in$ $\\mathbb{R}^{d_{e}\\times d_{v}}$ are the query, key,and value projection matrices,respctiely.",
            "Suppose that each input embedding is perturbed by a vector $\\delta_{x_{i}}$ (with $\\|\\delta_{x_{i}}\\|\\leq\\epsilon)$ , resulting in perturbed embeddings $\\tilde{e}_{x_{i}}=e_{x_{i}}+\\delta_{x_{i}}$ . The attention scores under perturbation become:",
            "$$\nA_{i j}^{\\Delta}=\\frac{\\exp\\left((\\mathrm{W}_{Q}\\tilde{e}_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{j}})\\right)}{\\sum_{t=1}^{n}\\exp\\left((\\mathrm{W}_{Q}\\tilde{e}_{x_{i}})^{\\mathrm{T}}(\\mathrm{W}_{K}e_{x_{t}})\\right)}\n$$",
            "and the updated contextual representation is: $\\begin{array}{r}{\\tilde{o}_{i}\\ =\\ \\sum_{j=1}^{n}A_{i j}^{\\Delta}\\cdot(\\mathrm{W}_{V}e_{x_{j}})}\\end{array}$ . To quantify the deviation in internal representations caused by the perturbations with a hallucination metric:",
            "$$\n\\mathcal{H}=\\sum_{i=1}^{n}\\|\\widetilde{o}_{i}-o_{i}\\|^{2}.\n$$",
            "A higher value of $\\mathcal{H}$ indicates that the attention distributions—and hence the contextual representations—have been significantly altered.Such deviationscan lead to errneous token predictions during autoregressve decoding,thereby increasing the likelihood of hallucinated outputs.",
            "![](images/d6f8295943413880a640a472aedd721992b46745498c74da4873c8352627ae2e.jpg)",
            "Figure 18.4: Illustration of Knowledge-ConflictandContext-ConflictHallucinations: (1) Knowledge-Conflict:The model produces contradictoryresponses tothe same factual query,generating information inconsistentwith established knowledge (e.gconflicting statements about the winnerof anelection).(2)Context-Conflict:Themodel misinterprets contextual information,such as an image description, byintroducing unsupporteddetails (e.g.,falsely identifing a surfboard in a beach scene where none exists).",
            "Knowledge-Conflict Hallucination.This arises when an agent generates information that contradicts established facts or its owninternal knowledge base,irespective ofany externalcontext provided during a specific task [1161]. Essntiall,the agent'sresponses are inconsistent withwhat it should“know,\"even ina“closed-book\"settingwhereit relies solelyonits pre-trainedknowledge[l162].These hallucinations,like knowledge-conflict shown in[246],posea severe threattothereliabilityandtrustworthinessof AIagents,astheycanleadtoincorrectdecisions,misinforation andafundamentallackofgrounding inreality[l163].Forinstance,an agent tasked with answering generalknowledge questions might incorrctly state the year ahistorical event occurred orfabricate details about a scientific concept, drawing fromitsflawed internal understanding[1164].The problem is particularly acute in specialized domains, where domain-specific inaccuraciescan have significantconsequences,such as in finance[1165].In multi-agent scenarios, thesekowledge-conflicthallucinationscan beamplified,leading tocascading errorsandabreakdown incollaborative tasks[626].Thecore issue lies in how agents store,process,andretrieve informationduring inference,with inherent limitations in their ability to graspand maintainfactual consistency[l166].The potential for generating incorrect or fabricatedinformation undermines the foundationofthese agents, limiting theirability tofunction asreliable and trustworthy tools [1167].",
            "Context-Conflict Hallucination.This occurs when an agent's output contradicts or is unsupported by the specific context provided during inference,such as a document,image,or set of instructions [168].In these“open-book\" setngs,theagentessentiallmisinterpretsorfabricates informationrelatedtothe givencontext,leadingtooutputs that are detached fromthe immediate reality it is meant tobe processng [l169].This can manifest ina variety of ways, including generating summaries that add details not present in the source text, misidentifying objects in images,or failing tofollow instructions accurately[l17o].For agents equipped with vision capabilities,this can lead to object hallcinations,whereisualinput is fundamentally misinterpreted, posing asignificantrisk inapplications ikerobotics or autonomousdriving[1171,1172].Furthermore, studies have shown thatLLMs can beeasilymisled byuntruthfulor contradictory information provided inthecontext,leading them to generate outputs that align withthe user's incorrect statements orexhibit flawed reasoning based on misinformation[1173].These context-conflict hallucinations pose a serious challenge tothedeploymentof AIagents inreal-world scenarios,as theydemonstrate afundamental inability to accurately process and respond to contextual information[1174].The potentialfor misinterpreting the provided contextcanlead toactions thatare inappropriate, unsafe,orsimplyincoect, undermining the agent'sabilitytofunction effectively in dynamic environments [1175].",
            "Mitigation.Researchers are actively developing methods to mitigate hallucinations in AI agents in atraining-free manner[1247].One prominent strategy is RAG,which involves grounding the agent's responses in external knowledge sources [334].Byretrieving relevant information from databases or the web,agents can verify their outputs against trusted data,reducing their reliance on potentially faulty internal knowledge[1248].Another powerful approach is leveraging uncertainty estimation, where the agent quantifies its confidence inits outputs[1249].Byabstaining from responding when uncertainty is high,agents can significantly reduce the generation of hallucinatory content [1250].Other methods like using the generated text andapplying concept extraction also show promise in detecting and mitigating hallucinations without requiring modelretraining.Yin et al.[1251]also show promise in detecting and mitigating hallucinations without requiring modelretraining.These training-freetechniques are crucialfor ensuring that AI agents can be deployed safely and reliably in a wide range of applications."
          ],
          "raw_title": "Hallucination Risks",
          "type": null,
          "children": []
        },
        {
          "title": "18.1.4 Misalignment Issues",
          "number": "18.1.4",
          "level": 3,
          "content": [
            "Misalignment in AIagents refers to situations where the agent's behavior deviates from the intended goals and values of its developers or users [1252].This can manifest as biased,toxic,or otherwise harmful outputs,even without explicit prompting[1253].As shown in Figure18.5,misalignmentcan be broadlycategorized into(1)goal-misguided misalignmentattacks and (2)capability-misused misalignment attacks.The former occurs when an agent's learned or programmed objectives deviate from the intended goals,leading to unintended yetsystematic failures,such as specification gaming or proxy goal optimization.Thelatter involves exploiting an agent'scapabilities for harmful purposes, often due to vulnerabilities in its design, insuffcient safeguards, or adversarial manipulation.",
            "Formalization. Let $\\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A})$ denote the ideal alignment reward for an output y given input x—i.e.,the reward reflecting perfect adherence to safety and ethical norms—and let $\\mathcal{R}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A})$ be the actual reward observed from the model. The degree of misalignment can be quantified by the absolute discrepancy:",
            "$$\n\\begin{array}{r}{\\Delta_{\\mathrm{align}}(\\mathbf{y},\\mathbf{x})=\\left|\\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\boldsymbol{A})-\\mathcal{R}(\\mathbf{y}\\mid\\mathbf{x},\\boldsymbol{A})\\right|.}\\end{array}\n$$",
            "Ideally, the model should generate the output:",
            "$$\n\\mathbf{y}^{\\star}=\\underset{\\mathbf{y}}{\\arg\\operatorname*{max}}\\ \\mathcal{R}^{*}(\\mathbf{y}\\mid\\mathbf{x},\\mathcal{A}).\n$$",
            "Due to misalignment, the actualoutput y may differ.To incorporate this deviation into the learning or evaluation process, a misalignment loss can be defined as:",
            "$$\n\\begin{array}{r}{\\mathcal{L}^{m i s a l i g n}(\\mathbf{y},\\mathbf{x})=\\lambda\\cdot\\Delta_{\\mathrm{align}}(\\mathbf{y},\\mathbf{x})}\\end{array}\n$$",
            "where $\\lambda$ is a trade-off parameter that adjusts the importance of alignment relative to other factors (e.g.,fluency or task performance).",
            "Goal-Misguided Misalignment. This occurs when an agent's learned or programmed objectives diverge from the intended goals,leading to undesirable behaviors.A fundamentalchallenge is the diffculty in precisely defining complex,real-world goals that agents can understand and reliably execute,particularly in dynamic environments [1176].Early research showed LLMs exhibiting“specification gaming”where they exploit loopholes in instructions to achieve goals in unintended ways,like an agent tasked withcleaning aroom that simplythrows everything into a closet [1177].AsLLMs evolved,subtlerformsemerged,such as pursuing proxygoals that areeasierto achieve but differ from the intendedones[1l78].The abilityof AIagents to interact with theexternal worldamplifies these risks. For example, an agent might prioritizeengagement over accuracy,generating misleading information toelicit astrong response[l179].Translating complex human values into machine-understandable objectives remains a significant hurdle[l176].Moreover,fne-tuning can inadvertentlycompromise oreven backfire safety alignment efforts[1180], and goal misalignment can worsen in dynamic setings where agents struggle to adapt tochanging social norms [921]. Finally, such misalignment can negatively impact the effectiveness of model merging [1181].",
            "![](images/6623cd57ff31355dc1875ce8a8797482d9a73fe8a2c8a42bf5f6cebab87e0edd.jpg)",
            "Figure 18.5:Ilustration of Goal-Misguided and Capability-Misused Misalignment: (1)Goal-Misguided Misalignment: Occurs when anagent's learned or programmedobjectives diverge from intended goals,leading tounintendedbehaviors. (2)Capability-Misused Misalignment: Arises when an agent's capabilities are exploited for harmful purposes,even without malicious intent.",
            "Capability-Misused Misalignment.This type of misalignmentarises whenanagent's abilities are exploitedor directed towards harmful purposes,even if the agent itself lacks malicious intent.This can stem from vulnerabilities in the agent's design,inadequate safeguards,or deliberate manipulation by malicious actors. Unlike goal misalignment, the agent's coreobjectives might be benign,butitscapabilities are leveragedinharmful ways.Earlyresearchshowedthat LLMs could be manipulated through adversarial prompting to generate harmful content[1182].The integration of LLMs into agent architectures has expanded the potentialformisuse, with safety alignment proving fragile and easily attacked[l183].Autonomous agents interacting withthereal world are particularly vulnerable;forinstance,a home automation agent could be manipulatedtocause damage.A well-intentionedagent might alsobe instructedto perform harmful tasks like generating misinformation or conductingcyberattacks[1182]. Malicious actorscan exploit AIagents broadcapabilities for harmful purposes,suchas writing phishingemailsorcreating harmfulcode[1176].Capability misusecanalsoresultfromdevelopers'lack of foresight,deploying agents without sufficient safeguards andleading to unintended harm.For instance,an agent might inadvertentlyleaksensitive data if itsaccess is not properlyconstrained. Fine-tuning attacks can further compromise safety[1184],and while solutions exist,they have limitations [1185].",
            "Mitigation. Addressing misalignment requires a multi-faceted approach.While retraining is common, training-free mitigation methodsoferavaluable alternative,especiallfordeployed systems.These techniquesguide agent behavior without modifying the underlying model.“Prompt enginering\" involves crafting prompts that emphasize safety and ethical considerations[1254]. Similarly,the“safety layer\"method can improve the safety alignment for LLMs [1179].“Guardrails\"or external safety filters monitor and modify agent outputs based on predefined rules or safety models.“Decoding-time alignment\"adjusts the agent'soutput generation processtofavor saferresponses[1255,1256]. Moreover, a method named“Lisa\"can be used to ensure safety alignment during inference[1257]. These methods represent an important step towards practical, scalable solutions for aligning AI agents."
          ],
          "raw_title": "Misalignment Issues",
          "type": null,
          "children": []
        },
        {
          "title": "18.1.5 Poisoning Attacks",
          "number": "18.1.5",
          "level": 3,
          "content": [
            "Poisoning atacks compromise LLMs by introducing malicious data during training or runtime, which subtly alters their behavior.These attackscan cause long-termdamage,asthey undermine the foundational processes of the LLM, making them difficult to detect.",
            "Formalization. Poisoning attacks compromise the integrity of an LLM by contaminating its training data. Let the daset liii $\\mathbfcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$ vesar itrodue rae $\\delta_{i}$ to a frtin of $\\tilde{\\mathcal{D}}=\\{(\\mathbf{x}_{i}+\\delta_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$",
            "During training, the model parameters $\\theta$ are learned by minimizing the loss function $\\mathcal{L}$ over the poisoned dataset:",
            "$$\n\\theta^{\\star}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}\\big(\\tilde{D};\\theta\\big).\n$$",
            "The impact of poisoning is captured by the deviation of the poisoned model parameters $\\theta^{\\star}$ from the clean parameters $\\theta_{\\mathrm{clean}}$ , which would be obtained using the clean dataset $\\Delta_{\\theta}^{\\mathrm{~~}}=\\lVert\\theta^{\\star}-\\theta_{\\mathrm{clean}}\\rVert$ . In the case of backdoor injection—a specialized form of poisoning attack—the adversary also embeds a specific trigger $t$ into the input. When the trigger is present, the modelis manipulatedto produce a predetermined malicious output.The successof such an attackcan be quantified by:",
            "$$\n\\mathcal{B}(t)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{X}}\\left[\\mathbb{I}\\{f(\\mathbf{x}\\oplus t;\\theta^{\\star})\\in\\mathcal{V}_{\\mathrm{malicious}}\\}\\right]\n$$",
            "where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function and $\\mathcal{N}_{\\mathrm{malicious}}$ represents the set of undesirable outputs.",
            "As shown in Figure 18.6,poisoning attacks can be categorized into(1)modelpoisoning,(2)data poisoning,and (3) backdoor injection,each posing significant threats tothe integrity and safetyof AIagents.Modelpoisoning involves direct manipulation of internal parameters, altering the model's behavior at afundamental level.Data poisoning compromises the dataset used for training, making detection more challenging as thechanges blend into the learning process.Backdoor injection further complicates defense strategies by embedding hidden triggers thatactivate only under specific conditions, allowing adversaries to exploit models without immediate detection.",
            "Model Poisoning.This technique directly manipulates the internal parameters ofthe AI agents, such as weights or biases,leading to incorrect outputs orunintended behaviors[l86]whichallowsattackers to introduce specific vulnerabilities that remain dormant until triggered by certain inputs [1187].Techniques like Low-Rank Adaptation (LoRA),meant foreffcient updates,can also be exploited to inject maliciouschanges[1188], which are also seen in parameter-effcient fine-tuning (PEFT)[1189]. Research has demonstrated that poisoned models can introduce safety flaws incode[l190],and potentiallycollaboratewithother poisoned agents,amplifyingthe attack's impact [1191].Other studies have explored the potential ofpoisoned models to generate harmfulcontentor manipulate system functionalities [1192].",
            "![](images/bd838b1520e97b15bd75c65f0f5a36a105244b00b8368062fb3c37805ea393b0.jpg)",
            "Figure 18.6:Ilustrationof ModelPoisoning andDataPoisoning: (1)ModelPoisoning:Theatacker injectsabackdoor into themodelby manipulatingkey-aluerepresentations inthetransformerdecoder,embedding ahidden trigger-target mapping.(2)Data Poisoning:The atacker manipulates training data throughadversarialtrigger optimization,injecting poisoned samples that cause the model to learn hidden backdoors,making it susceptible to malicious triggers.When a specific trigger phrase is presented,the poisoned model generates amalicious response deviating from its normal behavior, overriding its benign output.",
            "Data Poisoning.Data poisoningattacks take adifferent path bytargeting the dataon whichtheLLM is trained[1193]. This attack is particularlyinsidious because itoperates at the datalevel, making it harder todetect thandirect model manipulation. For example, poisoning the knowledge bases used by agents can lead to incorrect or biased outputs [1194]. Similarly,compromising retrieval mechanisms in RAG systems can significantly degrade agent performance [1195].Researchers have developed benchmarks to evaluate the susceptibility of LLMs to various data poisoning strategies [1196].Moreover,even user feedback, intended to improve model performance,can be manipulated to introduce biases[l197].Studies havealsoexploredtherelationshipbetweenthescaleof themodelanditsvulnerablity to data poisoning,withfindings suggesting thatlarger modelsmay be more susceptible[1198].Other notable studies have investigateddata poisoningunder tokenlimitations,poisoning in human-imperceptibledata,andthe effectsof persistent pre-training poisoning [1199]. Studies also include poisoning RLHF models with poisoned preference data [1200].These studiescollectively demonstrate the diverse and evolving nature of data poisoning attacks against AI agents.",
            "Backdoor Injection.Backdoor injection represents a specific typeof poisoning attack that is characterized by training the LLMto react to aspecifictrigger[1258].These triggers cause the agent tobehave maliciouslyonly when specific conditions are met, making them diffcult to detect under normal operation. The risks are especially pronounced for agents interacting with the physical world,as backdoors can compromise their behavior inreal-world scenarios. Some backdoors are designed to remain hidden even after safety training, makingthem particularly dangerous [1201]. Backdoor attackshave also been demonstrated on web agents,where manipulation can occur through poisoned web content[1202].Furthermore,researchhas examined the impact ofbackdoors ondecision-making processes,showing how they canlead to incorect orharmfuldecisions[1203].Other studies have provided detailed analyses of various backdoor attack methods,including those that leverage model-generated explanations,cros-lingual triggers, and chain-of-thought prompting[1204].Additionalinvestigations have explored the persistence of backdoors,he useof virtual prompt injection,andthechallenges of mitigating these threats[1205].Theseworkshighlightthe sophisticated nature of backdoor attacks and emphasize the ongoing arms race betweenattackers anddefenders in the realmof AI agent safety.",
            "Mitigation. Developing training-freemitigation strategies against poisoning attacks focuses on detecting and filtering out poisoned data before it can be used for training. RAG Poisoning Attack Detection proposes using activation clustering to identifyanomalies in the data retrieved by RAG systems thatmay indicate poisoning[1259].BEAT[1260] proposed the first black-boxbackdoor inputs detection against backdoor unalignment attacks under LLMaaSsettings by leveraging the probeconcatenate eect.Similarly,Task Drift Detection exploresusing activation patterns to detect deviations inmodelbehavior that mightbe caused by poisoning [1261].Liet al.[1262]involves leveraging the model's ownreasoning processtoidentifandneutralizebackdoortriggers,suchasthe multi-stepverificationprocess described by Chain-of-Scrutiny todetect and filter outpoisoned outputs.Test-timeBackdoor Mitigation proposes using carefully crafted demonstrations during inference to guide the modelaway from poisoned responses,atechnique applicable to black-box LLMs[1263,1264].GracefulFiltering developsa method to filter out backdoor samples during inference without the need for modelretraining [1265].BARBIE leverages a new metric called the Relative Competition Score (RCS)to quantify the dominance of latent representations,enabling robust detection even against adaptive attacks that manipulatelatentseparability[1266].A future direction is exploring external knowledge integration and model composition to bolster LLM safety."
          ],
          "raw_title": "Poisoning Attacks",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "18.2 Privacy Concerns",
      "number": "18.2",
      "level": 2,
      "content": [
        "Privacy threats on AIagents primarily stem from theirreliance on extensive datasets and real-time user interactions introduce significant privacy threats.These risks primarily stem from two sources: Training Data Inference, where atackers attempt toextract orinfer sensitiveinformationfromthe agent'straining data,andInteractionData Inference, where system and user prompts are vulnerable to leakage. Without efective safeguards,these threats can compromise data confidentiality, expose proprietary agent knowledge, and violate privacy regulations."
      ],
      "raw_title": "Privacy Concerns",
      "type": null,
      "children": [
        {
          "title": "18.2.1 Inference of Training Data",
          "number": "18.2.1",
          "level": 3,
          "content": [
            "AI agentsbuild their knowledge from massivedatasets, making them vulnerable toattacks that expose confidential training data.Asillustratedin Figure18.7,these attackscanbebroadlyclasified intotwocategories: (1)membership inference and (2) data extraction.",
            "![](images/009fdf6f04938b3f5875e84ab4440531d6f85572200a4c06852da79fc9e33435.jpg)",
            "Figure 18.7:lustration of MembershipInference and Data Extraction Attack Methods: (1) Membership Inference: The adversaryattempts todetermine ifa specificdata point wasused inthe agent's training set,often byanalyzing subtle variations inthe agent'sconfidence scores.(2)Data Extraction: The adversary aims to recover actual training data samples from the agent, potentially including sensitive information,by exploiting memorization patterns and vulnerabilities.",
            "Membeship Inference Attack. Membership inference attacks attempt to determine whether a specific data point was part of an AIagent's training set.Forexample,an atacker may tryto verify whether apatient's medicalrecord was included in the training data of a healthcare chatbot.",
            "Let the training dataset be: $\\mathcal{D}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$ Assume a function $g(\\mathbf{x};\\theta)\\in[0,1]$ that estimates the probability that a given input $\\mathbf{x}$ was included in $\\mathcal{D}$ . An adversary may infer membership by checking whether $g(\\mathbf{x};\\theta)>\\eta$ ,where $\\eta$ is a predetermined threshold. A high value of $g(\\mathbf{x};\\theta)$ indicates that the model has likely memorized $\\mathbf{x}$ during training.",
            "Early research by MIA[1206] demonstrated the feasibility of these atacks in machine learning models.Carlini et al.[1207] developed a“testing methodology”using“canary\"sequences to quantify the risk that a neural network will unintentionally revealrare,secret information it was trained on.Recent advancements have improved attack effectivenessFor instance, Choquette et al.[1208]leverage Label-only membership inference atacks leverage linear probing and internal modelstates to enhance inference accuracy. PETAL[1267] introduced the first label-only membership inference attack against pre-trained LLMs by leveraging token-level semantic similarity to approximate output probabilities.Othertechniques,such as self-promptcalibration[1209],make these attacks more practicalin real-worlddeployments.MIA[1210]developed a new, more powerful attck (LiRA)to test for“membership inference,\" whichis when someonecanfigure out if aparticular person's datawasused to train a machine learning model,even if they only seethe model'spredictions.He et al.[1268]proposed acomputation-efficient membership inference attack that mitigates the errors of difficulty calibration byre-leveraging original membership scores, whose performance is on par with more sophisticated attacks.Additionally, Huetal.[121lreviews andclassfies existing researchon membership inference atacks on machine learning models,offering insights into both attack and defense strategies.",
            "Data ExtractionAtack.Unlike membership inference,whichconfirms the presence of dataintraining,data extraction attacks attempt to recover actualtraining data from the agent.Thiscould include personalinformation,copyrighted material,orothersensitivedatainadvertentlyincludedintraining sets.Theadversaryatempts toreconstructatraining example by solving:",
            "$$\n\\mathbf{x}^{\\star}=\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\arg\\operatorname*{max}}~p\\big(\\mathbf{x}~|~f(\\mathbf{x};\\theta)\\big)\n$$",
            "vhere $f(\\cdot;\\theta)$ denotes the model's response given input $\\mathbf{x}$ , and $p\\big(\\mathbf{x}\\mid f(\\mathbf{x};\\theta)\\big)$ represents the likelihood that $\\mathbf{x}$ has been nemorized. A higher likelihood implies a greater risk of sensitive data leakage.",
            "Early researchbyCarliniet al.[212]providedfoundationalevidence that AIagentscanregurgitate training data under specific conditions.Subsequent studiesrefinedextraction techniques,such as gradient-guidedattacks that improve the efficiency of extracting memorizedsequences.Other methods,e.g.Baiet al.[1213],exploit prompt manipulation to trigger unintended dataleaks.Ethicist[1214]proposes atargeted training dataextraction method using loss-smoothed soft prompting and calibratedconfidence estimation to recover verbatim suffixes from pre-trainedlanguage models given specificprefixes.Modelinversionatackshaveeven allowedatackers toreconstructlarge portions oftraining data froman AIagent'sresponses[1215].Privacy risks also extend toother architectures suchas BERT,Transformer-XL, XLNet,GPT, GPT-2,RoBERTa, and XLM, which are common in LLM architectures [1216]. Carlini et al.[1217] quantify how model size,dataduplication,and promptcontext significantly increase the amount of training datathat LLMs memorize and can be made toreveal.Carlini et al.[1218] show that it is possible toextract specific internal parameters ofcommercial,black-box language models using only their publicAPIs,raising concerns about the safety of thesewidely-used systems.More et al.[1219]show that existing methods underestimate the risk of“extraction attacks\"on language models because real-world atackers can exploit prompt sensitivity and access multiple model versions toreveal significantly more training data.Sakarvadia etal.[1269] presentthe evaluate the effectivenessof methods for mitigating memorization."
          ],
          "raw_title": "Inference of Training Data",
          "type": null,
          "children": []
        },
        {
          "title": "18.2.2 Inference of Interaction Data",
          "number": "18.2.2",
          "level": 3,
          "content": [
            "Unlike traditional software,AIagents are guidedby naturallanguage instructions,known as prompts.As demonstrated in Figure 18.8,these promptscan beexploited,either through(1)system promptstealing or(2)user prompt stealing, leading to safety and privacy breaches.",
            "Formalizaiton. Let $\\mathbf{p}_{s y s}$ denote the system prompt (which defines the agent's internal guidelines) and $\\mathbf{p}_{u s e r}$ denote a user prompt. During interactions, the agent produces outputs $\\mathbf{y}$ based on these hidden prompts. An adversary may attempt to reconstruct these prompts by solving an inversion problem:",
            "$$\n\\mathbf{p}^{\\star}=\\underset{\\mathbf{p}}{\\arg\\operatorname*{max}}~p\\big(\\mathbf{p}~|~\\mathbf{y};\\theta\\big)\n$$",
            "where $p(\\mathbf{p}\\mid\\mathbf{y};\\theta)$ represents the probability that the hidden prompt $\\mathbf{p}$ (system or user) is responsible for the observed output $\\mathbf{y}$ .By optimizing Equation (18.17),an atacker can reconstruct sensitive context that influences the agent's behavior.",
            "System Prompt Stealing. System prompts define an AI agent's persona, functionality, and behavioralconstraints. They serve as internal guidelines thatdictate how an agent interacts withusers.Stealing these prompts alows attackers to reverse-engineer the agent's logic,replicateits functionality,orexploit weaknesses.Early work,such as[122], demonstrated how prompt stealing applies eventothe intelectual propertyoftext-to-image generative systems.While Jiang et al.[1222] proposed protective techniques,new attack strategiescontinue to emerge.Perezet al.[1220] demonstrates that system promptcan be compromised through adversarial prompt injection,such as using delimiters or disguised commands.Timing side-channel atacks, such as InputSnatch[1223] uncovers caching techniques in LLM inferencecreate atiming side-channelthat allows atackers to reconstruct usersprivate inputs.Zhang et al.[1224] demonstrates that system prompts ofproduction LLMs (e.g.,Claude,Bing Chat)can be extracted via translation-based attacks and other query strategies,bypassing defenses like output filtering,withhighsuccessrates across1l models. Wen et al.[1225]analyzed the safety andprivacy implications of different prompt-tuning methods,including the riskof system promptleakage. Zhaoet al.[1226]identif safety and privacyanalysis as acrucialresearcharea,encompassing potential threats like system prompt leakage within the app ecosystem.",
            "![](images/d734a5bbdf58ca5621b35f0f355da622390af25e9f98e6b0b5062673aff34796.jpg)",
            "Figure 18.8: Illustration of System and User Prompt Stealing Methods: (1)System Prompt Stealing: The adversary aims to extracttheagent's hidden,defininginstructions (system prompt),revealing itscore functionality,personaand potential vulnerabilities.(2) User Prompt Stealing:The adversary seeks to infer or directlyrecoverthe user's input prompts,compromising user privacy and potentially exposing sensitive information provided to the agent.",
            "User Prompt Stealing. Beyond system prompts, user prompts are also vulnerable. Atackers can infer or extract sensitive user inputs,compromising privacy. If a user queries an AI agent with confidential business strategies or personal medical concerns,an atacker could reconstruct these inputs from model responses.Yang et al.[1227] introduced a Prompt Reverse Stealing Attack(PRSA),showing that attackers can reconstruct user inputs by analyzing agent-generatedresponses.Agrwalet al.[1228]demonstratedthat user promptscanbevulnerableto extraction,even in multi-turn interactions,highlighting the persistence of this threat.Agrwal etal.[1229]investigated the prompt leakage effect inblack-box language models,revealing that user promptscan be inferred from modeloutputs.Liang et al.[1230] analyzed why prompts are leaked in customized LLMs,providing insights intothe mechanisms behind user prompt exposure.Huiet al.[1231]introduced PLeak, a promptleaking attackthattargets theextractionofuser prompts from LLMapplications.Yona et al.[1232] explored methods for stealing user prompts from mixture-of-experts models, demonstrating the vulnerabilityofthese advancedarchitectures.Zhang et al.[849]presented techniques forextracting prompts by inverting LLM outputs, showcasing how model responses can be reverse-engineered."
          ],
          "raw_title": "Inference of Interaction Data",
          "type": null,
          "children": []
        },
        {
          "title": "18.2.3 Privacy Threats Mitigation",
          "number": "18.2.3",
          "level": 3,
          "content": [
            "To address privacy threats in AI agents, researchers have developed privacy-preserving computation and machine unleaming techniques to protect sensitive data without compromising utility.Diffrential Privacy (DP)introduces carefully calibrated noise into the training processor model outputs to prevent individual data points from being inferred[1270].DP has been successfully adapted for fine-tuning LLMs,employing techniques such as gradient clipping and noise injection at diferent stages, including during optimization and user-level interactions [1271]. Another promising direction is Federated Learming (FL),e.g.,FICAL is a privacy-preserving FL method for training AI agents that transmits summarizedknowledge instead of model parameters or raw data,addressing communication and computational challenges[1272]. Recent studies have explored FL-based fine-tuning of AI agents, enabling collaborative modelimprovement acrossdiffrent entities without directdata sharing[273].Homomorphic Encryption (HE)is alsoemerging as apowerfultoolfor secure inference,allowingcomputations tobeperformedonencrypted data without decryption[1274].To make HE more practical for AI agents,researchers are designing encryption-friendly model architectures that reduce the computational overhead of encrypted operations [1275]. For hardware-based solutions,Trusted Execution Environments (TEEs)ofera secure enclave where computations can be isolated fromthe rest of the system,protecting sensitivedata and model parameters[1276].Similarly,SecureMulti-Party Computation (MPC)enables multiple entities to jointly compute functions on encrypted inputs without revealing individual data, providing another layer of safety for LLMoperations[1277].Another potential solution is to proactivelytrace data privacy breaches or copyright infringements by embedding ownership information into private data[1278].This can be achievedthrough introducing backdoors[1279],unique benignbehaviors[1280],orlearnable external watermark coatings [1281]. Complementing these approaches is the growing field of Machine Unlearming,which aims toremove specific training data froman AI agent's memory,efectively implementing a“right to be forgoten\"[1282,1283]. Recent research has developed LLM-specific unleaming techniques,including adaptive prompt tuning and parameter editing,to selectivelyerase unwanted knowledge while minimizing the impact on model performance[1284,1285].",
            "Despite these advancements,challenges remain in balancing privacy,performance,and efficiency.Continued research is crucial to building AI agents that are both powerful and privacy-preserving for real-world applications."
          ],
          "raw_title": "Privacy Threats Mitigation",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "18.3 Summary and Discussion",
      "number": "18.3",
      "level": 2,
      "content": [
        "The above sections have meticulously detaileda spectrum of safety andprivacy threats targeting thecore of AIagentsthe“brain\"(LLM).From jailbreaks and prompt injection tohalucinations, misalignments, and poisoning attacks,it is evident that the LLM'scentralrole indecision-making makes it a prime targetforadversaries.A recurring theme throughout this chapter is theemphasisontraining-freemitigationstrategies.Manyofthe defenses presented,suchas input sanitizationandfiltering for jailbreaks[125,286],uncertaintyestimationforhallucinations[249]andsafety layers for misalignment[l179],are crucial because theyare practical,scalable, adaptable, and often model-agnostic. Retraining large models iscostly; training-free methodscan be appied post-deployment and offer flexibility against evolving threats.",
        "However,apurelyreactive approach is insuficient.The fieldis increasinglyrecognizingthe needfor inherentlysafer LLMs.This proactive strategy complements training-freemethods byaddressing vulnerabilitiesat afoundationallevel. For instance,modelpoisoning mitigation,lke activation clustering in RAG poisoning atack detection[1259],not only mitigates immediate threats but also informs thedesign of more robust raining processes.Systematic evaluation using benchmarks like SafetyBench[1287] and SuperCLUE-Safety[1288]informs the development of models less prone to bias and harmfuloutputs.Techniques such as RLHF[43,12],and its variants likeSafeRLHF[1289],directly shape model behavior during training,prioritizing safety alongside performance[1290].Prompt engineering[1291,292] and parameter manipulation[1293]enhance robustnessagainst adversarial attacks,creating models thatare inherently less susceptible to misalignment.",
        "Importantly, while the term“jailbreak\"often emphasizes bypassing safety guardrails,the underlying mechanisms bearstrong resemblance to adversarialatacks morebroadly:inbothcases, inputs are crafted to induce undesiredor harmful outputs.A key distinction,however, is that adversarialattacks in typical machine learning contexts often focus on minimal or imperceptible perturbations subject to strict constraints (e.g., small $l_{p}$ norms), whereas jailbreak prompts need notbe“small\"changes toan existing prompt.Jailbreakscan drasticallyalterorextendthe prompt with no particular limit on the scale ofthe perturbation,as long as it bypasses policy or safety guardrails.Under specific conditions—such as whensafetyconstraints are formulated as a sort of“decision boundary\"—these two atack vectors become effectively equivalent.Yet, inreal-world LLM scenarios,the unconstrainednature of jailbreak inputs can pose a diffrent, andoftenbroader, practicalthreat model.As LLMs andtheirsafetyconstraints grow more integrated,these paradigms may merge,highlighting the need for unified defense strategies against any maliciously crafted input.",
        "Adversarialtraining,initiallpresented asa jailbreak mitigation technique[239]exemplifiesthe synergy between reactive and proactive approaches.Continuous exposureto adversarialexamples improves inherentrobustness [1294] Similarly,privacy-preserving techniqueslike diffrential privacy and federated learning [1270,1295],originally discussedformitigating privacythreats,fundamentallalterthetraining process,leading toa morerobust and privacyaware LLM brain."
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": []
    },
    {
      "title": "Agent Intrinsic Safety: Threats on Non-Brain Modules",
      "number": "",
      "level": 1,
      "content": [
        "The safety of an AIagent extends beyond thecore LLMtoits peripheral modules, including the perception and action modules.Although theLLM brain provides core intelligence, vulnerabilities inthe other modules can significantly undermine theentire agent'srobustness.These components act as interfaces, alowing the AIagent to perceive the world and execute actions within it, making them prime targets for adversarial attacks."
      ],
      "raw_title": "Agent Intrinsic Safety: Threats on Non-Brain Modules",
      "type": null,
      "children": []
    },
    {
      "title": "19.1  Perception Safety Threats",
      "number": "19.1",
      "level": 2,
      "content": [
        "The perception module of an AI agent iscrucial for processing and interpreting user inputs across various modalities, such a text,images,and audio.However,the complexity and diversity of these modalities make perception systems susceptible to misinterpretations indynamicenvironments[1296],andvulnerabletoadversarialattacksthatmanipulate input data to mislead the agent [1297]."
      ],
      "raw_title": "Perception Safety Threats",
      "type": null,
      "children": [
        {
          "title": "19.1.1 Adversarial Attacks on Perception",
          "number": "19.1.1",
          "level": 3,
          "content": [
            "Adversarialattacks are deliberate atempts todeceive AIagents by altering input datatargeting the perception module across various modalities.From subtle textualtweaks toinaudibleaudio distortions,these atacksrevealthe fragilityof even the most advanced systems.Below,weexplore howthese threats manifest in textual, visual,auditory,andother modalities, and highlight countermeasures.",
            "Textual.Textualadversarialatacks manipulate input text to deceive LLMs,rangingfrom simple sentence alterationsto more complexcharacter-level perturbations.Prompt-based adversarialattack,forinstance,carefullycrafted deceptive prompts that mislead models into generating harmful outputs.Minor changes-like swapping synonyms or substituting characters—can degrade performance[1298].Sophisticated strategies push this further: Zou et al.[1134] generate universal adversarial suffxes using greedyand gradient-based searches, while Wen et al.[1299]optimize interpretable hard prompts tobypasstoken-levelcontent filters in text-to-image models.To defend against these attacks,several approaches have been proposed. For example, Legilimens—a novelcontent moderation system-employs a decoderbased concept probing technique and red-team data augmentationto detect andthwart adversarialinput with impressive accuracy[13o].Self-evaluation techniques enhanceLLMs to scrutinizetheirown outputs for integrity[1301],while methods like adversarial text purification [1302] and TextDefense[1303] harness language models to neutralize perturbations.These defenses ilustrateadynamic armsrace,where resilience is forged throughcreativityand vigilance.",
            "Visual. Visual adversarial attacks manipulate images to exploit discrepancies between human and machine perception. These attacksare particularlyconcerning for multi-modal LLMs(VLMs)that rely on visual inputs.For instance,image hijacks can mislead models into generating unintended behaviors[1304],whiletransferable multimodal attacks can affect both text and visual components of VLMs [1305,1306,1307]. Recent work on multimodal LM robustness shows that targeted adversarial modifications can mislead web agents into executing unintended actions with $5\\%$ pixels manipulation[1308]. Jiet al.[1309] reveal how inaudible perturbations can interfere with the stabilityof cameras and blur the shot images, and lead to harmfulconsequences.Defensive strategies include adversarialtraining [1310,13112]whichinvolves jointtaining withcleanandadversarialimages tomproveobustness,andcerifd robustness methods that guaranteeresilience through the text generationcapabilitiesof VLMs.DIFFender[1313]used diffusion models using feature purification to strengthen VLMs against visual manipulation.",
            "![](images/7f209fc91f59982289f5dbeb939810015ba377835de54d14640bdf8b4d64c349.jpg)",
            "Figure 19.1: Agent Intrinsic Safety: Threats on LLM Non-Brains.",
            "Auditory.For voice-controlled AIagents,auditory adversarialatacks pose astealthy threat.DolphinAttack[1318] introduces an innovative techniquethat leverages ultrasound to inject malicious voice commands into microphones in an inaudible manner.Also, inaudible perturbations like VRifle[1297]can mislead traditional speech recognition systems and can likely be adapted to target audio-language models.Deepfake audio and adversarial voiceprint further pose serious risks for authentication-based systems[1316,317,1335],while emerging jailbreak and chataudio attacks exploitaudio processing vulnerabilities[1336].To mitigate these threats,solutions likeEarAray use acoustic attenuation tofilter inaudible perturbations [1337],while SpeechGuard enhances LLMrobustnessthrough adversarial training [1338]. Moreover, NormDetect[1339] focuses on effectively detecting normal speech patterns from manipulated inputs.",
            "Other Modality.Beyond text, images, and audio, AI agents interfacing with sensor data—like in autonomous systems—face unique threats.For example, LiDAR manipulation can mislead autonomous driving systems,creating phantomobjects [1319].Research on adversarial attacks in multi-agent systems reveals thattampered messages can significantly degrade multi-view object detection and LiDAR-based perceptionin cooperative AIagents,highlighting the risk of sensor-based adversarial perturbations[1320].Similarlyatacks targeting gyroscopes or GPS spoofing can disrupt navigation systems[1321,1322].Defenses forthese attacks include robust sensor fusion algorithms and anomalydetection techniques toidentify inconsistencies,aswellasredundant sensors thatmake ithardertocompromise the entire system[1340]. Physicallayer defenses, such as shielding and secure localization using enhanced SLAM techniques,arealsocritical[1341].Jiet al.[342]oferarigorous framework forsafeguarding sensor dataintegrity and privacy."
          ],
          "raw_title": "Adversarial Attacks on Perception",
          "type": null,
          "children": []
        },
        {
          "title": "19.1.2 Misperception Issues",
          "number": "19.1.2",
          "level": 3,
          "content": [
            "While adversarialattacks are deliberate atempts tocompromise system integrity, misperception issues emerge intrinsically from thelimitations of LLMs.Theseerrors occur without any malicious intent andcan be atributed toa varietyof factors ranging from dataset biases to architecturalconstraints.One primary source of misperception is dataset bias. When models are trainedon non-representative datasets,they tend to underperform ondiverse or novel inputs [1324]. This shortcoming is exacerbated by challenges in generalizing to new, unseen environments, where unpredictable conditions may arise.Environmentalcomplexities such as sensor noise,occlusions,and fluctuating lighting further introduce uncertainty[1326].Additionall,inherent modellimitations—likerestrictedreceptive fields ortheabsenceof robust reasoning mechanisms- -compound these errors [1327]. Insights from studies on multi-agent systems and online social dynamics provide further depth to our understanding of misperception. Research shows that individuals may misjudge the true distribution ofopinionsdue tophenomena like false consensus efects,vocalminorityamplification, and the spiralof silence[1328].Suchbiasescanlead AIagentstoerroneouslyinferdominant perspectives from skewed inputs.Similarly,whendiffrent modelsshare visualfeatures,discrepancies infeatureencodingcanresult insignificant perception errors,achallenge that mirrors issues inmulti-modalLLMs[1329].Moreover, in interactive enviroments, agents may developdistorted interpretationsofcooperative and adversarialbehaviors,as evidenced by findings in multi-agent reinforcement learning [1330]. Linguistic representation, too,can be influenced by perceptual biases, suggesting that misperception in LLMs may stem not only from sensory inaccuracies but alsofrom language-driven distortions [131].Finall,systematic erors often arise when mismatched confidence levels across models affect decision-making in uncertain contexts [1332].",
            "Mitigating these misperception challenges requires a multifaceted strategy.Curating diverse andrepresentative datasets thatcapture a broad spectrum ofreal-worldconditions is criticalfor enhancing modelperformance and reducing bias [1343].Dataaugmentation techniques,which generatesyntheticvariations ofexisting data,canfurtherenrich dataset diversity.Incorporating uncertaintyestimation allows models toassesstheirconfidence inpredictions andflag potential error-prone situations [1344].Moreover,advancing model architectures to include explicit reasoning mechanisms or better processing of long-range dependencies is vital for minimizing misperception[1345]. An especially promising avenue is theadoption of biologicaly inspiredlearming frameworks,such as Adaptive Resonance Theory(ART).Unlike traditional deep learning approaches—often hampered by issues like catastrophic forgeting and opaque decisionmaking-ART models can self-organize stable representations that adapt to dynamically changing environments, thereby reducing perceptualerrors [1346].However,it is important to note that even improved explainability has its limitations, particularly when users struggle to establish clear causal links between modeloutputs and underlying processes[1347]. Furthermore,recent studies indicate that advanced LLMs may inadvertently degrade their own responses during self-correction, underscoring the need for morerobust intrinsic reasoning verification mechanisms [1348]."
          ],
          "raw_title": "Misperception Issues",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "19.2 Action Safety Threats",
      "number": "19.2",
      "level": 2,
      "content": [
        "The action module is responsible fortranslating the AI agent's planned actions into actual task executions.This typicall includes invoking external toolscallng APIs,orinteractingwithphysicaldevices.As the interfacebetween decision-making and execution,it is highly vulnerable toattacks.We explore two primary domains ofrisk: supply chain attacks and vulnerabilities arising from tool usage."
      ],
      "raw_title": "Action Safety Threats",
      "type": null,
      "children": [
        {
          "title": "19.2.1 Supply Chain Attacks",
          "number": "19.2.1",
          "level": 3,
          "content": [
            "Supply chainatacksexploittheservices thatAIagents dependontherebyundermining theintegrityoftheentire system [1333].Unliketraditional attacks,these threatsdo nottarget the agentdirectly but insteadcompromise the extenal resources it relies upon.For example,malicious websitescan employ indirect prompt injection (IPI)attacks—illustrated by the Web-based Indirect Prompt Injection (WIPI)framework—to subtly alter an agent's behavior without needing access toitscode[122].Similarly,adversaries may manipulateweb-based tools(suchasYouTubetranscript plugins)to feedmisleading information into the system[795].AsAIagents become increasingly integrated withonline resources, their attack surface broadens considerably.Recent work byGreshake etal.proposes anewclasificationof indirect injection attacks,dividing them into categories like data theft, worming, and information ecosystem contamination [1149].Complementing this,the InjecAgent benchmark evaluated 30diferent AI agents andrevealed that most are vulnerable to IPI attacks [1152].",
            "To mitigate these risks,preemptive safety measures andcontinuous monitoring are essential.Current research suggests thattwokeyfactors behindthesucessof indirect injection areLLMsinabilitytodistinguish informationcontextfrom actionable instructions andtheir poor awareness of instruction safety;hence,it is proposed toenhance LLMsboundary and safety awareness through multi-round dialogue and in-context learning [1349]. Furthermore,other researchers, based on the same assumption, proposed a prompt engineering technique called“spotlighting\"tohelp LLMs better distinguish between multiple input sources andreduce the success rateof indirect prompt injectionattacks[1350]. Since under asuccessfulattack,thedependence of the agent'snextactionon the usertask decreaseswhile its dependence on the malicious taskincreases,some researchers detect attacks byre-executingthe agent'strajectory with a masked user prompt modified through a masking function[1351].Finally, sandboxing techniques,such as those employed in",
            "ToolEmu[795],create isolated environments for executing external tools,limiting the potential damage incaseofa breach."
          ],
          "raw_title": "Supply Chain Attacks",
          "type": null,
          "children": []
        },
        {
          "title": "19.2.2 Risks in Tool Usage",
          "number": "19.2.2",
          "level": 3,
          "content": [
            "Even when externaltools are secure,vulnerabilitiescan arise fromhow an agent interacts with them.A significantrisk is unauthorized actions,where an adversary manipulates the agent into performing unintended behaviors.For example, prompt injection atackscan trick an agent intosending emails,deletingfiles,orexecuting unauthorized transactions [795].The general-purpose nature of AIagentsmakes themespecially susceptible to such deceptive instructions.The tool leaning processitselfcan introduceadditionalrisks,suchas maliciousqueries,jailbreakatacks,andharmfulhints during the input,execution,and output phases[1334].During the toolexecution phase, using incorrect orrisky tools may deviatefrom the user'sintent and potentiallharm the externalenvironment.Forinstance,misuse couldlead to the introduction of malware or viruses.Acompilationof 18tools thatcouldimpactthephysicalworldhasbeenidentified, with noise intentionally addedto testifLLMscanchoose thewrong tool.Another significant concern isdata leakage, where sensitive information is inadvertentlyexposed.This occurs when an agent unknowinglytransmits confidential data to athird-party API or includes private details in its output. For example, an LLM may inject commands to extract privateuserdata,thenuseexternaltools,ikeaGmailsending tol,todistributethisdata[l52].Therisksare especially pronounced in applications dealing with personal or proprietary data, necesstating stricter controls over information flow.Additionallyexcessve permissons increase the potentialfor misuse.Agents with broad system access couldbemanipulatedto performdestructiveactions,suchasdeletingcriticalfles,leading toieversibledamage [795].Enforcing theprincipleof leastprivilege ensuresthat agents onlyhave thepermissons necessary tocomplete their tasks, minimizing the potentialimpact of exploitation.Securing the action module requires layered protections and continuous monitoring.Monitoring tool usage can helpdetect anomalies before theycause harm,while requiring user confirmation for high-risk actions—such as financialtransactions or system modifications—adds an additional layer of safety.Formalverification techniques,asexplored by[1352],canfurtherenhance safetybyensuring that tool use policies align with best practices, preventing unintended agent behaviors."
          ],
          "raw_title": "Risks in Tool Usage",
          "type": null,
          "children": []
        }
      ]
    },
    {
      "title": "Agent Extrinsic Safety: Interaction Risks",
      "number": "",
      "level": 1,
      "content": [
        "As AI agents evolve and interact with increasinglycomplex environments,the safety risks associated with these interactions have become a criticalconcern.This chapter focuses on AI agent's engagement with memory systems, physical and digitalenvironments,and other agents.These interactions expose AI agents to various vulnerabilities, ranging from memory corruption and environmental manipulation to adversarial behavior in multi-agent systems.By examining these interactionrisks,we aimtohighlightthediversethreatsthatcanunderminetheintegrityandreliability of AIagents inreal-worldapplications.Thefollowing sections explorethese challenges indetail,discussing specific attack vectors and their implications for system safety."
      ],
      "raw_title": "Agent Extrinsic Safety: Interaction Risks",
      "type": null,
      "children": []
    },
    {
      "title": "20.1 Agent-Memory Interaction Threats",
      "number": "20.1",
      "level": 2,
      "content": [
        "The extrinsic memory modulefunctions as the cognitive repositorythat empowers intellgent agents to store,retrieve, andcontextualize information,facilitatingcontinuous learningandtheexecutionofcomplextasks through accumulated experiences. Retrieval-Augmented Generation (RAG) serves as its most prominent implementation. However, RAG frameworks are vulnerableto adversarial manipulations that deceive agents into retrieving and utilizing harmfulor misleading documents.AgentPoison[1194]exploits this vulnerability by executing a backdoor attack on AI agents, poisonig RAG knowledge bases to ensure that backdoor-triggered inputs retrieve malicious demonstrations while maintaining normal performance on benign queries.ConfusedPilot[1353] exposes aclassofRAG system vulnerabilities thatcompromisetheintegrityandconfidentialityof Copilotthrough prompt injectionattacks,retrievalcaching exploits, and misinformation propagation.Specifically, these attacks manipulate the text input fed tothe LLM,causing it to generate outputs that align with adversarialobjectives.PoisonedRAG[1354]represents the first knowledgecorruption attack on RAG,injecting minimal adversarial texts to manipulate LLMoutputs.Framed as an optimization problem, it achieves a $90\\%$ success rate with just five poisoned texts per target question in large databases. Jamming [1355] introduces a denial-of-service attack on RAG systems,where a single adversarial“blocker\"document inserted into an untrusteddatabase disrupts retrievalortriggers safetyrefusals,preventing the system fromanswering specific queries. BadRAG[1356] exposes vulnerabilities in RAG-based LLMs through corpus poisoning,wherein an atacker injects multiplecrafted documents intothedatabase,forcing thesystemtoretrieve adversarialcontent and generate incorrect responses to targeted queries. By introducing just 10 adversarial passages ( $0.04\\%$ of the corpus), it achieves a $98.2\\%$ retrieval success rate, elevating GPT-4's rejection rate from $0.01\\%$ to $74.6\\%$ and its negative response rate from $0.22\\%$ to $72\\%$ . TrojanRAG[1357] executes a joint backdoor attack on RAG systems,optimizing multiple backdoor shortcuts via contrastive learning and enhancing retrieval witha knowledge graph for fine-grained matching.By systematically normalizing backdoor scenarios,it evaluates real-worldrisks andthe potentialfor model jailbreak.Lastly,acovert backdoor attack[1358]leverages grammar errors astriggers,allowing LLMs tofunction normallyfor standard queries while retrieving atacker-controlled content when minor linguistic mistakes are present. This method exploits the sensitivityofdenseretrievers togrammaticalirrgularities usingcontrastiveloss andhardnegative sampling,ensuring that backdoor triggers remain imperceptible while enabling precise adversarial control."
      ],
      "raw_title": "Agent-Memory Interaction Threats",
      "type": null,
      "children": []
    },
    {
      "title": "20.2 Agent-Environment Interaction Threats",
      "number": "20.2",
      "level": 2,
      "content": [
        "Agentscan beclassified intotwocategories based ontheir modeof interaction: physical interaction agents and digital interaction agents.Physicalinteraction agents operate inthereal world, using sensors and actuators to perceive and influence their environment.Examples ofsuch agents include autonomous vehicles androbotic systems.Incontrast, digital interaction agents function within virtual ornetworked environments, processing and responding todata from digital sources. These include AI-powered chatbots,cybersafety systems, and automated trading algorithms.",
        "![](images/d11a581850b36148e9c126fcd33e1ae00bccfc85791be25f232ac74d096e0b50.jpg)",
        "Figure 20.1: Agent Extrinsic Safety:Threats on agent-memory, agent-environment, and agent-agent interactions",
        "Threats in Physical Environment.Agents operating in the physical world,such asrobots and autonomous vehicles, face distinct safety chalenges dueto their interaction with dynamic and potentially adversarial environments [1359, 1360,1366].One major threat issensor spoofing,where atackers manipulate sensor inputs to deceive the agent about its surroundings.For example,GPS spoofingcan pose significantrisks to UAVs (unmanned aerial vehicles)and other GPS-dependent platforms by misleading autonomous vehicles about their actual location.This allowsfor malicious redirection or hijacking [1361].Similarly,LiDAR spofing can introduce false obstacles that don't actuall exist, potntiallyleading tonavigationfailuresor safetyhazards[362].Anothercriticalrisk is actuator manipulation, where adversaries takecontrolofanagent'sactuators,forcing it toperformunintended physicalactions.Thiscanoccurthrough direct tampering with thehardwareorbyexploiting vulnerabilties inthe software that governs actuatorfunctions[1363]. Such attackscan compromise the agent's actions,leading tophysicalharm or mision failure.Aditionally,exploiting environmentalhazards is a serious threat.Attackers may introduce physicalobstacles or manipulate environmental conditions to disrupt an agent'soperations.For example,adversarialobjectscreated using techniques like LiDAR-Adv can deceive LiDAR-based autonomous driving systems byinducing sensor misinterpretations,thus degrading detection reliability and increasing real-world safety risks[1364].Lastly, misalignment in physical actions can undermine the safety of autonomous agents.Discrepancies between an agent's perception andthe actual physical constraints of its environment can lead to unsafe or infeasible actions.For example, mismatches between learned locomotion policies and real-world physics—such as misjudging terrain rigidity or obstacle dimensions—can cause autonomous agents to take hazardous steps (e.g.,unstable strides on roughsurfaces).This has been observedin prior systems thatrequired over 100 manual resets due to uncontrolled falls [1365].",
        "Threats in Digital Environment.Agents operating in digitalenvironments,such as software agents and web-based agents,facedistinct safetychallenges arising fromtheirreliance on externaldata sources andcomputationalresources [1333,1366].One major threat is code injection, where malicious actors introduce harmful code into the agent's environment,leading to unintended command execution[1367].These atacksoften exploit software vulnerabilities or leveragecompromisedexternalresources thatthe agent interacts with,potentiallyresulting in unauthorizedcontrolover the agent'soperations[1202].Environmental Injection Atack (EIA)exploits privacy risks in generalist web agents to stealthily steal users' PII, achieving up to $70\\%$ success rate [137O]. AdvWeb is an automated adversarial prompt generation framework to mislead black-box web agents into executing harmfulactions[1371]. Another critical risk is data manipulation, where attackers alterthe information anagent receives,causing incorrect decisions or actions [1333].Forexample,atrading agent can be misledby manipulated financial data,leading to incorrct transactions,or an information-gathering agent may betricked byfalsifiednews articles,distorting itsoutputs.Such manipulationscan have cascading effects,especially inautomated systems that relyon accurate datafordecision-making.Beyond direct manipulation,denial-of-service(DoS)attcks pose aserious threat by overwhelming the agent'sdigitalenvironment with excessiverequestsordata,effectivelyrendering it unresponsive orcausing it tocrash[368].Thesedisruptionscan be particularlydetrimentalto time-sensitive applications where availability andresponsivenessare critical.Aditionaly, resource exhaustion isasignificant threatas adversaries mayexploitthe agent'sresource management mechanisms to depletecomputational resources,leading to service denial for other usersor overallsystem instability [1369]. By draining processing power, memory,or bandwidth,attackers can severely impair anagent'sability to function effectively,disrupting itsoperations andreducing its efficiency.Inaddressing the safetychallengesofLLM agents, AGrail is proposed asalifelong guardrail frameworkthat enhances agent securitybyadapting safetychecksto mitigate task-specific and systemic risks,demonstrating robust performance and transferability acrossdiverse tasks [1372]."
      ],
      "raw_title": "Agent-Environment Interaction Threats",
      "type": null,
      "children": []
    },
    {
      "title": "20.3 Agent-Agent Interaction Threats",
      "number": "20.3",
      "level": 2,
      "content": [
        "n multi-agent systems,interactionsbetween agentscan introduce new safetyvulnerabilities[1380].These interactions ire mainly competitive, where agents try to outdo each other, or cooperative, where they work together.",
        "Threats in Competitive Interactions.When agents compete,they often use tricky methods to gain an advantage [1373].Forexample,theymight spread false information or make other agents think the situation isdiffrent from reality todeceivethem[1374].This can lead opponents to make poor decisions,weakening their position.Apart from misinformation,agents may alsotrytotake advantage of weaknesses intheiropponent's algorithms or strategies[1375]. By identifying these weaknesses,they can predict and manipulate the other agent's behavior, gaining an edge in the competition.Additionall,some agents might use disruptive techniques likedenial-of-service(DoS)attacks,which overload an opponent's system with unnecessary requests, disrupting communication and hindering their ability to function[1376].Anotherthreat incompetitive interactions iscovert collaboration.Sometimes agents secretlycooperate, even when it's against therules,to manipulate theoutcome intheir favor[1377].Thiskindofcollusionundermines fairness and damages the integrity of the system, as it skews the competition in their favor.",
        "Threats in Cooperative Interactions. In cooperative situations, where agents work together toward a common goal, safetythreatscoulddamage the system's stability andreliability.Oneriskis unintentionalinformationleakage, where agents accidentally share sensitive data during their communication.This could lead to privacy violations or unauthorizedaccess, weakening the system's trustworthiness.Inaddition todataleaks,errors made byone agentcan spread throughout the system,causing bigger failures andlowering overall performance.[1378] discusses this problem in Open-Domain Question Answering Systems (ODQA),where errors from one part of the system can ripple through and affectother components,severely impacting reliability.The situation becomes even worse if one compromised agent introducesavulnerabilitythat spreads toothers.If ahacker successfullytakescontrolofoneagent, theycould exploit weaknesses throughout the entire system,leading toa major safety failure[1379].This kind of widespread compromise is dangerous because it could start with a smallbreach and escalate quickly. Another challenge comes from poor synchronization between agents. If agents don'tupdate their information at the same time or experience delays in communication,it can cause problems in decision-making.Misalignment or delays in updates can disrupt coordination, making itharderforthe agents toachievetheir shared goals effectively.Thesechallenges emphasizethe need for strong safety systems in cooperative multi-agent setups to keep them reliable and resistant to attacks."
      ],
      "raw_title": "Agent-Agent Interaction Threats",
      "type": null,
      "children": []
    },
    {
      "title": "20.4 Summary and Discussion",
      "number": "20.4",
      "level": 2,
      "content": [
        "The preceding sections have detailed the significant safety risks that arise fromAIagents interacting with memory systems,physical and digital environments, and other agents.These risks,ranging fromdata poisoning and code injection tosensorspoofing andcolusion,highlight thevulnerabilities inherent in increasinglycomplexagent-based systems.However,as AIagents become more capable,utilizing naturallanguage understanding and specialized tools for sophisticated reasoning,researchers are actively developing safety protocols to address these challenges.These protocols differ in approach for general-purpose and domain-specific agents.",
        "General-purpose agents,designed for versatility across various domains,facea broad spectrum of safety chalenges. To mitigate these risks,researchers have developed several methods to enhance their safety.Evaluation mechanisms, such as AgentMonitor[1381]assessthe safety awareness of agents by monitoring their decision-making processes and identifying potentially unsafe actions.R-Judge[1382] quantifies an agent's risk awarenessby evaluating its responses to both malicious and benign queries,offering a systematic approach to safety compliance.Aditionally, risk detection tools like ToolEmu[795]simulate toolusage in controlled environments to expose vulnerabilities in agent interactions.This approach identifies potentialhazards during task execution,alowing developers to address vulnerabilities proactively.Thesecombinedeffrts enhancethe safety of general-purpose agents throughcomprehensive evaluation and risk detection.",
        "Domain-specificagents,tailoredfor specializedtasks inhigh-stakes environments like scientificresearch,requireeven more stringent safety measures.Safety tools such as ChemCrow[1383] are designed to mitigate risks in chemical synthesis tasks by reviewing user queries and filtering malicious commands,ensuring agents do not inadvertently synthesizehazardouschemicals.Structured taskconstraints,as implemented inCLAIRify[84],enhance experimental safety by imposing high-levelconstraints on material synthesis order and low-levelrestrictions on manipulation and perception tasks,thereby preventing accidents and errors.Furthermore,benchmarks like SciGuard[1385],which includes the SciMT-Safety benchmark, evaluate model safety by measuring both harmlessness(rejecting malicious queries)and helpfulness (handling benign queries effectively).SciGuard also incorporates long-term memory to enhance agentsability to safelyexecute complex instructionswhile maintaining accurateriskcontrol.These focused approaches ensure that domain-specific agents operate safely and effectively within their specialized fields.",
        "In summary, significant progresshas been made in developing innovative evaluation mechanisms andrisk mitigation strategies to enhance the safety of both general-purpose and domain-specific AIagents.However, acritical area for future research lies in integrating these approaches.Building stronger connections between the broad capabilities of general-purpose agents andthe focused safeguards ofdomain-specificagents willbeessentialforcreating trulyrobust andtrustworthy LM systems.Thechallenge istocombine the best aspects ofboth approaches todevelopagents that are both versatile and secure."
      ],
      "raw_title": "Summary and Discussion",
      "type": null,
      "children": []
    },
    {
      "title": "Concluding Remarks and Future Outlook",
      "number": "",
      "level": 1,
      "content": [
        "We have exploredinthis survey the evolving landscape offoundation agents bydrawing paralelsbetween humancognitive processes andartificialintelligenceWebegan byoutliningthecore componentsof intelligent agents—detailing how modules such as memory,perception,emotion,reasoning, and action can be modeled inaframework inspiredby the comparisonwith human brain.Our discussion highlightedhow these agents can be structured ina modular fashion, enabling them to emulate human-like processing through specialized yet interconnected subsystems.",
        "We then delved into the dynamic aspects ofagent evolution,examining self-improvement mechanisms that leverage optimization techniques,including both online and ofline strategies.By investigating how large language models can act as both reasoning entities andautonomousoptimizers,we ilustrated the transformative potentialof agents that continuously adapt tochanging environments.Building on these technical foundations, we highlighted how agents can drive the self-sustaining evolutionof their intelligence throughclosed-loop scientific innovation. We introduced a generalmeasure of intellgence for knowledge discovery tasks and surveyed current successes and limitations in agent-knowledge interactions.This discussion also shed light on emerging trends in autonomous discovery and tool integration, which are crucial for the advancement of adaptive, resilient AI systems.",
        "Our paper alsoaddressed the collaborative dimension of intellgent systems,analyzing how multi-agent interactions can give rise tocollective intelgence.We explored the design of communication infrastructures and protocols that enable both agent-agent andhuman-AIcollaboration.Thisdiscussion underscoredthe importance offostering synergy between diverse agent capabilities to achieve complex problem solving and efective decision-making.",
        "Finally,weemphasized thecriticalchalnge of building safe and beneficialAI.Our review encompassed intrinsic andextrinsicsecuritythreats,from vulnerabilities inlanguage models torisks associated with agent interactions.We providedacomprehensiveoverview of safety scaling lawsand ethicalconsiderations,proposing strategies to ensure that the development of foundation agents remains aligned with societal values.Overall our workoffers a unified roadmap that notonlyidentifiescurrnt researchgaps but alsolaysthefoundation for future innovations increating more powerful, adaptive, and ethically sound intelligent agents.",
        "Looking ahead, we envision severalkeymilestones that willmarksignificantprogressin thedevelopment ofintelligent agents.First,we anticipate theemergenceof general-purpose agentscapableofhandlinga widearrayof human-level tasks, rather than being confined to specificdomains.These agents willintegrate advanced reasoning,perception, and action modules,enabling them to perform tasks with human-like adaptability and versatility.Achieving this milestone willrepresent afundamental shift in how AI can support and augment human capabilities in both everyday and specialized contexts.",
        "Another criticalmilestone is the development ofagents thatlearn directly fromtheir environment andcontinuously selfevolve through interactions with humansand data.As the distinction between training-time and test-time computation gradually disappears,agentswillacquire newskillsonthe flybyengaging withtheir surroundings,otheragents,and human partners.This dynamic learming processis essential forachieving human-levelcapabilities and for enabling agentstokeep pace with aconstantlychanging world.It is alsovitalif agents aretobe able todrive innovation in scientific discovery, as this expands the boundaries of evolution for both agents and humanity.",
        "We predict that agents willtranscend traditional human limitations by transforming individual human know-how into collctive agent intelligence.The current inefficiencies in human information sharing—where complex knowledge requiresextensive practiceto transfer-willbe overcome byagents,which ofera format ofhumanknow-how that is both transferable and infinitelyduplicableThis breakthroughwillremove thebottleneck ofcomplexityenabling a new intelligence networkeffect wherebyalarge ensembleofhuman and AIagentscanoperate atalevelofintelligence that scales withnetwork size.Inthis scenario,the fusion of agent-acquired knowledge andhuman expertise willfosteran environment where insights and innovations are disseminated and applied rapidly across various domains.",
        "We also anticipate this inteligence network effect enabling the establishment of a new paradigm for human-AI collaboration-one that islarger in scale, more interdisciplinary,and more dynamicall organized than ever before. The resulting human-AI societywillachieve previously unatanable levels of complexityand productivity,heralding a transformative era in both technological and social development.",
        "In summary,these milestones outline a future where intellgent agents become increasingly autonomous,adaptive, and deeply integrated withhuman societydriving scientific discovery,enhancing knowledge sharing, andredefining collaboration on a global scale."
      ],
      "raw_title": "Concluding Remarks and Future Outlook",
      "type": null,
      "children": []
    },
    {
      "title": "Acknowledge",
      "number": "",
      "level": 1,
      "content": [
        "Argonne NationalLaboratory's work was supported bythe U.S.Departmentof Energy,Offce of Science, undercontract DE-AC02-06CH11357. XLQ acknowledges the support of the Simons Foundation."
      ],
      "raw_title": "Acknowledge",
      "type": null,
      "children": []
    }
  ]
}